[
    {
        "source_paper": {
            "name": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "arxiv_id": "2201.11903",
            "isAPA": true,
            "abstract": "We explore how generating a chain of thought a series of intermediate reasoning steps significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "reference": [
                "Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. ACL",
                "Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models behind language models. NAACL",
                "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research",
                "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv",
                "Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures generalize via recursion. ICLR",
                "Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020. Neural execution engines: Learning to execute subroutines. NeurIPS",
                "Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems",
                "Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization improve robustness? NAACL",
                "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. ICLR",
                "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms",
                "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research",
                "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv",
                "Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language. TACL",
                "Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Refining language models with compositional explanations. NeurIPS",
                "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint arXiv",
                "Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-AI collaboration for generating free-text explanations. NAACL",
                "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv",
                "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. NAACL",
                "Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using  \"annotator rationales \" to improve machine learning for text categorization. NAACL",
                "Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP",
                "Jacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. NAACL",
                "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv",
                "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv",
                "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. EMNLP",
                "Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning. arXiv preprint arXiv",
                "Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. ICLR",
                "Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual programming. CHI Extended Abstracts",
                "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv",
                "Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning and Interpretation",
                "Piotr Pi\u0119kos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. ACL",
                "Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher R\u00e9. 2018. Training classifiers with natural language explanations. ACL",
                "Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. EMNLP",
                "Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv",
                "Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. IJCAI",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL",
                "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv",
                "Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration. arXiv preprint arXiv",
                "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! Leveraging language models for commonsense reasoning. ACL",
                "Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with reasoning. arXiv preprint arXiv",
                "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL",
                "Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal reasoning through internal monologue. NAACL",
                "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL",
                "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv",
                "Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamification. NeurIPS Track on Datasets and Benchmarks",
                "Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language models learn from explanations in context? arXiv preprint arXiv",
                "Ana Marasovi\u00e8, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization with natural language prompts. NAACL Findings",
                "Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS",
                "Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables. NeurIPS",
                "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. NeurIPS",
                "Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2019. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. ICLR",
                "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. ACL",
                "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv",
                "Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv",
                "BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the capabilities of language models",
                "Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. NeurIPS",
                "Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL",
                "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv",
                "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. NAACL",
                "Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. ICLR",
                "Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. ACL",
                "Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving and reasoning math word problems",
                "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL",
                "Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain: A self-explaining architecture for neural text classifiers. EMNLP",
                "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization",
                "Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. EMNLP",
                "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv",
                "Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley. 2021. Rationale-inspired natural language explanations with commonsense. arXiv preprint arXiv",
                "Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot out-of-domain transfer learning of natural language explanations. arXiv preprint arXiv",
                "Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math word problem solvers. arXiv preprint arXiv",
                "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? NAACL",
                "Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts. CHI",
                "Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions. EMNLP",
                "Sarah Wiegreffe, Ana Marasovi\u00e8, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. EMNLP",
                "Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework for understanding the roles of explanation data. ACL",
                "Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. ICML",
                "Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Benefits of intermediate annotations in reading comprehension. ACL",
                "Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv",
                "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv",
                "Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate & rank: A multi-task framework for math word problems",
                "Sarah Wiegreffe and Ana Marasovi\u00e8. 2021. Teach me to explain: A review of datasets for explainable NLP. NeurIPS",
                "Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. EMNLP",
                "Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training text-to-text models to explain their predictions. arXiv preprint arXiv",
                "Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction. arXiv preprint arXiv",
                "Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. EMNLP",
                "Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv",
                "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv"
            ],
            "related work": "7Related Work This work is inspired by many research areas, which we detail in an extended related work section (AppendixC) . Here we describe two directions and associated papers that are perhaps most relevant.The first relevant direction is using intermediate steps to solve reasoning problems.Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason(Roy et al.,2015; Chiang and Chen,2019; Amini et al.,2019; Chen et al.,2019) .Cobbe et al. (2021) extendLing et al. (2017) by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis,Nye et al. (2021) leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.Naturally, this paper also relates closely to the large body of recent work on prompting. Since the popularization of few-shot prompting as given byBrown et al. (2020) , several general approaches have improved the prompting ability of models, such as automatically learning prompts(Lester et al.,2021) or giving models instructions describing a task(Wei et al.,2022a; Sanh et al.,2022; Ouyang et al.,2022) . Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs) , our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought.",
            "date": "2022"
        },
        "topic": "in-context learning",
        "year_start": "2021",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Long-context LLMs Struggle with Long In-context Learning",
                "arxiv_id": "2404.02060",
                "subtitles": [
                    "Long In-context Learning on LLMs",
                    "Long Context Techniques over LLMs",
                    "Long Context Evaluation",
                    "Extreme-label Classification"
                ],
                "reference": [
                    "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
                    "Augmenting language models with long-term memory",
                    "Long range arena : A benchmark for efficient transformers",
                    "Rwkv: Reinventing rnns for the transformer era",
                    "Structured prompting: Scaling in-context learning to 1, 000 examples",
                    "What makes good in-context examples for GPT",
                    "Few-NERD: A few-shot named entity recognition dataset",
                    "In-context learning with many demonstration examples",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Mining discourse markers for unsupervised sentence representation learning",
                    "Parallel context windows for large language models",
                    "In-context learning for text classification with many labels",
                    "Loogle: Can long-context language models understand long contexts",
                    "Lost in the middle: How language models use long contexts",
                    " \u221ebench: Extending long context evaluation beyond 100k tokens",
                    "A survey on in-context learning",
                    "Roformer: Enhanced transformer with rotary position embedding",
                    "Longbench: A bilingual, multitask benchmark for long context understanding",
                    "Same task, more tokens: the impact of input length on the reasoning performance of large language models",
                    "Position-aware attention and supervised data improve slot filling",
                    "Code llama: Open foundation models for code",
                    "Extending context window of large language models via positional interpolation",
                    "Focused transformer: Contrastive training for context scaling",
                    "ConvFiT: Conversational fine-tuning of pretrained language models",
                    "L-eval: Instituting standardized evaluation for long context language models",
                    "Train short, test long: Attention with linear biases enables input length extrapolation",
                    "GoEmotions: A dataset of fine-grained emotions",
                    "When does in-context learning fall short and why? a study on specification-heavy tasks",
                    "PoSE: Efficient context window extension of LLMs via positional skip-wise training",
                    "Sparse local embeddings for extreme multi-label classification"
                ],
                "related_work": "2Related Work Long In-context Learning on LLMsAs pre-trained language models continue to grow in size, in-context learning (ICL) has emerged as a favored approach for addressing a wide array of tasks without the need for extensive fine-tuning(Dong et al.,2023) . A body of research has established that increasing the number of examples demonstrations can enhance ICL performance(Liu et al.,2022; Wu et al.,2023) . Nonetheless, there are studies indicating that longer input prompts can actually diminish performance(Liu et al.,2023) , with the effectiveness of prior large language models (LLMs) being constrained by the maximum sequence length encountered during their training. It is also claimed in previous works that LLM+ICL falls short on specification-heavy tasks due to inadequate long-text understanding ability(Peng et al.,2023c) . To counter this issue, various works have introduced memory augmentation and extrapolation techniques to support ICL with an extensive set of demonstrations(Li et al.,2023c; Wang et al.,2023) .Long Context Techniques over LLMsThe effectiveness of Transformer-based models is hindered by the quadratic increase in computational cost relative to sequence length, particularly in handling long context inputs. Recent efforts have explored various strategies to address this challenge. Some studies have pursued continued fine-tuning of the LLM with longer context inputs(Rozi\u00e8re et al.,2024; Tworkowski et al.,2023) . Others have leveraged position extrapolation or interpolation, building upon relative rotary positional embedding(Su et al.,2021) , to extend input length beyond the training phase(Press et al.,2022; Chen et al.,2023a) . Additionally, more approaches have been proposed to mitigate computational issues, including sliding memory window and chunk segmentation(Hao et al.,2022; Ratner et al.,2023; Zhu et al.,2024) . Furthermore, alternative architectures beyond Transformer have been explored to handle long inputs more naturally, such as selective-state-spaces models(Peng et al.,2023a; Gu & Dao,2023) . These diverse approaches claim that they can enhance the capabilities of LLMs in processing long context inputs more efficiently.Long Context EvaluationDue to the imperious demands for the support of long-range LLMs, there is a series of benchmarks focusing on long context evaluation. Long-Range Arena(Tay et al.,2021) includes tasks consisting of sequences ranging from 1K to 16K tokens to evaluate variations of fast Transformers. LongBench(Bai et al.,2023b) comprises 21 bilingual datasets with an average length of around 6k words, which have been processed in a unified format to enable effortless evaluation. L-Eval Benchmark(An et al.,2023) supports 20 sub-tasks with input lengths of 3K to 200K tokens. LooGLE(Li et al.,2023b) focuses on summarization and long dependency QA tasks with test instances exceeding 100k words. Most recently, \u221eBench(Zhang et al.,2024) encompasses 12 tasks with an average length of 200K tokens. Another recent work explores the impact of extending input lengths, especially on reasoning tasks(Levy et al.,2024) .Extreme-label ClassificationExtreme-label Classification involves categorizing data into one of an extremely large number of labels, and finds application across a variety of real-world domains such as emotion classification, named entity recognition, and biological function prediction, each requiring precise differentiation among vast label spaces(Zhang et al.,2017; Sileo et al.,2019; Demszky et al.,2020; Ding et al.,2021) . Previous methods to tackle Extreme-label Classification tasks range from embedding-based approaches to fine-tuned retrievals(Bhatia et al.,2015; Vuli\u00e8 et al.,2021) .However, integrating this task with long-context large language models presents unique challenges. The large scale of the label space complicates the in-context learning process, where LLMs are expected to discern fine-grained differences among labels based on extensive context(Milios et al.,2023) . These challenges make the proposedLongICLBenchwith a range of difficulty levels a good testing scenario to evaluate the capability of long-context large language models.",
                "abstract": "Large Language Models (LLMs) have made significant strides in handling long sequences. Some models like Gemini could even to be capable of dealing with millions of tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their true abilities in more challenging, real-world scenarios. We introduce a benchmark (LongICLBench) for long in-context learning in extreme-label classification using six datasets with 28 to 174 classes and input lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct predictions. We evaluate on 15 long-context LLMs and find that they perform well on less challenging classification tasks with smaller label space and shorter demonstrations. However, they struggle with more challenging task like Discovery with 174 labels, suggesting a gap in their ability to process long, context-rich sequences. Further analysis reveals a bias towards labels presented later in the sequence and a need for improved reasoning over multiple pieces of information. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LongICLBench could serve as a more realistic evaluation for the future long-context LLMs."
            },
            {
                "name": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
                "arxiv_id": "2401.09417",
                "subtitles": [
                    "Architectures for generic vision backbone",
                    "State space models for long sequence modeling",
                    "State space models for visual applications"
                ],
                "reference": [
                    "Imagenet: A large-scale hierarchical image database",
                    "Eva: Exploring the limits of masked visual representation learning at scale",
                    "U-mamba: Enhancing long-range dependency for biomedical image segmentation",
                    "Microsoft coco: Common objects in context",
                    "Long range language modeling via gated state spaces",
                    "Introducing our multimodal models",
                    "Emerging properties in self-supervised vision transformers",
                    "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                    "Generating long sequences with sparse transformers",
                    "Hierarchically gated recurrent neural network for sequence modeling",
                    "Deep high-resolution representation learning for visual recognition",
                    "Cvt: Introducing convolutions to vision transformers",
                    "Linformer: Self-attention with linear complexity",
                    "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
                    "Hungry hungry hippos: Towards language modeling with state space models",
                    "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Resmlp: Feedforward networks for image classification with data-efficient training",
                    "Msg-transformer: Exchanging local spatial information by manipulating messenger tokens",
                    "Going deeper with convolutions",
                    "S4nd: Modeling images and videos as multidimensional signals with state spaces",
                    "Semantic understanding of scenes through the ade20k dataset",
                    "Training data-efficient image transformers & distillation through attention",
                    "Densely connected convolutional networks",
                    "Gradient-based learning applied to document recognition",
                    "Mlp-mixer: An all-mlp architecture for vision",
                    "Deep residual learning for image recognition",
                    "Beit: BERT pre-training of image transformers",
                    "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
                    "Simplified state space layers for sequence modeling",
                    "Selective structured state-spaces for long-form video understanding",
                    "Very deep convolutional networks for large-scale image recognition",
                    "Efficiently modeling long sequences with structured state spaces",
                    "Aggregated residual transformations for deep neural networks",
                    "Visual instruction tuning",
                    "Retentive network: A successor to transformer for large language modelss",
                    "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
                    "Designing network design spaces",
                    "Imagenet classification with deep convolutional neural networks",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "When an image is worth 1,024 x 1,024 words: A case study in computational pathology",
                    "Scaling up visual and vision-language representation learning with noisy text supervision",
                    "Learning transferable visual models from natural language supervision",
                    "A convnet for the 2020s",
                    "Reformer: The efficient transformer",
                    "Long movie clip classification with state-space video models",
                    "Longnet: Scaling transformers to 1,000,000,000 tokens",
                    "Convit: Improving vision transformers with soft convolutional inductive biases",
                    "More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity",
                    "Efficient movie scene detection using state-space transformers",
                    "Coatnet: Marrying convolution and attention for all data sizes",
                    "Efficientnetv2: Smaller models and faster training",
                    "Efficientnet: Rethinking model scaling for convolutional neural networks",
                    "Diffusion models without attention",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Swin transformer: Hierarchical vision transformer using shifted windows",
                    "Rethinking attention with performers"
                ],
                "related_work": "2Related WorkArchitectures for generic vision backbone.In the early eras, ConvNet[33]serves as the de-facto standard network design for computer vision. Many convolutional neural architectures[32,55,50,24,56,62,25,71,57,49]have been proposed as the vision backbone for various visual applications. The pioneering work, Vision Transformer (ViT) [13]changes the landscape. It treats an image as a sequence of flattened 2D patches and directly applies a pure Transformer architecture. The surprising results of ViT on image classification and its scaling ability encourage a lot of follow-up works[60,58,61,15]. One line of works focuses on hybrid architecture designs by introducing 2D convolutional priors into ViT[68,8,14,12]. PVT[65]proposes a pyramid structure Transformer. Swin Transformer[41]applies self-attention within shift windows. Another line of works focuses on improving traditional 2D ConvNets with more advanced settings[66,40]. ConvNeXt[42]reviews the design space and proposes pure ConvNets, which can be scalable as ViT and its variants. RepLKNet[11]proposes to scale up the kernel size of existing ConvNets to bring improvements.Though these dominant follow-up works demonstrate superior performance and better efficiency on ImageNet[9]and various downstream tasks[38,73]by introducing 2D priors, with the surge of large-scale visual pretraining[1,16,5]and multi-modality applications[48,34,35,39,3,28], vanilla Transformer-style model strikes back to the center stage of computer vision. The advantages of larger modeling capacity, unified multi-modality representation, being friendly to self-supervised learningetc., make it the preferred architecture. However, the number of visual tokens is limited due to the quadratic complexity of Transformer. There are plenty of works[7,64,31,6,10,47,54]to address this long-standing and prominent challenge, but few of them focus on visual applications. Recently, LongViT[67]built an efficient Transformer architecture for computational pathology applications via dilated attention. The linear computation complexity of LongViT allows it to encode the extremely long visual sequence. In this work, we draw inspiration from Mamba[19]and explore building a pure-SSM-based model as a generic vision backbone without using attention, while preserving the sequential, modality-agnostic modeling merit of ViT.State space models for long sequence modeling.[20]proposes a Structured State-Space Sequence (S4) model, a novel alternative to CNNs or Transformers, to model the long-range dependency. The promising property of linearly scaling in sequence length attracts further explorations.[52]proposes a new S5 layer by introducing MIMO SSM and efficient parallel scan into S4 layer.[17]designs a new SSM layer, H3, that nearly fills the performance gap between SSMs and Transformer attention in language modeling.[45]builds the Gated State Space layer on S4 by introducing more gating units to improve the expressivity. Recently,[19]proposes a data-dependent SSM layer and builds a generic language model backbone, Mamba, which outperforms Transformers at various sizes on large-scale real data and enjoys linear scaling in sequence length. In this work, we explore transferring the success of Mamba to vision,i.e., building a generic vision backbone purely upon SSM without attention.State space models for visual applications.[26]uses 1D S4 to handle the long-range temporal dependencies for video classification.[46]further extends 1D S4 to handle multi-dimensional data including 2D images and 3D videos.[27]combines the strengths of S4 and self-attention to build TranS4mer model, achieving state-of-the-art performance for movie scene detection.[63]introduces a novel selectivity mechanism to S4, largely improving the performance of S4 on long-form video understanding with a much lower memory footprint.[72]supplants attention mechanisms with a more scalable SSM-based backbone to generate high-resolution images and process fine-grained representation under affordable computation.[44]proposes U-Mamba, a hybrid CNN-SSM architecture, to handle the long-range dependencies in biomedical image segmentation. The above works either apply SSM to specific visual applications or build a hybrid architecture by combining SSM with convolution or attention. Different from them, we build a pure-SSM-based model, which can be adopted as a generic vision backbone.",
                "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available atthis https URL."
            },
            {
                "name": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
                "arxiv_id": "2402.10207",
                "subtitles": [
                    "Multi-attribute Conditioned SFT",
                    "Reinforcement Learning via Supervised Learning (RvS)",
                    "Hindsight Experience Replay (HER)"
                ],
                "reference": [
                    "Deep reinforcement learning from human preferences",
                    "Nash learning from human feedback",
                    "Statistical rejection sampling improves preference optimization",
                    "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation",
                    "Dueling rl: reinforcement learning with trajectory preferences",
                    "Reinforced self-training (rest) for language modeling",
                    "A practical guide to multi-objective reinforcement learning and planning",
                    "Learning all optimal policies with multiple criteria",
                    "Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf",
                    "Deep reinforcement learning for multiobjective optimization",
                    "Proximal policy optimization algorithms",
                    "Tailoring self-rationalizers with multi-reward distillation",
                    "Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf",
                    "Multi-objective reinforcement learning using sets of pareto dominating policies",
                    "Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints",
                    "Raft: Reward ranked finetuning for generative foundation model alignment",
                    "Fine-tuning language models from human preferences",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Aligning language models with offline reinforcement learning from human feedback",
                    "Gpt-4 technical report. arxiv",
                    "Beyond one-preference-for-all: Multi-objective direct preference optimization",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Rrhf: Rank responses to align language models with human feedback without tears",
                    "A survey of multi-objective sequential decision-making",
                    "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "5Related WorkRLHF.RLHF(Christiano et al.,2017; Ziegler et al.,2019) , also known as dueling RL(Pacchiano et al.,2021) or preference-based RL(Chen et al.,2022) , is a methodology that incorporates human feedback into the reinforcement learning to guide the learning of AI systems. In recent years, RLHF has become a powerful tool for aligning LLMs such as GPT(OpenAI,2023) and LLaMA2(Touvron et al.,2023) , using RL algorithms such as PPO(Schulman et al.,2017; Ouyang et al.,2022) . However, RLHF faces challenges related to instability and inefficiency. To overcome these limitations, several works(e.g., Dong et al.,2023a; Yuan et al.,2023; Gulcehre et al.,2023; Xiong et al.,2023; Liu et al.,2023; Rafailov et al.,2023; Wang et al.,2023a; Munos et al.,2023) propose various RLHF algorithms. Our work is mostly related to reward conditional training(Hu et al.,2023) , which involves augmenting the prompt with reward and performing supervised fine-tuning. Unlike previous works that focus on aligning LLMs with a single reward, our work specifically focuses on multi-objective alignment.MORL and MORLHF.Given that optimizing a single reward model may not align with everyone's preferences(Ouyang et al.,2022) , a natural and promising approach is multi-objective RLHF (MORLHF) , a paradigm originated from Multi-Objective Reinforcement Learning (MORL) (Barrett & Narayanan,2008; Roijers et al.,2013; Van Moffaert & Now\u00e9,2014; Li et al.,2020b; Hayes et al.,2022) . Recent studies have explored this issue and proposed algorithms such as rewarded soups(Rame et al.,2023) and MODPO(Zhou et al.,2023) . Compared with these works, our approach is based on multi-reward conditional supervised fine-tuning and dynamic inference-time adaptation, which offers superior simplicity, computational efficiency, flexibility, and improved empirical performance compared to previous solutions.Multi-attribute Conditioned SFT.SteerLM(Dong et al.,2023b; Ramnath et al.,2023) employ multi-dimensional attributes as conditions for the supervised fine-tuning of language models, a formulation that shares similarity to RiC. However, there are three crucial distinctions: (1) Unlike RiC, they does not consider the trade-offs between rewards, and its objective is not to achieve an optimal empirical Pareto front. (2) they uses a limited number of discrete values (e.g., 0-10) for each attribute, which may limit the language model's ability to generalize and extrapolate to higher rewards. (3) During online generation, SteerLM directly generates new responses based on the selected data with original prompts and rewards. In contrast, RiC assigns desired rewards that align more closely with the Pareto front, thereby improving the quality of data generated along the optimal frontier. Additional related works are deferred to the AppendixD.",
                "abstract": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline."
            },
            {
                "name": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "arxiv_id": "2401.06766",
                "subtitles": [
                    "In-Context Learning",
                    "In-Context Learning Analysis"
                ],
                "reference": [
                    "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
                    "Palm: Scaling language modeling with pathways",
                    "Calibrate before use: Improving few-shot performance of language models",
                    "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
                    "Making pre-trained language models better few-shot learners",
                    "Data curation alone can stabilize in-context learning",
                    "Z-icl: Zero-shot in-context learning with pseudo-demonstrations",
                    "Mitigating label biases for in-context learning",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "The language of prompting: What linguistic properties make a prompt successful",
                    "Are we really making much progress? a worrying analysis of recent neural recommendation approaches",
                    "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
                    "Prototypical calibration for few-shot learning of language models",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "State of what art? a call for multi-prompt llm evaluation",
                    "A metric learning reality check",
                    "Noisy channel language model prompting for few-shot text classification",
                    "How can we know what language models know",
                    "Training compute-optimal large language models",
                    "Larger language models do in-context learning differently",
                    "In-context example selection with influences",
                    "The icl consistency test",
                    "A critical look at the evaluation of GNNs under heterophily: Are we really making progress",
                    "Language models are few-shot learners",
                    "Language models are unsupervised multitask learners",
                    "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
                    "Do prompt-based models really understand the meaning of their prompts"
                ],
                "related_work": "2Background and Related Work2.1In-Context LearningAn important property of LLMs is their ability to learn new tasks from only a few demonstrations Radford et al. (2019); Brown et al. (2020). This capability, known as in-context learning, forms the focus of our work. We focus on sequence classification, as it is the most widely studied task for understanding and improving ICL performance.Formally, classifying an input x_{test} with in-context learning can be described as finding the class c the space of label tokens \\mathcal{C} that yields a sequence with the highest probability according to a language model. The input sequence consists of demonstration inputs and labels (x_{i},y_{i}); to obtain a natural language input, demonstrations are formatted with a template.Each template consists of four components: input and output verbalizersv v_{I}(x) and v_{O}(y,\\mathcal{C}) into a natural language text, an intra-separator to divide input from output, and an inter-separator to join several demonstrations.Figure1shows an example of transforming a set of demonstrations into a context for ICL.2.2In-Context Learning AnalysisRecent work has shown that ICL can perform at levels comparable to finetuning Chowdhery et al. (2022); Hoffmann et al. (2022). Still, in-context learning is known to be highly dependent on the way the model input is formed: a prompt is defined by several components, and altering any of them can lead to unpredictable changes in performance.Template SelectionThere are multiple ways to construct a template for a task. The most straightforward approach is to use minimal templates v_{I}=\\{x\\},v_{O}=\\{\\mathcal{C}[y]\\} or universal verbalizers like  \"input/output\", as done in Wang et al. (2023)and Wei et al. (2023).Another strategy is to create task-specific templates.Jiang et al. (2020)generate paraphrases of templates for the relation extraction task. Authors show the sensitivity of masked language models to the prompt format and propose to ensemble predictions over the best templates. Compared to this method, our approach is task-agnostic and does not require evaluating all templates in advance.Several studies aim to find templates that directly optimize in-context learning performanceShin et al. (2020); Gao et al. (2021). Our work unifies the results of previous research, using the verbalizers proposed by Gao et al. (2021), as well as minimal and universal templates.Choice and Order of DemonstrationsThe choice of examples for ICL is highly important, as they enable the model to condition on correct input and label distributions for the task Wu et al. (2023); Nguyen and Wong (2023); Min et al. (2022d); Chang and Jia (2023). Furthermore, the order of examples also significantly affects the results and does not transfer between models even within the same family Lu et al. (2022b); Zhao et al. (2021). In this work, we analyze two recent methods for selecting demonstrations.Wang et al. (2023)propose learning latent concept variables for a task and using them to find examples that can best predict the task concept. We refer to this method asImplicit Topic ModelsorITM. In turn,z-ICL Lyu et al. (2023)generates pseudo-demonstrations by retrieving most similar examples to the test sentence from an unlabeled dataset and assigning random labels to retrieved examples.Crucially, both methods are evaluated on single templates that differ across two works. Therefore, it is unclear whether the reported performance gains arise from the methods themselves or from a particular combination of the example selection strategy, the model, and the chosen template. Prediction MethodsThe standard approach for classification with LLMs is to compute the sequence probability with each of the possible labels and select the label with the highest probability. We refer to this method asDirectfurther on. Alternatively, one can use more advanced prediction methods that aim to reduce the variance across prompt formats. The Calibration method Zhao et al. (2021)computes a correction factor based on the deviation of the model's predictions for a placeholder input from a uniform distribution over labels and applies this factor to test set predictions. Recent work has proposed multiple improvements of this method Fei et al. (2023); Han et al. (2023); Zhou et al. (2024); to limit the scope of our study, we focus only on the baseCalibrationapproach in this work. Lastly, the Channel prompting technique, proposed in Min et al. (2022b), maximizes P(x|y) instead of P(y|x).Both of these methods aim to mitigate the issue of ICL sensitivity to the prompt template choice. However, as we show in AppendixA, these methods are evaluated on their own sets of templates. In this paper, we strive for a more unified view on the robustness of advanced prompting methods and compare their performance across a broader range of templates and models.Prompt and Template RobustnessAlthough the problem of prompt robustness is relatively well-known, until recently, the discussion oftemplate robustnesshas been limited. Notably, Sclar et al. (2023)present a highly relevant study of prompt format sensitivity, reporting a significant performance variation across formats even for large models or minor template changes. While their experiments are conducted in the standard setup (randomly selected examples and default prompting), our work instead focuses on alternative prompting and example selection methods, several of whichZhao et al. (2021); Min et al. (2022b)were proposed to improve the prompt robustness of ICL. Similarly to papers in other subfields of machine learning arguing for a more consistent methodology Dacrema et al. (2019); Musgrave et al. (2020); Platonov et al. (2023), the goal of our work is to demonstrate that disparate experiment setups lead to an invalid comparison of competing methods.Moreover, several works study prompt robustness in a broader sense by considering models that use natural language instructions instead of labeled demonstrations Webson and Pavlick (2022); Leidinger et al. (2023); Weber et al. (2023). Recently,Mizrahi et al. (2023)have shown that very similar instructions can lead to drastic differences in task performance for a variety of instruction-tuned models. Although we study a similar issue, the focus of our work is on in-context learning and the transfer of best prompts between evaluation setups. Still, we find that instruction-tuned models lack in-context robustness as well, which confirms previous observations and emphasizes the need for language model evaluation that takes prompt design into account.",
                "abstract": "Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across 21 models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates."
            },
            {
                "name": "Grimoire is All You Need for Enhancing Large Language Models",
                "arxiv_id": "2401.03385",
                "subtitles": [
                    "In-context Learning of Large Language Model",
                    "Prompt Engineering of Demo Examples"
                ],
                "reference": [
                    "What can transformers learn in-context? a case study of simple function classes",
                    "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
                    "Data distributional properties drive emergent in-context learning in transformers",
                    "Calibrate before use: Improving few-shot performance of language models",
                    "Ground-truth labels matter: A deeper look into input-label demonstrations",
                    "What makes good in-context examples for GPT",
                    "In-context learning with iterative demonstration selection",
                    "Automatic chain of thought prompting in large language models",
                    "Finding support examples for in-context learning",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "In-context learning for text classification with many labels",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Learning to retrieve prompts for in-context learning",
                    "Larger language models do in-context learning differently",
                    "Language models are few-shot learners",
                    "Active example selection for in-context learning",
                    "An explanation of in-context learning as implicit bayesian inference",
                    "Towards informative few-shot prompt with maximum information gain for in-context learning",
                    "Selective annotation makes language models better few-shot learners"
                ],
                "related_work": "2Related Works2.1In-context Learning of Large Language Model ICL has emerged as a novel learning paradigm, initially proposed and applied in the pre-training of GPT-3Brownet al.(2020) . ICL can efficiently learn tasks with a small number of prompt examples, without the parameter updates. Why ICL is effective has sparked widespread discussion.Chanet al.(2022) suggests that the ICL capability of models is driven by the distributional characteristics of the data itself, emphasizing the importance of data structure.Xieet al.(2022) suggests that contextual learning takes place as the language model infers shared latent concepts among examples within prompts.Garget al.(2022) indicates that models can learn specific functions based on encoded prompt samples, achieving performance comparable to specific task learning algorithms. Combining the above works, we find that the ICL capabilities of models are more derived from learning the distributional features of example samples or underlying rules, rather than necessarily relying on the specific examples. Moreover, the greater the parameter size of large language models, the more robust their corresponding ICL capabilitiesBrownet al.(2020) ; Yooet al.(2022) , establishing the theoretical foundation for our work. 2.2Prompt Engineering of Demo ExamplesExtensive research indicates that the construction of demonstration examples is crucial for the performance of ICLYooet al.(2022) ; Qinet al.(2023) . Furthermore, recent research has enhanced ICL performance by optimizing both the characteristics of demonstration examples and the order and selection strategies of example samples.Characteristics of Demonstration Examples. The study byMinet al.(2022) highlights that the influence of prompt samples on ICL performance is attributable to four principal elements: input-label mappings, label space, and sample distribution. Nonetheless, it is notable that opinions diverge concerning the effect of input-label mappings relationships namely, label accuracy on ICL performance. Certain studies propose that input-label mappings relationships potentially influence ICL performance, withPawelczyket al.(2023) observing that label inversion within contextual examples markedly impacts the outputs of the Bloom model. In contrast, findings byWeiet al.(2023) demonstrate that larger models generally exhibit more robust ICL capabilities and excel in deciphering label mapping relationships from contextual examples relative to their smaller counterparts.Ordering of Demonstration Examples. The ordering of samples represents a critical aspect of prompt sample construction, with various sorting methods leading to differences in ICL performanceZhaoet al.(2021) . The study byLiuet al.(2022) introduced KATE, a system that selects demonstration examples based on semantic similarity. Expanding on this, they examined various ordering methods and found that their impact on KATE's performance was minimal. Research fromLuet al.(2022) employed GlobalE and LocalE to sequence demonstration examples and uncovered a positive relationship between information entropy and ICL performance. As the parameter scale increases, models become more proficient at ICL, and their sensitivity to sample ordering diminishes accordinglyMilioset al.(2023) .Selecting Demonstration Examples. Selection of demonstration examples is a pivotal stage in crafting prompt samples, with a substantial effect on ICL performanceLiuet al.(2022) . Presently, selection of demonstration examples methodologies are primarily categorized into three types: semantic similarityLiuet al.(2022) ; Suet al.(2022) , clusteringZhanget al.(2023) , and information entropyLiu and Wang (2023) . Moreover, certain studies have introduced specialized models to score demonstration examples, aiming to select representative demonstration examplesRubinet al.(2022) ; Wuet al.(2023) ; Li and Qiu (2023) . Previous research on the selection of demonstration examples can be categorized based on the granularity of selection. One strategy involves obtaining instance-level examples, where retrieval is performed for every test queryLiuet al.(2022) ; Suet al.(2022) ; Rubinet al.(2022) ; Wuet al.(2023) . Alternatively, task-level example retrieval is utilized as prompts for all test samplesZhanget al.(2022) ; Liu and Wang (2023) ; Li and Qiu (2023) , which is less resource-intensive compared to instance-level retrieval. However, such selected samples may not be sufficiently representative or could be substandard, potentially resulting in modest improvements in ICL performance or reduced stability. Researchers and practitioners must deliberate these aspects when advancing and implementing ICL methodologies to ensure the most effective deployment of demonstration examples.",
                "abstract": "In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method. Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL."
            },
            {
                "name": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks",
                "arxiv_id": "2405.10548",
                "subtitles": [
                    "In-context Learning of Large Language Model",
                    "Prompt Engineering of Demo Examples"
                ],
                "reference": [
                    "Meta-cot: Generalizable chain-of-thought prompting in mixed-task scenarios with large language models",
                    "Learning to retrieve prompts for in-context learning",
                    "What makes good in-context examples for GPT",
                    "Language models are few-shot learners",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Multilingual LLMs are better cross-lingual in-context learners with alignment",
                    "Demystifying prompts in language models via perplexity estimation",
                    "Task compass: Scaling multi-task pre-training with task prefix"
                ],
                "related_work": "7Related workIn-context learning without gradient updates to the model was introduced byBrown et al. (2020) using the GPT-3 model. Multiple recent works sought robust ICL setup via different techniques: selecting the examples that are semantically most similar to the input(Liu et al.,2022) , choosing low perplexity examples(Gonen et al.,2022) , training a two-stage retrieval system to select the best examples(Rubin et al.,2022) , etc. However, these works primarily aim to construct better in-task prompts where the examples and the input come from the same task.Tanwar et al. (2023) showed that cross-lingual ICL can be elicited with proper alignment between the source and target language examples.Raffel et al. (2020) introduced theT5 transformermodel which has been trained to perform different text-processing tasks.Zhang et al. (2022) proposed a task prefix guided multi-task pre-training framework to solve this problem. More recently, a new prompting method calledMeta-CoThas been proposed byZou et al. (2023) which generalizes chain-of-thought prompting to include multi-task examples in the prompt and it has shown improvement in a number of tasks.",
                "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable In-context Learning (ICL) capabilities. Automated assistants based on LLMs are gaining popularity; however, adapting them to novel tasks is still challenging. While colossal models excel in zero-shot performance, their computational demands limit widespread use, and smaller language models struggle without context. This paper investigates whether LLMs can generalize from labeled examples of predefined tasks to novel tasks. Drawing inspiration from biological neurons and the mechanistic interpretation of the Transformer architecture, we explore the potential for information sharing across tasks. We design a cross-task prompting setup with three LLMs and show that LLMs achieve significant performance improvements despite no examples from the target task in the context. Cross-task prompting leads to a remarkable performance boost of 107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average over zero-shot prompting, and performs comparable to standard in-context learning. The effectiveness of generating pseudo-labels for in-task examples is demonstrated, and our analyses reveal a strong correlation between the effect of cross-task examples and model activation similarities in source and target input tokens. This paper offers a first-of-its-kind exploration of LLMs' ability to solve novel tasks based on contextual signals from different task examples."
            },
            {
                "name": "Many-Shot In-Context Learning",
                "arxiv_id": "2404.11018",
                "subtitles": [
                    "Scaling in-context learning",
                    "Long-context scaling laws",
                    "Learning from self-generated data",
                    "Self-generated data and in-context learning",
                    "Learning Input-Output Relationships with ICL",
                    "Learning Mathematical Functions with LLMs",
                    "Comparing ICL with fine-tuning",
                    "Exemplar vs. Rule-based ICL generalization"
                ],
                "reference": [
                    "What can transformers learn in-context? a case study of simple function classes",
                    "Trained transformers learn linear models in-context",
                    "Scaling laws for neural language models",
                    "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation",
                    "The claude 3 model family: Opus, sonnet, haiku",
                    "Ground-truth labels matter: A deeper look into input-label demonstrations",
                    "Understanding in-context learning in transformers and llms by learning to learn discrete functions",
                    "In-context learning with many demonstration examples",
                    "In-Context Learning with Long-Context Models: An In-Depth Exploration, Apr",
                    "Reinforced self-training (rest) for language modeling",
                    "Automatic chain of thought prompting in large language models",
                    "Dual operating modes of in-context learning",
                    "Transformers generalize differently from information stored in context vs in weights, Oct",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Data Engineering for Scaling Language Models to 128K Context, Feb",
                    "Lift: Language-interfaced fine-tuning for non-language machine learning tasks",
                    "Gpqa: A graduate-level google-proof q&a benchmark",
                    "Beyond human data: Scaling self-training for problem-solving with language models",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "Many-shot jailbreaking",
                    "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                    "Effective long-context scaling of foundation models",
                    "Scaling relationship on learning mathematical reasoning with large language models",
                    "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                    "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                    "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
                    "Benefits of transformer: In-context learning in linear regression tasks with unstructured data",
                    "Buffet: Benchmarking large language models for few-shot cross-lingual transfer",
                    "From words to numbers: Your large language model is secretly a capable regressor when given in-context examples",
                    "An information-theoretic analysis of in-context learning",
                    "In-context learning learns label relationships but is not conventional learning",
                    "Exploring the landscape of distributional robustness for question answering models",
                    "Language models are few-shot learners",
                    "Are human-generated demonstrations necessary for in-context learning",
                    "Efficient attention via control variates"
                ],
                "related_work": "5Related Work Scaling in-context learningBrown et al. [8]reported improved performance as you increase the number of examples (up to 64) for in-context learning in LLMs , and later works corroborated this finding[39]. However, very few works have explored using a large number of examples (1000 or above) in the prompt. This is likely due to the fact the context lengths in large language models have been quite limited until recently[16,3]. One closely related work to ours is fromLi et al. [31], who scale the number of examples for in-context learning to 2000. However,Li et al. [31]use a custom model architecture[74]to achieve long context lengths, and only evaluate models of up to 1.3B parameters, which is several orders of magnitude smaller than state-of-the-art language models, and are ineffective for complex tasks, such as GPQA[52].Concurrently to our work,Anil et al. [2]used many-shot prompting (upto 256 shots) to jailbreak language models. In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots) and use models with much longer context lengths (up to 1M tokens) . Also, we explore mitigations for needing many human-generated examples with many-shot ICL. Furthermore, whileAnil et al. [2]use many-shot learning to override preferences learned during RLHF phase to elicit the biases stemming from pretraining, our results in \u00a74.1demonstrate that we can also override pre-training biases themselves.Bertsch et al. [6]also concurrently shows benefits of scaling up in-context learning to many demonstrations on several classification datasets with up to 151 labels, albeit also using smaller context windows of up to 80k tokens (using Llama2-80k[13]) .Long-context scaling lawsPrior works[68,2,27,16]have reported smaller next-token prediction loss with longer contexts, whichJeon et al. [25]also show using theoretical analysis. Our findings confirm this trend for even longer context lengths, but our analysis reveals some of the limitations of using next-token prediction loss as a metric for evaluating long-context performance, as next-token prediction loss continues to go down even as overall performance plateaus.Learning from self-generated dataNumerous recent works[19,70,55]propose fine-tuning language models on self-generated data to improve performance. Their approach consists of (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. In this work, we extend this idea to in-context learning, and study the efficacy of Reinforced ICL in reasoning and problem-solving domains.Self-generated data and in-context learningKim et al. [29]propose using self-generated data for few-shot ICL on classification problems, where they generate demonstrations using the LLM conditioned on the test input for each possible class label, and including these demonstrations in the context when performing the final prediction.Li et al. [32]extend this approach to reasoning and language understanding tasks, where they also generate demonstrations conditioned on the test input. Consistent with our findings, these works show that model-generated demonstrations can outperform human-generated demonstrations in the few-shot regime. Another related approach is AutoCoT[73]that uses a zero-shot CoT prompt to produce model-generated demonstrations forfew-shotICL. To do so, AutoCoT samples diverse questions one-by-one based on embedding-based clustering followed by heuristics-based post-processing for selecting demonstrations.Different from above approaches, Reinforced ICL generates demonstrations using the same procedure asSingh et al. [55], does not require clustering, post-processing heuristics, or access to the test inputs for generating demonstrations, and can be applied to any problem for which we can obtain reliable reward signals. Moreover, our work mainly focuses on the utility of randomly-sampled model-generated demonstrations for many-shot ICL.Learning Input-Output Relationships with ICLNumerous works[41,30,69,36]have investigated whether LLMs truly learn input-output relationships during in-context learning.Min et al. [41]found that replacing the ground truth labels in in-context examples with random labels barely effected final performance. Further investigations byYoo et al. [69]andKossen et al. [30]found that this finding does not necessarily hold across tasks and model sizes. In particular,Kossen et al. [30], Lin and Lee [36]showed that LLMs can indeed learn input-output relationships via in-context learning, but require more examples in order to do so well. In our work, we extrapolate the trend found in those works to much longer context lengths, showing that pre-training biases can be mostly overcome given enough training examples.Learning Mathematical Functions with LLMsSeveral prior works investigate whether mathematical functions can be learned with transformers[14,72,67,7]. All these works train transformers specifically to perform in-context learning for such functions. In contrast, we demonstrate that many-shot ICL can learn high-dimensional functions even with pre-trained LLMs. Concurrent to our work,Vacareanu et al. [58]demonstrate that pretrained LLMs are able to perform regression tasks, with performance rivaling that of traditional supervised methods with 500 in-context examples. Our work complement their findings to other synthetic tasks with a much larger number of in-context examples.Dinh et al. [11]fine-tuned GPT-3 on synthetic classification tasks and observed similarities in the decision boundaries learned by the fine-tuned model and kNNs. Our results in Figure 11 show that many-shot ICL also performs comparably to kNNs on high-dimensional classification tasks.Comparing ICL with fine-tuningContrary to task-specific fine-tuning, ICL does not require optimizing any model weights, allowing LLMs to perform a variety of tasks at inference. As such, several prior works compare fine-tuning with ICL but in few-shot regime.Liu et al. [37]proposed a parameter-efficient few-shot fine-tuning (FT) approach for T0 that outperforms few-shot ICL with GPT-3. However,Awadalla et al. [5]argue that few-shot ICL is more robust to distribution shifts than fine-tuning for question answering tasks. Similarly,Asai et al. [4]show better transfer with ICL compared to fine-tuning on some tasks.Mosbach et al. [42]fairly compare ICL with FT by using the same model for both approaches and show that full fine-tuning (FT) generally outperforms ICL in the few-shot regime with 16 examples. More recently,Lin et al. [34]show that few-shot ICL can outperform fine-tuning based approaches for aligning LLMs.Complementary to prior works, we compare full fine-tuning with many-shot ICL with the same number of examples for low-resource translation. Notably, we find that many-shot ICL performs comparably to FT. Aligned with our findings,Bertsch et al. [6]concurrently show that many-shot ICL generally outperforms parameter-efficient fine-tuning (LoRA) on classification tasks. Overall, many-shot ICL and FT can exhibit comparable behaviors, which we leave for further investigation.Exemplarvs.Rule-based ICL generalizationChan et al. [9]indicate that ICL tends to generalize in a more exemplar-based way, compared to rule-based generalization during in-weights learning. Using a clever experiment with blocked attention,Bertsch et al. [6]also argue that the benefits of many in-context demonstrations arise from having access to more similar examples. While our results on in-context linear classification agree with this conclusion, our sequential parity results seem to contradict it. Strikingly, sequential parity was the task on which we saw themostimprovement, whereas it should be a task that benefitsleastfrom seeing similar examples - after all, the nearest neighbor is always going to give the wrong answer (off by 1 bit) .Chan et al. [9]do show that a transformer's inductive biases towards exemplar-based generalization can be shifted both by the training data and the model size, with larger models being less exemplar-based - perhaps this explains the contradictory findings, given that our work used a larger and much more capable model, though this remains an open question.",
                "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance."
            },
            {
                "name": "In-Context Learning with Long-Context Models: An In-Depth Exploration",
                "arxiv_id": "2405.00200",
                "subtitles": [
                    "Augmenting decoder-only models with long context",
                    "Properties of in-context learning",
                    "Comparing in-context learning and finetuning"
                ],
                "reference": [
                    "Lm-infinite: Zero-shot extreme length generalization for large language models",
                    "Yarn: Efficient context window extension of large language models",
                    "What in-context learning  \"learns \" in-context: Disentangling task recognition and task learning",
                    "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation",
                    "Structured prompting: Scaling in-context learning to 1, 000 examples",
                    "MetaICL: Learning to learn in context",
                    "How long can context length of open-source LLMs truly promise",
                    "Pose: Efficient context window extension of llms via positional skip-wise training",
                    "Prompt-augmented linear probing: Scaling beyond the limit of few-shot in-context learners",
                    "Many-shot in-context learning",
                    "Dual operating modes of in-context learning",
                    "MEGABYTE: Predicting million-byte sequences with multiscale transformers",
                    "Parallel context windows for large language models",
                    "In-context learning for text classification with many labels",
                    "Loogle: Can long-context language models understand long contexts?, 2023b",
                    "Ring attention with blockwise transformers for near-infinite context",
                    "Lost in the Middle: How Language Models Use Long Contexts",
                    "Transformers learn in-context by gradient descent",
                    "Efficient streaming language models with attention sinks",
                    "Hierarchical context merging: Better long context understanding for pre-trained LLMs",
                    "In-context learning creates task vectors",
                    "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                    "Effective long-context scaling of foundation models",
                    "In-context learning and gradient descent revisited",
                    "Code llama: Open foundation models for code",
                    "Extending context window of large language models via positional interpolation",
                    "Long-context language modeling with parallel context encoding",
                    "Buffet: Benchmarking large language models for few-shot cross-lingual transfer",
                    "impact of sample selection on in-context learning for entity extraction from scientific writing",
                    "Focused transformer: Contrastive training for context scaling",
                    "Unlimiformer: Long-range transformers with unlimited length input",
                    "Towards understanding in-context learning with contrastive demonstrations and saliency maps"
                ],
                "related_work": "6Related WorkAugmenting decoder-only models with long contextMany methods for extending the context of language models have been introduced in the last few years. One series of work has focused on positional embedding extrapolation strategies(Peng et al.,2023; Rozi\u00e8re et al.,2024; Chen et al.,2023; Liu et al.,2023; Zhu et al.,2024; Xiao et al.,2024; Han et al.,2024) . When extrapolating past pretraining length, models also generally benefit from additional finetuning on long-context data(Xiong et al.,2023) . Other methods include adding retrieval-based attention(Bertsch et al.,2023; Tworkowski et al.,2023; Yen et al.,2024) or hierarchical merging of information(Song et al.,2024; YU et al.,2023) . The two long-context Llama variants we consider in this work are both examples of finetuning for length extrapolation.Separately, methods for longer context for ICL have also been proposed. Parallel context windows(Ratner et al.,2022) and structured prompting(Hao et al.,2022) propose methods of re-using the same positional embeddings multiple times to encode more demonstrations; this is quite effective for small numbers of overlaps, albeit with diminishing returns as the number of overlapping windows increases.Cho et al. (2023) propose a hybrid of ICL and linear prompting which improves beyond few-shot ICL performance.Several works have also critiqued the efficacy of long context models.Liu et al. (2024) demonstrate that some long-context models fail to effectively use the middle of the context window; the models we use were released after this work and have generally high scores for middle-of-context retrieval in their trained context length.Li et al. (2023a) suggest that some long-context models are only effective at utilizing inputs that are shorter than their context window's intended supported length; we do not observe this effect strongly, but it is a possible contributing factor to the saturation of performance for some models before the maximum number of examples in-context.Li et al. (2023b) show that many models fail at tasks that require reasoning over long dependency lengths; this is unlikely to be an issue in our setting.Properties of in-context learningMilios et al. (2023) study ICL for many-class classification with models up to 4k context length. They find that, when retrieving demonstrations, smaller (7b) models show early performance saturation on many tasks. This is consistent with our findings for Llama2-7b with retrieval; however, the same model continues to learn from demonstrations in the random selection case, and the same size model finetuned for longer context does not show the same performance dropoff and continues to see improvements from additional context for several tasks. Our results suggest that this failure to use longer context effectively is not an inherent property of 7b models, but instead a type of shallow heuristic used by this particular model when the demonstrations are of sufficiently high quality.Xu et al. (2023) study the impacts of ground-truth label, input distribution, and explanations on ICL performance;B\u00f6l\u00fcc\u00fc et al. (2023) study the impact of example selection in a specific domain.Lin & Lee (2024) argue that ICL occurs in two modes: learning tasks and retrieving tasks, and that retrieval of similar-but-not-quite-correct tasks can explain  \"early ascent \" behaviors where ICL performance peaks once in a fewshot regime and then performance improves again with a much higher number of examples. Similarly,Pan et al. (2023) argue for a distinction between task recognition and task learning, and suggest that task learning continues to benefit from additional examples at scale.von Oswald et al. (2023) suggest in-context learning can be viewed as gradient descent, althoughDeutch et al. (2024) argue against this interpretation.Hendel et al. (2023) view in-context learning as compressing the demonstrations into a  \"task vector \" that maps from inputs to outputs; the surprising effectiveness of block encoding initially appears contrary to this theory, although it is also possible that multiple similar task vectors are learned from the separate blocks and then ensembled via attention for the final prediction.Concurrently to our work,Agarwal et al. (2024) study many-shot prompting of Gemini 1.5 and show improvements from the fewshot setting across both classification and generation tasks. Our work differs in its evaluation of multiple open-source models, our comparison to finetuning the same base model, and our use of ICL as a testbed for analysis of long context behaviors.Comparing in-context learning and finetuningMin et al. (2022a) show that models trained on fewshot learning can generalize to perform fewshot learning on new tasks; in some cases, this can outperform finetuning directly on the new task.Mosbach et al. (2023) compare finetuning to ICL more directly; they find that finetuning generally outperforms ICL with the same number of examples both in-domain and out-of-domain, when comparing 16-example ICL to finetuning on the same 16 examples. Their setting differs from ours in their choice of model (OPT) , the amount of data considered (16 for ICL, 16 or 128 for finetuning) , and the use of full finetuning rather than PEFT.Liu et al. (2022) find that PEFT generally outperforms ICL in their setting, where they finetune an encoder-decoder model with a language modeling objective using their T-few method and 20-70 samples.Asai et al. (2023) compare finetuning and ICL for mT5 on cross-lingual transfer and find that ICL outperforms finetuning in some, but not all, of the tasks studied. To the best of our knowledge, no prior work has considered the relative performance of finetuning and ICL in the many-shot regime, where there are hundreds or thousands of examples in-context.",
                "abstract": "As model context lengths continue to increase, the number of demonstrations that can be provided in-context approaches the size of entire training datasets. We study the behavior of in-context learning (ICL) at this extreme scale on multiple datasets and models. We show that, for many datasets with large label spaces, performance continues to increase with hundreds or thousands of demonstrations. We contrast this with example retrieval and finetuning: example retrieval shows excellent performance at low context lengths but has diminished gains with more demonstrations; finetuning is more data hungry than ICL but can sometimes exceed long-context ICL performance with additional data. We use this ICL setting as a testbed to study several properties of both in-context learning and long-context models. We show that long-context ICL is less sensitive to random input shuffling than short-context ICL, that grouping of same-label examples can negatively impact performance, and that the performance boosts we see do not arise from cumulative gain from encoding many examples together. We conclude that although long-context ICL can be surprisingly effective, most of this gain comes from attending back to similar examples rather than task learning."
            },
            {
                "name": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models",
                "arxiv_id": "2404.00884",
                "subtitles": [
                    "In-Context Learning",
                    "Optimizing Demonstrations for ICL",
                    "Eliciting LLMs' Power with Prompts"
                ],
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Emergent abilities of large language models",
                    "Diverse demonstrations improve in-context compositional generalization",
                    "Large language models as analogical reasoners",
                    "Openai: Introducing chatgpt",
                    "Ambiguity-aware in-context learning with large language models",
                    "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
                    "Automatic chain of thought prompting in large language models",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Self-icl: Zero-shot in-context learning with self-generated demonstrations",
                    "Self-prompting large language models for open-domain QA",
                    "In-context examples selection for machine translation",
                    "Dr.icl: Demonstration-retrieved in-context learning",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                    "What makes good in-context examples for gpt",
                    "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
                    "Self-refine: Iterative refinement with self-feedback",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "How many demonstrations do you need for in-context learning",
                    "Generalizing to unseen domains: A survey on domain generalization",
                    "Learning to retrieve prompts for in-context learning",
                    "Overthinking the truth: Understanding how language models process false demonstrations",
                    "Language models are few-shot learners",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Synthetic prompting: Generating chain-of-thought demonstrations for large language models"
                ],
                "related_work": "2Related Work2.1In-Context LearningThe rise of LLMs such as ChatGPT(OpenAI,2022) and LLaMA(Touvron et al.,2023) has revolutionized the field. With the model size scaling, LLMs demonstrate remarkable capabilities of ICL(Brown et al.,2020b; Wei et al.,2022b) , which learns to perform tasks by specific instructions and demonstrations. Additionally, insights from scaling laws(Wei et al.,2022b) also highlight the LLMs' potential for out-of-distribution generalization. It refers to the challenge where model inputs deviate from their training distribution(Wang et al.,2023a) . If stimulated effectively, this generalization capability can empower LLMs to address queries outside the training corpus(Collins et al.,2022) , enhancing utility in dynamic and open-ended scenarios.2.2Optimizing Demonstrations for ICLThe performance of LLMs may be influenced by the quantity, relevance, diversity, and truthfulness of demonstrations(Chen et al.,2023a; Levy et al.,2023; Min et al.,2022; Halawi et al.,2023) . There are two primary paradigms to optimize demonstrations and steer models towards generalization.Demo Retrieval for ICL.LLMs are sensitive to the choice of demonstrations. Therefore, researchers have focused on using retrieval modules to find the most representative demos for ICL. One effective strategy is leveraging existing retrievers based on semantic similarity metrics between the available demos and queries(Liu et al.,2022; Agrawal et al.,2023; Gao et al.,2023; Luo et al.,2023) . Another method employs ranking scores derived from fine-tuned language models(Rubin et al.,2022; Shi et al.,2022) .Demo Generation for ICL.Rather than extracting existing demos, demo generation aims to self-generate exemplars that closely align with the input.Kim et al. (2022) initially employed language models to produce demos from pre-defined labels. Subsequent works adopted a two-stage approach of generating and selecting demos(Li et al.,2022; Zhang et al.,2023; Shao et al.,2023) . In contrast, our work leverages the intrinsic capabilities of LLMs to identify superior demos via best-of-N sampling.Besides, there are approaches akin to ours.Chen et al. (2023b) adopt multi-steps to construct demonstration pairs, whileYasunaga et al. (2023) prompt LLMs to recall relevant demos before answering. However, our method stands out by combining pre- and post-processing steps around demo generation to guarantee the high quality of generated demos.2.3Eliciting LLMs' Power with PromptsEfforts to enhance LLMs include finetuning with specific instructions(Wei et al.,2022a) and employing prompting strategies like Chain-of-Thought (CoT,Wei et al.,2022c) . Our approach adopts the prompt-based strategy and draws inspiration from studies of the  \"self \" series(Madaan et al.,2023; Wang et al.,2023b; Chen et al.,2023b) . The essence of  \"self \" is to leverage the model's inherent power, without external modules. Our method positions the LLM itself as an analyzer, generator, and selector, aiming to elicit its intrinsic generalizability to resolve OOD queries.",
                "abstract": "Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights."
            },
            {
                "name": "Visual In-Context Learning for Large Vision-Language Models",
                "arxiv_id": "2402.11574",
                "subtitles": [
                    "Large Vision-Language Models",
                    "In-Context Learning"
                ],
                "reference": [
                    "Flamingo: a visual language model for few-shot learning",
                    "A survey of machine unlearning",
                    "Explaining emergent in-context learning as kernel regression",
                    "A survey for in-context learning",
                    "No matter how you slice it: Machine unlearning with sisa comes at the expense of minority classes",
                    "Unifying vision-and-language tasks via text generation",
                    "A survey on multimodal large language models",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Videocon: Robust video-language alignment via contrast captions",
                    "An explanation of in-context learning as implicit bayesian inference",
                    "Transformers learn in-context by gradient descent",
                    "A very preliminary analysis of DALL-E",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Learning transferable visual models from natural language supervision",
                    "A review on machine unlearning",
                    "Are emergent abilities in large language models just in-context learning",
                    "A data generation perspective to the mechanism of in-context learning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Photorealistic text-to-image diffusion models with deep language understanding",
                    "Data minimization for GDPR compliance in machine learning models",
                    "Machine unlearning",
                    "A theory of emergent in-context learning as implicit structure induction",
                    "Gpt-4v(ision) system card"
                ],
                "related_work": "2Related Work2.1Large Vision-Language ModelsLarge Vision-Language Models (LVLMs) are designed to comprehend and generate content across vision and language modalities, allowing them to perform tasks that involve understanding and generating not only text, but also information in visual formsYin et al. (2023) .LVLM can be broadly categorized into two main types, according to the output modalities: visual understanding and visual generation. Visual understanding models are capable of comprehending visual modality information provided to them and generating textual responses, enabling them to accomplish tasks such as image captioning, image question answeringZhu et al. (2023) ; OpenAi (2023) ; Alayrac et al. (2022) , video understandingCho et al. (2021) , video captioningBansal et al. (2023) , etc. The typical structure of these models involves integrating the visual encoders based on transformer architecture (like ClipRadford et al. (2021) ) into a large language model.On the other hand, visual generation models are equipped with visual decoders, enabling the decoding of feature vectors into images or videos. They have shown the ability to create high-quality outputs in generative tasks, such as generating text, images, and videosMarcus et al. (2022) ; Saharia et al. (2022) ; Zhang et al. (2023b) .However, since the LVLM has strong capabilities, it memorizes much unnecessary knowledge. In certain scenarios involving security or privacy concerns, it becomes imperative to selectively erase specific knowledge acquired by machine learning modelsGoldsteen et al. (2022) . Unlike conventional databases where information is explicitly stored in tabular forms, the entirety of a model's acquired knowledge is implicitly embedded within its parameters. Consequently, the challenge arises of accurately expunging unwanted information without necessitating a complete retraining of the model, thereby minimizing interference with other retained knowledge. This intricate problem is addressed by a collective set of techniques known as machine unlearningBourtoule et al. (2021) ; Nguyen et al. (2022) ; Zhang et al. (2023a) ; Koch and Soll (2023) .2.2In-Context LearningIn-Context Learning exemplifies a paradigm where model weights require no optimization; rather, adjusting the model input (adding context) leads to correct output generationDong et al. (2023) . An in-context learning prompt typically consists of two components: demonstration and new query. Demonstrations comprise multiple question-answer pairs, each presenting a complete question and its corresponding answer, while new queries involve inquiries posed to the model. Due to the emergent ability in large language modelsLu et al. (2023) , they can to some extent reference demonstrations to answer new questionsMin et al. (2022) . With the advantage of not necessitating fine-tuning of model parameters, in-context learning has become a popular paradigm for applying large language models.The inherent black-box nature of deep neural models renders the reasons behind the efficacy of in-context learning even more challenging to elucidateMao et al. (2024) ; Hahn and Goyal (2023) ; Han et al. (2023) ; von Oswald et al. (2023) ; Xie et al. (2022) . One of the most widely accepted theoretical explanations at present is that when the pre-training text has long-range coherence, if the demonstrations in the prompt share potential concepts, in-context learning ability will emergeXie et al. (2022) .Motivated by in-context learning, the in-context unlearningPawelczyk et al. (2023) emerges as a promising solution, which specifically applies the in-context learning paradigm without updating any model parameters, making it suitable for large language models. The framework leverages a combination of incorrectly and correctly labeled examples from training datasets. By analyzing and understanding the nuances within these discrepancies, a unique prompt is constructed for each instance. This tailored prompt aims to highlight specific challenges posed by the labeling discrepancies, encouraging the model to refine its predictions during inference.",
                "abstract": "In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval & Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use of in-context unlearning further shows promise in resetting specific model knowledge without retraining."
            }
        ],
        "survey": {
            "name": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
            "arxiv_id": "2401.11624",
            "subtitles": [
                {
                    "name": "Few-shot In-context Learning for Language Models",
                    "key_history": [
                        {
                            "reference_title": "Two decades of statistical language modeling: Where do we go from here",
                            "key_word": "probabilistic model"
                        },
                        {
                            "reference_title": "N-gram language models",
                            "key_word": "N-gram"
                        },
                        {
                            "reference_title": "Efficient estimation of word representations in vector space",
                            "key_word": "word embedding"
                        },
                        {
                            "reference_title": "Attention is all you need",
                            "key_word": "Transformer"
                        },
                        {
                            "reference_title": "Language models are few-shot learners",
                            "key_word": "Number of Demonstrations"
                        },
                        {
                            "reference_title": "How can we know what language models know",
                            "key_word": "Demonstration Formatting"
                        },
                        {
                            "reference_title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                            "key_word": "Order of Demonstrations"
                        },
                        {
                            "reference_title": "Diversity of thought improves reasoning abilities of large language models",
                            "key_word": "Diversity of Demonstrations"
                        },
                        {
                            "reference_title": "Chain of thought prompting elicits reasoning in large language models",
                            "key_word": "Chain of Thought (CoT)"
                        }
                    ],
                    "references_in_this_section": [
                        "What can transformers learn in-context? a case study of simple function classes",
                        "Two decades of statistical language modeling: Where do we go from here",
                        "Tasklama: probing the complex task understanding of language models",
                        "Unified demonstration retriever for in-context learning",
                        "Calibrate before use: Improving few-shot performance of language models",
                        "A neural probabilistic language model",
                        "Metaicl: Learning to learn in context",
                        "A survey of large language models",
                        "Improving in-context few-shot learning via self-supervised training",
                        "Large language models are zero-shot reasoners",
                        "Automatic chain of thought prompting in large language models",
                        "N-gram language models",
                        "Rethinking the role of demonstrations: What makes in-context learning work",
                        "Towards understanding chain-of-thought prompting: An empirical study of what matters",
                        "Fairness-guided few-shot prompting for large language models",
                        "Long short-term memory",
                        "Measuring faithfulness in chain-of-thought reasoning",
                        "Chain of thought prompting elicits reasoning in large language models",
                        "Detecting formal thought disorder by deep contextualized word representations",
                        "Instruction tuning for large language models: A survey",
                        "Transformers learn in-context by gradient descent",
                        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                        "Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models",
                        "Diversity of thought improves reasoning abilities of large language models",
                        "Challenging big-bench tasks and whether chain-of-thought can solve them",
                        "The flan collection: Designing data and methods for effective instruction tuning",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "An overview of overfitting and its solutions",
                        "Understanding finetuning for factual knowledge extraction from language models",
                        "\" according to \" prompting language models improves quoting from pre-training data",
                        "Efficient estimation of word representations in vector space",
                        "How can we know what language models know",
                        "Bert: Pre-training of deep bidirectional transformers for language understanding",
                        "Large language models as optimizers",
                        "Can language models be biomedical knowledge bases",
                        "Attention is all you need",
                        "Language models are few-shot learners",
                        "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                        "In-context instruction learning",
                        "Language models are unsupervised multitask learners",
                        "Language models as knowledge bases",
                        "An explanation of in-context learning as implicit bayesian inference",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "In-context Learning with Demonstration Retrieval",
                    "key_history": [
                        {
                            "reference_title": "Language models are few-shot learners",
                            "key_word": "Few-shot Learning"
                        },
                        {
                            "reference_title": "An overview of overfitting and its solutions",
                            "key_word": "Fine-tuning"
                        },
                        {
                            "reference_title": "Understanding finetuning for factual knowledge extraction from language models",
                            "key_word": "Overfitting"
                        },
                        {
                            "reference_title": "Attention is all you need",
                            "key_word": "Transformer Models"
                        },
                        {
                            "reference_title": "Language models as knowledge bases",
                            "key_word": "Pre-trained Language Models"
                        },
                        {
                            "reference_title": "Detecting formal thought disorder by deep contextualized word representations",
                            "key_word": "Pre-trained Models"
                        },
                        {
                            "reference_title": "Training language models to follow instructions with human feedback",
                            "key_word": "Instruction Tuning"
                        },
                        {
                            "reference_title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
                            "key_word": "Chain of Thought"
                        },
                        {
                            "reference_title": "Calibrate before use: Improving few-shot performance of language models",
                            "key_word": "Demonstration Order"
                        },
                        {
                            "reference_title": "Large language models are zero-shot reasoners",
                            "key_word": "Demonstration Formatting"
                        }
                    ],
                    "references_in_this_section": [
                        "Self-prompting large language models for zero-shot open-domain qa",
                        "Diverse demonstrations improve in-context compositional generalization",
                        "Demix layers: Disentangling domains for modular language modeling",
                        "Compositional exemplars for in-context learning",
                        "Complexity-based prompting for multi-step reasoning",
                        "Unified demonstration retriever for in-context learning",
                        "Generate rather than retrieve: Large language models are strong context generators",
                        "Determinantal point processes for machine learning",
                        "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
                        "Z-icl: Zero-shot in-context learning with pseudo-demonstrations",
                        "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                        "Automatic chain of thought prompting in large language models",
                        "Reticl: Sequential retrieval of in-context examples with reinforcement learning",
                        "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                        "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
                        "Dr. icl: Demonstration-retrieved in-context learning",
                        "Learning to retrieve in-context examples for large language models",
                        "Learning to retrieve prompts for in-context learning",
                        "Sentence-bert: Sentence embeddings using siamese bert-networks",
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Cross-lingual retrieval augmented prompt for low-resource languages",
                        "Selective annotation makes language models better few-shot learners"
                    ]
                },
                {
                    "name": "Off-the-shelf Demonstration Retrievers",
                    "key_history": [
                        {
                            "reference_title": "Dr. icl: Demonstration-retrieved in-context learning",
                            "key_word": "Demonstration Retrieval"
                        },
                        {
                            "reference_title": "Unified demonstration retriever for in-context learning",
                            "key_word": "Demonstration Ordering"
                        },
                        {
                            "reference_title": "Learning to retrieve prompts for in-context learning",
                            "key_word": "Retrieval Objectives"
                        },
                        {
                            "reference_title": "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
                            "key_word": "Pretrained Dual Encoder"
                        },
                        {
                            "reference_title": "Automatic chain of thought prompting in large language models",
                            "key_word": "Clustering Retrieval"
                        },
                        {
                            "reference_title": "Diverse demonstrations improve in-context compositional generalization",
                            "key_word": "Demonstration Diversity"
                        },
                        {
                            "reference_title": "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                            "key_word": "Cross-Domain Retrieval"
                        },
                        {
                            "reference_title": "Complexity-based prompting for multi-step reasoning",
                            "key_word": "Complexity-based Retrieval"
                        },
                        {
                            "reference_title": "Learning to retrieve in-context examples for large language models",
                            "key_word": "Mix-domain Retrieval"
                        }
                    ],
                    "references_in_this_section": [
                        "Compositional exemplars for in-context learning",
                        "Palm: Scaling language modeling with pathways",
                        "Ms marco: A human generated machine reading comprehension dataset",
                        "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
                        "A large annotated corpus for learning natural language inference",
                        "Colbert: Efficient and effective passage search via contextualized late interaction over BERT",
                        "Text embeddings by weakly-supervised contrastive pre-training",
                        "In-context examples selection for machine translation",
                        "Semeval-2017 task 1: Semantic textual similarity - multilingual and cross-lingual focused evaluation",
                        "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                        "The probabilistic relevance framework: Bm25 and beyond",
                        "Dr. icl: Demonstration-retrieved in-context learning",
                        "Large dual encoders are generalizable retrievers",
                        "SimCSE: Simple contrastive learning of sentence embeddings",
                        "What makes good in-context examples for gpt",
                        "Learning to retrieve in-context examples for large language models",
                        "Roberta: A robustly optimized bert pretraining approach",
                        "Learning to retrieve prompts for in-context learning",
                        "mt5: A massively multilingual pre-trained text-to-text transformer",
                        "Sentence-bert: Sentence embeddings using siamese bert-networks",
                        "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
                        "Scaling instruction-finetuned language models",
                        "Colbertv2: Effective and efficient retrieval via lightweight late interaction",
                        "ComQA: A community-sourced dataset for complex factoid question answering with paraphrase clusters",
                        "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
                        "Multilingual universal sentence encoder for semantic retrieval"
                    ]
                },
                {
                    "name": "Fine-tuned Demonstrations Retrievers",
                    "key_history": [
                        {
                            "reference_title": "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                            "key_word": "Demonstration Retrieval"
                        },
                        {
                            "reference_title": "Compositional exemplars for in-context learning",
                            "key_word": "InfoNCE Loss"
                        },
                        {
                            "reference_title": "Dr. icl: Demonstration-retrieved in-context learning",
                            "key_word": "BM25"
                        },
                        {
                            "reference_title": "Unified demonstration retriever for in-context learning",
                            "key_word": "List-wise Ranking Loss"
                        },
                        {
                            "reference_title": "Learning to retrieve prompts for in-context learning",
                            "key_word": "Dual Encoder"
                        },
                        {
                            "reference_title": "Compositional exemplars for in-context learning",
                            "key_word": "Distillation by KL Divergence"
                        },
                        {
                            "reference_title": "Learning to retrieve in-context examples for large language models",
                            "key_word": "Iterative Training"
                        },
                        {
                            "reference_title": "Determinantal point processes for machine learning",
                            "key_word": "Diversity Training"
                        },
                        {
                            "reference_title": "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                            "key_word": "Re-ranker Training"
                        }
                    ],
                    "references_in_this_section": [
                        "Learning to retrieve prompts for in-context learning",
                        "Determinantal point processes for machine learning",
                        "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                        "Dense passage retrieval for open-domain question answering",
                        "Diverse demonstrations improve in-context compositional generalization",
                        "Dr. icl: Demonstration-retrieved in-context learning",
                        "Large dual encoders are generalizable retrievers",
                        "Automatic chain of thought prompting in large language models",
                        "From ranknet to lambdarank to lambdamart: An overview",
                        "Compositional exemplars for in-context learning",
                        "Unified demonstration retriever for in-context learning",
                        "Learning to retrieve in-context examples for large language models",
                        "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
                        "In-context learning for few-shot dialogue state tracking",
                        "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                        "Synchromesh: Reliable code generation from pre-trained language models"
                    ]
                },
                {
                    "name": "Applications",
                    "key_history": [
                        {
                            "reference_title": "Automatic chain of thought prompting in large language models",
                            "key_word": "Commonsense Reasoning"
                        },
                        {
                            "reference_title": "Compositional exemplars for in-context learning",
                            "key_word": "Open-Domain QA"
                        },
                        {
                            "reference_title": "Learning to retrieve prompts for in-context learning",
                            "key_word": "Text Generation"
                        }
                    ],
                    "references_in_this_section": [
                        "Training verifiers to solve math word problems",
                        "Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources",
                        "Hellaswag: Can a machine really finish your sentence",
                        "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
                        "Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system",
                        "Piqa: Reasoning about physical commonsense in natural language",
                        "Compositional exemplars for in-context learning",
                        "Squad: 100,000+ questions for machine comprehension of text",
                        "Break it down: A question understanding benchmark",
                        "Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark",
                        "Totto: A controlled table-to-text generation dataset",
                        "Think you have solved question answering? try arc, the ai2 reasoning challenge",
                        "A large annotated corpus for learning natural language inference",
                        "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                        "Automatic chain of thought prompting in large language models",
                        "Reticl: Sequential retrieval of in-context examples with reinforcement learning",
                        "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
                        "Task-oriented dialogue as dataflow synthesis",
                        "Semantic noise matters for neural natural language generation",
                        "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                        "Looking beyond the surface:a challenge set for reading comprehension over multiple sentences",
                        "PAWS: Paraphrase Adversaries from Word Scrambling",
                        "Can machines learn morality? the delphi experiment",
                        "Boolq: Exploring the surprising difficulty of natural yes/no questions",
                        "What makes good in-context examples for gpt",
                        "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                        "Learning to parse database queries using inductive logic programming",
                        "Finding contradictions in text",
                        "A broad-coverage challenge corpus for sentence understanding through inference",
                        "Can a suit of armor conduct electricity? a new dataset for open book question answering",
                        "Natural questions: a benchmark for question answering research",
                        "Dart: Open-domain structured data record to text generation",
                        "Semantic parsing on freebase from question-answer pairs",
                        "Learning to retrieve prompts for in-context learning",
                        "Twitter sentiment classification using distant supervision",
                        "Character-level convolutional networks for text classification",
                        "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
                        "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
                        "Recursive deep models for semantic compositionality over a sentiment treebank",
                        "Cross-lingual retrieval augmented prompt for low-resource languages",
                        "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
                        "Selective annotation makes language models better few-shot learners"
                    ]
                },
                {
                    "name": "Discussion of Future Direction",
                    "key_history": [
                        {
                            "reference_title": "Z-icl: Zero-shot in-context learning with pseudo-demonstrations",
                            "key_word": "Pseudo Demonstrations"
                        },
                        {
                            "reference_title": "Self-consistency improves chain of thought reasoning in language models",
                            "key_word": "Self-consistency"
                        },
                        {
                            "reference_title": "Active example selection for in-context learning",
                            "key_word": "Active Learning"
                        },
                        {
                            "reference_title": "Take one step at a time to know incremental utility of demonstration: An analysis on reranking for few-shot in-context learning",
                            "key_word": "Retriever Training"
                        }
                    ],
                    "references_in_this_section": [
                        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                        "Retrieval-augmented multimodal language modeling",
                        "Exploring diverse in-context configurations for image captioning",
                        "Weakly-supervised visual-retriever-reader for knowledge-based question answering",
                        "Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering",
                        "Making pre-trained language models better few-shot learners",
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Flamingo: a visual language model for few-shot learning",
                        "Take one step at a time to know incremental utility of demonstration: An analysis on reranking for few-shot in-context learning",
                        "Retrieval augmented language model pre-training",
                        "Icd-lm: Configuring vision-language in-context demonstrations by language modeling",
                        "Z-icl: Zero-shot in-context learning with pseudo-demonstrations",
                        "Atlas: Few-shot learning with retrieval augmented language models",
                        "Active example selection for in-context learning",
                        "Compositional exemplars for in-context learning",
                        "Mimic-it: Multi-modal in-context instruction tuning",
                        "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                        "Better zero-shot reasoning with self-adaptive prompting"
                    ]
                }
            ],
            "all_references": [
                "Lambada: Backward chaining for automated reasoning in natural language",
                "Tasklama: probing the complex task understanding of language models",
                "Boolq: Exploring the surprising difficulty of natural yes/no questions",
                "Dart: Open-domain structured data record to text generation",
                "Weakly-supervised visual-retriever-reader for knowledge-based question answering",
                "Twitter sentiment classification using distant supervision",
                "Cross-lingual retrieval augmented prompt for low-resource languages",
                "Transformers learn in-context by gradient descent",
                "Scaling instruction-finetuned language models",
                "Complexity-based prompting for multi-step reasoning",
                "How can we know what language models know",
                "Z-icl: Zero-shot in-context learning with pseudo-demonstrations",
                "Chain of thought prompting elicits reasoning in large language models",
                "Break it down: A question understanding benchmark",
                "Language models as knowledge bases",
                "Understanding finetuning for factual knowledge extraction from language models",
                "Metaicl: Learning to learn in context",
                "Determinantal point processes for machine learning",
                "Multilingual universal sentence encoder for semantic retrieval",
                "In-context learning for few-shot dialogue state tracking",
                "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
                "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                "Hellaswag: Can a machine really finish your sentence",
                "The probabilistic relevance framework: Bm25 and beyond",
                "Attention is all you need",
                "In-context learning for text classification with many labels",
                "Colbertv2: Effective and efficient retrieval via lightweight late interaction",
                "Can machines learn morality? the delphi experiment",
                "Better zero-shot reasoning with self-adaptive prompting",
                "Detecting formal thought disorder by deep contextualized word representations",
                "Diverse demonstrations improve in-context compositional generalization",
                "Roberta: A robustly optimized bert pretraining approach",
                "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                "Text embeddings by weakly-supervised contrastive pre-training",
                "Atlas: Few-shot learning with retrieval augmented language models",
                "Exploring diverse in-context configurations for image captioning",
                "Finding contradictions in text",
                "An overview of overfitting and its solutions",
                "Diversity of thought improves reasoning abilities of large language models",
                "Memory-assisted prompt editing to improve gpt-3 after deployment",
                "Take one step at a time to know incremental utility of demonstration: An analysis on reranking for few-shot in-context learning",
                "Solving quantitative reasoning problems with language models",
                "The flan collection: Designing data and methods for effective instruction tuning",
                "Self-consistency improves chain of thought reasoning in language models",
                "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
                "Demix layers: Disentangling domains for modular language modeling",
                "A neural probabilistic language model",
                "Can language models be biomedical knowledge bases",
                "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                "Flamingo: a visual language model for few-shot learning",
                "Unified demonstration retriever for in-context learning",
                "Instruction tuning for large language models: A survey",
                "Parade: Passage ranking using demonstrations with large language models",
                "Language models are few-shot learners",
                "Training verifiers to solve math word problems",
                "Challenging big-bench tasks and whether chain-of-thought can solve them",
                "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                "Measuring faithfulness in chain-of-thought reasoning",
                "In-context learning as maintaining coherency: A study of on-the-fly machine translation using large language models",
                "Training language models to follow instructions with human feedback",
                "Semeval-2017 task 1: Semantic textual similarity - multilingual and cross-lingual focused evaluation",
                "Making pre-trained language models better few-shot learners",
                "Ms marco: A human generated machine reading comprehension dataset",
                "Looking beyond the surface:a challenge set for reading comprehension over multiple sentences",
                "Efficient estimation of word representations in vector space",
                "Finding supporting examples for in-context learning",
                "Dr. icl: Demonstration-retrieved in-context learning",
                "Sentence-bert: Sentence embeddings using siamese bert-networks",
                "Can a suit of armor conduct electricity? a new dataset for open book question answering",
                "SimCSE: Simple contrastive learning of sentence embeddings",
                "mt5: A massively multilingual pre-trained text-to-text transformer",
                "Large language models as optimizers",
                "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
                "Learning to parse database queries using inductive logic programming",
                "Semantic parsing on freebase from question-answer pairs",
                "Recursive deep models for semantic compositionality over a sentiment treebank",
                "Learning to retrieve prompts for in-context learning",
                "Ambiguity-aware in-context learning with large language models",
                "PAWS: Paraphrase Adversaries from Word Scrambling",
                "Two decades of statistical language modeling: Where do we go from here",
                "Active example selection for in-context learning",
                "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "From ranknet to lambdarank to lambdamart: An overview",
                "ComQA: A community-sourced dataset for complex factoid question answering with paraphrase clusters",
                "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
                "Colbert: Efficient and effective passage search via contextualized late interaction over BERT",
                "A broad-coverage challenge corpus for sentence understanding through inference",
                "Selective annotation makes language models better few-shot learners",
                "Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark",
                "A survey of large language models",
                "Icd-lm: Configuring vision-language in-context demonstrations by language modeling",
                "Lora: Low-rank adaptation of large language models",
                "Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system",
                "\" according to ...\" prompting language models improves quoting from pre-training data",
                "Towards understanding chain-of-thought prompting: An empirical study of what matters",
                "Self-prompting large language models for zero-shot open-domain qa",
                "A large annotated corpus for learning natural language inference",
                "Rethinking the role of demonstrations: What makes in-context learning work",
                "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                "Learning to retrieve in-context examples for large language models",
                "Calibrate before use: Improving few-shot performance of language models",
                "Squad: 100,000+ questions for machine comprehension of text",
                "Synchromesh: Reliable code generation from pre-trained language models",
                "Retrieval-augmented multimodal language modeling",
                "Dense passage retrieval for open-domain question answering",
                "In-context examples selection for machine translation",
                "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
                "What makes good in-context examples for gpt",
                "Demystifying prompts in language models via perplexity estimation",
                "Palm: Scaling language modeling with pathways",
                "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
                "Reticl: Sequential retrieval of in-context examples with reinforcement learning",
                "An explanation of in-context learning as implicit bayesian inference",
                "Compositional exemplars for in-context learning",
                "Long short-term memory",
                "In-context instruction learning",
                "Measuring mathematical problem solving with the math dataset",
                "Natural questions: a benchmark for question answering research",
                "N-gram language models",
                "Think you have solved question answering? try arc, the ai2 reasoning challenge",
                "Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources",
                "What can transformers learn in-context? a case study of simple function classes",
                "Large dual encoders are generalizable retrievers",
                "Totto: A controlled table-to-text generation dataset",
                "The power of scale for parameter-efficient prompt tuning",
                "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
                "Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering",
                "Qlora: Efficient finetuning of quantized llms",
                "Language models are unsupervised multitask learners",
                "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
                "Generate rather than retrieve: Large language models are strong context generators",
                "Retrieval augmented language model pre-training",
                "Compositional semantic parsing with large language models",
                "Large language models are zero-shot reasoners",
                "Piqa: Reasoning about physical commonsense in natural language",
                "Birds have four legs?! numersense: Probing numerical commonsense knowledge of pre-trained language models",
                "Semantic noise matters for neural natural language generation",
                "Fairness-guided few-shot prompting for large language models",
                "Task-oriented dialogue as dataflow synthesis",
                "Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                "Character-level convolutional networks for text classification",
                "Mimic-it: Multi-modal in-context instruction tuning",
                "Improving in-context few-shot learning via self-supervised training",
                "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "Automatic chain of thought prompting in large language models",
                "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp"
            ]
        },
        "topic_history": [
            {
                "name": "Long-context LLMs Struggle with Long In-context Learning",
                "arxiv_id": "2404.02060",
                "reference": [
                    "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
                    "Augmenting language models with long-term memory",
                    "Long range arena : A benchmark for efficient transformers",
                    "Rwkv: Reinventing rnns for the transformer era",
                    "Structured prompting: Scaling in-context learning to 1, 000 examples",
                    "What makes good in-context examples for GPT",
                    "Few-NERD: A few-shot named entity recognition dataset",
                    "In-context learning with many demonstration examples",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Mining discourse markers for unsupervised sentence representation learning",
                    "Parallel context windows for large language models",
                    "In-context learning for text classification with many labels",
                    "Loogle: Can long-context language models understand long contexts",
                    "Lost in the middle: How language models use long contexts",
                    " \u221ebench: Extending long context evaluation beyond 100k tokens",
                    "A survey on in-context learning",
                    "Roformer: Enhanced transformer with rotary position embedding",
                    "Longbench: A bilingual, multitask benchmark for long context understanding",
                    "Same task, more tokens: the impact of input length on the reasoning performance of large language models",
                    "Position-aware attention and supervised data improve slot filling",
                    "Code llama: Open foundation models for code",
                    "Extending context window of large language models via positional interpolation",
                    "Focused transformer: Contrastive training for context scaling",
                    "ConvFiT: Conversational fine-tuning of pretrained language models",
                    "L-eval: Instituting standardized evaluation for long context language models",
                    "Train short, test long: Attention with linear biases enables input length extrapolation",
                    "GoEmotions: A dataset of fine-grained emotions",
                    "When does in-context learning fall short and why? a study on specification-heavy tasks",
                    "PoSE: Efficient context window extension of LLMs via positional skip-wise training",
                    "Sparse local embeddings for extreme multi-label classification"
                ]
            },
            {
                "name": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
                "arxiv_id": "2401.09417",
                "reference": [
                    "Imagenet: A large-scale hierarchical image database",
                    "Eva: Exploring the limits of masked visual representation learning at scale",
                    "U-mamba: Enhancing long-range dependency for biomedical image segmentation",
                    "Microsoft coco: Common objects in context",
                    "Long range language modeling via gated state spaces",
                    "Introducing our multimodal models",
                    "Emerging properties in self-supervised vision transformers",
                    "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                    "Generating long sequences with sparse transformers",
                    "Hierarchically gated recurrent neural network for sequence modeling",
                    "Deep high-resolution representation learning for visual recognition",
                    "Cvt: Introducing convolutions to vision transformers",
                    "Linformer: Self-attention with linear complexity",
                    "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
                    "Hungry hungry hippos: Towards language modeling with state space models",
                    "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Resmlp: Feedforward networks for image classification with data-efficient training",
                    "Msg-transformer: Exchanging local spatial information by manipulating messenger tokens",
                    "Going deeper with convolutions",
                    "S4nd: Modeling images and videos as multidimensional signals with state spaces",
                    "Semantic understanding of scenes through the ade20k dataset",
                    "Training data-efficient image transformers & distillation through attention",
                    "Densely connected convolutional networks",
                    "Gradient-based learning applied to document recognition",
                    "Mlp-mixer: An all-mlp architecture for vision",
                    "Deep residual learning for image recognition",
                    "Beit: BERT pre-training of image transformers",
                    "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
                    "Simplified state space layers for sequence modeling",
                    "Selective structured state-spaces for long-form video understanding",
                    "Very deep convolutional networks for large-scale image recognition",
                    "Efficiently modeling long sequences with structured state spaces",
                    "Aggregated residual transformations for deep neural networks",
                    "Visual instruction tuning",
                    "Retentive network: A successor to transformer for large language modelss",
                    "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
                    "Designing network design spaces",
                    "Imagenet classification with deep convolutional neural networks",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "When an image is worth 1,024 x 1,024 words: A case study in computational pathology",
                    "Scaling up visual and vision-language representation learning with noisy text supervision",
                    "Learning transferable visual models from natural language supervision",
                    "A convnet for the 2020s",
                    "Reformer: The efficient transformer",
                    "Long movie clip classification with state-space video models",
                    "Longnet: Scaling transformers to 1,000,000,000 tokens",
                    "Convit: Improving vision transformers with soft convolutional inductive biases",
                    "More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity",
                    "Efficient movie scene detection using state-space transformers",
                    "Coatnet: Marrying convolution and attention for all data sizes",
                    "Efficientnetv2: Smaller models and faster training",
                    "Efficientnet: Rethinking model scaling for convolutional neural networks",
                    "Diffusion models without attention",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Swin transformer: Hierarchical vision transformer using shifted windows",
                    "Rethinking attention with performers"
                ]
            },
            {
                "name": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
                "arxiv_id": "2402.10207",
                "reference": [
                    "Deep reinforcement learning from human preferences",
                    "Nash learning from human feedback",
                    "Statistical rejection sampling improves preference optimization",
                    "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation",
                    "Dueling rl: reinforcement learning with trajectory preferences",
                    "Reinforced self-training (rest) for language modeling",
                    "A practical guide to multi-objective reinforcement learning and planning",
                    "Learning all optimal policies with multiple criteria",
                    "Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf",
                    "Deep reinforcement learning for multiobjective optimization",
                    "Proximal policy optimization algorithms",
                    "Tailoring self-rationalizers with multi-reward distillation",
                    "Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf",
                    "Multi-objective reinforcement learning using sets of pareto dominating policies",
                    "Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints",
                    "Raft: Reward ranked finetuning for generative foundation model alignment",
                    "Fine-tuning language models from human preferences",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Aligning language models with offline reinforcement learning from human feedback",
                    "Gpt-4 technical report. arxiv",
                    "Beyond one-preference-for-all: Multi-objective direct preference optimization",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Rrhf: Rank responses to align language models with human feedback without tears",
                    "A survey of multi-objective sequential decision-making",
                    "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Mind Your Format: Towards Consistent Evaluation of In-Context Learning Improvements",
                "arxiv_id": "2401.06766",
                "reference": [
                    "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
                    "Palm: Scaling language modeling with pathways",
                    "Calibrate before use: Improving few-shot performance of language models",
                    "Batch calibration: Rethinking calibration for in-context learning and prompt engineering",
                    "Making pre-trained language models better few-shot learners",
                    "Data curation alone can stabilize in-context learning",
                    "Z-icl: Zero-shot in-context learning with pseudo-demonstrations",
                    "Mitigating label biases for in-context learning",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "The language of prompting: What linguistic properties make a prompt successful",
                    "Are we really making much progress? a worrying analysis of recent neural recommendation approaches",
                    "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning",
                    "Prototypical calibration for few-shot learning of language models",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "State of what art? a call for multi-prompt llm evaluation",
                    "A metric learning reality check",
                    "Noisy channel language model prompting for few-shot text classification",
                    "How can we know what language models know",
                    "Training compute-optimal large language models",
                    "Larger language models do in-context learning differently",
                    "In-context example selection with influences",
                    "The icl consistency test",
                    "A critical look at the evaluation of GNNs under heterophily: Are we really making progress",
                    "Language models are few-shot learners",
                    "Language models are unsupervised multitask learners",
                    "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
                    "Do prompt-based models really understand the meaning of their prompts"
                ]
            },
            {
                "name": "Grimoire is All You Need for Enhancing Large Language Models",
                "arxiv_id": "2401.03385",
                "reference": [
                    "What can transformers learn in-context? a case study of simple function classes",
                    "Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering",
                    "Data distributional properties drive emergent in-context learning in transformers",
                    "Calibrate before use: Improving few-shot performance of language models",
                    "Ground-truth labels matter: A deeper look into input-label demonstrations",
                    "What makes good in-context examples for GPT",
                    "In-context learning with iterative demonstration selection",
                    "Automatic chain of thought prompting in large language models",
                    "Finding support examples for in-context learning",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "In-context learning for text classification with many labels",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Learning to retrieve prompts for in-context learning",
                    "Larger language models do in-context learning differently",
                    "Language models are few-shot learners",
                    "Active example selection for in-context learning",
                    "An explanation of in-context learning as implicit bayesian inference",
                    "Towards informative few-shot prompt with maximum information gain for in-context learning",
                    "Selective annotation makes language models better few-shot learners"
                ]
            },
            {
                "name": "Language Models can Exploit Cross-Task In-context Learning for Data-Scarce Novel Tasks",
                "arxiv_id": "2405.10548",
                "reference": [
                    "Meta-cot: Generalizable chain-of-thought prompting in mixed-task scenarios with large language models",
                    "Learning to retrieve prompts for in-context learning",
                    "What makes good in-context examples for GPT",
                    "Language models are few-shot learners",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Multilingual LLMs are better cross-lingual in-context learners with alignment",
                    "Demystifying prompts in language models via perplexity estimation",
                    "Task compass: Scaling multi-task pre-training with task prefix"
                ]
            },
            {
                "name": "Many-Shot In-Context Learning",
                "arxiv_id": "2404.11018",
                "reference": [
                    "What can transformers learn in-context? a case study of simple function classes",
                    "Trained transformers learn linear models in-context",
                    "Scaling laws for neural language models",
                    "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation",
                    "The claude 3 model family: Opus, sonnet, haiku",
                    "Ground-truth labels matter: A deeper look into input-label demonstrations",
                    "Understanding in-context learning in transformers and llms by learning to learn discrete functions",
                    "In-context learning with many demonstration examples",
                    "In-Context Learning with Long-Context Models: An In-Depth Exploration, Apr",
                    "Reinforced self-training (rest) for language modeling",
                    "Automatic chain of thought prompting in large language models",
                    "Dual operating modes of in-context learning",
                    "Transformers generalize differently from information stored in context vs in weights, Oct",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Data Engineering for Scaling Language Models to 128K Context, Feb",
                    "Lift: Language-interfaced fine-tuning for non-language machine learning tasks",
                    "Gpqa: A graduate-level google-proof q&a benchmark",
                    "Beyond human data: Scaling self-training for problem-solving with language models",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "Many-shot jailbreaking",
                    "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                    "Effective long-context scaling of foundation models",
                    "Scaling relationship on learning mathematical reasoning with large language models",
                    "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                    "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                    "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
                    "Benefits of transformer: In-context learning in linear regression tasks with unstructured data",
                    "Buffet: Benchmarking large language models for few-shot cross-lingual transfer",
                    "From words to numbers: Your large language model is secretly a capable regressor when given in-context examples",
                    "An information-theoretic analysis of in-context learning",
                    "In-context learning learns label relationships but is not conventional learning",
                    "Exploring the landscape of distributional robustness for question answering models",
                    "Language models are few-shot learners",
                    "Are human-generated demonstrations necessary for in-context learning",
                    "Efficient attention via control variates"
                ]
            },
            {
                "name": "In-Context Learning with Long-Context Models: An In-Depth Exploration",
                "arxiv_id": "2405.00200",
                "reference": [
                    "Lm-infinite: Zero-shot extreme length generalization for large language models",
                    "Yarn: Efficient context window extension of large language models",
                    "What in-context learning  \"learns \" in-context: Disentangling task recognition and task learning",
                    "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation",
                    "Structured prompting: Scaling in-context learning to 1, 000 examples",
                    "MetaICL: Learning to learn in context",
                    "How long can context length of open-source LLMs truly promise",
                    "Pose: Efficient context window extension of llms via positional skip-wise training",
                    "Prompt-augmented linear probing: Scaling beyond the limit of few-shot in-context learners",
                    "Many-shot in-context learning",
                    "Dual operating modes of in-context learning",
                    "MEGABYTE: Predicting million-byte sequences with multiscale transformers",
                    "Parallel context windows for large language models",
                    "In-context learning for text classification with many labels",
                    "Loogle: Can long-context language models understand long contexts?, 2023b",
                    "Ring attention with blockwise transformers for near-infinite context",
                    "Lost in the Middle: How Language Models Use Long Contexts",
                    "Transformers learn in-context by gradient descent",
                    "Efficient streaming language models with attention sinks",
                    "Hierarchical context merging: Better long context understanding for pre-trained LLMs",
                    "In-context learning creates task vectors",
                    "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                    "Effective long-context scaling of foundation models",
                    "In-context learning and gradient descent revisited",
                    "Code llama: Open foundation models for code",
                    "Extending context window of large language models via positional interpolation",
                    "Long-context language modeling with parallel context encoding",
                    "Buffet: Benchmarking large language models for few-shot cross-lingual transfer",
                    "impact of sample selection on in-context learning for entity extraction from scientific writing",
                    "Focused transformer: Contrastive training for context scaling",
                    "Unlimiformer: Long-range transformers with unlimited length input",
                    "Towards understanding in-context learning with contrastive demonstrations and saliency maps"
                ]
            },
            {
                "name": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models",
                "arxiv_id": "2404.00884",
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Emergent abilities of large language models",
                    "Diverse demonstrations improve in-context compositional generalization",
                    "Large language models as analogical reasoners",
                    "Openai: Introducing chatgpt",
                    "Ambiguity-aware in-context learning with large language models",
                    "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
                    "Automatic chain of thought prompting in large language models",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Self-icl: Zero-shot in-context learning with self-generated demonstrations",
                    "Self-prompting large language models for open-domain QA",
                    "In-context examples selection for machine translation",
                    "Dr.icl: Demonstration-retrieved in-context learning",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "XRICL: cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic parsing",
                    "What makes good in-context examples for gpt",
                    "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
                    "Self-refine: Iterative refinement with self-feedback",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "How many demonstrations do you need for in-context learning",
                    "Generalizing to unseen domains: A survey on domain generalization",
                    "Learning to retrieve prompts for in-context learning",
                    "Overthinking the truth: Understanding how language models process false demonstrations",
                    "Language models are few-shot learners",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Synthetic prompting: Generating chain-of-thought demonstrations for large language models"
                ]
            },
            {
                "name": "Visual In-Context Learning for Large Vision-Language Models",
                "arxiv_id": "2402.11574",
                "reference": [
                    "Flamingo: a visual language model for few-shot learning",
                    "A survey of machine unlearning",
                    "Explaining emergent in-context learning as kernel regression",
                    "A survey for in-context learning",
                    "No matter how you slice it: Machine unlearning with sisa comes at the expense of minority classes",
                    "Unifying vision-and-language tasks via text generation",
                    "A survey on multimodal large language models",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Videocon: Robust video-language alignment via contrast captions",
                    "An explanation of in-context learning as implicit bayesian inference",
                    "Transformers learn in-context by gradient descent",
                    "A very preliminary analysis of DALL-E",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Learning transferable visual models from natural language supervision",
                    "A review on machine unlearning",
                    "Are emergent abilities in large language models just in-context learning",
                    "A data generation perspective to the mechanism of in-context learning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Photorealistic text-to-image diffusion models with deep language understanding",
                    "Data minimization for GDPR compliance in machine learning models",
                    "Machine unlearning",
                    "A theory of emergent in-context learning as implicit structure induction",
                    "Gpt-4v(ision) system card"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
            "arxiv_id": "2305.00447",
            "isAPA": true,
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse domains, thereby prompting researchers to explore their potential for use in recommendation systems. Initial attempts have leveraged the exceptional capabilities of LLMs, such as rich knowledge and strong generalization through In-context Learning, which involves phrasing the recommendation task as prompts. Nevertheless, the performance of LLMs in recommendation tasks remains suboptimal due to a substantial disparity between the training tasks for LLMs and recommendation tasks, as well as inadequate recommendation data during pre-training. To bridge the gap, we consider building a Large Recommendation Language Model by tunning LLMs with recommendation data. To this end, we propose an efficient and effective Tuning framework for Aligning LLMs with Recommendations, namely TALLRec. We have demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples. Additionally, the proposed framework is highly efficient and can be executed on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM exhibits robust cross-domain generalization. Our code and data are available at https://github.com/SAI990323/TALLRec",
            "reference": [
                "Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive recommender systems",
                "Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose, and Xiangnan He. 2019. A Simple Convolutional Generative Network for Next Item Recommendation",
                "Yunfan Gao et al. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System. arXiv preprint",
                "F.Maxwell and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context",
                "Liang Wang, Nan Yang, and Furu Wei. 2023b. Query2doc: Query Expansion with Large Language Models. (2023) . arXiv",
                "Sihao Ding, Fuli Feng, Xiangnan He, Jinqiu Jin, Wenjie Wang, Yong Liao, and Yongdong Zhang. 2022. Interpolative Distillation for Unifying Biased and Debiased Recommendation",
                "Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023a. Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                "Bal\u00e1zs Hidasi et al. 2016. Session-based Recommendations with Recurrent Neural Networks",
                "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning",
                "Lei Li, Yongfeng Zhang, and Li Chen. 2023b. Personalized prompt learning for explainable recommendation. ACM Transactions on Information Systems",
                "Hongyi Wen, Xinyang Yi, Tiansheng Yao, Jiaxi Tang, Lichan Hong, and Ed H. Chi. 2022. Distributionally-robust Recommendations for Improving Worst-case User Experience",
                "Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2015. Learning hierarchical representation model for nextbasket recommendation",
                "Yang Zhang et al. 2023b. Reformulating CTR Prediction: Learning Invariant Feature Interactions for Recommendation",
                "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation",
                "Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation",
                "Jason Wei, Yi Tay, Rishi Bommasani, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv",
                "Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems. arXiv preprint arXiv",
                "Chengfeng Xu, Jian Feng, Pengpeng Zhao, Fuzhen Zhuang, Deqing Wang, Yanchi Liu, and Victor S. Sheng. 2021. Long- and short-term self-attention network for sequential recommendation. Neurocomputing",
                "Zhenlei Wang, Shiqi Shen, Zhipeng Wang, Bo Chen, Xu Chen, and Ji-Rong Wen. 2022b. Unbiased Sequential Recommendation with Latent Confounders",
                "Greg Brockman, Mira Murati, Peter Welinder, and OpenAI. 2022. OpenAI API",
                "Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020. Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation",
                "Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and Georg Lausen. 2005. Improving Recommendation Lists through Topic Diversification",
                "Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. abs/2301.01820 (2023) . arXiv",
                "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. CoRR abs/2303.18223 (2023) . https://doi.org/10.48550/arXiv.2303.18223 arXiv",
                "Long Ouyang et al. 2022. Training language models to follow instructions with human feedback. NeurIPS",
                "Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                "Ruining He and Julian McAuley. 2016. Fusing similarity models with markov chains for sparse sequential recommendation",
                "Jordan Hoffmann et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv",
                "Zhengyi Yang et al. 2023. A Generic Learning Framework for Sequential Recommendation with Distribution Shifts",
                "Victor Sanh, Albert Webson, Colin Raffel, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv",
                "Tingting Zhang, Pengpeng Zhao, Yanchi Liu, et al. 2019. Feature-level Deeper Self-Attention Network for Sequential Recommendation",
                "Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv",
                "Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation",
                "Yu Zheng et al. 2021. Disentangling User Interest and Conformity for Recommendation with Causal Embedding",
                "Hugo Touvron et al. 2023. LlaMA: Open and efficient foundation language models. arXiv preprint arXiv",
                "Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited",
                "Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. arXiv preprint arXiv",
                "Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023. InferFix: End-to-End Program Repair with LLMs. (2023) . arXiv",
                "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv",
                "Qiang Cui, Shu Wu, Qiang Liu, Wen Zhong, and Liang Wang. 2020. MV-RNN: A Multi-View Recurrent Neural Network for Sequential Recommendation. IEEE Trans. Knowl. Data Eng",
                "Xi Victoria Lin, Todor Mihaylov, et al. 2021. Few-shot learning with multilingual language models. arXiv preprint arXiv",
                "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, et al. 2019. Parameter-Efficient Transfer Learning for NLP",
                "Percy Liang, Rishi Bommasani, Tony Lee, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv",
                "Yabo Ni, Dan Ou, Shichen Liu, et al. 2018. Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks",
                "Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation",
                "Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations. ACM Trans. Inf. Syst",
                "Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation",
                "Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning for Recommender Systems",
                "An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, and Julian J. McAuley. 2019. CosRec: 2D Convolutional Neural Networks for Sequential Recommendation",
                "Tom Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language models are few-shot learners. NeurIPS",
                "Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the Benefits of Training Expert Language Models over Instruction Tuning. (2023) . arXiv",
                "Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation",
                "Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. 2022. LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action",
                "Ziyang Wang, Huoyu Liu, Wei Wei, Yue Hu, Xian-Ling Mao, Shaojian He, Rui Fang, and Dangyang Chen. 2022a. Multi-level Contrastive Learning Framework for Sequential Recommendation. (2022) . arXiv",
                "Zizhuo Zhang and Bang Wang. 2023. Prompt Learning for News Recommendation. arXiv preprint arXiv",
                "Tim Donkers, Benedikt Loepp, and J\u00fcrgen Ziegler. 2017. Sequential user-based recurrent neural network recommendations",
                "Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan Tompson. 2022. Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. abs/2211.11736 (2022) . arXiv",
                "Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer",
                "Baolin Peng et al. 2023. Instruction Tuning with GPT-4. (2023) . arXiv",
                "Lei Wang et al. 2023a. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv",
                "Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding",
                "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv",
                "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model",
                "Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, et al. 2022. OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. arXiv preprint arXiv",
                "Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, and Mehmet A. Orgun. 2019. Sequential Recommender Systems: Challenges, Progress and Prospects",
                "Ruyu Li, Wenhao Deng, Yu Cheng, Zheng Yuan, Jiaqi Zhang, and Fajie Yuan. 2023a. Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights. arXiv preprint arXiv",
                "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, et al. 2023. PaLM-E: An Embodied Multimodal Language Model. abs/2303.03378 (2023) . arXiv",
                "Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation",
                "Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2021. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. (2021) . arXiv"
            ],
            "related work": "4.Related Work  \\bulletLMs for Recommendation.There have been several attempts to integrate language models (LMs) with recommendation systems. Despite the incorporation of LMs(Li et al.,2023b; Geng et al.,2022) , some attempts persist in utilizing traditional user/item IDs to represent users/items. Thereby, they disregard the semantic understanding capabilities of LMs, such as reviews, which other work has incorporated the language information as part of the users/items embedding(Hou et al.,2022) . In addition, other methods either utilize an undisclosed model that already possesses preliminary recommendation capabilities(Cui et al.,2022) . or employ small models to train on large-scale downstream task data(Zhang and Wang,2023) . Moreover, the aforementioned models are also limited to small models, while this paper is orthogonal about how to adapt large language models to recommendation tasks. In recommendation systems, there is currently little research on applying LLMs in recommendation scenarios. Those works utilize the interaction ability of GPT3.5 series models and apply In-context Learning(Gao et al.,2023; Wang et al.,2023a) . In detail, Chat-Rec(Gao et al.,2023) endeavors to harness the interaction capabilities of ChatGPT and link the ChatGPT with traditional recommendation models (e.g. MF(Koren et al.,2009) , LightGCN(He et al.,2020) ) to formulate a conversational recommendation system. NIR(Wang et al.,2023a) shares a similar concept with Chat-Rec, which employs conventional recommendation models to generate candidate items subjected to a three-stage multi-step prompting process for re-ranking.  \\bulletSequential Recommendation.Our setup is close to the sequential recommendation, which aims to infer the user's next interaction based on users' historical interaction sequences(Fang et al.,2020; Wang et al.,2019) . In the early time, the Markov chain plays an important role in sequential recommendation(Rendle et al.,2010; He and McAuley,2016; Wang et al.,2015; Mahmood and Ricci,2007) . Recently, deep learning-based methods have become mainstream. Extensive work using different kinds of neural network structures, like RNN(Hidasi et al.,2016; Cui et al.,2020; Donkers et al.,2017) , CNN(Tang and Wang,2018; Yuan et al.,2019; Yan et al.,2019) , and attention(Kang and McAuley,2018; Zhang et al.,2019; Xu et al.,2021) , to model the user interaction sequences. However, limited by only using IDs to represent users and items, such work cannot fastly adapt and generalize to new scenarios. Thus, some works focus on the generalization ability of sequential recommendation models by pre-training(Ni et al.,2018; Yuan et al.,2020) , data augmentation(Qiu et al.,2022,2021; Wang et al.,2022a; Xie et al.,2022) , debiasing(Ding et al.,2022; Wang et al.,2022b; Zheng et al.,2021; Zhang et al.,2021) , and robust optimization(Wen et al.,2022; Yang et al.,2023) . However, they ignore the strong generalization ability of existing LLMs, leading to inadequate exploration.",
            "date": "2023"
        },
        "topic": "LLMs for Recommendation",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation",
                "arxiv_id": "2404.01855",
                "subtitles": [
                    "Next-POI Recommendation",
                    "LLM-Based Recommender Systems",
                    "LLMs for User Mobility Patterns"
                ],
                "reference": [
                    "Recommender systems in the era of large language models (llms",
                    "Next point-of-interest recommendation with inferring multi-step future preferences",
                    "Getnext: trajectory flow map enhanced transformer for next poi recommendation",
                    "Large language models are zero-shot rankers for recommender systems",
                    "A multi-channel next poi recommendation framework with multi-granularity check-in signals",
                    "Where would i go next? large language models as human mobility predictors",
                    "Spatio-temporal hypergraph learning for next poi recommendation",
                    "Next poi recommendation with dynamic graph and explicit dependency",
                    "Hme: A hyperbolic metric embedding approach for next-poi recommendation",
                    "Autonomous gis: the next-generation ai-powered gis",
                    "Uncovering chatgpt's capabilities in recommender systems",
                    "On the opportunities and challenges of foundation models for geospatial artificial intelligence",
                    "Gpt4rec: A generative framework for personalized recommendation and user interests interpretation",
                    "Zero-shot next-item recommendation using large pretrained language models",
                    "K2: A foundation language model for geoscience knowledge understanding and utilization",
                    "Geollm: Extracting geospatial knowledge from large language models",
                    "Gpt4geo: How a language model sees the world's geography",
                    "Geogpt: Understanding and processing geospatial tasks through an autonomous gpt",
                    "Language models represent space and time",
                    "Clsprec: Contrastive learning of long and short-term preferences for next poi recommendation",
                    "Large language models for spatial trajectory patterns mining",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Cityfm: City foundation models to solve urban challenges",
                    "Sta-tcn: Spatial-temporal attention over temporal convolutional network for next point-of-interest recommendation",
                    "Exploring large language models for human mobility prediction under public events",
                    "Leveraging large language models for sequential recommendation"
                ],
                "related_work": "IIRelated WorkII-ANext-POI RecommendationAs an important human mobility mining task, the next-POI recommendation problem captures the users' complex personalized check-in behaviors, where various factors play essential roles including individual interests, continuous movement patterns, and spatial-temporal influence, etc. Recently, the next POI recommendation has attracted extensive research interests and a large variety of approaches have been developed[19,4,6,5,20,7,21,8]. However, existing approaches require constructing and training recommendation models using extensive users' check-in data, demanding significant computational resources. Moreover, these task-specific recommendation models fall short in providing zero-shot POI recommendations for users. Different from them, this work aims to generate the next POI suggestions without the need for task-specific training, a direction not previously explored.II-BLLM-Based Recommender SystemsVery recently, the LLMs have been exploited for the recommendation tasks[22]. Although LLMs are not specifically designed for capturing user-item interactions, their proficiency in understanding textual information and robust generative capabilities, including providing explanations and justifications, holds significant promise for improving recommendations. An illustrative example is the generative GPT4Rec framework proposed in[23], which treats the recommendation task as a query generation and searching procedure.Several LLM-based recommendation methods address sequential recommendation problems. Harteet al.[24]propose three variants: LLM Embeddings, Fine-Tuned LLM, and LLM-enhanced Sequential Model. Wanget al.[15]introduce Zero-Shot Next-Item Recommendation with a prompting strategy guiding GPT-3 through user preferences, historical items, and top-K recommendations. Liuet al.[16]evaluate ChatGPT in five recommendation scenarios, employing zero-shot and few-shot prompt strategies for next-item prediction based on past sequential behaviors.[17]enhances sequential recommendations with a recency-focused prompting method. Daiet al.[18]combine ChatGPT with information retrieval for improved recommendation capabilities. However, these prompt-based methods lack consideration for geographical information, hindering their effectiveness in solving the next POI recommendation task.II-CLLMs for User Mobility PatternsThe exploration of leveraging pre-trained models for modeling geographical spatial data has garnered increasing research attention. Two main approaches emerge: training geospatial pre-trained models and utilizing open-accessible Large Language Models (LLMs) for geospatial tasks. Maiet al.[25]and Balsebreet al.[26]focus on geospatial foundation models. Based on the open-sourced LLaMA model, Denget al.[27]develop a foundation language model for understanding and utilizing geoscience knowledge. For open-accessible LLMs, studies like[28,29,11]explore tasks like population description, economic livelihood measurement, and route planning.[9]and[10]highlight that LLMs capture spatial information and acquire coherent knowledge about space and time.Approaches like[13,14,12]employ pre-trained LLMs for various human mobility prediction tasks, such as anomaly detection using LLMs[13], predicting travel demand under public events[14]. It's worth mentioning that[12]introduces the LLM-Mob framework, utilizing accessible LLMs for learning mobility data. While it accounts for both long-term and short-term dependencies, its primary focus is on incorporating temporal information into human mobility sequences. However, the framework is specifically designed for the time-aware location prediction task, limiting recommendations to historically visited places. In essence, it does not provide recommendations for new locations and does not consider the geographical information of places.",
                "abstract": "Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms."
            },
            {
                "name": "Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens",
                "arxiv_id": "2406.08477",
                "subtitles": [
                    "Characterizing Items by IDs",
                    "Instruction Tuning for Recommendation",
                    "ID Construction with Tokens"
                ],
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "PALR: personalization aware llms for recommendation",
                    "Recommender systems in the era of large language models (llms",
                    "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization",
                    "Recommender systems with generative retrieval",
                    "Model spider: Learning to rank pre-trained models efficiently",
                    "Self-attentive sequential recommendation",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Contrastive learning for sequential recommendation",
                    "Gpt4rec: A generative framework for personalized recommendation and user interests interpretation",
                    "Recommendation as language processing (RLP) : A unified pretrain, personalized prompt & predict paradigm (P",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "M6-rec: Generative pretrained language models are open-ended recommender systems",
                    "How to index item ids for recommendation foundation models",
                    "Cross-batch negative sampling for training two-tower recommenders",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Personalized transformer for explainable recommendation",
                    "Streaming CTR prediction: Rethinking recommendation task for real-world streaming data",
                    "Matrix factorization techniques for recommender systems",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Advances in collaborative filtering",
                    "BPR: bayesian personalized ranking from implicit feedback",
                    "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer"
                ],
                "related_work": "2Related WorkCharacterizing Items by IDs. Modern recommendation models usually use unique IDs to represent users and items, which are subsequently converted to embedding vectors as learnable parameters[11]. Common approaches include matrix factorization[21,32], two-tower models[38]and deep neural networks[35,19,49,46,43,39], which make predictions by examining historical user-item interactions to identify behavioral patterns and enable collaborative recommendations[22]. Some recent approaches adopt the concept of ID to represent the token combinations that characterize items in LLMs[13,17]. We follow previous studies and also call these token combinations as ID. This paper aims to combine LLM with ID more efficiently by proposing a new ID construction method.Instruction Tuning for Recommendation. The integration of Large Language Models (LLMs) into diverse tasks has seen significant growth recently. Recent works fine-tuned pretrained language models on large-scale NLP datasets verbalized via human-readable prompts[33,41]. These instruction tuning methods design prompts containing detailed task descriptions and adhere more to the natural language format. Driven by their exceptional natural language processing capabilities, researchers aim to transfer their linguistic ability to enhance recommender systems[25,45]. These LLMs process user interactions as sequences of tokens and, through fine-tuning, predict users' future interests based on past activities[47,5]. Moreover, some studies reframe tasks like retrieval, rating, and explanation generation as language comprehension tasks[6,13,23], allowing LLMs to function as multi-task recommenders, producing recommendations and explanations with a unified architecture. In this paper, we apply LLMs based on instruction tuning of multi-task scenarios.ID Construction with Tokens. Recent studies explore using in-vocabulary tokens to characterize items in LLMs, where users and items are represented by token combinations called IDs. Early efforts such as P5[13]convert user-item interactions into natural language formats using numeric IDs constructed of in-vocabulary tokens of the T5 model[29]. Further, sequential ID and collaborative ID are developed to enhance item information sharing[17]. A relevant study[30], constructs IDs using hierarchical tokens through RQ-VAE. Despite these advancements in ID construction, challenges remain in the lack of ID construction criteria and the focus on item IDs. In contrast, our META ID approach assigns memorization and diversity scores for evaluation and constructs both user and item IDs, which improves the characterization of users and items.",
                "abstract": "Characterizing users and items through vector representations is crucial for various tasks in recommender systems. Recent approaches attempt to apply Large Language Models (LLMs) in recommendation through a question and answer format, where real users and items (e.g., Item No.2024) are represented with in-vocabulary tokens (e.g., \"item\", \"20\", \"24\"). However, since LLMs are typically pretrained on natural language tasks, these in-vocabulary tokens lack the expressive power for distinctive users and items, thereby weakening the recommendation ability even after fine-tuning on recommendation tasks. In this paper, we explore how to effectively tokenize users and items in LLM-based recommender systems. We emphasize the role of out-of-vocabulary (OOV) tokens in addition to the in-vocabulary ones and claim the memorization of OOV tokens that capture correlations of users/items as well as diversity of OOV tokens. By clustering the learned representations from historical user-item interactions, we make the representations of user/item combinations share the same OOV tokens if they have similar properties. Furthermore, integrating these OOV tokens into the LLM's vocabulary allows for better distinction between users and items and enhanced capture of user-item relationships during fine-tuning on downstream tasks. Our proposed framework outperforms existing state-of-the-art methods across various downstream recommendation tasks."
            },
            {
                "name": "Improve Temporal Awareness of LLMs for Sequential Recommendation",
                "arxiv_id": "2405.02778",
                "subtitles": [
                    "LLMs for recommendation",
                    "Sequential recommendation"
                ],
                "reference": [
                    "Recommender systems in the era of large language models (llms",
                    "Palr: Personalization aware llms for recommendation",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Prompting large language models for recommender systems: A comprehensive framework and empirical analysis",
                    "Session-based recommendations with recurrent neural networks",
                    "Personalized top-n sequential recommendation via convolutional sequence embedding",
                    "Self-attentive sequential recommendation",
                    "Zero-shot next-item recommendation using large pretrained language models",
                    "Towards universal sequence representation learning for recommender systems",
                    "Factorizing personalized markov chains for next-basket recommendation",
                    "Neural attentive session-based recommendation",
                    "Large language models as zero-shot conversational recommenders",
                    "Learning vector-quantized item representation for transferable sequential recommenders",
                    "Feature-level deeper self-attention network for sequential recommendation",
                    "Do llms understand user preferences? evaluating llms on user rating prediction",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Session-based recommendation with graph neural networks",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Sequential recommendation with graph neural networks",
                    "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer",
                    "Recmind: Large language model powered agent for recommendation",
                    "Text is all you need: Learning language representations for sequential recommendation"
                ],
                "related_work": "2Related Works LLMs for recommendation.Recently, the use of LLMs in recommendation systems has garnered significant research interest due to their capability to comprehend and encapsulate a user's preferences and past interactions through natural language(Fan et al.,2023; He et al.,2023) . Current LLM-based recommender systems are primarily designed for rating prediction(Kang et al.,2023; Bao et al.,2023) and sequential recommendation tasks(Wang and Lim,2023; Hou et al.,2023b; Xu et al.,2024) . In both tasks, a user's previous interactions with items, along with other optional data like the user profile or item attributes, are concatenated to formulate a natural language prompt. This is then fed into an LLM with options for no fine-tuning(Wang and Lim,2023) , full-model fine-tuning(Chen,2023) or parameter-efficient fine-tuning(Bao et al.,2023) .Liu et al. (2023a) designs a series of prompts to evaluate ChatGPT's performance over five recommendation tasks.Wang et al. (2023) develops a ChatGPT-based agent to improve recommendation ability by using tools such as SQL and Web search. Contrary to existing works that focus on the tentative evaluation of LLMs' ability in recommendation, we focus on improving the LLM's inefficacy of utilizing temporal information by designing temporal-aware prompting strategies.Sequential recommendation.Sequential recommendation (SRS) Hidasi et al. (2015) ; Kang and McAuley (2018) aims to predict the next interacted items based on historical interaction sequences. Early works follow the Markov assumptionRendle et al. (2010) , by designing various neural network models to capture user preference within interaction sequences, including Recurrent Neural NetworkHidasi et al. (2015) ; Li et al. (2017) , Convolutional Neural NetworkTang and Wang (2018) , TransformerKang and McAuley (2018) ; Sun et al. (2019) , Graph Neural NetworkChang et al. (2021) ; Wu et al. (2019) . However, most of these approaches are developed based on item IDsKang and McAuley (2018) or attributesZhang et al. (2019) defined on specific domains, making it difficult to be generalized to other domains. Recently,Hou et al. (2023a) ,Hou et al. (2022) andLi et al. (2023) propose to learn unified item representations for SRS based on pretrained language models. They follow the paradigm that pretraining an unified text-based sequence encoder on source domains and then fine-tune the encoder on the target domain. However, all aforementioned methods need massive user interaction sequences on a specific domains and can not be easily transfer to unseen domains. In contrast, we propose utilizing LLMs to establish a domain-agnostic learning process for sequential recommendation systems. Our approach is training-free and readily generalizable to unseen domains using only prompts.",
                "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities in solving a wide range of general-purpose tasks. However, it is empirically found that LLMs fall short in recognizing and utilizing temporal information, rendering poor performance in tasks that require an understanding of sequential data, such as sequential recommendation. In this paper, we aim to improve temporal awareness of LLMs by designing a principled prompting framework inspired by human cognitive processes. Specifically, we propose three prompting strategies to exploit temporal information within historical interactions for LLM-based sequential recommendation. Besides, we emulate divergent thinking by aggregating LLM ranking results derived from these strategies. Evaluations on MovieLens-1M and Amazon Review datasets indicate that our proposed method significantly enhances the zero-shot capabilities of LLMs in sequential recommendation tasks."
            },
            {
                "name": "HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation",
                "arxiv_id": "2408.13521",
                "subtitles": [
                    "Applications of Knowledge Graphs",
                    "HR Data and Knowledge Graphs"
                ],
                "reference": [
                    "Knowledge graphs on the web - an overview",
                    "BanglaAutoKG: Automatic Bangla knowledge graph construction with semantic neural graph filtering",
                    "Construction of human resource ontology model for knowledge graph",
                    "Knowledge graphs",
                    "Medical knowledge graph: Data sources, construction, reasoning, and applications",
                    "Deep learning on knowledge graph for recommender system: A survey",
                    "Job recommendation algorithm based on knowledge graph",
                    "Representation learning of knowledge graph for wireless communication networks",
                    "Knowledge graph with job recommendation",
                    "Knowledge Graphs and Big Data Processing",
                    "Knowledge graph prompting for multi-document question answering",
                    "Explainable job-posting recommendations using knowledge graphs and named entity recognition"
                ],
                "related_work": "2Related Works2.1Applications of Knowledge GraphsKGs showcase versatility, excelling in applications such as semantic search, question answering, and recommendation systems(Wang et al.,2023b; Gao et al.,2020; Wasi et al.,2024) . Their structured representation enhances search engine results and tailors suggestions. KGs, pivotal in Natural Language Processing (NLP) , elevate information extraction and contribute to superior machine learning predictions. From enterprise knowledge management to biomedical research(Wu et al.,2023) , KGs exhibit adaptability. Their integration of diverse data, contextualization, and inherent flexibility underpin effectiveness in managing and extracting insights across varied domains, including Medical AI(Wu et al.,2023) , Wireless Communication Networks(He et al.,2022) , Search Engines(Heist et al.,2020) , and Big Data Decision Analysis(Janev et al.,2020) . KGs emerge as indispensable tools, navigating dynamic information landscapes seamlessly(Hogan et al.,2021) . Our work is inspired by these different uses of knowledge graphs.2.2HR Data and Knowledge GraphsThough human resource knowledge graphs have good potential, limited efforts have been made in this domain.Zhang et al. (2021) adopted a top-down approach to create the ontology model of a human resource knowledge graph. The paper describes the significance of initially establishing ontology and defines entities and relationships for HR KGs.Cui (2022) presented a hypothesis to build a job description KG using NLP-based semantic entity extraction, but no detailed methodologies or experiments were presented.Wang et al. (2022) presented a job recommendation algorithm based on KGs, using word similarity to find recommendations.Upadhyay et al. (2021) uses a NER-based approach to build knowledge graphs to aid in job recommendation. However, no practical efforts have been made to implement a tangible HR knowledge graph in real-world scenarios based on LLMs and utilize one graph for multiple downstream tasks.",
                "abstract": "Knowledge Graphs (KGs) serving as semantic networks, prove highly effective in managing complex interconnected data in different domains, by offering a unified, contextualized, and structured representation with flexibility that allows for easy adaptation to evolving knowledge. Processing complex Human Resources (HR) data, KGs can help in different HR functions like recruitment, job matching, identifying learning gaps, and enhancing employee retention. Despite their potential, limited efforts have been made to implement practical HR knowledge graphs. This study addresses this gap by presenting a framework for effectively developing HR knowledge graphs from documents using Large Language Models. The resulting KG can be used for a variety of downstream tasks, including job matching, identifying employee skill gaps, and many more. In this work, we showcase instances where HR KGs prove instrumental in precise job matching, yielding advantages for both employers and employees. Empirical evidence from experiments with information propagation in KGs and Graph Neural Nets, along with case studies underscores the effectiveness of KGs in tasks such as job and employee recommendations and job area classification. Code and data are available at :this https URL"
            },
            {
                "name": "LLMs for User Interest Exploration in Large-scale Recommendation Systems",
                "arxiv_id": "2405.16363",
                "subtitles": [
                    "LLMs for Recommendation Systems",
                    "User Interest Exploration"
                ],
                "reference": [
                    "Values of user exploration in recommender systems",
                    "Feedback loop and bias amplification in recommender systems",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Uncovering ChatGPT's Capabilities in Recommender Systems",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "TagGPT: Large Language Models are Zero-shot Multimodal Taggers",
                    "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models",
                    "VIP5: Towards Multimodal Foundation Models for Recommendation",
                    "GPT4Rec: A generative framework for personalized recommendation and user interests interpretation",
                    "Long-Term Value of Exploration: Measurements, Findings and Algorithms",
                    "Large Language Models as Data Augmenters for Cold-Start Item Recommendation",
                    "A First Look at LLM-Powered Generative News Recommendation",
                    "PIE: Personalized Interest Exploration for Large-Scale Recommender Systems",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Exploration in recommender systems",
                    "Tiny-newsrec: Effective and efficient plm-based news recommendation",
                    "How algorithmic confounding in recommendation systems increases homogeneity and decreases utility",
                    "Learning vector-quantized item representation for transferable sequential recommenders"
                ],
                "related_work": "2.Related WorkLLMs for Recommendation Systems. Application of LLMs to recommendation systems is a rapidly growing research area. Some studies explore using LLMs directly for generating recommendations(Bao et al.,2023; Dai et al.,2023; Geng et al.,2023; Hou et al.,2023b; Li et al.,2023c; Liu et al.,2023b) , while others focus on augmenting traditional recommendation models with LLM-powered feature engineering(Hou et al.,2023a; Yu et al.,2022) or enriched user/item representations(Li et al.,2023a; Liu et al.,2023a; Xi et al.,2023) . The computational cost of LLMs however presents a critical challenge. Directly using them for large-scale retrieval is expensive and hinders adoption. Wang et al.(Wang et al.,2024) use LLMs as data augmenters for conventional recommendation systems during training, to improve model performance without additional serving cost. Different from prior work, we focus on directly incorporating LLM-generated content to break the feedback loop, aiming for more diverse and serendipitous recommendations while maintaining efficiency.User Interest Exploration. Prior research has established the benefits of User Interest Exploration in recommendation systems, demonstrating its ability to expand user preferences and enhance long-term engagement(Chen et al.,2021; Chen,2021; Su et al.,2024) . However, a key challenge lies in the inherent closed-loop nature of existing systems(Chaney et al.,2018; Mansoury et al.,2020; Chen et al.,2021) . Training data is primarily derived from past user-item interactions, limiting the system's ability to explore truly novel interests. While methods like PIE(Mahajan et al.,2023) address this to certain extent through user-creator affinity and online bandit formulations, they remain confined by the system's internal knowledge(Chen et al.,2021) . Our work introduces a novel approach that integrates world knowledge from LLMs to overcome these limitations.",
                "abstract": "Traditional recommendation systems are subject to a strong feedback loop by learning from and reinforcing past user-item interactions, which in turn limits the discovery of novel user interests. To address this, we introduce a hybrid hierarchical framework combining Large Language Models (LLMs) and classic recommendation models for user interest exploration. The framework controls the interfacing between the LLMs and the classic recommendation models through \"interest clusters\", the granularity of which can be explicitly determined by algorithm designers. It recommends the next novel interests by first representing \"interest clusters\" using language, and employs a fine-tuned LLM to generate novel interest descriptions that are strictly within these predefined clusters. At the low level, it grounds these generated interests to an item-level policy by restricting classic recommendation models, in this case a transformer-based sequence recommender to return items that fall within the novel clusters generated at the high level. We showcase the efficacy of this approach on an industrial-scale commercial platform serving billions of users. Live experiments show a significant increase in both exploration of novel interests and overall user enjoyment of the platform."
            },
            {
                "name": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
                "arxiv_id": "2406.18825",
                "subtitles": [
                    "Feature Interaction Methods",
                    "User Behavior Methods",
                    "Semantic-Enhanced Methods"
                ],
                "reference": [
                    "FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction",
                    "Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features",
                    "Wide & deep learning for recommender systems",
                    "xdeepfm: Combining explicit and implicit feature interactions for recommender systems",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction",
                    "U-BERT: Pre-training user representations for improved recommendation",
                    "Towards Universal Sequence Representation Learning for Recommender Systems",
                    "Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction",
                    "Deep & cross network for ad click predictions",
                    "CTRL: Connect Collaborative and Language Model for CTR Prediction",
                    "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation",
                    "Bookgpt: A general framework for book recommendation empowered by large language model",
                    "DeepFM: a factorization-machine based neural network for CTR prediction",
                    "Prompt Learning for News Recommendation",
                    "Text Matching Improves Sequential Recommendation by Reducing Popularity Biases",
                    "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "Unitrec: A unified text-to-text transformer and joint contrastive learning framework for text-based recommendation",
                    "Tabllm: Few-shot classification of tabular data with large language models",
                    "Session-based recommendations with recurrent neural networks",
                    "Personalized top-n sequential recommendation via convolutional sequence embedding",
                    "Product-based neural networks for user response prediction",
                    "Self-attentive sequential recommendation",
                    "Zero-shot recommendation as language modeling",
                    "MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction",
                    "Factorization machines",
                    "Deep Interest Network for Click-Through Rate Prediction",
                    "Collm: Integrating collaborative embeddings into large language models for recommendation",
                    "Practice on long sequential user behavior modeling for click-through rate prediction",
                    "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
                    "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction",
                    "DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems",
                    "An introduction to convolutional neural networks",
                    "Deep session interest network for click-through rate prediction",
                    "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
                    "Towards open-world recommendation with knowledge augmentation from large language models",
                    "Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models",
                    "Exploring large language model for graph data understanding in online job recommendations",
                    "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                    "Field-embedded factorization machines for click-through rate prediction",
                    "Do llms understand user preferences? evaluating llms on user rating prediction",
                    "Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Ptab: Using the pre-trained language model for modeling tabular data",
                    "Deep Interest Evolution Network for Click-Through Rate Prediction",
                    "Text is all you need: Learning language representations for sequential recommendation"
                ],
                "related_work": "2.Related Work2.1.Feature Interaction MethodsEarly CTR prediction methods focus on leveraging collaborative signals(Lin et al.,2023b) . Classical FM(Rendle,2010) learns a hidden vector for each feature, capturing second-order feature interactions. Based on it, FFM(Pande,2020) introduces the concept of feature fields and divides each feature into different fields to capture combinatorial interaction relationships. Deep Crossing(Shan et al.,2016) learns the interactions among features through deep neural networks. PNN(Qu et al.,2016) replaces the stacking layer with the proposed product layer to achieve more comprehensive feature interactions. Based on logistic regression, Wide&Deep(Cheng et al.,2016) aims to enhance the model's memory and generalization capabilities with the wide part and the deep part, respectively. DCN(Wang et al.,2017) designed the cross network to replace the original wide part and introduced MoE structure along with low-rank operations in its improved version(Wang et al.,2021b) . DeepFM(Guo et al.,2017) replaces wide layer with an FM layer, enhancing the shallow model's feature interaction capabilities. xDeepFM(Lian et al.,2018) uses a multi-layer CIN network to better capture high-order information.2.2.User Behavior MethodsRecently more works leverage user behavior sequences to better model user interest(Liu et al.,2024) . GRU4Rec(Hidasi et al.,2015) proposes to apply GRU(Chung et al.,2014) units for sequential prediction. DIN(Zhou et al.,2018) uses attention mechanisms to explicitly model the relevance between target items and user historical behaviors. DIEN(Zhou et al.,2019) models the evolution process of user interests using recurrent neural networks and captures the dynamic changes in user interests over time. SASRec(Kang and McAuley,2018) applies self-attention mechanisms to user historical behavior sequences and applies position encoding to capture sequential relationships. DSIN(Feng et al.,2019) divides user behavior sequence into sessions to capture dynamic interests. Caser(Tang and Wang,2018) applies CNNs(O'shea and Nash,2015) to extract short-term preference sequence. MIMN(Pi et al.,2019) proposes to capture multiple aspects of user interests via a memory network architecture. SIM(Pi et al.,2020) leverages retrieval to model long user behaviors.2.3.Semantic-Enhanced MethodsLarge language models (LLMs) are being researched to assist recommendation tasks(Zhang et al.,2021; Liu et al.,2022; Mao et al.,2023; Zhang and Wang,2023; Li et al.,2023c; Hegselmann et al.,2023; Sileo et al.,2022; Kang et al.,2023; Zhiyuli et al.,2023; Bao et al.,2023; Wu et al.,2024; Wang et al.,2023b; Xi et al.,2023; Liu et al.,2023; Lin et al.,2024a; Zhang et al.,2023; Wang et al.,2023a; Li et al.,2023b; Qiu et al.,2021) . CTRL(Li et al.,2023a) aligns tabular data space with response space using contrastive learning. UniSRec(Hou et al.,2022) achieves unified cross-domain recommendation by learning item representations through a fixed BERT model. Building on it, VQ-Rec(Hou et al.,2023) applies vector quantization to learn better item representations. U-BERT(Qiu et al.,2021) enhances user representation quality by encoding review information through the BERT model. LMRecSys(Zhang et al.,2021) directly uses a pre-trained language model as a recommendation system. P5(Geng et al.,2022) trains a large language model on the T5(Raffel et al.,2020) model for various downstream recommendation tasks. PTab(Liu et al.,2022) models tabular data via modality conversion, language model fine-tuning, and prediction model fine-tuning steps. TALLRec(Bao et al.,2023) trains a large language recommendation model (LRLM) and designs a complete training paradigm and prompt templates for it. CoLLM(Zhang et al.,2023) proposed to provide collaborative knowledge to the large model using ID and interaction information. ReLLa(Lin et al.,2024b) trains the large language model on retrieved data, addressing the issue of long-sequence interest modeling.",
                "abstract": "Large language models have been flourishing in the natural language processing (NLP) domain, and their potential for recommendation has been paid much attention to. Despite the intelligence shown by the recommendation-oriented finetuned models, LLMs struggle to fully understand the user behavior patterns due to their innate weakness in interpreting numerical features and the overhead for long context, where the temporal relations among user behaviors, subtle quantitative signals among different ratings, and various side features of items are not well explored. Existing works only fine-tune a sole LLM on given text data without introducing that important information to it, leaving these problems unsolved. In this paper, we propose ELCoRec to Enhance Language understanding with CoPropagation of numerical and categorical features for Recommendation. Concretely, we propose to inject the preference understanding capability into LLM via a GAT expert model where the user preference is better encoded by parallelly propagating the temporal relations, and rating signals as well as various side information of historical items. The parallel propagation mechanism could stabilize heterogeneous features and offer an informative user preference encoding, which is then injected into the language models via soft prompting at the cost of a single token embedding. To further obtain the user's recent interests, we proposed a novel Recent interaction Augmented Prompt (RAP) template. Experiment results over three datasets against strong baselines validate the effectiveness of ELCoRec. The code is available atthis https URL."
            },
            {
                "name": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
                "arxiv_id": "2403.03536",
                "subtitles": [
                    "Machine Unlearning and Downstream Applications",
                    "Large Language Models as Recommenders"
                ],
                "reference": [
                    "Graph unlearning",
                    "Amnesiac machine learning",
                    "Recranker: Instruction tuning large language model as ranker for top-k recommendation",
                    "Adapting large language models by integrating collaborative semantics for recommendation",
                    "Remember what you want to forget: Algorithms for machine unlearning",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "Forgetting fast in recommender systems",
                    "Towards making systems forget with machine unlearning",
                    "A survey of machine unlearning",
                    "Recommendation unlearning via matrix correction",
                    "Flip: Towards fine-grained alignment between id-based models and pretrained language models for ctr prediction",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Knowledge unlearning for llms: Tasks, methods, and challenges",
                    "Prompting large language models for recommender systems: A comprehensive framework and empirical analysis",
                    "Making recommender systems forget: Learning and unlearning for erasable recommendation",
                    "A survey of large language models",
                    "How can recommender systems benefit from large language models: A survey",
                    "Forget me now: Fast and exact unlearning in neighborhood-based recommendation",
                    "Towards safer large language models through machine unlearning",
                    "Deep unlearning via randomized conditionally independent hessians",
                    "Understanding black-box predictions via influence functions",
                    "Certified data removal from machine learning models",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Collm: Integrating collaborative embeddings into large language models for recommendation",
                    "Recommendation unlearning via influence function",
                    "Unlearning protected user attributes in recommendations with adversarial training",
                    "Clickprompt: Ctr models are strong prompt generators for adapting language models to ctr prediction",
                    "Recommendation unlearning",
                    "Unlearn what you want to forget: Efficient unlearning for llms",
                    "Netflix and forget: Efficient and exact machine unlearning from bi-linear recommendations",
                    "Federated unlearning for on-device recommendation",
                    "Machine unlearning of pre-trained large language models",
                    "Tofu: A task of fictitious unlearning for llms",
                    "Ultrare: Enhancing receraser for recommendation unlearning via error decomposition",
                    "Selective and collaborative influence function for efficient recommendation unlearning",
                    "Arcane: An efficient architecture for exact machine unlearning",
                    "Rethinking machine unlearning for large language models",
                    "Large language model unlearning",
                    "Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation",
                    "Who's harry potter? approximate unlearning in llms",
                    "Agentcf: Collaborative learning with autonomous language agents for recommender systems",
                    "Llara: Aligning large language models with sequential recommenders",
                    "Sequence unlearning for sequential recommender systems",
                    "E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation",
                    "Language models are few-shot learners",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher",
                    "On the effectiveness of unlearning in session-based recommendation",
                    "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
                    "Machine unlearning for recommendation systems: An insight",
                    "Approximate data deletion from machine learning models",
                    "Machine unlearning",
                    "Making users indistinguishable: Attribute-wise unlearning in recommender systems",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work2.1Machine Unlearning and Downstream ApplicationsWe would like to first introduce the concept and categorization of general machine unlearning methods (i.e., exact unlearning and approximate unlearning) , and then elaborate on their special downstream applications for both recommendation unlearning and large language model unlearning.Machine Unlearninginvolves eliminating the influence of particular training data from a trained model[4,41]. One naive method to accomplish this is by retraining the model from scratch using a revised dataset that excludes the forgotten data. However, this method can be impractical due to its computational demands. Based on the degree of unlearning completeness, existing unlearning techniques can be categorized into exact unlearning and approximate unlearning.Exact unlearningrequires eliminating all information relevant to the forgotten data so that the unlearned model performs identically to a completely retrained model. For example, SISA[2], as a prominent approach, only requires retraining the sub-model trained on the relevant data shard for each unlearning request. GraphEraser[7]extends SISA for unlearning in graph neural networks, while ARCANE[54]partitions the dataset by class and employs one-class anomaly detection training on each subgroup. However, these methods require model retraining, which is computationally expensive and time-consuming.Approximate unlearningonly ensures that the performance of the unlearned model closely aligns with that of a retrained model[15,46]. These methods estimate the influence of the forgotten samples and mitigate it by inverse gradient-based update techniques[13,8], allowing efficient forgetting without retraining. This trade-off enhances efficiency and model utility but compromises completeness, leading to statistical forgetting[20]. Certain studies use the influence function to quantify the influence of the forgotten data[21], which entails computing the Hessian matrix and incurs additional computational overheads[40]. Recent research also highlights the computational intractability of the influence of individual training data on deep models[14].While machine unlearning techniques are widespread and well studied, many research works start to explore their applications for various downstream areas, e.g., recommendation unlearning and large language model unlearning.Recommendation Unlearninghas garnered increasing research interest due to both privacy and utility purposes[44,45,25,11]. Unlearning in recommender systems not only aids in safeguarding user privacy but also enhances recommendation models by mitigating the impact of noisy data[5]. RecEraser[5], along with LASER[27]and UltraRE[26], proposes novel data partition algorithms that divide the training set into balanced shards to preserve the collaborative information. AltEraser[35]enhances the efficiency by dividing the optimization problem of unlearning into many small tractable sub-problems. SCIF[24]and IFRU[62]extend influence function by selectively updating user embedding and employing importance-based pruning algorithms, respectively. Additionally, IMCorrect[33]and Unlearn-ALS[53]improve exact unlearning by correcting the mapping matrix and using Alternating Least Squares optimization, respectively. Lastly, unlearning methods have also emerged specifically for sequential recommendation[57], session-based recommendation[51]and federated recommendation[58]. However, none of the existing methods are tailored for LLM-based recommendation. Moreover, current approaches primarily prioritize forgetting effectiveness, neglecting the significance of preserving the original recommendation performance.Large Language Model Unlearning. While there's a growing body of research on machine unlearning, LLM unlearning is still a largely under-explored topic[47,34]. The extensive parameters and training data of LLMs render existing unlearning methods ill-suited and inefficient, since they are typically tailored for classification tasks and demand either model retraining or Hessian matrix computations.Yao et al. [56]formulate the settings and goals in LLM unlearning, advocating the removal of harmful content using a Gradient Ascent (GA) approach.Chen and Yang [6]introduce EUL, an efficient unlearning framework employing unlearning layers. Besides,Eldan and Russinovich [9]propose a novel network to unlearn copyrighted knowledge embedded within LLMs. However, they all use gradient ascent methods to unlearn knowledge, which will deteriorate the model performance on normal data[36]. Lastly, two benchmark[38,55]are established to evaluate the efficacy of various unlearning methods for LLMs. To the best of our knowledge, we are the first to study the LLM unlearning in the recommendation scenario. To overcome the shortcomings of gradient ascent methods, we adopt the teacher-student mode to maintain the original recommendation performance.2.2Large Language Models as RecommendersWith the remarkable development of large language models[63], researchers start to investigate their potential to directly perform the recommendation tasks, i.e., large language models as recommenders (LLMRec) . As suggested in previous work[30,52], according to different tasks to be solved by LLMs, LLMRec can be generally classified in three categories: (1) item scoring task, (2) item generation task, and (3) hybrid task.In theitem scoringtask[61,23,48,29,1,32], LLMs are required to perform pointwise preference estimation for each user-item pair, and the final ranked item list is generated by sorting the estimated preference scores. In theitem generationtask[28,16,64,59], LLMs serve as generative functions to directly produce the final ranked list. Generally speaking, such a generative task places a significant demand on the powerful reasoning and instruction-following capabilities of LLMs to infer the user preference for item generation. As for thehybridtask[60,12,37], LLMs would accomplish the recommendation in a multi-task manner which combines both the item scoring and generation tasks. The basis to support such hybrid functionality is that LLMs are inherent multi-task learners[3,42]through a unified language interface.In this paper, we mainly focus on the first paradigm of LLMRec (i.e., item scoring task) , which is more similar and aligned with traditional recommendation tasks. However, our proposed E2URec is generalized, and is also capable of item generation and hybrid tasks, which we leave as future works.",
                "abstract": "The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{this https URL}."
            },
            {
                "name": "Large Language Model with Graph Convolution for Recommendation",
                "arxiv_id": "2402.08859",
                "subtitles": [
                    "Graph-based Recommendation",
                    "Large Language Models for Recommendation",
                    "Large Language Models in Graphs"
                ],
                "reference": [
                    "A Survey on Large Language Models for Recommendation",
                    "Generative Job Recommendations with Large Language Model",
                    "Enhancing Job Recommendation through LLM-based Generative Adversarial Networks",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "Pre-train, prompt and recommendation: A comprehensive survey of language modelling paradigm adaptations in recommender systems",
                    "Greaselm: Graph reasoning enhanced language models",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Graph convolutional neural networks for web-scale recommender systems",
                    "Improving graph collaborative filtering with neighborhood-enriched contrastive learning",
                    "Self-supervised graph learning for recommendation",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Deep bidirectional language-knowledge graph pretraining",
                    "A First Look at LLM-Powered Generative News Recommendation",
                    "Exploring large language model for graph data understanding in online job recommendations",
                    "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
                    "Knowledge graph convolutional networks for recommender systems",
                    "Are graph augmentations necessary? simple graph contrastive learning for recommendation",
                    "Enhanced story comprehension for large language models through dynamic document-based knowledge graphs",
                    "Lightgcn: Simplifying and powering graph convolution network for recommendation",
                    "Language models are few-shot learners",
                    "Exploring the potential of large language models (llms) in learning on graphs",
                    "Llama: Open and efficient foundation language models",
                    "Multi-behavior recommendation with graph convolutional networks",
                    "Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers",
                    "Semi-Supervised Classification with Graph Convolutional Networks"
                ],
                "related_work": "2.RELATED WORK 2.1.Graph-based Recommendation In recent years, graph-based techniques have gained popularity in recommender systems(Kipf and Welling,2016) . They utilize deep neural networks to capture the complex interactive patterns between users and items of graph-structured data(Jin et al.,2020; Ying et al.,2018) . For example, Wang et al.(Wang et al.,2019) adopt GCN to discover the high-order structure information in the knowledge graph to enrich users' preference modeling. LightGCN(He et al.,2020) investigates the key components in GCNs and makes them more concise and appropriate for collaborative filtering, thus it gains popularity on recommendation tasks due to its simplicity and effectiveness. As a result, many studies adopt LightGCN using techniques such as contrastive learning(Yu et al.,2022) , neighborhood-structure(Lin et al.,2022) , and self-supervised learning(Wu et al.,2021) . However, these methods primarily focus on aggregating the embeddings of different nodes in the graph. They struggle to extract insights from text-level descriptions for semantic reasoning and description improvement. 2.2.Large Language Models for RecommendationWith the popularity of LLMs(Touvron et al.,2023; Brown et al.,2020) in the field of natural language processing, there is growing interest in leveraging LLMs to enhance the efficacy of recommender systems(Wu et al.,2023b) . According to the roles of LLMs in recommendation tasks, existing methods can be primarily categorized into two groups: LLM-as-predictor methods and LLM-as-extractor methods.LLM-as-predictor methods, also known as generative recommendation, adopt the LLM as a predictor to directly generate recommendation results for users. These methods usually convert recommendation tasks into natural language processing tasks. Subsequently, techniques such as in-context learning(Hou et al.,2023; Dai et al.,2022) , prompt tuning(Kang et al.,2023) , and instruction tuning(Zhang et al.,2023) are employed to trigger LLMs for the direct generation of recommendation results. For example,Zhang et al.(2023) employ instruction tuning in the prompt to inform LLMs about the requirements of the recommendation task to better trigger LLMs to obtain satisfactory results. In contrast, LLM-as-extractor methods use the LLM as a feature extractor for downstream recommendation tasks. These methods boost the quality of recommendations due to the external knowledge and reasoning capacity of LLMs(Liu et al.,2023c) . Specifically, these methods aim to capture contextual information with superior efficacy, enabling a precise understanding of user profiles(Zheng et al.,2023; Du et al.,2023) , item descriptions(Liu et al.,2023a) , and other textual data(Geng et al.,2022) . For example,Zheng et al.(2023) propose to fine-tune the LLM backbone based on the pairs of resumes and job descriptions if their candidates and employers reach an agreement, and then improve the user resume for the downstream recommendation task. However, current methods that prompt LLMs with raw texts ignore structured knowledge and easily lead to hallucinations, e.g., inconsistency between generation and users' behaviors. To this end, we incorporate graph information into LLMs, to predict the missing descriptions of users and items for accurate recommendation results. 2.3.Large Language Models in GraphsDue to the availability and effectiveness of graph information, combining LLMs with graph data has emerged as a promising direction. According to the manner of how to explore graphical information with LLMs, existing methods can be primarily categorized into two groups: supervised methods and unsupervised methods.Supervised methods typically adapt LLMs for graph-aware tasks with encoding strategies(Chen et al.,2023; Zhang et al.,2021) and training strategies(Sun et al.,2021; Yasunaga et al.,2022) . For example,Chen et al.(2023) use an LLM to encode the text information, and then integrate these encoding vectors into initial node features for graph models.Sun et al.(2021) propose a training objective by integrating entities and relations in the graph directly into the training data. However, these methods rely on compressing graph-based knowledge into the LLM's parameters via supervision, leaving the reasoning capability of LLMs unexplored for graphs. Unsupervised methods(Wang et al.,2023a; Andrus et al.,2022; Wu et al.,2023a) attempt to transfer graph information into textual information with templates and sampling strategies for LLMs. For example,Andrus et al.(2022) transfer knowledge graphs into text sentences based on prompts, then feed them into LLM to enhance story comprehension.Wu et al.(2023a) sample semantic information in heterogeneous information networks based on meta-path, and leverage LLM-based recommender to understand such semantic information for job recommendation. However, the limitation of these methods lies in their reliance on templates and sampling strategies to generate text information in one shot, which lacks the global view of the graph and fails to elicit LLMs' reasoning capacity on graph-based knowledge inference. To this end, we propose to utilize LLMs to explore graphs in a least-to-most manner, therefore propagating information progressively in the graph with a limited token requirement.",
                "abstract": "In recent years, efforts have been made to use text information for better user profiling and item characterization in recommendations. However, text information can sometimes be of low quality, hindering its effectiveness for real-world applications. With knowledge and reasoning capabilities capsuled in Large Language Models (LLMs), utilizing LLMs emerges as a promising way for description improvement. However, existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation. To this end, we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture high-order relations in the user-item graph. To adapt text-based LLMs with structured graphs, We use the LLM as an aggregator in graph processing, allowing it to understand graph-based information step by step. Specifically, the LLM is required for description enhancement by exploring multi-hop neighbors layer by layer, thereby propagating information progressively in the graph. To enable LLMs to capture large-scale graph information, we break down the description task into smaller parts, which drastically reduces the context length of the token input with each step. Extensive experiments on three real-world datasets show that our method consistently outperforms state-of-the-art methods."
            },
            {
                "name": "Reinforced Prompt Personalization for Recommendation with Large Language Models",
                "arxiv_id": "2407.17115",
                "subtitles": [
                    "LLMs for Recommender Systems",
                    "Prompt Sensitivity of LLMs",
                    "Discrete Prompt Engineering"
                ],
                "reference": [
                    "Plum: Prompt Learning using Metaheuristic",
                    "IDPG: An Instance-Dependent Prompt Generation Method",
                    "Instance-Aware Prompt Learning for Language Understanding and Generation",
                    "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
                    "Bookgpt: A general framework for book recommendation empowered by large language model",
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Calibrate Before Use: Improving Few-shot Performance of Language Models",
                    "Recommendation as Language Processing (RLP) : A Unified Pretrain, Personalized Prompt & Predict Paradigm (P",
                    "Instance-wise Prompt Tuning for Pretrained Language Models",
                    "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
                    "How Can Recommender Systems Benefit from Large Language Models: A Survey",
                    "What Makes Good In-Context Examples for GPT",
                    "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Language Models are Realistic Tabular Data Generators",
                    "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                    "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
                    "Learning To Retrieve Prompts for In-Context Learning",
                    "How Can We Know What Language Models Know",
                    "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                    "Uncovering ChatGPT's Capabilities in Recommender Systems",
                    "Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders",
                    "Language Models as Knowledge Bases"
                ],
                "related_work": "5.Related WorkHere, we provide a review regarding LLMs for recommender systems, prompt sensitivity of LLMs, and discrete prompt engineering.5.1.LLMs for Recommender SystemsRecently, large language models (LLMs) have demonstrated remarkable capabilities in instruction following and intent reasoning, drawing researchers' attention to combining LLMs with recommendations. The roles of LLMs at different stages of the recommendation pipeline can be classified into four categories(Lin et al.,2023) : feature engineering(Borisov et al.,2023) , feature encoder(Hou et al.,2023) , scoring function(Li et al.,2023a) , and pipeline controller(Gao et al.,2023) . In our study, we conceptualize LLMs as RSs, wherein the LLMs directly generate recommendation outcomes in the role of scoring function. This mechanism relies heavily on the quality of the prompts designed. Tailoring prompts of LLMs for the recommendation tasks has been explored by recent research. InstructRec(Zhang et al.,2023b) develops a general instruction format that contains users' preferences, intentions, task requirements, and contextual information. Chat-REC(Gao et al.,2023) recasts user profiles and prior interactions into prompts, enhancing the interactivity and explainability of the recommendation process. LLM4RS(Dai et al.,2023) designs domain-specific prompt format to analyze ChatGPT's recommendation ability on all three ranking policies, including point-wise, pair-wise, and list-wise ranking. An evaluation study(Kang et al.,2023) investigates the recommendation ability of various LLMs by designing zero and few shots LLM prompts for rating prediction. P5(Geng et al.,2022) converts interaction history, item metadata, and user review into a common format of prompt to realize various recommendation tasks. Most of them pay attention to developing task-wise prompts that are shared by all users in the specific task. Instead of designing a common prompt format for the specific recommendation task(Geng et al.,2022; Zhang et al.,2023b) , we personalize instance-wise prompt for each user to explore the diversity of users' preferences and intents.5.2.Prompt Sensitivity of LLMsRecent studies(Lu et al.,2022; Zhu et al.,2023) have examined the sensitivity of LLMs to prompts that even minor variations in prompts can lead to significantly different outputs, indicating that the one-size-fits-all prompt may not be ideal for different inputs. For example, KATE(Liu et al.,2022) confirmed that the number and order of in-context examples can influence the output of LLMs. Besides, variations in prompt formats within task descriptions may result in LLMs interpreting input instances differently, consequently influencing the outcomes(Zhao et al.,2021) . These studies underscore the importance of optimizing prompts for specific inputs to enhance the performance of LLMs. Fixed prompt templates that assume uniformity across all samples in a task, regardless of their varying difficulty, may impair LLMs' abilities due to a lack of instance-specific knowledge(Jiang et al.,2022) . To address the difference in inputs, recent research has proposed instance-level prompt optimization(Jin et al.,2023; Wu et al.,2022) , which involves rewriting the prompt for each input to better leverage the LLM's capabilities for specific instances. IDPG(Wu et al.,2022) introduced a lightweight and trainable component to generate prompts based on each input instance. IPL(Jin et al.,2023) assumed that each learnable prompt token contributes differently to various instances, which can be learned by calculating relevance scores. To meet the specific demands for personalization in recommendation tasks, we enhance LLMs' personalized recommendations by leveraging instance-wise prompts and introducing a method to optimize these prompts using MARL.5.3.Discrete Prompt EngineeringSeveral approaches have explored prompt engineering in continuous and discrete space to obtain better answers from LLMs. Since the prompts for recent LLMs (ChatGPT, LLaMa2) are discrete and hard prompts, our work is strongly related to the discrete prompts engineering. Prior works have attempted to construct discrete prompts manually(Petroni et al.,2019; Schick and Sch\u00fctze,2021) or by heuristic(Jiang et al.,2020; Pan et al.,2023) , which are limited by human effort or resource consumption. Recent research has explored constructing prompts automatically. For instance, prompt retrieval(Rubin et al.,2022) leverages annotated data and a language model, training a compact retriever to retrieve prompts based on input-output pairs. AutoPrompt(Shin et al.,2020) utilizes gradient-guided search to identify the optimal tokens within the prompt, although these prompts are typically not human-interpretable. RLPrompt(Deng et al.,2022) introduces a framework based on RL to generate prompts iteratively word-by-word from the vast vocabulary. An important distinction between our method and the existing methods is that we decompose the completed prompt into multiple key patterns, optimizing each pattern and concatenating them, which can enhance the efficiency of optimization and ensure the quality of prompts. To the best of our knowledge, our work is the first to explore instance-wise prompting in the domain of RSs.",
                "abstract": "Designing effective prompts can empower LLMs to understand user preferences and provide recommendations by leveraging LLMs' intent comprehension and knowledge utilization capabilities. However, existing research predominantly concentrates on task-wise prompting, developing fixed prompt templates composed of four patterns (i.e., role-playing, history records, reasoning guidance, and output format) and applying them to all users for a given task. Although convenient, task-wise prompting overlooks individual user differences, leading to potential mismatches in capturing user preferences. To address it, we introduce the concept of instance-wise prompting to personalize discrete prompts for individual users and propose Reinforced Prompt Personalization (RPP) to optimize the four patterns in prompts using multi-agent reinforcement learning (MARL). To boost efficiency, RPP formulates prompt personalization as selecting optimal sentences holistically across the four patterns, rather than optimizing word-by-word. To ensure the quality of prompts, RPP meticulously crafts diverse expressions for each of the four patterns, considering multiple analytical perspectives for specific recommendation tasks. In addition to RPP, our proposal of RPP+ aims to enhance the scalability of action space by dynamically refining actions with LLMs throughout the iterative process. We evaluate the effectiveness of RPP/RPP+ in ranking tasks over various datasets. Experimental results demonstrate the superiority of RPP/RPP+ over traditional recommender models, few-shot methods, and other prompt-based methods, underscoring the significance of instance-wise prompting for LLMs in recommendation tasks and validating the effectiveness of RPP/RPP+. Our code is available atthis https URL."
            },
            {
                "name": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation",
                "arxiv_id": "2408.03533",
                "subtitles": [
                    "Large Language Model for Recommendation",
                    "Long Sequence User Modeling"
                ],
                "reference": [
                    "PALR: Personalization Aware LLMs for Recommendation",
                    "Large Language Models for User Interest Journeys",
                    "How to Index Item IDs for Recommendation Foundation Models",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "U-BERT: Pre-training user representations for improved recommendation",
                    "VIP5: Towards Multimodal Foundation Models for Recommendation",
                    "Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction",
                    "Large language models are zero-shot rankers for recommender systems",
                    "TransRec: Learning Transferable Recommendation from Mixture-of-Modality Feedback",
                    "PBNR: Prompt-based News Recommender System",
                    "How can recommender systems benefit from large language models: A survey",
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Where to go next for recommender systems? id-vs. modality-based recommender models revisited",
                    "Learning implicit user interest hierarchy for context in personalization",
                    "Text Is All You Need: Learning Language Representations for Sequential Recommendation",
                    "TagGPT: Large Language Models are Zero-shot Multimodal Taggers",
                    "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models",
                    "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Practice on long sequential user behavior modeling for click-through rate prediction",
                    "CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models",
                    "Towards universal sequence representation learning for recommender systems",
                    "Large Language Model Augmented Narrative Driven Recommendations",
                    "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
                    "Generative Sequential Recommendation with GPTRec",
                    "M6-rec: Generative pretrained language models are open-ended recommender systems",
                    "Prompt learning for news recommendation",
                    "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "A First Look at LLM-Powered Generative News Recommendation",
                    "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
                    "Language models are realistic tabular data generators",
                    "Sampling is all you need on modeling long-term user behaviors for CTR prediction",
                    "TwHIN-BERT: a socially-enriched pre-trained language model for multilingual Tweet representations",
                    "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
                    "User behavior retrieval for click-through rate prediction",
                    "Learning vector-quantized item representation for transferable sequential recommenders",
                    "End-to-end user behavior retrieval in click-through rateprediction model",
                    "Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights",
                    "Lifelong sequential modeling with personalized memorization for user response prediction",
                    "PTab: Using the Pre-trained Language Model for Modeling Tabular Data",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Uncovering ChatGPT's Capabilities in Recommender Systems",
                    "PTM4Tag: sharpening tag recommendation of stack overflow posts with pre-trained models",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
                    "Tiny-newsrec: Effective and efficient plm-based news recommendation",
                    "Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models"
                ],
                "related_work": "5.Related Works5.1.Large Language Model for RecommendationPrevious work(Lin et al.,2023a) has suggested that the employment of language models to recommender systems can generally be categorized based on the roles they play in the recommendation pipeline.i.e., feature engineering(Liu et al.,2023a; Borisov et al.,2022; Li et al.,2023d; Mysore et al.,2023; Carranza et al.,2023; Christakopoulou et al.,2023) , feature encoder(Muhamed et al.,2021b; Hou et al.,2022; Yu et al.,2021; Wang et al.,2022; Hou et al.,2023a; Zhang et al.,2022; Fu et al.,2023; Yuan et al.,2023; Qiu et al.,2021; Li et al.,2023c; He et al.,2022; Muhamed et al.,2021b) , scoring/ranking function(Liu et al.,2022; Kang et al.,2023; Zhang et al.,2021; Li et al.,2023f; Bao et al.,2023; Li et al.,2023e; Zhang and Wang,2023; Mao et al.,2023; Hua et al.,2023a; Geng et al.,2023; Hua et al.,2023b; Zhang et al.,2023a; Hou et al.,2023b; Chen,2023; Petrov and Macdonald,2023; Wang and Lim,2023) .In feature engineering, large language models (LLMs) process raw data (e.g., user profiles and item descriptions) as input, and output open-world knowledge or potential new attributes for data augmentation with carefully designed prompts or templates. For example, KAR(Xi et al.,2023) utilizes the potential knowledge of user preferences and item attributes by requesting LLMs with factorization prompting techniques. GENRE(Liu et al.,2023a) employs LLMs to generate news summarization, synthetic pieces, and user profiles. The obtained knowledge serves as augmented features and improves performances of recommenders in a model-agnostic manner.In feature encoders, LLMs are used as auxiliary textual encoders to both enrich the user/item representations with semantic information and enable cross-domain recommendation with the open-world natural language interface. For instance, U-BERT(Qiu et al.,2021) enhances user representation by encoding review texts into dense vectors via BERT. UniSRec(Hou et al.,2022) and VQ-Rec(Hou et al.,2023a) apply a fixed BERT as the encoder for item descriptive texts, to achieve unified cross-domain sequential recommendation.In scoring/ranking function, instead of doing assistant tasks for recommendation (e.g., feature engineering or feature encoder) , LLMs are adopted to do the scoring or ranking task, which is the core component of recommendation. In this case, LLMs try to accomplish either the item scoring task(Liu et al.,2022; Kang et al.,2023; Zhang et al.,2021; Li et al.,2023f; Bao et al.,2023; Li et al.,2023e; Zhang and Wang,2023; Mao et al.,2023; Lin et al.,2024b) , or item generation task(Hua et al.,2023a; Geng et al.,2023; Hua et al.,2023b; Zhang et al.,2023a; Hou et al.,2023b; Chen,2023; Petrov and Macdonald,2023; Wang and Lim,2023) . Also, various works(Geng et al.,2022; Cui et al.,2022; Zhang et al.,2023c; Liu et al.,2023b; Sun et al.,2023; Dai et al.,2023) attempt to explore the multi-task capacity of LLMs, and instruct LLMs with various ways to solve the multiple tasks (e.g., both scoring and generation) through a unified language interface.In this paper, we focus on utilizing LLMs as scoring functions, using LoRA techniques to parameter-efficiently tune LLM to fit the CTR distribution. We design a personalized LoRA module, which aggregates LoRA weights guided by conventional recommendation models. To the best of our knowledge, we are the first to consider LoRA parameters personalization by introducing CRM instructions in the large language model for recommendation setting.5.2.Long Sequence User ModelingDuring recommendation task, Kim(Kim and Chan,2003) argues that long-term sequence means general interest, which is back to one's mind and important for personalization. Existing approaches for addressing long-term user sequence mainly focus on memory network and retrieval methods. Hierarchical Periodic Memory Network (HPMN) (Ren et al.,2019) proposes a hierarchical and periodical updating mechanism for capturing multi-scale sequential user interests. The Memory Augmented Neural Network (MIMN) (Pi et al.,2019) stores behaviors in a memory matrix at the user interest center and updates the memory for new users.Sequential Interest Modeling (SIM) (Pi et al.,2020a) and User Behavior Retrieval for CTR (UBR4CTR) (Qin et al.,2020) have introduced retrieval-enhanced history and two-stage frameworks to catch user patterns in the past, which are related to current targets. UBR4CTR employs BM25 as the relevance metric in its initial stage. In contrast, the original SIM has two variations with different designs. SIM Hard selects relevant items based on the same category as the target item, while SIM Soft utilizes the inner product of pre-trained item embeddings to determine relevance. Efficient Temporal Attention (ETA) (Chen et al.,2021) employs locality-sensitive hashing (LSH) to encode item embeddings and retrieve relevant items from long sequences via Hamming distance. Sequential Dynamic Interest Modeling (SDIM) (Cao et al.,2022) samples behavior items that share the same hash signature as the target item through multiple rounds of hash collisions, and then linearly aggregates these sampled behavior items to capture user interests.Inspired by these works above, we propose a novel Long-Short Modality Retriever to address long sequence problems in LLM-based recommendation, showing both its effectiveness and efficiency.",
                "abstract": "We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. Current mainstream efforts mainly center around injecting personalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, leading to suboptimal performance. (2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths. (3) Existing approaches aren't scalable for large datasets due to training efficiency constraints. Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space. To address these problems, we propose RecLoRA. This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost. Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models."
            }
        ],
        "survey": {
            "name": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys) ",
            "arxiv_id": "2404.00579",
            "subtitles": [
                {
                    "name": "Generative Models for Interaction- Driven Recommendation",
                    "key_history": [
                        {
                            "reference_title": "Diffusion Recommender Model",
                            "key_word": "Diffusion Models"
                        },
                        {
                            "reference_title": "Diffusion augmentation for sequential recommendation",
                            "key_word": "Data Augmentation"
                        },
                        {
                            "reference_title": "Recommendation via collaborative diffusion generative model",
                            "key_word": "Generative Models"
                        },
                        {
                            "reference_title": "Generative flow network for listwise recommendation",
                            "key_word": "Generative Flow Networks"
                        },
                        {
                            "reference_title": "Generative Neural Processes",
                            "key_word": "Idnp: Interest dynamics modeling using generative neural processes for sequential recommendation"
                        },
                        {
                            "reference_title": "Flow network based generative models for non-iterative diverse candidate generation",
                            "key_word": "Generative Flow Networks"
                        },
                        {
                            "reference_title": "Better training of gflownets with local credit and incomplete trajectories",
                            "key_word": "Recommendation Lists"
                        },
                        {
                            "reference_title": "Conditional generative adversarial nets",
                            "key_word": "Generative Adversarial Networks"
                        },
                        {
                            "reference_title": "Session-based recommendations with recurrent neural networks",
                            "key_word": "Recurrent Auto-Regressive Models"
                        },
                        {
                            "reference_title": "Attention is all you need",
                            "key_word": "Self-Attentive Auto-Regressive Models"
                        }
                    ],
                    "references_in_this_section": [
                        "KBGAN: Adversarial Learning for Knowledge Graph Embeddings",
                        "Predicting temporal sets with deep neural networks",
                        "Emergent abilities of large language models",
                        "Learning structured output representation using deep conditional generative models",
                        "Better training of gflownets with local credit and incomplete trajectories",
                        "Generative adversarial user model for reinforcement learning based recommendation system",
                        "Generative slate recommendation with reinforcement learning",
                        "Sequential variational autoencoders for collaborative filtering",
                        "Beyond Greedy Ranking: Slate Optimization via List-CVAE",
                        "Recommendation via collaborative diffusion generative model",
                        "Flow network based generative models for non-iterative diverse candidate generation",
                        "Variational autoencoders for collaborative filtering",
                        "FISSA: Fusing item similarity models with self-attention networks for sequential recommendation",
                        "Extracting and composing robust features with denoising autoencoders",
                        "Variational inference with normalizing flows",
                        "CFGAN: A generic collaborative filtering framework based on generative adversarial networks",
                        "Generative flow network for listwise recommendation",
                        "A neural probabilistic language model",
                        "Session-based recommendations with recurrent neural networks",
                        "Collaborative denoising auto-encoders for top-n recommender systems",
                        "Diff4Rec: Sequential Recommendation with Curriculum-scheduled Diffusion Augmentation",
                        "Self-attentive sequential recommendation",
                        "Locker: Locally constrained self-attentive sequential recommendation",
                        "Enhancing collaborative filtering with generative augmentation",
                        "Recurrent neural networks with top-k gains for session-based recommendations",
                        "Diffusion augmentation for sequential recommendation",
                        "Long short-term memory",
                        "Personalized bundle list recommendation",
                        "Neural memory streaming recommender networks with adversarial training",
                        "Black-box attacks on sequential recommenders via data-free model extraction",
                        "Generative adversarial nets",
                        "Auto-encoding variational bayes",
                        "Irgan: A minimax game for unifying generative and discriminative information retrieval models",
                        "BERT: Pre-training of deep bidirectional transformers for language understanding",
                        "Sparks of Artificial General Intelligence: Early experiments with GPT",
                        "Conditional generative adversarial nets",
                        "Sets2sets: Learning from sequential sets with neural networks",
                        "SSE-PT: Sequential recommendation via personalized transformer",
                        "Neural processes",
                        "Wavenet: A generative model for raw audio",
                        "Diffusion Recommender Model",
                        "Deep unsupervised learning using nonequilibrium thermodynamics",
                        "Linear recurrent units for sequential recommendation",
                        "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer",
                        "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                        "Generative sequential recommendation with gptrec",
                        "Language models are few-shot learners",
                        "Attention is all you need",
                        "Autorec: Autoencoders meet collaborative filtering",
                        "Conditional neural processes",
                        "Idnp: Interest dynamics modeling using generative neural processes for sequential recommendation",
                        "Variation control and evaluation for generative slate recommendations",
                        "Session-based recommendation via flow-based deep generative networks and Bayesian inference"
                    ]
                },
                {
                    "name": "Large Language Models in Recommendation",
                    "key_history": [
                        {
                            "reference_title": "Leveraging large language models for sequential recommendation",
                            "key_word": "Recommendation as Dense Retrieval"
                        },
                        {
                            "reference_title": "UNBERT: User-News Matching BERT for News Recommendation",
                            "key_word": "Recommendation via LLM Item-Preference Fusion"
                        },
                        {
                            "reference_title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                            "key_word": "Transformer and Joint Contrastive Learning Framework"
                        },
                        {
                            "reference_title": "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p5) ",
                            "key_word": "Tuning LLMs for Generative Recommendation"
                        },
                        {
                            "reference_title": "Personalized prompt learning for explainable recommendation",
                            "key_word": "Generative Explanation"
                        },
                        {
                            "reference_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                            "key_word": "Retrieval Augmented Recommendation"
                        },
                        {
                            "reference_title": "Leveraging large language models for sequential recommendation",
                            "key_word": "LLM-based Input Generation"
                        },
                        {
                            "reference_title": "Leveraging Large Language Models in Conversational Recommender Systems",
                            "key_word": "Conversational Recommendation"
                        }
                    ],
                    "references_in_this_section": [
                        "Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation",
                        "Emergent abilities of large language models",
                        "Retrieval-augmented generation for knowledge-intensive NLP tasks",
                        "Large language models are competitive near cold-start recommenders for language-and item-based preferences",
                        "Recommender systems with generative retrieval",
                        "U-BERT: Pre-training user representations for improved recommendation",
                        "Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking",
                        "GPT4Rec: A generative framework for personalized recommendation and user interests interpretation",
                        "Empowering news recommendation with pre-trained language models",
                        "Large language models are zero-shot rankers for recommender systems",
                        "Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search",
                        "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                        "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                        "What does BERT know about books, movies and music? probing bert for conversational recommendation",
                        "Automated interactive domain-specific conversational agents that understand human dialogs",
                        "Zero-shot recommendation as language modeling",
                        "Logic-scaffolding: Personalized aspect-instructed recommendation explanation generation using llms",
                        "A Workflow Analysis of Context-driven Conversational Recommendation",
                        "Multi-Aspect Reviewed-Item Retrieval via LLM Query Decomposition and Aspect Fusion",
                        "Recommendation as language processing (RLP): A unified pretrain, personalized prompt & predict paradigm (P",
                        "Leveraging passage retrieval with generative models for open domain question answering",
                        "Augmented Language Models: a Survey",
                        "UNBERT: User-News Matching BERT for News Recommendation",
                        "Leveraging Large Language Models in Conversational Recommender Systems",
                        "Towards knowledge-based recommender dialog system",
                        "Towards unified conversational recommender systems via knowledge-enhanced prompt learning",
                        "M6-rec: Generative pretrained language models are open-ended recommender systems",
                        "Sparks of Artificial General Intelligence: Early experiments with GPT",
                        "Recipe-MPR: A Test Collection for Evaluating Multi-aspect Preference-based Natural Language Retrieval",
                        "Improving language models by retrieving from trillions of tokens",
                        "Prompt learning for news recommendation",
                        "Where to go next for recommender systems? ID-vs. modality-based recommender models revisited",
                        "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
                        "Llmrec: Large language models with graph augmentation for recommendation",
                        "Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta-Information",
                        "COLLM: Integrating collaborative embeddings into large language models for recommendation",
                        "Content-based recommender systems: State of the art and trends",
                        "Bayesian preference elicitation with language models",
                        "MINER: Multi-interest matching network for news recommendation",
                        "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
                        "Pre-training methods in information retrieval",
                        "Large language models as zero-shot conversational recommenders",
                        "Personalized prompt learning for explainable recommendation",
                        "Generate neural template explanations for recommendation",
                        "Factual and informative review generation for explainable recommendation",
                        "Is chatgpt a good recommender? a preliminary study",
                        "Rexplug: Explainable recommendation using plug-and-play language model",
                        "A Survey on Conversational Recommender Systems",
                        "Uncovering ChatGPT's Capabilities in Recommender Systems",
                        "Language models are few-shot learners",
                        "Query-Aware Sequential Recommendation",
                        "Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue",
                        "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                        "Neural news recommendation with multi-head self-attention",
                        "Personalized review generation by expanding phrases and attending on aspect-aware representations",
                        "ReprBERT: distilling BERT to an efficient representation-based relevance model for e-commerce",
                        "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
                        "Self-supervised Contrastive BERT Fine-tuning for Fusion-Based Reviewed-Item Retrieval",
                        "Large language model augmented narrative driven recommendations",
                        "Leveraging large language models for sequential recommendation",
                        "Deciphering Compatibility Relationships with Textual Descriptions via Extraction and Explanation",
                        "Recmind: Large language model powered agent for recommendation",
                        "Towards Deep Conversational Recommendations"
                    ]
                },
                {
                    "name": "Generative Multimodal Recommendation Systems",
                    "key_history": [
                        {
                            "reference_title": "Instructpix2pix: Learning to follow image editing instructions",
                            "key_word": "Synthetic data generation"
                        },
                        {
                            "reference_title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
                            "key_word": "Labeling"
                        },
                        {
                            "reference_title": "Learning transferable visual models from natural language supervision",
                            "key_word": "Contrastive learning"
                        },
                        {
                            "reference_title": "Scaling up visual and vision-language representation learning with noisy text supervision",
                            "key_word": "Latent space alignment"
                        },
                        {
                            "reference_title": "Imagebind: One embedding space to bind them all",
                            "key_word": "Multimodal alignment"
                        },
                        {
                            "reference_title": "Adding conditional control to text-to-image diffusion models",
                            "key_word": "Diffusion models"
                        },
                        {
                            "reference_title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
                            "key_word": "Consistency in image generation"
                        },
                        {
                            "reference_title": "Visual instruction tuning",
                            "key_word": "Image-text interaction"
                        }
                    ],
                    "references_in_this_section": [
                        "Masked autoencoders are scalable vision learners",
                        "Chils: Zero-shot image classification with hierarchical label sets",
                        "FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition",
                        "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                        "InteraRec: Interactive Recommendations Using Multimodal Large Language Models",
                        "Hierarchical text-conditional image generation with clip latents",
                        "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                        "Categorical Reparameterization with Gumbel-Softmax",
                        "Contrastvae: Contrastive variational autoencoder for sequential recommendation",
                        "Structured denoising diffusion models in discrete state-spaces",
                        "Align before fuse: Vision and language representation learning with momentum distillation",
                        "Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction",
                        "Adding conditional control to text-to-image diffusion models",
                        "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
                        "Mm-llms: Recent advances in multimodal large language models",
                        "Improved baselines with visual instruction tuning",
                        "Segment anything",
                        "Look, listen, and attend: Co-attention network for self-supervised audio-visual representation learning",
                        "Zero-shot composed image retrieval with textual inversion",
                        "Extending CLIP for Category-to-image Retrieval in E-commerce",
                        "Deep multimodal representation learning: A survey",
                        "Visual instruction tuning",
                        "Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions",
                        "Deep unsupervised learning using nonequilibrium thermodynamics",
                        "Scaling up visual and vision-language representation learning with noisy text supervision",
                        "Learning transferable visual models from natural language supervision",
                        "Zegclip: Towards adapting clip for zero-shot semantic segmentation",
                        "Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data",
                        "Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation",
                        "Instructpix2pix: Learning to follow image editing instructions",
                        "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
                        "Tryondiffusion: A tale of two unets",
                        "Imagebind: One embedding space to bind them all",
                        "High-resolution image synthesis with latent diffusion models"
                    ]
                },
                {
                    "name": "Evaluating for Impact and Harm",
                    "key_history": [
                        {
                            "reference_title": "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                            "key_word": "Accuracy Metrics for Offline Impact"
                        },
                        {
                            "reference_title": "Towards Deep Conversational Recommendations",
                            "key_word": "Benchmarks for Offline Impact"
                        },
                        {
                            "reference_title": "Recagent: A novel simulation paradigm for recommender systems",
                            "key_word": "Online and Longitudinal Evaluations"
                        },
                        {
                            "reference_title": "A diversity-promoting objective function for neural conversation models",
                            "key_word": "Conversational Evaluation"
                        },
                        {
                            "reference_title": "Recommender systems and their ethical challenges",
                            "key_word": "Evaluating for Harm"
                        },
                        {
                            "reference_title": "Multistakeholder recommendation: Survey and research directions",
                            "key_word": "Holistic Evaluations"
                        }
                    ],
                    "references_in_this_section": [
                        "Taxonomy of risks posed by language models",
                        "Recommender systems with generative retrieval",
                        "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
                        "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
                        "How generative AI could disrupt creative work",
                        "Power hungry processing: Watts driving the cost of ai deployment",
                        "A diversity-promoting objective function for neural conversation models",
                        "FairEvalLLM. A Comprehensive Framework for Benchmarking Fairness in Large Language Model Recommender Systems",
                        "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                        "INSPIRED: Toward Sociable Recommendation Dialog Systems",
                        "Towards Understanding and Mitigating Unintended Biases in Language Model-driven Conversational Recommendation",
                        "Holistic evaluation of language models",
                        "Ethical aspects of multi-stakeholder recommendation systems",
                        "Recommendation as language processing (RLP): A unified pretrain, personalized prompt & predict paradigm (P",
                        "The Ethics of Advanced AI Assistants",
                        "Improving recommendation lists through topic diversification",
                        "RevCore: Review-augmented conversational recommendation",
                        "Recommender systems and their ethical challenges",
                        "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                        "Multistakeholder recommendation with provider constraints",
                        "CRSLab: An Open-Source Toolkit for Building Conversational Recommender System",
                        "Recagent: A novel simulation paradigm for recommender systems",
                        "Yelp Dataset",
                        "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                        "Automating democracy: Generative AI, journalism, and the future of democracy",
                        "Estimating the environmental impact of Generative-AI services using an LCA-based methodology",
                        "AI Personal Assistants and Sustainability: Risks and Opportunities",
                        "User tampering in reinforcement learning recommender systems",
                        "Large language models as zero-shot conversational recommenders",
                        "Typology of risks of generative text-to-image models",
                        "Estimating and penalizing induced preference shifts in recommender systems",
                        "Visually-aware fashion recommendation and design with generative image models",
                        "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                        "Multistakeholder recommendation: Survey and research directions",
                        "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                        "The Movielens datasets: History and context",
                        "The lfm-1b dataset for music retrieval and recommendation",
                        "Towards Deep Conversational Recommendations"
                    ]
                }
            ],
            "all_references": [
                "Estimating the environmental impact of Generative-AI services using an LCA-based methodology",
                "Black-box attacks on sequential recommenders via data-free model extraction",
                "Augmented Language Models: a Survey",
                "Llmrec: Large language models with graph augmentation for recommendation",
                "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
                "ReprBERT: distilling BERT to an efficient representation-based relevance model for e-commerce",
                "AI Personal Assistants and Sustainability: Risks and Opportunities",
                "Leveraging Large Language Models in Conversational Recommender Systems",
                "Deep unsupervised learning using nonequilibrium thermodynamics",
                "Conditional generative adversarial nets",
                "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                "Generate neural template explanations for recommendation",
                "Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta-Information",
                "Predicting temporal sets with deep neural networks",
                "Towards Understanding and Mitigating Unintended Biases in Language Model-driven Conversational Recommendation",
                "Attention is all you need",
                "Automated interactive domain-specific conversational agents that understand human dialogs",
                "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
                "Improving language models by retrieving from trillions of tokens",
                "Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation",
                "Bayesian preference elicitation with language models",
                "AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models",
                "M6-rec: Generative pretrained language models are open-ended recommender systems",
                "Flow network based generative models for non-iterative diverse candidate generation",
                "Content-based recommender systems: State of the art and trends",
                "Large language models are effective text rankers with pairwise ranking prompting",
                "Estimating and penalizing induced preference shifts in recommender systems",
                "Personalized prompt learning for explainable recommendation",
                "Power hungry processing: Watts driving the cost of ai deployment",
                "Rexplug: Explainable recommendation using plug-and-play language model",
                "Neural processes",
                "Scaling up visual and vision-language representation learning with noisy text supervision",
                "Learning transferable visual models from natural language supervision",
                "Prompt learning for news recommendation",
                "Enhancing collaborative filtering with generative augmentation",
                "Towards knowledge-based recommender dialog system",
                "Towards unified conversational recommender systems via knowledge-enhanced prompt learning",
                "Improving recommendation lists through topic diversification",
                "Session-based recommendation via flow-based deep generative networks and Bayesian inference",
                "Recommender systems with generative retrieval",
                "Leveraging large language models for sequential recommendation",
                "Sequential variational autoencoders for collaborative filtering",
                "Large language models are competitive near cold-start recommenders for language-and item-based preferences",
                "Neural news recommendation with multi-head self-attention",
                "Idnp: Interest dynamics modeling using generative neural processes for sequential recommendation",
                "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
                "Instructpix2pix: Learning to follow image editing instructions",
                "Large language models for generative recommendation: A survey and visionary discussions",
                "Extracting and composing robust features with denoising autoencoders",
                "A neural probabilistic language model",
                "Deep multimodal representation learning: A survey",
                "Categorical Reparameterization with Gumbel-Softmax",
                "User tampering in reinforcement learning recommender systems",
                "Variational inference with normalizing flows",
                "Retrieval-augmented generation for knowledge-intensive NLP tasks",
                "Sparks of Artificial General Intelligence: Early experiments with GPT",
                "Diffusion Recommender Model",
                "Taxonomy of risks posed by language models",
                "Segment anything",
                "Is chatgpt a good recommender? a preliminary study",
                "Beyond Greedy Ranking: Slate Optimization via List-CVAE",
                "Visually-aware fashion recommendation and design with generative image models",
                "Query-Aware Sequential Recommendation",
                "Language models are few-shot learners",
                "Session-based recommendations with recurrent neural networks",
                "Collaborative denoising auto-encoders for top-n recommender systems",
                "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                "Sets2sets: Learning from sequential sets with neural networks",
                "Generative sequential recommendation with gptrec",
                "Typology of risks of generative text-to-image models",
                "Generative slate recommendation with reinforcement learning",
                "GPT4Rec: A generative framework for personalized recommendation and user interests interpretation",
                "How generative AI could disrupt creative work",
                "Masked autoencoders are scalable vision learners",
                "A Survey on Conversational Recommender Systems",
                "Auto-encoding variational bayes",
                "UNBERT: User-News Matching BERT for News Recommendation",
                "A survey on adversarial recommender systems: from attack/defense strategies to generative adversarial networks",
                "Recommendation via collaborative diffusion generative model",
                "Neural memory streaming recommender networks with adversarial training",
                "Better training of gflownets with local credit and incomplete trajectories",
                "The lfm-1b dataset for music retrieval and recommendation",
                "CRSLab: An Open-Source Toolkit for Building Conversational Recommender System",
                "Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering",
                "Learning structured output representation using deep conditional generative models",
                "Recommender systems and their ethical challenges",
                "Chatgpt: Optimizing language models for dialogue",
                "FairEvalLLM. A Comprehensive Framework for Benchmarking Fairness in Large Language Model Recommender Systems",
                "Variational autoencoders for collaborative filtering",
                "Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions",
                "Multistakeholder recommendation with provider constraints",
                "Self-supervised Contrastive BERT Fine-tuning for Fusion-Based Reviewed-Item Retrieval",
                "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
                "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                "Pre-training methods in information retrieval",
                "The Movielens datasets: History and context",
                "FISSA: Fusing item similarity models with self-attention networks for sequential recommendation",
                "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
                "Diff4Rec: Sequential Recommendation with Curriculum-scheduled Diffusion Augmentation",
                "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review",
                "Contrastvae: Contrastive variational autoencoder for sequential recommendation",
                "Ethical aspects of multi-stakeholder recommendation systems",
                "Factual and informative review generation for explainable recommendation",
                "Imagebind: One embedding space to bind them all",
                "Look, listen, and attend: Co-attention network for self-supervised audio-visual representation learning",
                "Mm-llms: Recent advances in multimodal large language models",
                "FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning",
                "Recommender systems in the era of large language models (llms",
                "COLLM: Integrating collaborative embeddings into large language models for recommendation",
                "Recurrent neural networks with top-k gains for session-based recommendations",
                "Self-attentive sequential recommendation",
                "U-BERT: Pre-training user representations for improved recommendation",
                "A survey on large language models for recommendation",
                "Uncovering ChatGPT's Capabilities in Recommender Systems",
                "Diffusion augmentation for sequential recommendation",
                "What does BERT know about books, movies and music? probing bert for conversational recommendation",
                "Wavenet: A generative model for raw audio",
                "Irgan: A minimax game for unifying generative and discriminative information retrieval models",
                "Attack prompt generation for red teaming and defending large language models",
                "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
                "Generative adversarial user model for reinforcement learning based recommendation system",
                "RevCore: Review-augmented conversational recommendation",
                "Personalized review generation by expanding phrases and attending on aspect-aware representations",
                "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                "Adding conditional control to text-to-image diffusion models",
                "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
                "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
                "CFGAN: A generic collaborative filtering framework based on generative adversarial networks",
                "How can recommender systems benefit from large language models: A survey",
                "Conditional neural processes",
                "Visual instruction tuning",
                "Logic-scaffolding: Personalized aspect-instructed recommendation explanation generation using llms",
                "Chils: Zero-shot image classification with hierarchical label sets",
                "Extending CLIP for Category-to-image Retrieval in E-commerce",
                "Large language models are zero-shot rankers for recommender systems",
                "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data",
                "SSE-PT: Sequential recommendation via personalized transformer",
                "Recommendation as language processing (RLP) : A unified pretrain, personalized prompt & predict paradigm (P",
                "Holistic evaluation of language models",
                "Improved baselines with visual instruction tuning",
                "Foundation Models for Recommender Systems: A Survey and New Perspectives",
                "Gemini: a family of highly capable multimodal models",
                "Large language models as zero-shot conversational recommenders",
                "Large language model augmented narrative driven recommendations",
                "Zero-shot recommendation as language modeling",
                "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
                "Variation control and evaluation for generative slate recommendations",
                "Generative flow network for listwise recommendation",
                "Where to go next for recommender systems? ID-vs. modality-based recommender models revisited",
                "Towards Deep Conversational Recommendations",
                "Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation",
                "Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search",
                "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                "InteraRec: Interactive Recommendations Using Multimodal Large Language Models",
                "Long short-term memory",
                "Align before fuse: Vision and language representation learning with momentum distillation",
                "Generative adversarial nets",
                "Recommendation as a communication game: Self-supervised bot-play for goal-oriented dialogue",
                "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer",
                "Tryondiffusion: A tale of two unets",
                "Zero-shot composed image retrieval with textual inversion",
                "Emergent abilities of large language models",
                "Leveraging passage retrieval with generative models for open domain question answering",
                "Multi-Aspect Reviewed-Item Retrieval via LLM Query Decomposition and Aspect Fusion",
                "Hierarchical text-conditional image generation with clip latents",
                "Empowering news recommendation with pre-trained language models",
                "The Ethics of Advanced AI Assistants",
                "High-resolution image synthesis with latent diffusion models",
                "Generative recommendation: Towards next-generation recommender paradigm",
                "Zegclip: Towards adapting clip for zero-shot semantic segmentation",
                "FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition",
                "Yelp Dataset",
                "Recipe-MPR: A Test Collection for Evaluating Multi-aspect Preference-based Natural Language Retrieval",
                "Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer's Disease Prediction",
                "Linear recurrent units for sequential recommendation",
                "Personalized bundle list recommendation",
                "Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking",
                "A diversity-promoting objective function for neural conversation models",
                "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                "Multistakeholder recommendation: Survey and research directions",
                "A Workflow Analysis of Context-driven Conversational Recommendation",
                "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                "Automating democracy: Generative AI, journalism, and the future of democracy",
                "Locker: Locally constrained self-attentive sequential recommendation",
                "Recmind: Large language model powered agent for recommendation",
                "Autorec: Autoencoders meet collaborative filtering",
                "MINER: Multi-interest matching network for news recommendation",
                "Structured denoising diffusion models in discrete state-spaces",
                "KBGAN: Adversarial Learning for Knowledge Graph Embeddings",
                "INSPIRED: Toward Sociable Recommendation Dialog Systems",
                "Aligning large language models with human: A survey",
                "Recagent: A novel simulation paradigm for recommender systems",
                "Deciphering Compatibility Relationships with Textual Descriptions via Extraction and Explanation"
            ]
        },
        "topic_history": [
            {
                "name": "Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation",
                "arxiv_id": "2404.01855",
                "reference": [
                    "Recommender systems in the era of large language models (llms",
                    "Next point-of-interest recommendation with inferring multi-step future preferences",
                    "Getnext: trajectory flow map enhanced transformer for next poi recommendation",
                    "Large language models are zero-shot rankers for recommender systems",
                    "A multi-channel next poi recommendation framework with multi-granularity check-in signals",
                    "Where would i go next? large language models as human mobility predictors",
                    "Spatio-temporal hypergraph learning for next poi recommendation",
                    "Next poi recommendation with dynamic graph and explicit dependency",
                    "Hme: A hyperbolic metric embedding approach for next-poi recommendation",
                    "Autonomous gis: the next-generation ai-powered gis",
                    "Uncovering chatgpt's capabilities in recommender systems",
                    "On the opportunities and challenges of foundation models for geospatial artificial intelligence",
                    "Gpt4rec: A generative framework for personalized recommendation and user interests interpretation",
                    "Zero-shot next-item recommendation using large pretrained language models",
                    "K2: A foundation language model for geoscience knowledge understanding and utilization",
                    "Geollm: Extracting geospatial knowledge from large language models",
                    "Gpt4geo: How a language model sees the world's geography",
                    "Geogpt: Understanding and processing geospatial tasks through an autonomous gpt",
                    "Language models represent space and time",
                    "Clsprec: Contrastive learning of long and short-term preferences for next poi recommendation",
                    "Large language models for spatial trajectory patterns mining",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Cityfm: City foundation models to solve urban challenges",
                    "Sta-tcn: Spatial-temporal attention over temporal convolutional network for next point-of-interest recommendation",
                    "Exploring large language models for human mobility prediction under public events",
                    "Leveraging large language models for sequential recommendation"
                ]
            },
            {
                "name": "Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens",
                "arxiv_id": "2406.08477",
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "PALR: personalization aware llms for recommendation",
                    "Recommender systems in the era of large language models (llms",
                    "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization",
                    "Recommender systems with generative retrieval",
                    "Model spider: Learning to rank pre-trained models efficiently",
                    "Self-attentive sequential recommendation",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Contrastive learning for sequential recommendation",
                    "Gpt4rec: A generative framework for personalized recommendation and user interests interpretation",
                    "Recommendation as language processing (RLP) : A unified pretrain, personalized prompt & predict paradigm (P",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "M6-rec: Generative pretrained language models are open-ended recommender systems",
                    "How to index item ids for recommendation foundation models",
                    "Cross-batch negative sampling for training two-tower recommenders",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Personalized transformer for explainable recommendation",
                    "Streaming CTR prediction: Rethinking recommendation task for real-world streaming data",
                    "Matrix factorization techniques for recommender systems",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Advances in collaborative filtering",
                    "BPR: bayesian personalized ranking from implicit feedback",
                    "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer"
                ]
            },
            {
                "name": "Improve Temporal Awareness of LLMs for Sequential Recommendation",
                "arxiv_id": "2405.02778",
                "reference": [
                    "Recommender systems in the era of large language models (llms",
                    "Palr: Personalization aware llms for recommendation",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Prompting large language models for recommender systems: A comprehensive framework and empirical analysis",
                    "Session-based recommendations with recurrent neural networks",
                    "Personalized top-n sequential recommendation via convolutional sequence embedding",
                    "Self-attentive sequential recommendation",
                    "Zero-shot next-item recommendation using large pretrained language models",
                    "Towards universal sequence representation learning for recommender systems",
                    "Factorizing personalized markov chains for next-basket recommendation",
                    "Neural attentive session-based recommendation",
                    "Large language models as zero-shot conversational recommenders",
                    "Learning vector-quantized item representation for transferable sequential recommenders",
                    "Feature-level deeper self-attention network for sequential recommendation",
                    "Do llms understand user preferences? evaluating llms on user rating prediction",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Session-based recommendation with graph neural networks",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Sequential recommendation with graph neural networks",
                    "Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer",
                    "Recmind: Large language model powered agent for recommendation",
                    "Text is all you need: Learning language representations for sequential recommendation"
                ]
            },
            {
                "name": "HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation",
                "arxiv_id": "2408.13521",
                "reference": [
                    "Knowledge graphs on the web - an overview",
                    "BanglaAutoKG: Automatic Bangla knowledge graph construction with semantic neural graph filtering",
                    "Construction of human resource ontology model for knowledge graph",
                    "Knowledge graphs",
                    "Medical knowledge graph: Data sources, construction, reasoning, and applications",
                    "Deep learning on knowledge graph for recommender system: A survey",
                    "Job recommendation algorithm based on knowledge graph",
                    "Representation learning of knowledge graph for wireless communication networks",
                    "Knowledge graph with job recommendation",
                    "Knowledge Graphs and Big Data Processing",
                    "Knowledge graph prompting for multi-document question answering",
                    "Explainable job-posting recommendations using knowledge graphs and named entity recognition"
                ]
            },
            {
                "name": "LLMs for User Interest Exploration in Large-scale Recommendation Systems",
                "arxiv_id": "2405.16363",
                "reference": [
                    "Values of user exploration in recommender systems",
                    "Feedback loop and bias amplification in recommender systems",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Uncovering ChatGPT's Capabilities in Recommender Systems",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "TagGPT: Large Language Models are Zero-shot Multimodal Taggers",
                    "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models",
                    "VIP5: Towards Multimodal Foundation Models for Recommendation",
                    "GPT4Rec: A generative framework for personalized recommendation and user interests interpretation",
                    "Long-Term Value of Exploration: Measurements, Findings and Algorithms",
                    "Large Language Models as Data Augmenters for Cold-Start Item Recommendation",
                    "A First Look at LLM-Powered Generative News Recommendation",
                    "PIE: Personalized Interest Exploration for Large-Scale Recommender Systems",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Exploration in recommender systems",
                    "Tiny-newsrec: Effective and efficient plm-based news recommendation",
                    "How algorithmic confounding in recommendation systems increases homogeneity and decreases utility",
                    "Learning vector-quantized item representation for transferable sequential recommenders"
                ]
            },
            {
                "name": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
                "arxiv_id": "2406.18825",
                "reference": [
                    "FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction",
                    "Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features",
                    "Wide & deep learning for recommender systems",
                    "xdeepfm: Combining explicit and implicit feature interactions for recommender systems",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction",
                    "U-BERT: Pre-training user representations for improved recommendation",
                    "Towards Universal Sequence Representation Learning for Recommender Systems",
                    "Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction",
                    "Deep & cross network for ad click predictions",
                    "CTRL: Connect Collaborative and Language Model for CTR Prediction",
                    "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation",
                    "Bookgpt: A general framework for book recommendation empowered by large language model",
                    "DeepFM: a factorization-machine based neural network for CTR prediction",
                    "Prompt Learning for News Recommendation",
                    "Text Matching Improves Sequential Recommendation by Reducing Popularity Biases",
                    "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "Unitrec: A unified text-to-text transformer and joint contrastive learning framework for text-based recommendation",
                    "Tabllm: Few-shot classification of tabular data with large language models",
                    "Session-based recommendations with recurrent neural networks",
                    "Personalized top-n sequential recommendation via convolutional sequence embedding",
                    "Product-based neural networks for user response prediction",
                    "Self-attentive sequential recommendation",
                    "Zero-shot recommendation as language modeling",
                    "MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction",
                    "Factorization machines",
                    "Deep Interest Network for Click-Through Rate Prediction",
                    "Collm: Integrating collaborative embeddings into large language models for recommendation",
                    "Practice on long sequential user behavior modeling for click-through rate prediction",
                    "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
                    "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction",
                    "DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems",
                    "An introduction to convolutional neural networks",
                    "Deep session interest network for click-through rate prediction",
                    "TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation",
                    "Towards open-world recommendation with knowledge augmentation from large language models",
                    "Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models",
                    "Exploring large language model for graph data understanding in online job recommendations",
                    "Empirical evaluation of gated recurrent neural networks on sequence modeling",
                    "Field-embedded factorization machines for click-through rate prediction",
                    "Do llms understand user preferences? evaluating llms on user rating prediction",
                    "Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Ptab: Using the pre-trained language model for modeling tabular data",
                    "Deep Interest Evolution Network for Click-Through Rate Prediction",
                    "Text is all you need: Learning language representations for sequential recommendation"
                ]
            },
            {
                "name": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
                "arxiv_id": "2403.03536",
                "reference": [
                    "Graph unlearning",
                    "Amnesiac machine learning",
                    "Recranker: Instruction tuning large language model as ranker for top-k recommendation",
                    "Adapting large language models by integrating collaborative semantics for recommendation",
                    "Remember what you want to forget: Algorithms for machine unlearning",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "Forgetting fast in recommender systems",
                    "Towards making systems forget with machine unlearning",
                    "A survey of machine unlearning",
                    "Recommendation unlearning via matrix correction",
                    "Flip: Towards fine-grained alignment between id-based models and pretrained language models for ctr prediction",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Knowledge unlearning for llms: Tasks, methods, and challenges",
                    "Prompting large language models for recommender systems: A comprehensive framework and empirical analysis",
                    "Making recommender systems forget: Learning and unlearning for erasable recommendation",
                    "A survey of large language models",
                    "How can recommender systems benefit from large language models: A survey",
                    "Forget me now: Fast and exact unlearning in neighborhood-based recommendation",
                    "Towards safer large language models through machine unlearning",
                    "Deep unlearning via randomized conditionally independent hessians",
                    "Understanding black-box predictions via influence functions",
                    "Certified data removal from machine learning models",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Collm: Integrating collaborative embeddings into large language models for recommendation",
                    "Recommendation unlearning via influence function",
                    "Unlearning protected user attributes in recommendations with adversarial training",
                    "Clickprompt: Ctr models are strong prompt generators for adapting language models to ctr prediction",
                    "Recommendation unlearning",
                    "Unlearn what you want to forget: Efficient unlearning for llms",
                    "Netflix and forget: Efficient and exact machine unlearning from bi-linear recommendations",
                    "Federated unlearning for on-device recommendation",
                    "Machine unlearning of pre-trained large language models",
                    "Tofu: A task of fictitious unlearning for llms",
                    "Ultrare: Enhancing receraser for recommendation unlearning via error decomposition",
                    "Selective and collaborative influence function for efficient recommendation unlearning",
                    "Arcane: An efficient architecture for exact machine unlearning",
                    "Rethinking machine unlearning for large language models",
                    "Large language model unlearning",
                    "Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation",
                    "Who's harry potter? approximate unlearning in llms",
                    "Agentcf: Collaborative learning with autonomous language agents for recommender systems",
                    "Llara: Aligning large language models with sequential recommenders",
                    "Sequence unlearning for sequential recommender systems",
                    "E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation",
                    "Language models are few-shot learners",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher",
                    "On the effectiveness of unlearning in session-based recommendation",
                    "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
                    "Machine unlearning for recommendation systems: An insight",
                    "Approximate data deletion from machine learning models",
                    "Machine unlearning",
                    "Making users indistinguishable: Attribute-wise unlearning in recommender systems",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Large Language Model with Graph Convolution for Recommendation",
                "arxiv_id": "2402.08859",
                "reference": [
                    "A Survey on Large Language Models for Recommendation",
                    "Generative Job Recommendations with Large Language Model",
                    "Enhancing Job Recommendation through LLM-based Generative Adversarial Networks",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "Pre-train, prompt and recommendation: A comprehensive survey of language modelling paradigm adaptations in recommender systems",
                    "Greaselm: Graph reasoning enhanced language models",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Graph convolutional neural networks for web-scale recommender systems",
                    "Improving graph collaborative filtering with neighborhood-enriched contrastive learning",
                    "Self-supervised graph learning for recommendation",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Deep bidirectional language-knowledge graph pretraining",
                    "A First Look at LLM-Powered Generative News Recommendation",
                    "Exploring large language model for graph data understanding in online job recommendations",
                    "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
                    "Knowledge graph convolutional networks for recommender systems",
                    "Are graph augmentations necessary? simple graph contrastive learning for recommendation",
                    "Enhanced story comprehension for large language models through dynamic document-based knowledge graphs",
                    "Lightgcn: Simplifying and powering graph convolution network for recommendation",
                    "Language models are few-shot learners",
                    "Exploring the potential of large language models (llms) in learning on graphs",
                    "Llama: Open and efficient foundation language models",
                    "Multi-behavior recommendation with graph convolutional networks",
                    "Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers",
                    "Semi-Supervised Classification with Graph Convolutional Networks"
                ]
            },
            {
                "name": "Reinforced Prompt Personalization for Recommendation with Large Language Models",
                "arxiv_id": "2407.17115",
                "reference": [
                    "Plum: Prompt Learning using Metaheuristic",
                    "IDPG: An Instance-Dependent Prompt Generation Method",
                    "Instance-Aware Prompt Learning for Language Understanding and Generation",
                    "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
                    "Bookgpt: A general framework for book recommendation empowered by large language model",
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Calibrate Before Use: Improving Few-shot Performance of Language Models",
                    "Recommendation as Language Processing (RLP) : A Unified Pretrain, Personalized Prompt & Predict Paradigm (P",
                    "Instance-wise Prompt Tuning for Pretrained Language Models",
                    "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
                    "How Can Recommender Systems Benefit from Large Language Models: A Survey",
                    "What Makes Good In-Context Examples for GPT",
                    "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Language Models are Realistic Tabular Data Generators",
                    "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
                    "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
                    "Learning To Retrieve Prompts for In-Context Learning",
                    "How Can We Know What Language Models Know",
                    "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
                    "Uncovering ChatGPT's Capabilities in Recommender Systems",
                    "Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders",
                    "Language Models as Knowledge Bases"
                ]
            },
            {
                "name": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation",
                "arxiv_id": "2408.03533",
                "reference": [
                    "PALR: Personalization Aware LLMs for Recommendation",
                    "Large Language Models for User Interest Journeys",
                    "How to Index Item IDs for Recommendation Foundation Models",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "U-BERT: Pre-training user representations for improved recommendation",
                    "VIP5: Towards Multimodal Foundation Models for Recommendation",
                    "Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction",
                    "Large language models are zero-shot rankers for recommender systems",
                    "TransRec: Learning Transferable Recommendation from Mixture-of-Modality Feedback",
                    "PBNR: Prompt-based News Recommender System",
                    "How can recommender systems benefit from large language models: A survey",
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Where to go next for recommender systems? id-vs. modality-based recommender models revisited",
                    "Learning implicit user interest hierarchy for context in personalization",
                    "Text Is All You Need: Learning Language Representations for Sequential Recommendation",
                    "TagGPT: Large Language Models are Zero-shot Multimodal Taggers",
                    "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models",
                    "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Practice on long sequential user behavior modeling for click-through rate prediction",
                    "CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models",
                    "Towards universal sequence representation learning for recommender systems",
                    "Large Language Model Augmented Narrative Driven Recommendations",
                    "ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
                    "Generative Sequential Recommendation with GPTRec",
                    "M6-rec: Generative pretrained language models are open-ended recommender systems",
                    "Prompt learning for news recommendation",
                    "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "A First Look at LLM-Powered Generative News Recommendation",
                    "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
                    "Language models are realistic tabular data generators",
                    "Sampling is all you need on modeling long-term user behaviors for CTR prediction",
                    "TwHIN-BERT: a socially-enriched pre-trained language model for multilingual Tweet representations",
                    "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
                    "User behavior retrieval for click-through rate prediction",
                    "Learning vector-quantized item representation for transferable sequential recommenders",
                    "End-to-end user behavior retrieval in click-through rateprediction model",
                    "Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights",
                    "Lifelong sequential modeling with personalized memorization for user response prediction",
                    "PTab: Using the Pre-trained Language Model for Modeling Tabular Data",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Uncovering ChatGPT's Capabilities in Recommender Systems",
                    "PTM4Tag: sharpening tag recommendation of stack overflow posts with pre-trained models",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
                    "Tiny-newsrec: Effective and efficient plm-based news recommendation",
                    "Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
            "arxiv_id": "2305.16617",
            "isAPA": true,
            "abstract": "The detection of machine-generated text, especially from large language models (LLMs) , iscrucial in preventing serious social problems resulting from their misuse. Some methods traindedicated detectors on specific datasets but fallshort in generalizing to unseen test data, whileother zero-shot ones often yield suboptimal performance. Although the recent DetectGPT hasshown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying thesource LLM with hundreds of its perturbations.This paper aims to bridge this gap. Concretely,we propose to incorporate a Bayesian surrogate model, which allows us to select typicalsamples based on Bayesian uncertainty and interpolate scores from typical samples to othersamples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under alow query budget. Notably, when detecting thetext generated by LLaMA family models, ourmethod with just 2 or 3 queries can outperformDetectGPT with 200 queries.",
            "reference": [
                "Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. 2019a. GLTR: Statistical detection and visualization of generated text",
                "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled",
                "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv",
                "OpenAI. 2023a. Gpt-4. https://openai.com/research/gpt-4, Last accessed on",
                "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv",
                "Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax",
                "Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv",
                "Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian optimization of machine learning algorithms. Advances in neural information processing systems",
                "Saranya Venkatraman, Adaku Uchendu, and Dongwon Lee. 2023. Gpt-who: An information density-based machine-generated text detector. arXiv preprint arXiv",
                "Zhijie Deng and Jun Zhu. 2023. Bayesadapter: Being bayesian, inexpensively and reliably, via bayesian fine-tuning",
                "Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. 2021. Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
                "Salman Mohamadi and Hamidreza Amindavar. 2020. Deep bayesian active learning, a brief survey on recent advances. arXiv preprint arXiv",
                "Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata",
                "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv",
                "Christopher Williams and Carl Rasmussen. 1995. Gaussian processes for regression. Advances in neural information processing systems",
                "Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep bayesian active learning with image data",
                "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog",
                "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv",
                "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. arXiv preprint arXiv",
                "OpenAI. 2022. Introducing chatgpt. https://openai.com/blog/chatgpt, Last accessed on",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv",
                "Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems",
                "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv",
                "OpenAI. 2023b. New ai classifier for indicating ai-written text. Https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text",
                "Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023. MGTBench: Benchmarking Machine-Generated Text Detection. CoRR abs",
                "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                "Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Spotting llms with binoculars: Zero-shot detection of machine-generated text. arXiv preprint arXiv",
                "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research",
                "Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. 2019b. Gltr: Statistical detection and visualization of generated text",
                "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv",
                "Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can ai-generated text be reliably detected? arXiv preprint arXiv",
                "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems",
                "Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. 2019. A simple baseline for bayesian uncertainty in deep learning. Advances in neural information processing systems",
                "Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. 2023. Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text. arXiv preprint arXiv",
                "Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and Arthur Szlam. 2019. Real or fake? learning to discriminate machine from human generated text. arXiv preprint arXiv",
                "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv",
                "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv",
                "Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv",
                "Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020. Authorship attribution for neural text generation",
                "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv",
                "Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shuming Shi, and Yue Zhang. 2023. Deepfake text detection in the wild. arXiv preprint arXiv"
            ],
            "related work": "2 Related Works Large language models. LLMs (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022; OpenAI, 2022) have revolutionized the field of natural language processing by offering several advantages over previous pre-trained models (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019), including a better characterization of complex patterns and dependencies in the text, and the appealing in-context learning ability for solving downstream tasks with minimal examples. Representative models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and ChatGPT (OpenAI, 2022) have showcased their remarkable ability to generate text with high coherence, fluency, and semantic relevance. They can even effectively address complex inquiries related to science, mathematics, history, current events, and social trends. Therefore, it is increasingly important to effectively regulate the use of LLMs to prevent significant social issues. LLM-generated text detection. Previous methods can be broadly categorized into two groups. The first group of methods performs detection in a zero-shot manner (Solaiman et al., 2019; Gehrmann et al., 2019a; Mitchell et al., 2023; Yang et al., 2023), but they require access to the source model that generates the texts to derive quantities like output logits or losses for detection. For instance, Solaiman et al. (2019) suggest that a higher log probability for each token indicates that the text will likely be machine-generated. When the output logits/losses of the source model are unavailable, these methods rely on a proxy model for detection. However, there is often a substantial gap between the proxy and source models from which the text is generated. Another group of methods trains DNN-based classifiers on collected human-written and machine-generated texts for detection (Guo et al., 2023; Uchendu et al., 2020; OpenAI, 2023b). However, such detectors are data-hungry and may exhibit poor generalization ability when facing domain shift (Bakhtin et al., 2019; Uchendu et al., 2020). Furthermore, training DNN-based classifiers is susceptible to backdoor attacks (Qi et al., 2021) and adversarial attacks (He et al., 2023). Besides, both He et al. (2023) and Li et al. (2023) develop benchmarks for evaluating existing detection methods and call for more robust detection methods.",
            "date": "2023"
        },
        "topic": "LLM-Generated Texts Detection",
        "year_start": "2021",
        "year_end": "2024",
        "target_list": [
            {
                "name": "LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?",
                "arxiv_id": "2401.05952",
                "subtitles": [
                    "Machine Generated Text Detection",
                    "Previous study to mix of HWT and MGT",
                    "Datasets for MGT Detection"
                ],
                "reference": [
                    "How you prompt matters! even task-oriented constraints in instructions affect llm-generated text detection",
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "On the generalization of training-based chatgpt detection methods",
                    "Gpt-sentinel: Distinguishing human and chatgpt generated content",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Pubmedqa: A dataset for biomedical research question answering",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Radar: Robust ai-text detection via adversarial learning",
                    "Crosslingual generalization through multitask finetuning",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Smaller language models are better black-box machine-generated text detectors",
                    "A mathematical theory of communication",
                    "Openai models - gpt",
                    "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
                    "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
                    "Ghostbuster: Detecting text ghostwritten by large language models",
                    "Mgtbench: Benchmarking machine-generated text detection",
                    "Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature"
                ],
                "related_work": "2Related works2.1Machine Generated Text DetectionCurrent MGT detection methods can be broadly categorized into metric-based and model-based methods according to the previous study(He et al.,2023) . Please refer to AppendixAfor comprehensive related works.Metric-based Methods.Building upon the observation that MGTs occupy regions with sharp negative log probability curvature,Mitchell et al. (2023) introduced a zero-shot whitebox detection method called DetectGPT, setting a trend in metric-based detection(Su et al.,2023; Mireshghallah et al.,2023; Bao et al.,2023) . Recently,Yang et al. (2023a) also introduced a powerful detection method known as DNA-GPT, which leverages N-gram(Shannon,1948) in a black-box setting.Model-based Methods.In the era of Large Language Models (LLMs) ,Guo et al. (2023) developed the ChatGPT Detector based on a fine-tuned Roberta model. As for decoder-based detectors, GPT-sentinel(Chen et al.,2023) leverage the t5-small model(Muennighoff et al.,2022) and show convincing results when detecting MGT even in revised cases.2.2Previous study to mix of HWT and MGTPrior studies have viewed the mixture of HWT and MGT in different settings. DNA-GPT(Yang et al.,2023a) and DetectGPT(Mitchell et al.,2023) notably utilized the T5 model(Raffel et al.,2020) to simulate scenarios where humans make limited, random modifications to MGT, creating complex test cases. Conversely, DIPPER(Krishna et al.,2023) and OUTFOX(Koike et al.,2023b) opted for a paraphrasing technique, using this method to craft adversarial attacks aimed at eluding the detection mechanisms of classifiers, thereby presenting a nuanced way to alter MGT while maintaining undetectability.2.3Datasets for MGT DetectionPrevious studies have proposed many datasets of MGT, accompanied by their newly proposed detectors(Verma et al.,2023; Chen et al.,2023) .Guo et al. (2023) leverages multiple previous Question-Answer (QA) datasets(Jin et al.,2019; Lin et al.,2021) , allowing ChatGPT to generate corresponding answers without explicit prompts. This results in creating a comprehensive dataset comprising a large set of pairs of MGT and HWT. Following the QA pattern, many researchers(Mitchell et al.,2023; Su et al.,2023; Hu et al.,2023; He et al.,2023) propose datasets with the MGT from variant mainstream LLMs(OpenAI,2022,2023b) .However, these datasets typically consist of two distinct classes of texts, namely pure MGT or HWT, without accounting for the potential mixture cases. Furthermore, issues arise due to variations in prompts(Koike et al.,2023a) , sampling methods, and the inherent differences in length, style, and quality among textsHe et al. (2023) , posing variations challenges on the generalization ability of proposed detectors(Xu et al.,2023) . In some instances, MGT included in datasets may not be thoroughly checked, with many noisy sentences not filtered well. For example, some sentences likeLet me know if you have any other questionsexist in the dataset, which will impact the effectiveness of the detectors(Guo et al.,2023) .",
                "abstract": "With the rapid development and widespread application of Large Language Models (LLMs), the use of Machine-Generated Text (MGT) has become increasingly common, bringing with it potential risks, especially in terms of quality and integrity in fields like news, education, and science. Current research mainly focuses on purely MGT detection without adequately addressing mixed scenarios, including AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle this challenge, we define mixtext, a form of mixed text involving both AI and human-generated content. Then, we introduce MixSet, the first dataset dedicated to studying these mixtext scenarios. Leveraging MixSet, we executed comprehensive experiments to assess the efficacy of prevalent MGT detectors in handling mixtext situations, evaluating their performance in terms of effectiveness, robustness, and generalization. Our findings reveal that existing detectors struggle to identify mixtext, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixtext, offering valuable insights for future research. Code and Models are available atthis https URL."
            },
            {
                "name": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
                "arxiv_id": "2403.13335",
                "subtitles": [
                    "Algorithms for machine-generated text detection",
                    "Ensemble Learning",
                    "Ensemble Learning for LLM-generated Text Detection Task"
                ],
                "reference": [
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Switchtab: Switched autoencoders are effective tabular learners",
                    "Joyful: Joint modality fusion and graph contrastive learning for multimodal emotion recognition",
                    "Ensemble methodology: Innovations in credit default prediction using lightgbm, xgboost, and localensemble",
                    "A semi-hard voting combiner scheme to ensemble multi-class probabilistic classifiers",
                    "Attention is all you need",
                    "How many validation labels do you need? exploring the design space of label-efficient model ranking",
                    "Generative ai text classification using ensemble llm approaches",
                    "Chatgpt generated text detection",
                    "Leveraging relational graph neural network for transductive model ensemble",
                    "Unlabeled data selection for active learning in image classification",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Greedy function approximation: a gradient boosting machine",
                    "Recontab: Regularized contrastive representation learning for tabular data",
                    "Enhancing liver segmentation: A deep learning approach with eas feature extraction and multi-scale fusion",
                    "Emp: emotion-guided multi-modal fusion and contrastive learning for personality traits recognition",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection"
                ],
                "related_work": "IIRelated WorkThe LLM-generated text detection task is framed as a binary classification problem. Transformer-based classifiers, which has been extensively explored in various domains[46,47,48,49,50], serve as one of the most popular and effective approaches for this classification task. Additionally, we recognized that ensemble learning algorithms offer an efficient means of enhancing classification performance. In this paper, we combined individual classifiers with ensemble algorithms, resulting in a significant improvement in the performance of LLM-generated text detection on both in-distribution and out-of-distribution datasets.II-AAlgorithms for machine-generated text detectionGuo et al, applied two methods: logistic regression with GLTR features and an end-to-end RoBerta classifier to distinguish whether a text was generated by ChatGPT or humans across multiple fields[43]. Shijaku and Canhasi detected TOEFL essays using XGBoost with manually extracted 244 lexical and semantic features[51]. There are also widely-used off-the-shelf GPT detectors, such as the OpenAI detection classifier, GPTZero and ZeroGPT[52]. OpenAI's AI text classifier is fine-tuned on the output of an already trained language model. They used text generated by 34 models pre-trained by five different organizations, and then trained their models on samples from multiple sources of human writing and language model-generated text. GPTZero is trained on an extensive and diverse corpus of text created by humans and artificial intelligence, with a primary focus on English. As a classification model, GPTzero predicts whether a given text fragment is generated by a LLM with different text granularities, including sentence, paragraph, and entire document levels. These classifiers are all based on the transformer structure[53], providing us with some reference for the algorithm of selecting classifiers.II-BEnsemble LearningEnsemble learning refers to the machine learning paradigm where multiple learners (also known as models or predictors) are trained to solve the same problem. The main advantage of ensemble learning lies in its ability to improve model generalization ability. By combining multiple models, it can effectively reduce the risk of overfitting and underfitting. In addition, the diversity of different models can improve the overall prediction accuracy. Ensemble learning typically performs well in various machine learning competitions and practical applications[54,55]. We call algorithms without parameter updates in ensemble learning as  \"non-adaptive ensemble algorithm \", like the hard voting ensemble[56]. And we call algorithms with parameter updates in ensemble learning as  \"adaptive ensemble algorithm \", like the neural network ensemble and random forest algorithm. Usually, adaptive classifier detection performs better by adaptively integrating the outputs of different classifiers, assigning dynamic weights to each classifier's performance.II-CEnsemble Learning for LLM-generated Text Detection TaskSpecifically for LLM-generated text detection, since this is a unified data framework, ensemble learning will be a good fit to combine multiple models, instead of using fusion mechanisms[57,58,59]or model selection techniques[60]. LLM-Blender[61]is an ensemble framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs) . The study[62]presents ensemble neural models utilizing probabilities from multiple pre-trained Large Language Models (LLMs) as features for Traditional Machine Learning (TML) classifiers to distinguish between AI-generated and human-written text, achieving competitive performance in both English and Spanish languages and ranking first in model attribution.",
                "abstract": "Large language models (LLMs) have reached human-like proficiency in generating diverse textual content, underscoring the necessity for effective fake text detection to avoid potential risks such as fake news in social media. Previous research has mostly tested single models on in-distribution datasets, limiting our understanding of how these models perform on different types of data for LLM-generated text detection task. We researched this by testing five specialized transformer-based models on both in-distribution and out-of-distribution datasets to better assess their performance and generalizability. Our results revealed that single transformer-based classifiers achieved decent performance on in-distribution dataset but limited generalization ability on out-of-distribution dataset. To improve it, we combined the individual classifiers models using adaptive ensemble algorithms, which improved the average accuracy significantly from 91.8% to 99.2% on an in-distribution test set and from 62.9% to 72.5% on an out-of-distribution test set. The results indicate the effectiveness, good generalization ability, and great potential of adaptive ensemble algorithms in LLM-generated text detection."
            },
            {
                "name": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting",
                "arxiv_id": "2405.16133",
                "subtitles": [
                    "LLMs for Code Generation",
                    "Detection of Synthetic Text"
                ],
                "reference": [
                    "A Watermark for Large Language Models",
                    "Gltr: Statistical detection and visualization of generated text",
                    "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                    "Starcoder: may the source be with you",
                    "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x",
                    "Authorship attribution for neural text generation",
                    "Evaluating large language models trained on code",
                    "Competition-level code generation with alphacode",
                    "SantaCoder: don't reach for the stars",
                    "Real or Fake? Learning to Discriminate Machine from Human Generated Text",
                    "Defending against neural fake news",
                    "PaLM: Scaling Language Modeling with Pathways",
                    "GPT-4 Technical Report",
                    "Introducing ChatGPT",
                    "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Robust Multi-bit Natural Language Watermarking through Invariant Features",
                    "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Neural Deepfake Detection with Factual Structure of Text",
                    "Developer sentiment around AI/ML"
                ],
                "related_work": "2.Related Work 2.1.LLMs for Code Generation With the rapid rise in the popularity of LLMs, an increasing number of studies have begun focusing on Code LLMs aimed at achieving automatic software engineering. Those Code LLMs are continually pre-trained on large-scale code corpus from an initial pre-trained model on text with auto-denoising(Wang et al.,2021a) or causal language modeling task(Chen et al.,2021; Li et al.,2023; Nijkamp et al.,2022; Fried et al.,2022) . A crucial capability of Code LLMs is code generation. Code generation is a left-to-right decoding process that utilizes functional requirements or code context as a prompt. Pioneering works such as CodeX(Chen et al.,2021) , AlphaCode(Li et al.,2022a) , CodeGeex(Zheng et al.,2023) , SantaCoder(Allal et al.,2023) , and GPT-4(OpenAI et al.,2024) have all demonstrated powerful code generation capabilities. Recent advances in general purpose LLMs(Chowdhery et al.,2022; OpenAi,2022; OpenAI,2023; Touvron et al.,2023) further enhanced their understanding of human instructions, resulting in the generation of code that better meets the requirements and exhibits higher quality. As code large language models continue to mature, more and more people from various industries are integrating these models into their daily lives. According to a community survey led byStackOverflow (2023) , 44% of experienced developers and 55% of beginners already used AI coding assistant and most of them use two tools, ChatGPT (83%) and Github Copilot (56%) . Therefore, in certain areas such as education and industrial settings, automated detection of whether code is written by humans or generated by models has become increasingly important. 2.2.Detection of Synthetic TextDetecting AI-generated (synthetic) text has been studied before the emergency of LLM. The main works on synthetic text detection follow two lines of research. One research line formulates detection as a binary classification problem by collecting synthetic texts from generative models and training a supervised model based on pre-trained transformers(Zellers et al.,2019; Ippolito et al.,2020; Zhong et al.,2020) or other neural models(Bakhtin et al.,2019; Uchendu et al.,2020) . Another research line detects synthetic text by designing zero-shot metrics. These metrics measure the relationship between a given text and the text distribution of generative models. Gehrmann et al.(Gehrmann et al.,2019) claim that synthetic texts are sampled from the head of generative models' distribution, so the average log probability score under the generative model \\mathcal{G} of a given text, i.e., \\frac{1}{L}\\sum_{i=1}^{L}log(P_{\\mathcal{G}}(x_{i}|x_{1},...,x_{i-1}) ), can be a simple and effective zero-shot metric (called GLTR) .Su et al.(2023) proposed two metrics, LRR and NPR, where the former combines log-rand and log-likelihood to better magnify the differences between human-written and machine-generated text, while the latter is primarily based on the idea that the log-rank of machine-generated texts should be more sensitive to smaller perturbations.Mitchell et al.(2023) further improved the GLTR metrics by proposing that synthetic texts tend to occupy negative curvature regions of the model's log probability function, i.e., the local maximum of the generative model's distribution. They use the average token probability disparity between the given text and perturbed texts to detect whether the given text is located at the negative curvature regions of log\\ p(x) . Overall, all of these methods converge on the same idea: LLMs tend to generate tokens with higher confidence. For tokens generated by LLMs themselves, there will be assigned a higher log probability. Therefore, effective differentiation can be achieved by statistically analyzing token log probabilities.In addition to detection methods, some approaches have explored adding watermarks to the generated text during the generation process behind LLM services(Kirchenbauer et al.,2023; Yoo et al.,2023) . Through watermark extraction, it becomes feasible to discern whether a given text originates from a model. However, it's worth noting that the inclusion of a watermarking algorithm could potentially compromise the quality of the generated text. Consequently, it cannot be presumed that all users or service providers will be inclined to produce watermarked text.",
                "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency in generating code. However, the misuse of LLM-generated (Synthetic) code has prompted concerns within both educational and industrial domains, highlighting the imperative need for the development of synthetic code detectors. Existing methods for detecting LLM-generated content are primarily tailored for general text and often struggle with code content due to the distinct grammatical structure of programming languages and massive \"low-entropy\" tokens. Building upon this, our work proposes a novel zero-shot synthetic code detector based on the similarity between the code and its rewritten variants. Our method relies on the intuition that the differences between the LLM-rewritten and original codes tend to be smaller when the original code is synthetic. We utilize self-supervised contrastive learning to train a code similarity model and assess our approach on two synthetic code detection benchmarks. Our results demonstrate a notable enhancement over existing synthetic content detectors designed for general texts, with an improvement of 20.5% in the APPS benchmark and 29.1% in the MBPP benchmark."
            },
            {
                "name": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges",
                "arxiv_id": "2403.18249",
                "subtitles": [
                    "Fake news generation",
                    "Fake news detection"
                ],
                "reference": [
                    "On the risk of misinformation pollution with large language models",
                    "Declare: Debunking fake news and false claims using evidence-aware deep learning",
                    "Disinformation detection: An evolving challenge in the age of llms",
                    "How effectively can machines defend against machine-generated fake news? an empirical study",
                    "Mining dual emotion for fake news detection",
                    "Adapting fake news detection to the era of large language models",
                    "Implementing bert and fine-tuned roberta to detect ai generated news by chatgpt",
                    "Fake news in sheep's clothing: Robust fake news detection against llm-empowered style attacks",
                    "A survey of fake news: Fundamental theories, detection methods, and opportunities",
                    "defend: Explainable fake news detection",
                    "Defending against neural fake news",
                    "Med-mmhl: A multi-modal dataset for detecting human-and llm-generated misinformation in the medical domain"
                ],
                "related_work": "2Related Work Fake news generation. In the pre-LLM era, the automatic generation of fake news articles typically involved word shuffling and random substitutions of real news articles(Zellers et al.,2019; Bhat & Parthasarathy,2020) . However, such artificial content often lacked coherency, making it easily identifiable by human readers. With the advent of LLMs, a significant body of research has emerged, focusing on the potential to craft coherent and logical fake news. Works in the early stage utilized straightforward prompts to produce fake news(Wang et al.,2023; Sun et al.,2023) . However, these methods failed to trick automated detectors due to the lack of details or consistency. Subsequent methodologies introduced the use of actual news, facts, and intentionally false information provided by humans. Specifically,Su et al. (2023) ask LLMs to fabricate articles from human-collected summaries of fake events.Wu & Hooi (2023) refines the writing of fake news articles with LLMs. Except for the methods above,Jiang et al. (2023) uses fake events with real news articles for fake news generation.Pan et al. (2023) employs a question-answer dataset with real news to generate fake news text by manipulating the answers. Overall, the strategy of infusing manually crafted fake news prevents the automated mass production of fake news articles. Additionally, techniques that rely on fabricated summaries tend to produce content deficient in details, and alterations to specific events or elements often give rise to issues with contextual coherence. Fake news detection. Mainstream fake news detection models often employ auxiliary information beyond the text of the articles themselves(Zhou & Zafarani,2020) . For instance, Grover(Zellers et al.,2019) considers metadata like publication dates, authors, and publishers to ascertain the legitimacy of an article. DeClarE(Popat et al.,2018) checks the credibility of claims against information retrieved from web-searched articles.Zhang et al. (2021) mines emotional and semantic traits from the content as additional indicators. Defend(Shu et al.,2019) analyzes both the articles in question and the reactions they elicit on social media platforms. However, these auxiliary data are not always available in real-world scenarios. Recent papers(Su et al.,2023; Sun et al.,2023; Wang et al.,2023) show that fine-tuned pre-trained language models (PLMs) and LLMs might also deliver commendable results in the realm of fake news detection. In this research, we examined the performances of each category of the fake news detection models on our dataset.",
                "abstract": "Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings."
            },
            {
                "name": "CUDRT: Benchmarking the Detection of Human vs. Large Language Models Generated Texts",
                "arxiv_id": "2406.09056",
                "subtitles": [
                    "Large Language Models",
                    "AI-generated Text Detection Method",
                    "AI-generated Text Detector Evaluation Benchmark"
                ],
                "reference": [
                    "A survey on evaluation of large language models A survey on evaluation of large language models",
                    "The Next Chapter: A Study of Large Language Models in Storytelling The next chapter: A study of large language models in storytelling",
                    "GLM: General Language Model Pretraining with Autoregressive Blank Infilling GLM: General language model pretraining with autoregressive blank infilling",
                    "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection Hidding the ghostwriters: An adversarial evaluation of ai-generated student essay detection",
                    "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning CoCo: Coherence-enhanced machine-generated text detection under low resource with contrastive learning",
                    "Using an llm to help with code understanding Using an llm to help with code understanding",
                    "Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection",
                    "SeqXGPT: Sentence-Level AI-Generated Text Detection SeqXGPT: Sentence-level AI-generated text detection",
                    "Rewritelm: An instruction-tuned large language model for text rewriting Rewritelm: An instruction-tuned large language model for text rewriting",
                    "Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Gpt-4 technical report Gpt-4 technical report",
                    "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection M4gt-bench: Evaluation benchmark for black-box machine-generated text detection",
                    "Gltr: Statistical detection and visualization of generated text Gltr: Statistical detection and visualization of generated text",
                    "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark Multitude: Large-scale multilingual machine-generated text detection benchmark",
                    "Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation",
                    "InstantOps: A Joint Approach to System Failure Prediction and Root Cause Identification in Microserivces Cloud-Native Applications Instantops: A joint approach to system failure prediction and root cause identification in microserivces cloud-native applications",
                    "Hc3 plus: A semantic-invariant human chatgpt comparison corpus Hc3 plus: A semantic-invariant human chatgpt comparison corpus",
                    "Mgtbench: Benchmarking machine-generated text detection Mgtbench: Benchmarking machine-generated text detection",
                    "Qwen technical report Qwen technical report",
                    "Llm-as-a-coauthor: The challenges of detecting llm-human mixcase Llm-as-a-coauthor: The challenges of detecting llm-human mixcase",
                    "A Framework for Detecting AI-Generated Text in Research Publications A framework for detecting ai-generated text in research publications",
                    "Large Language Models Evaluate Machine Translation via Polishing Large language models evaluate machine translation via polishing",
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "The future landscape of large language models in medicine The future landscape of large language models in medicine",
                    "Llama: Open and efficient foundation language models Llama: Open and efficient foundation language models"
                ],
                "related_work": "2 RELATED WORK 2.1 Large Language Models The evolutionary path of language models began with initial rule-based models, developed into statistical-based models, followed by early neural language models, then advanced models that incorporated pre-training strategies, and has now culminated in today's complex large language models (Chang et al., 2024). Each generation of models has significantly pushed the boundaries of natural language processing technology, enhancing both the depth and breadth of language handling capabilities. Currently, large language models represent the cutting edge in processing and generating natural language text. LLMs can be categorized into open-source and closedsource based on the availability of their parameters (Liang et al., 2023). Open-source models, such as Alibaba Cloud's Qwen (J. Bai et al., 2023), Meta's LLaMA (Touvron et al., 2023), and Tsinghua University's ChatGLM (Du et al., 2022), make their code and training methods public, facilitating further development by researchers. Conversely, closed-source models like OpenAI's GPT series (Achiam et al., 2023) do not disclose their full code or data, instead offering services via APIs and other methods. LLMs have been extensively applied across various domains of text generation, including creative writing, automated customer service, legal document drafting, and scientific research, demonstrating their versatility and efficacy in producing coherent, contextually relevant, and high-quality text across a wide range of applications. Li et al. (Li et al., 2024) developed the FlexKBQA framework based on LLMs, addressing the performance decline in Knowledge Base Question Answering (KBQA) due to the lack of highquality annotated data. Shu et al. (Shu et al., 2024) proposed novel instruction tuning and reinforcement learning strategies for large language models, aimed at optimizing their performance on cross-sentence rewriting tasks. Wang et al. (Y. Wang, 2023) proposed a reference-free machine translation evaluation method, EvLP, inspired by post-editing, to assess the quality of translations polished by large language models. Clusmann et al. (Clusmann et al., 2023) found that LLMs enhanced nursing outcomes by translating medical terminology into layman's terms and summarizing patient information. Xie et al. (Xie, Cohn, & Lau, 2023) demonstrated that by simply presenting a few example stories as prompts to LLMs, they can generate new stories that rival those written by human authors. Furthermore, LLMs show significant potential in accessing scientific literature and aiding in programming tasks (Nam, Macvean, Hellendoorn, Vasilescu, & Myers, 2024). In this paper, we explore how LLMs are applied across nearly all text generation scenarios, examining the differences in text generation among various open-source and closed-source models. 2.2 AI-generated Text Detection Method Due to the increasingly human-like level of text generated by LLMs, relying on manual differentiation of AI-generated text is not only costly, but also generally low in accuracy. As a variety of LLMs continue to emerge, researchers have increasingly focused on developing targeted AI-generated text detection algorithms to prevent the misuse of these powerful models (Sarzaeim, Doshi, & Mahmoud, 2023). Currently, detectors are primarily categorized based on their working principles and internal architectures, into metric-based and model-based methods (X. Liu et al., 2023; Rouf et al., 2024). Metric-based methods evaluate and identify whether text is generated by artificial intelligence by utilizing a series of quantitative metrics, such as textual consistency, complexity, word rank, and entropy (Rouf et al., 2024). Gehrmann et al. (Gehrmann et al., 2019) proposed a tool GLTR for detecting and visualizing machinegenerated text. This method uses statistical approaches to examine the generation probability of each word, its rank in model predictions, and the entropy of the prediction distribution, helping users determine whether a text was generated by an automated language model. Mitchell et al. (Mitchell, Lee, Khazatsky, Manning, & Finn, 2023) proposed DetectGPT, a new method that does not require additional training or data collection. It determines whether a text is generated by a language model by analyzing the curvature in the model's log probability function, thereby enhancing the accuracy of detecting news articles generated by GPT-NeoX. Su et al. (J. Su, Zhuo, Wang, & Nakov, 2023) developed two zero-shot detection methods, DetectLLM-LRR and DetectLLM-NPR, to identify machine-generated text. DetectLLM-LRR uses the Log-Likelihood Log-Rank Ratio (LRR) and can detect text without perturbations efficiently. In contrast, DetectLLM-NPR employs the Normalized Perturbed Log-Rank (NPR), which improves detection accuracy by slightly rewriting the text. Model-based methods involve initially training machine learning or deep learning models on corpora of human-generated and machine-generated texts, and then using these trained models to classify the two types of texts. Guo et al. (Guo et al., 2023) proposed the ChatGPT Detector, a model that uses the RoBERTa architecture as its backbone. The model is fine-tuned on texts generated under various scenarios to enable its multifunctional detection capabilities. Wang et al. (P. Wang et al., 2023) proposed a method called SeqXGPT for detecting AIgenerated text at the sentence level. This model uses a list of log probabilities of tokens extracted from a white-box language model as features. It processes these features through convolutional networks and self-attention networks to achieve fine-grained classification of the text. Liu et al. (X. Liu et al., 2023) proposed a machine-generated text detection method COCO, enhancing text coherence under low-resource conditions using contrastive learning. This method optimizes text representation by constructing a graph structure based on entity coherence and employs an improved contrastive loss function, significantly enhancing detection performance on public datasets. This paper selects mainstream models from these two types of detectors to comprehensively assess their detection performance across various LLMs text generation scenarios. 2.3 AI-generated Text Detector Evaluation Benchmark In the development and optimization of AI-generated text detectors, the effective assessment of their performance is a critical and fundamental issue. Following the initial discussion on performance assessment, significant advancements have been made in refining algorithms and enhancing dataset quality to improve the detection accuracy of AI-generated text detectors. This section provides a comprehensive review of existing benchmark literature, with a focusing on the development of benchmarks, the LLM-generated text tasks they cover, and their data sources. Key details regarding these benchmarks are summarized in Table 2 for ease of reference. In the domain of AI-generated text detection, question answering (QA) datasets were initially widely adopted as benchmarks (Guo et al., 2023; Y. Wang et al., 2024). Briefly, by utilizing LLMs to act as question answering experts, responding to human-posed queries and producing corresponding answers, creating pairs of AI-generated question and answer datasets. Guo et al. (Guo et al., 2023) created a dataset named Human ChatGPT Comparison Corpus (HC3), which included about 40,000 questions along with responses from human experts and ChatGPT. These questions and answers cover multiple domains, including open-domain, computer science, finance, medicine, law, and psychology. The human responses were collected from various publicly available question-andanswer datasets, which include highly rated responses by internet users and wiki-based texts. Given the powerful capabilities of LLMs, and with their widespread application in various text generation tasks, researchers are exploring text generation from other scenarios. This exploration aims to test and improve the performance of detectors, thereby enhancing the models' generalization capabilities and accuracy across multiple contexts. MGTBench (He et al., 2023) generated essays and news articles by providing titles for essays and news, using GPT3.5-turbo to create essays in response to specific writing prompts, or using Claude to generate articles responding to news headlines. Upon the existing HC3 dataset, Su et al. (Z. Su, Wu, Zhou, Ma, & Hu, 2023) further expanded its scope, introducing a new version named HC3 Plus. This dataset includes additional tasks such as translation, summarization, and paraphrasing, which involve AI-generated text in complex contexts. Wang et al. (Y. Wang et al., 2023) introduced a multigenerator, multi-domain, and multi-lingual corpus M4, which not only encompassed various text generation tasks of LLMs such as news writing, question answering, and academic paper abstracts, but also included texts in multiple languages, reflecting the text generation capabilities in multilingual environments. Despite ongoing improvements and refinements in current benchmarks for AI-generated text detectors, they still fall short in covering the breadth of text generation operations, failing to comprehensively encompass the extensive applications of LLMs. Therefore, to enhance the adaptability and accuracy of AI-generated text detection models, there is an urgent need to update and expand the benchmarks so that they can more comprehensively reflect and test the performance of LLMs in practical applications. The detection of AI-generated text can be considered a classification problem. Consequently, the performance of AI-generated text detectors is typically evaluated using common metrics from classification tasks, including Accuracy, Precision, Recall, and the F1 Score. These metrics collectively reflect the detector's effectiveness in various aspects, primarily providing comprehensive information about the model's recognition accuracy.",
                "abstract": "The proliferation of large language models (LLMs) has significantly enhanced text generation capabilities across various industries. However, these models' ability to generate human-like text poses substantial challenges in discerning between human and AI authorship. Despite the effectiveness of existing AI-generated text detectors, their development is hindered by the lack of comprehensive, publicly available benchmarks. Current benchmarks are limited to specific scenarios, such as question answering and text polishing, and predominantly focus on English texts, failing to capture the diverse applications and linguistic nuances of LLMs. To address these limitations, this paper constructs a comprehensive bilingual benchmark in both Chinese and English to evaluate mainstream AI-generated text detectors. We categorize LLM text generation into five distinct operations: Create, Update, Delete, Rewrite, and Translate (CUDRT), encompassing all current LLMs activities. We also establish a robust benchmark evaluation framework to support scalable and reproducible experiments. For each CUDRT category, we have developed extensive datasets to thoroughly assess detector performance. By employing the latest mainstream LLMs specific to each language, our datasets provide a thorough evaluation environment. Extensive experimental results offer critical insights for optimizing AI-generated text detectors and suggest future research directions to improve detection accuracy and generalizability across various scenarios."
            },
            {
                "name": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
                "arxiv_id": "2402.01158",
                "subtitles": [
                    "Black-Box Detection",
                    "White-Box Detection"
                ],
                "reference": [
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Detecting fake content with relative entropy scoring",
                    "Gltr: Statistical detection and visualization of generated text",
                    "On the possibilities of ai-generated text detection (arxiv: 2304.04736) . arxiv",
                    "Identifying real or fake articles: Towards better language modeling",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
                    "A watermark for large language models",
                    "Computer-generated text detection using machine learning: A systematic review",
                    "The science of detecting llm-generated texts",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection"
                ],
                "related_work": "2Related Work LLMs have been pre-trained on extensive text corpora, enabling them to generate contextually relevant and fluent texts. However, this also increases the difficulty of detecting AI-generated texts. The existing methods for detecting generated texts can be broadly categorized into two types: black-box and white-box detection(Tang et al.,2023) , contingent upon the level of access to the model that is suspected to have generated the target texts. 2.1Black-Box DetectionFor black-box detection, classifiers are restricted to API-level access to LLMs (only available for the text) . To develop a proficient detector, black-box methods are typically designed to first extract and select features based on text samples. Originating from both human and AI-generated texts, the black-box detection method would train a classification model leveraging relevant features, for which heavily relies on the large amount of text data and detectors.Datasets.Recently, a growing body of research has concentrated on amassing responses generated by LLMs and comparing them to human-written texts spanning a wide range of domains.(Guo et al.,2023b) collected the HC3 (Human ChatGPT Comparison Corpus) Chinese dataset, which consists of nearly 40K questions and their corresponding answers from human experts and ChatGPT, which has a wide range of domains coverage (open-domain, computer science, finance, medicine, law, and psychology) .(Wang et al.,2023) collected the M4 (Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection) dataset, which consists of questions and their corresponding answers from human experts and LLMs, covering a wide range of languages (English, Chinese, Russian, Arabic, Indonesian and Urdu) . Overall, Previous work has not established a comprehensive Chinese text detection dataset that encompasses diverse Chinese text data with different parameter types from LLMs and human expert responses.Detectors.Existing black-box detectors can be grouped into two main categories: supervised classifiers and zero-shot classifiers. Logistic regression with GLTR(Gehrmann et al.,2019) features and an end-to-end RoBerta(Guo et al.,2023b) classifier, to detect whether a certain text (English and Chinese) is generated by ChatGPT or humans across several domains. However, the study conducted demonstrates that a limitation of supervised models is the potential occurrence of overfitting within the domain, resulting in poor detection performance OOD (OOD) (Chakraborty et al.,2023) . To address the limitations of supervised classifiers, zero-shot classifiers, using a pre-trained language model directly without fine-tuning, are immune to domain-specific degradation. Zero-shot classifiers such as GPT-Zero, DetectGPT(Mitchell et al.,2023) and Fast-DetectGPT(Bao et al.,2023) have been developed. These methods utilize checks on perplexity and burstiness in the text to determine whether it is artificially generated or authored by a human. The current zero-shot classifiers require input documents of considerable length (exceeding 100 tokens) for the classifier to effectively capture contextual features of the text. In terms of classifying short sentences, their performance is relatively poor.2.2White-Box DetectionWhite-box detection require fully access to LLMs, thereby enabling control over the generation behavior of the model or embedding watermark within the generated texts. This enables the tracking and detection of AI-generated texts within white-box settings.White-Box detection involves using statistical boundaries between linguistic patterns found in human-written and AI-generated text as proxies. These boundaries are determined based on n-gram frequencies(Badaskar et al.,2008) , entropy(Lavergne et al.,2008) , and perplexity(Beresneva,2016) . One limitation of these statistics-based methods is the assumption, which assumes access to the model's prediction distributions. This constraint hinders broader applications, especially for models behind APIs.Inspired by copyright protection watermarks in the image and video fields, as proposed by(Kirchenbauer et al.,2023) , partitions the model's vocabulary into whitelist and black list tokens when predicting the next token given a prompt. During text generation, the goal is to produce whitelist tokens as much as possible, effectively creating a strong watermark. The third-parties can determine if the text is machine-generated by analyzing the frequency of whitelist tokens within the text. While watermarking methods offer robustness and interpretability, they can compromise the quality of the generated text and may not be highly practical in certain scenarios.",
                "abstract": "ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggle with sentence-level AI-generated text detection and OOD detection. In contrast, our proposed method not only significantly outperforms baseline methods in both sentence-level and document-level text detection but also demonstrates strong generalization capabilities. Furthermore, since LLM-Detector is trained based on open-source LLMs, it is easy to customize for deployment."
            },
            {
                "name": "Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts: Challenges, Strategies, and Insights",
                "arxiv_id": "2403.03506",
                "subtitles": [
                    "Al-Generated Text Detection",
                    "Text Segmentation",
                    "Human-AI Collaborative Writing and Datasets"
                ],
                "reference": [
                    "Segformer: a topic segmentation model with controllable range of attention",
                    "Statistical section segmentation in free-text clinical records",
                    "Threat scenarios and best practices to detect neural fake news",
                    "A sequence-to-sequence approach with mixed pointers to topic segmentation and segment labeling",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Text segmentation by cross segment attention",
                    "Text segmentation as a supervised learning task",
                    "Transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence",
                    "SynSciPass: detecting appropriate uses of scientific text generation",
                    "Improving long document topic segmentation models with enhanced coherence modeling",
                    "DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing",
                    "Towards automatic boundary detection for human-ai collaborative hybrid essay in education",
                    "Human guided exploitation of interpretable attention patterns in summarization and topic segmentation",
                    "Improving context modeling in neural topic segmentation",
                    "SeqXGPT: Sentence-level AI-generated text detection",
                    "Bad actor, good advisor: Exploring the role of large language models in fake news detection",
                    "Dialogue topic segmentation via parallel extraction network with neighbor smoothing",
                    "Is this abstract generated by ai? a research for the gap between ai-generated scientific text and human-written scientific text",
                    "Tipster: A topic-guided language model for topic-aware text segmentation",
                    "Real or fake text?: Investigating human ability to detect boundaries between human-written and machine-generated text",
                    "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities",
                    "Lessons learnt from linear text segmentation: a fair comparison of architectural and sentence encoding strategies for successful segmentation",
                    "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
                    "Two-level transformer and auxiliary coherence modeling for improved text segmentation",
                    "The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers"
                ],
                "related_work": "2Related Work Al-Generated Text Detection. As modern LLMs' generative capabilities strengthen, distinguishing between AI-generated texts and human-written texts becomes increasingly challengingMaet al.(2023) . This may lead to the misuse of generative AI in scenarios where it should not be used, such as students employing it for writing assignments, which can result in academic misconductMitchellet al.(2023) . These potential risks emphasize the necessity for effective detection of AI-generated text. Existing studiesKoikeet al.(2024) ; Huet al.(2024) ; Heet al.(2023) ; Mitchellet al.(2023) ; Pagnoniet al.(2022) ; Rosati (2022) have been criticized for their narrow focus on document-level detection and the assumption that a document is either entirely AI-generated or not. This point is noted inDuganet al.(2023) , which suggests that AI-generated text detection should be conducted with finer granularity to adapt to the increasingly popular trend of human-AI collaborative writing. Specifically,Zenget al.(2024) introduced a boundary detector for detecting boundaries between human-written sentences and AI-generated sentences. Meanwhile,Wanget al.(2023) adopted a more straightforward approach to detection, namely, identifying the exact authorship of each sentence within the hybrid text. They proposed a method called SeqXGPT to extract word-wise log probability lists and use a combination of CNN and Transformer for sentence-level authorship detection within hybrid texts. Text Segmentation.Text segmentation aims to discover the latent structure of a document by dividing the text into segments with different labelsGhinassiet al.(2023) ; Baiet al.(2023) ; Loet al.(2021) ; Xia and Wang (2023) (e.g., segmentation based on different topics) . The broad spectrum of text segmentation studies can be further divided into two categories: (1) those focusing solely on boundary detection for segmentationZenget al.(2024) ; Lukasiket al.(2020) ; Ghinassiet al.(2023) ; Yuet al.(2023) ; Xinget al.(2020) ; Liet al.(2022) ; Somasundaran and others (2020) ; Koshoreket al.(2018) ; and (2) those aiming at both segment detection and subsequent segment classificationBaiet al.(2023) ; Loet al.(2021) ; Xia and Wang (2023) ; Gonget al.(2022) ; Tepperet al.(2012) . Among the many possible approaches for segmentation, we have selected three approaches with open-source code for our segment detection module: (i) TriBERTZenget al.(2024) , because it was specifically developed for distinguishing AI-generated segments and human-written segments within hybrid texts; (ii) Transformer2Loet al.(2021) , due to its widespread adoption as a baseline in existing text segmentation studiesXiaet al.(2022) ; Xia and Wang (2023) ; Baiet al.(2023) ; Yuet al.(2023) ; and (iii) SegFormerBaiet al.(2023) , chosen as a representative of the state-of-the-art (also one of the latest) text segmentation approaches.Human-AI Collaborative Writing and Datasets. Modern large language models have made human-AI collaborative writing increasingly convenient. Existing studies show that the suggestions provided by intelligent writing assistants have evolved from phrase-levelBuscheket al.(2021) to sentence-levelLeeet al.(2022) . Furthermore, these assistants seem poised for further improvement, especially with the release of GPT-4. Pilot studiesDuganet al.(2023) ; Zenget al.(2024) ; Wanget al.(2023) have been conducted to address AI-generated text detection within hybrid text data at a granularity finer than the document level. However, these studies were mainly conducted on synthetic and simplistic datasets due to their low cost and ease of acquisition. For instance,Wanget al.(2023) instructed the GPT-3.5-turbo API to generate continuations on given human-written texts.Zenget al.(2024) goes further by designing a set of six prompting templates that enable ChatGPT to generate multi-boundary hybrid texts (no more than three boundaries) from incomplete student-written essays. For each of these original essays, specific segments were removed and substituted with new sentences generated by ChatGPT. However, these synthetic texts are still overly simplistic compared to the hybrid texts generated through multi-turn human-AI interactions in realistic scenarios, which could involve up to dozens of boundaries. We contend that to guide and enlighten real-world practices, AI-generated text detection studies should not rely solely on synthetic datasets. Therefore, our study aims at the CoAuthor datasetLeeet al.(2022) , which includes diverse, realistic hybrid texts generated through human-AI interactions and can serve as an ideal benchmark for exploring AI-generated text detection in hybrid texts.",
                "abstract": "This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical findings highlight (1) detecting AI-generated sentences in hybrid texts is overall a challenging task because (1.1) human writers' selecting and even editing AI-generated sentences based on personal preferences adds difficulty in identifying the authorship of segments; (1.2) the frequent change of authorship between neighboring sentences within the hybrid text creates difficulties for segment detectors in identifying authorship-consistent segments; (1.3) the short length of text segments within hybrid texts provides limited stylistic cues for reliable authorship determination; (2) before embarking on the detection process, it is beneficial to assess the average length of segments within the hybrid text. This assessment aids in deciding whether (2.1) to employ a text segmentation-based strategy for hybrid texts with longer segments, or (2.2) to adopt a direct sentence-by-sentence classification strategy for those with shorter segments."
            },
            {
                "name": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
                "arxiv_id": "2402.11175",
                "subtitles": [
                    "Binary Detection",
                    "Multi-Class Detection",
                    "Authorship Obfuscation"
                ],
                "reference": [
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Gpt-who: An information density-based machine-generated text detector",
                    "Red teaming language model detectors with language models",
                    "Provable robust watermarking for ai-generated text",
                    "Gltr: Statistical detection and visualization of generated text",
                    "A watermark for large language models",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
                    "Radar: Robust ai-text detection via adversarial learning",
                    "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
                    "Release strategies and the social impacts of language models",
                    "Authorship authentication using short messages from social networking sites",
                    "Adversarial robustness of neural-statistical features in detection of generative transformers",
                    "Authorship obfuscation in multilingual machine-generated text detection",
                    "Authorship attribution for neural text generation",
                    "Sparks: Inspiration for science writing using language models",
                    "Defending against neural fake news",
                    "Stylometric detection of ai-generated text in twitter timelines",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection",
                    "Real or fake text?: Investigating human ability to detect boundaries between human-written and machine-generated text",
                    "Llm-as-a-coauthor: The challenges of detecting llm-human mixcase",
                    "Through the looking glass: Learning to attribute synthetic text generated by language models",
                    "Rewritelm: An instruction-tuned large language model for text rewriting",
                    "GLTR: Statistical detection and visualization of generated text",
                    "Automatic detection of generated text is easiest when humans are fooled",
                    "Coco: Coherence-enhanced machine-generated text detection under data limitation with contrastive learning",
                    "Fine-tuning large language models for multigenerator, multidomain, and multilingual machine-generated text detection",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "The next chapter: A study of large language models in storytelling",
                    "Turingbench: A benchmark environment for turing test in the age of neural text generation",
                    "Unsupervised cross-lingual representation learning at scale",
                    "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
                    "Neural deepfake detection with factual structure of text",
                    "Few-shot detection of machine-generated text using style representations",
                    "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
                    "Mgtbench: Benchmarking machine-generated text detection",
                    "RoFT: A tool for evaluating human detection of machine-generated text",
                    "Protecting language generation models via invisible watermarking",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Robust fake news detection over time and attack"
                ],
                "related_work": "2Related Work The task of detecting machine-generated text has long been formulated as a binary classification problem Zellers et al. (2019); Gehrmann et al. (2019a); Solaiman et al. (2019); Ippolito et al. (2019), where the goal is to distinguish texts generated by certain language models from those authored by humans. However, with the rapid advancement of a wide range of LLMs such as GPT-4, Bard, Cohere and Claude, they can be easily used to generate texts through simple API calls. There is a growing need for more fine-grained classification that can not only identify the nature of the texts (i.e., whether it is machine-generated or human written), but also its specific source (i.e., which LLM generates it?). This is also known as authorship attribution Uchendu et al. (2020); Venkatraman et al. (2023); Rivera Soto et al. (2024). Binary Detection In general, there are mainly two types of machine-generated text detector: supervised approaches Wang et al. (2023); Uchendu et al. (2021); Zellers et al. (2019); Zhong et al. (2020); Liu et al. (2022) and unsupervised methods based on white-box features such as likelihood and log-rank Solaiman et al. (2019); Ippolito et al. (2019); Mitchell et al. (2023); Su et al. (2023); He et al. (2023); Hans et al. (2024), and watermarking Kirchenbauer et al. (2023); Zhao et al. (2023b, a). We focus on supervised approaches. Wang et al. (2023) provide the evaluation results based on several supervised detectors, including RoBERTa Liu et al. (2019), XLM-R Conneau et al. (2019), logistic regression classifier with GLTR features Gehrmann et al. (2019b), stylistic features Li et al. (2014), andNELA Horne et al. (2019) features. Similar work using supervised methods can also be seen in Guo et al. (2023); Hu et al. (2023); Xiong et al. (2024). However, most of them only consider binary detection. Multi-Class Detection The fine-grained multi-class classification problem is closely related to authorship attribution. Munir et al. (2021) find that texts generated from LMs contain distinguishable signals that can be used to attribute the source of texts. Uchendu et al. (2020) investigate three authorship attribution problems: (1) whether two texts are generated by the same method, (2) whether a text is generated by machine or human, and (3) which language model generated the texts. Venkatraman et al. (2023) examine if the principle that humans prefer to spread information evenly during language production can help capture the unique signature of each LLM and human author while Rivera Soto et al. (2024) leverages representations of writing styles. Authorship Obfuscation goes beyond the binary/multi-class classification lens and considers an adversarial setting of co-authorship of human and machine Macko et al. (2024). It is known that machine-generated detection methods are susceptible to authorship obfuscation attacks such as paraphrasing, back-translation, and human-machine collaboration mixture Crothers et al. (2022); Krishna et al. (2023); Shi et al. (2023); Koike et al. (2023). Gao et al. (2024) introduces a dataset with a mixture of machine and human written texts using operations such as polishing, completing Xie et al. (2023), rewriting Shu et al. (2023), adding natural noise Wang et al. (2021), and adapting Gero et al. (2022). Kumarage et al. (2023) use stylometric signals to quantify changes in tweets and detect if and when AI starts to generate tweets. In this work, we focus on detecting and localizing the change point from human-written segments to machine-generated segments. We construct a dataset consisting of peer reviews and student essays based on ChatGPT, GPT-4, LLaMA-2 (7B, 13B and 70B). Models using semantic features show promising results when training and test sets are from the same domain and generators.",
                "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available atthis https URL."
            },
            {
                "name": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
                "arxiv_id": "2402.11167",
                "subtitles": [
                    "AI-Generated Content and Detection Models",
                    "Adversarial Attacks on AI-Content Detection"
                ],
                "reference": [
                    "Can ai-generated text be reliably detected",
                    "The science of detecting llm-generated texts",
                    "Is chatgpt a general-purpose natural language processing task solver",
                    "A watermark for large language models",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature"
                ],
                "related_work": "2 Related Work AI-Generated Content and Detection Models.Along with the advancements in content generationQin et al. (2023) , efforts have been made to develop detection models capable of distinguishing human-written from AI-generated textsMitchell et al. (2023) ; Bao et al. (2023) . Techniques leveraging word entropy analysis, machine learning classifiers, and next-token probability analysis have been exploredKirchenbauer et al. (2023) ; Tang et al. (2023) . For instance,Tang et al. (2023) have demonstrated using statistical methods and fine-tuned models to improve detection accuracy. However, these methods often struggle against straightforward manipulation strategies, such as paraphrasing attacks, highlighting the gap in the current detection capabilitiesSadasivan et al. (2023) .Adversarial Attacks on AI-Content Detection.The concept of adversarial attacks in AI content detection involves manipulating input textual information to deceive detection models into misclassifying AI-generated content as human-writtenSadasivan et al. (2023) . Recent work byBao et al. (2023) andMitchell et al. (2023) has shed light on the vulnerabilities of AI models to adversarial inputs, suggesting that even minor alterations can significantly impact model performance.",
                "abstract": "The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies."
            },
            {
                "name": "EAGLE: A Domain Generalization Framework for AI-generated Text Detection",
                "arxiv_id": "2403.15690",
                "subtitles": [
                    "AI-generated Text Detection",
                    "Zero-shot AI-generated Text Detection"
                ],
                "reference": [
                    "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
                    "On the zero-shot generalization of machine-generated text detectors",
                    "Release strategies and the social impacts of language models",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Real or fake? learning to discriminate machine from human generated text",
                    "Zero-shot detection of machine-generated codes",
                    "Gltr: Statistical detection and visualization of generated text",
                    "Conda: Contrastive domain adaptation for ai-generated text detection",
                    "Fighting fire with fire: Can chatgpt detect ai-generated text? arXiv preprint arXiv",
                    "Automatic detection of generated text is easiest when humans are fooled",
                    "Ghostbuster: Detecting text ghostwritten by large language models",
                    "Mgtbench: Benchmarking machine-generated text detection",
                    "Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
                    "Smaller language models are better black-box machine-generated text detectors",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature"
                ],
                "related_work": "2Related WorkAI-generated Text Detection.With the rapid progress of language models over the last several years, there have also been approaches proposed for detection of text generated by such models. In the case where plenty of labeled data is available, fine-tuned pre-trained language models are often the best performing detectors[20,18]. An example of this is the OpenAI detector that is simply a RoBERTa[27]model fine-tuned on GPT-2 data[36]. A recent supervised method called Ghostbuster uses a series of weaker models followed by a search over combination functions and then a linear classifier[42]. Some recent work also explore the use of LLMs as the detector for detecting AI-generated text[6]. Authors in[5]propose an unsupervised domain adaptation framework to detect AI-generated text by leveraging labeled source and unlabeled target data.Zero-shot AI-generated Text Detection.In addition to supervised methods for detection, recently there has been a lot of effort in the area of zero-shot detection of text generated by AI text generators, i.e., LLMs. While some works[33,29]analyze the zero-shot transfer capabilities of AI-text detectors to text from new generators, some also propose novel zero-shot or unsupervised detection methods. Some of these methods[15,3]leverage statistical measures to identify generation artifacts across common sampling schemes, while some rely on assumptions surrounding the log probabilities of the generated text under a proxy model[30,4,37]. Under a full black-box setting, with no access to the token probabilities, a recent approach[45]uses n-gram analysis to detect such AI-generated text. Another interesting recent zero-shot detection method compares the perplexity of the input text under two related language models as a signal towards detection[17]. Authors in[5]proposes a domain adaptation framework for unsupervised detection. Zero-shot detection approaches have also been proposed for code generated by LLMs[46].While these various approaches have shown promising results in the detection of AI-generated text both in fully supervised and zero-shot scenarios, there is currently no work that leverages labeled data from older generators to perform unsupervised detection of text from newer generators. To the best of our knowledge, this is the first work to propose a domain generalization framework for AI-generated text detection, whereby we investigate if we can tackle the real-world scenario of detecting text from an unseen generator while leveraging labeled data from older, potentially much smaller generators.",
                "abstract": "With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised contrastive learning with domain adversarial training. Through our experiments we demonstrate how EAGLE effectively achieves impressive performance in detecting text generated by unseen target generators, including recent state-of-the-art ones such as GPT-4 and Claude, reaching detection scores of within 4.7% of a fully supervised detector."
            }
        ],
        "survey": {
            "name": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",
            "arxiv_id": "2403.01152",
            "subtitles": [
                {
                    "name": "AI-generated Text Forensic Systems",
                    "key_history": [
                        {
                            "reference_title": "How persuasive is ai-generated propaganda",
                            "key_word": "GPT in Misinformation Detection"
                        },
                        {
                            "reference_title": "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
                            "key_word": "Watermarking in AI Text Detection"
                        },
                        {
                            "reference_title": "Stylometric detection of ai-generated text in twitter timelines",
                            "key_word": "Stylometry in Supervised AI Text Detection"
                        },
                        {
                            "reference_title": "Detecting Generated Text and Attributing Language Model Source with Fine-tuned Models and Semantic Understanding",
                            "key_word": "Structural and Sequence-Based Detection Features"
                        },
                        {
                            "reference_title": "Real or fake? learning to discriminate machine from human generated text",
                            "key_word": "Energy-Based Models and Transferable Detection"
                        },
                        {
                            "reference_title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
                            "key_word": "Self-Consistency and Log-Rank Ratio in Zero-shot Detection"
                        },
                        {
                            "reference_title": "Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text",
                            "key_word": "LLMs as Zero-shot Detectors"
                        },
                        {
                            "reference_title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                            "key_word": "Factual Consistency and Fact-Checking Systems"
                        },
                        {
                            "reference_title": "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
                            "key_word": "Benchmarking AI-Misinformation Detection"
                        },
                        {
                            "reference_title": "Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation",
                            "key_word": "Challenges in AI-Misinformation Detection"
                        },
                        {
                            "reference_title": "Neural Authorship Attribution: Stylometric Analysis on Large Language Models",
                            "key_word": "Co-Reference Chains in AI Detection"
                        },
                        {
                            "reference_title": "Neural Authorship Attribution: Stylometric Analysis on Large Language Models",
                            "key_word": "Journalism-Standard Stylometry Features"
                        },
                        {
                            "reference_title": "Understanding news creation intents: Frame, dataset, and method",
                            "key_word": "Characterization in AI Forensics"
                        },
                        {
                            "reference_title": "Working with ai to persuade: Examining a large language model's ability to generate pro-vaccination message",
                            "key_word": "AI-Misinformation Impact on Public Health Messaging"
                        },
                        {
                            "reference_title": "OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples",
                            "key_word": "Outfox in Adversarial Training for Detection"
                        }
                    ],
                    "references_in_this_section": [
                        "Detecting Generated Text and Attributing Language Model Source with Fine-tuned Models and Semantic Understanding",
                        "AI-Writing Detection Using an Ensemble of Transformers and Stylometric Features",
                        "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                        "Attribution and obfuscation of neural text authorship: A data mining perspective",
                        "LM vs LM: Detecting Factual Errors via Cross Examination",
                        "Towards detecting harmful agendas in news articles",
                        "All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation",
                        "Can llm-generated misinformation be detected",
                        "Working with ai to persuade: Examining a large language model's ability to generate pro-vaccination messages",
                        "A neural probabilistic language model",
                        "Detecting AI Authorship: Analyzing Descriptive Features for AI Detection",
                        "Experiments with convolutional neural networks for multi-label authorship attribution",
                        "Automatic detection of machine generated text: A critical survey",
                        "Release strategies and the social impacts of language models",
                        "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
                        "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
                        "A Survey of Text Watermarking in the Era of Large Language Models",
                        "Disinformation detection: An evolving challenge in the age of llms",
                        "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                        "How persuasive is ai-generated propaganda",
                        "Neural Authorship Attribution: Stylometric Analysis on Large Language Models",
                        "Authorship attribution for neural text generation",
                        "Tweepfake: About detecting deepfake tweets",
                        "Fake news in sheep's clothing: Robust fake news detection against llm-empowered style attacks",
                        "Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT",
                        "Fake news detectors are biased against texts generated by large language models",
                        "Defending against neural fake news",
                        "Bot or human? detecting chatgpt imposters with a single question",
                        "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
                        "Stylometric detection of ai-generated text in twitter timelines",
                        "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
                        "Real or fake? learning to discriminate machine from human generated text",
                        "GPT-who: An Information Density-based Machine-Generated Text Detector",
                        "SeqXGPT: Sentence-Level AI-Generated Text Detection",
                        "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
                        "Automatic detection of generated text is easiest when humans are fooled",
                        "Anatomy of an ai-powered malicious social botnet",
                        "Llmdet: A third party large language models generated text detection tool",
                        "Matching pairs: Attributing fine-tuned models to their pre-trained large language models",
                        "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
                        "Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text",
                        "Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation",
                        "A mathematical theory of communication",
                        "GLTR: Statistical Detection and Visualization of Generated Text",
                        "OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples",
                        "Classification of Human- and AI-Generated Texts for English, French, German, and Spanish",
                        "Attention is all you need",
                        "Neural deepfake detection with factual structure of text",
                        "J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News",
                        "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
                        "Comparing the willingness to share for human-generated vs. ai-generated fake news",
                        "Adapting fake news detection to the era of large language models",
                        "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output",
                        "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
                        "Computational methods in authorship attribution",
                        "Language models are unsupervised multitask learners",
                        "Understanding news creation intents: Frame, dataset, and method",
                        "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
                        "Source code authorship attribution using long short-term memory based networks",
                        "Artificial text detection via examining the topology of attention maps",
                        "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "Resources",
                    "key_history": [
                        {
                            "reference_title": "Distinguishing fact from fiction: A benchmark dataset for identifying machine-generated scientific papers in the llm era",
                            "key_word": "Facts from Fiction"
                        },
                        {
                            "reference_title": "Overview of autextification at iberlef 2023: Detection and attribution of machine-generated text in multiple domains",
                            "key_word": "AI-generated Tweets"
                        },
                        {
                            "reference_title": "Multitude: Large-scale multilingual machine-generated text detection benchmark",
                            "key_word": "Performance Metrics"
                        },
                        {
                            "reference_title": "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
                            "key_word": "AI-Misinformation Benchmarks"
                        },
                        {
                            "reference_title": "Do models explain themselves? counterfactual simulatability of natural language explanations",
                            "key_word": "LLMFake"
                        },
                        {
                            "reference_title": "Overview of autextification at iberlef 2023: Detection and attribution of machine-generated text in multiple domains",
                            "key_word": "Generators and Domains"
                        }
                    ],
                    "references_in_this_section": [
                        "Turingbench: A benchmark environment for turing test in the age of neural text generation",
                        "On the risk of misinformation pollution with large language models",
                        "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
                        "Can llm-generated misinformation be detected",
                        "Distinguishing fact from fiction: A benchmark dataset for identifying machine-generated scientific papers in the llm era",
                        "Fake news detectors are biased against texts generated by large language models",
                        "Overview of autextification at iberlef 2023: Detection and attribution of machine-generated text in multiple domains",
                        "Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation",
                        "Multitude: Large-scale multilingual machine-generated text detection benchmark",
                        "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection"
                    ]
                },
                {
                    "name": "Future of AI-generated Text Forensics",
                    "key_history": [
                        {
                            "reference_title": "Can AI-Generated Text be Reliably Detected",
                            "key_word": "Diminishing Boundary"
                        },
                        {
                            "reference_title": "How reliable are ai-generated-text detectors? an assessment framework using evasive soft prompts",
                            "key_word": "Attacks Against Forensics"
                        },
                        {
                            "reference_title": "The false promise of imitating proprietary llms",
                            "key_word": "LLM Variants"
                        },
                        {
                            "reference_title": "Generative agents: Interactive simulacra of human behavior",
                            "key_word": "Coordinated AI Agents"
                        },
                        {
                            "reference_title": "Can knowledge graphs reduce hallucinations in llms",
                            "key_word": "Knowledge-Aware LLMs"
                        },
                        {
                            "reference_title": "Causality",
                            "key_word": "Causality-aware Forensic Systems"
                        }
                    ],
                    "references_in_this_section": [
                        "Causality",
                        "Language model detectors are easily optimized against",
                        "Rex: Rapid exploration and exploitation for ai agents",
                        "Can AI-Generated Text be Reliably Detected",
                        "Do models explain themselves? counterfactual simulatability of natural language explanations",
                        "The false promise of imitating proprietary llms",
                        "Alpaca: A strong, replicable instruction-following model",
                        "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                        "How reliable are ai-generated-text detectors? an assessment framework using evasive soft prompts",
                        "A knowledge graph question answering approach to iot forensics",
                        "Visualizing and reasoning about presentable digital forensic evidence with knowledge graphs",
                        "Generative agents: Interactive simulacra of human behavior",
                        "Can knowledge graphs reduce hallucinations in llms?: A survey"
                    ]
                }
            ],
            "all_references": [
                "Automatic detection of generated text is easiest when humans are fooled",
                "Experiments with convolutional neural networks for multi-label authorship attribution",
                "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
                "Fake news detectors are biased against texts generated by large language models",
                "Language model detectors are easily optimized against",
                "Towards detecting harmful agendas in news articles",
                "Detecting generated text and attributing language model source with fine-tuned models and semantic understanding",
                "Stadee: Statistics-based deep detection of machine generated text",
                "The false promise of imitating proprietary llms",
                "Attribution and obfuscation of neural text authorship: A data mining perspective",
                "Tweepfake: About detecting deepfake tweets",
                "Fake news in sheep's clothing: Robust fake news detection against llm-empowered style attacks",
                "OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples",
                "Attention is all you need",
                "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
                "Authorship attribution for neural text generation",
                "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
                "GPT-who: An Information Density-based Machine-Generated Text Detector",
                "Comparing the willingness to share for human-generated vs. ai-generated fake news",
                "Hansen: human and ai spoken text benchmark for authorship analysis",
                "Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text",
                "Check me if you can: Detecting chatgpt-generated academic writing using checkgpt",
                "Source code authorship attribution using long short-term memory based networks",
                "A mathematical theory of communication",
                "Overview of autextification at iberlef 2023: Detection and attribution of machine-generated text in multiple domains",
                "Understanding news creation intents: Frame, dataset, and method",
                "Neural Authorship Attribution: Stylometric Analysis on Large Language Models",
                "J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News",
                "Gpt-2 versus gpt-3 and bloom: Llms for llms generative text detection",
                "Conda: Contrastive domain adaptation for ai-generated text detection",
                "Bot or human? detecting chatgpt imposters with a single question",
                "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
                "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
                "Classification of Human- and AI-Generated Texts for English, French, German, and Spanish",
                "To chatgpt, or not to chatgpt: That is the question",
                "Detecting artificially generated academic text: The importance of mimicking human utilization of large language models",
                "All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation",
                "Computational methods in authorship attribution",
                "A neural probabilistic language model",
                "Release strategies and the social impacts of language models",
                "Artificial text detection via examining the topology of attention maps",
                "A Survey of Text Watermarking in the Era of Large Language Models",
                "Alpaca: A strong, replicable instruction-following model",
                "Generative agents: Interactive simulacra of human behavior",
                "Turingbench: A benchmark environment for turing test in the age of neural text generation",
                "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output",
                "Llama 2: Open foundation and fine-tuned chat models",
                "Can AI-Generated Text be Reliably Detected",
                "Classification of human-and ai-generated texts for english, french, german, and spanish",
                "Ghostbuster: Detecting text ghostwritten by large language models",
                "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                "Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT",
                "Training language models to follow instructions with human feedback",
                "Matching pairs: Attributing fine-tuned models to their pre-trained large language models",
                "Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection",
                "Defending against neural fake news",
                "Multitude: Large-scale multilingual machine-generated text detection benchmark",
                "Demasq: Unmasking the chatgpt wordsmith",
                "Adapting fake news detection to the era of large language models",
                "Automatic detection of machine generated text: A critical survey",
                "Rex: Rapid exploration and exploitation for ai agents",
                "J-guard: Journalism guided adversarially robust detection of ai-generated news",
                "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                "Neural deepfake detection with factual structure of text",
                "Detecting Generated Text and Attributing Language Model Source with Fine-tuned Models and Semantic Understanding",
                "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection",
                "Coco: Coherence-enhanced machine-generated text detection under low resource with contrastive learning",
                "Real or fake? learning to discriminate machine from human generated text",
                "The science of detecting llm-generated texts",
                "Causality",
                "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
                "Stylometric detection of ai-generated text in twitter timelines",
                "LM vs LM: Detecting Factual Errors via Cross Examination",
                "Mfd: Multi-feature detection of llm-generated text",
                "Supervised machine-generated text detectors: Family and scale matters",
                "A prompt in the right direction: Prompt based classification of machine-generated text detection",
                "How persuasive is ai-generated propaganda",
                "AI-Writing Detection Using an Ensemble of Transformers and Stylometric Features",
                "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
                "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                "Detecting AI Authorship: Analyzing Descriptive Features for AI Detection",
                "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
                "A knowledge graph question answering approach to iot forensics",
                "Gemini: a family of highly capable multimodal models",
                "Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text",
                "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning",
                "Working with ai to persuade: Examining a large language model's ability to generate pro-vaccination messages",
                "The falcon series of open language models",
                "Ai-writing detection using an ensemble of transformers and stylometric features",
                "GLTR: Statistical Detection and Visualization of Generated Text",
                "Can knowledge graphs reduce hallucinations in llms?: A survey",
                "Ai model gpt-3 (dis) informs us better than humans",
                "Visualizing and reasoning about presentable digital forensic evidence with knowledge graphs",
                "Can llm-generated misinformation be detected",
                "Llmdet: A third party large language models generated text detection tool",
                "SeqXGPT: Sentence-Level AI-Generated Text Detection",
                "Disinformation detection: An evolving challenge in the age of llms",
                "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
                "Gpt-4 technical report",
                "How reliable are ai-generated-text detectors? an assessment framework using evasive soft prompts",
                "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
                "Language models are unsupervised multitask learners",
                "Hc3 plus: A semantic-invariant human chatgpt comparison corpus",
                "Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation",
                "On the risk of misinformation pollution with large language models",
                "Machine-generated text: A comprehensive survey of threat models and detection methods",
                "Anatomy of an ai-powered malicious social botnet",
                "Stacking the odds: Transformer-based ensemble for ai-generated text detection",
                "Distinguishing fact from fiction: A benchmark dataset for identifying machine-generated scientific papers in the llm era",
                "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                "Do models explain themselves? counterfactual simulatability of natural language explanations",
                "Who said that? benchmarking social media ai detection"
            ]
        },
        "topic_history": [
            {
                "name": "LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?",
                "arxiv_id": "2401.05952",
                "reference": [
                    "How you prompt matters! even task-oriented constraints in instructions affect llm-generated text detection",
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "On the generalization of training-based chatgpt detection methods",
                    "Gpt-sentinel: Distinguishing human and chatgpt generated content",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Pubmedqa: A dataset for biomedical research question answering",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Radar: Robust ai-text detection via adversarial learning",
                    "Crosslingual generalization through multitask finetuning",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Smaller language models are better black-box machine-generated text detectors",
                    "A mathematical theory of communication",
                    "Openai models - gpt",
                    "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
                    "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
                    "Ghostbuster: Detecting text ghostwritten by large language models",
                    "Mgtbench: Benchmarking machine-generated text detection",
                    "Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature"
                ]
            },
            {
                "name": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
                "arxiv_id": "2403.13335",
                "reference": [
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Switchtab: Switched autoencoders are effective tabular learners",
                    "Joyful: Joint modality fusion and graph contrastive learning for multimodal emotion recognition",
                    "Ensemble methodology: Innovations in credit default prediction using lightgbm, xgboost, and localensemble",
                    "A semi-hard voting combiner scheme to ensemble multi-class probabilistic classifiers",
                    "Attention is all you need",
                    "How many validation labels do you need? exploring the design space of label-efficient model ranking",
                    "Generative ai text classification using ensemble llm approaches",
                    "Chatgpt generated text detection",
                    "Leveraging relational graph neural network for transductive model ensemble",
                    "Unlabeled data selection for active learning in image classification",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Greedy function approximation: a gradient boosting machine",
                    "Recontab: Regularized contrastive representation learning for tabular data",
                    "Enhancing liver segmentation: A deep learning approach with eas feature extraction and multi-scale fusion",
                    "Emp: emotion-guided multi-modal fusion and contrastive learning for personality traits recognition",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection"
                ]
            },
            {
                "name": "Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting",
                "arxiv_id": "2405.16133",
                "reference": [
                    "A Watermark for Large Language Models",
                    "Gltr: Statistical detection and visualization of generated text",
                    "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                    "Starcoder: may the source be with you",
                    "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x",
                    "Authorship attribution for neural text generation",
                    "Evaluating large language models trained on code",
                    "Competition-level code generation with alphacode",
                    "SantaCoder: don't reach for the stars",
                    "Real or Fake? Learning to Discriminate Machine from Human Generated Text",
                    "Defending against neural fake news",
                    "PaLM: Scaling Language Modeling with Pathways",
                    "GPT-4 Technical Report",
                    "Introducing ChatGPT",
                    "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Robust Multi-bit Natural Language Watermarking through Invariant Features",
                    "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Neural Deepfake Detection with Factual Structure of Text",
                    "Developer sentiment around AI/ML"
                ]
            },
            {
                "name": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges",
                "arxiv_id": "2403.18249",
                "reference": [
                    "On the risk of misinformation pollution with large language models",
                    "Declare: Debunking fake news and false claims using evidence-aware deep learning",
                    "Disinformation detection: An evolving challenge in the age of llms",
                    "How effectively can machines defend against machine-generated fake news? an empirical study",
                    "Mining dual emotion for fake news detection",
                    "Adapting fake news detection to the era of large language models",
                    "Implementing bert and fine-tuned roberta to detect ai generated news by chatgpt",
                    "Fake news in sheep's clothing: Robust fake news detection against llm-empowered style attacks",
                    "A survey of fake news: Fundamental theories, detection methods, and opportunities",
                    "defend: Explainable fake news detection",
                    "Defending against neural fake news",
                    "Med-mmhl: A multi-modal dataset for detecting human-and llm-generated misinformation in the medical domain"
                ]
            },
            {
                "name": "CUDRT: Benchmarking the Detection of Human vs. Large Language Models Generated Texts",
                "arxiv_id": "2406.09056",
                "reference": [
                    "A survey on evaluation of large language models A survey on evaluation of large language models",
                    "The Next Chapter: A Study of Large Language Models in Storytelling The next chapter: A study of large language models in storytelling",
                    "GLM: General Language Model Pretraining with Autoregressive Blank Infilling GLM: General language model pretraining with autoregressive blank infilling",
                    "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection Hidding the ghostwriters: An adversarial evaluation of ai-generated student essay detection",
                    "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning CoCo: Coherence-enhanced machine-generated text detection under low resource with contrastive learning",
                    "Using an llm to help with code understanding Using an llm to help with code understanding",
                    "Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection",
                    "SeqXGPT: Sentence-Level AI-Generated Text Detection SeqXGPT: Sentence-level AI-generated text detection",
                    "Rewritelm: An instruction-tuned large language model for text rewriting Rewritelm: An instruction-tuned large language model for text rewriting",
                    "Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text Is chatgpt involved in texts? measure the polish ratio to detect chatgpt-generated text",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Gpt-4 technical report Gpt-4 technical report",
                    "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection M4gt-bench: Evaluation benchmark for black-box machine-generated text detection",
                    "Gltr: Statistical detection and visualization of generated text Gltr: Statistical detection and visualization of generated text",
                    "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark Multitude: Large-scale multilingual machine-generated text detection benchmark",
                    "Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation",
                    "InstantOps: A Joint Approach to System Failure Prediction and Root Cause Identification in Microserivces Cloud-Native Applications Instantops: A joint approach to system failure prediction and root cause identification in microserivces cloud-native applications",
                    "Hc3 plus: A semantic-invariant human chatgpt comparison corpus Hc3 plus: A semantic-invariant human chatgpt comparison corpus",
                    "Mgtbench: Benchmarking machine-generated text detection Mgtbench: Benchmarking machine-generated text detection",
                    "Qwen technical report Qwen technical report",
                    "Llm-as-a-coauthor: The challenges of detecting llm-human mixcase Llm-as-a-coauthor: The challenges of detecting llm-human mixcase",
                    "A Framework for Detecting AI-Generated Text in Research Publications A framework for detecting ai-generated text in research publications",
                    "Large Language Models Evaluate Machine Translation via Polishing Large language models evaluate machine translation via polishing",
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "The future landscape of large language models in medicine The future landscape of large language models in medicine",
                    "Llama: Open and efficient foundation language models Llama: Open and efficient foundation language models"
                ]
            },
            {
                "name": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
                "arxiv_id": "2402.01158",
                "reference": [
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Detecting fake content with relative entropy scoring",
                    "Gltr: Statistical detection and visualization of generated text",
                    "On the possibilities of ai-generated text detection (arxiv: 2304.04736) . arxiv",
                    "Identifying real or fake articles: Towards better language modeling",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
                    "A watermark for large language models",
                    "Computer-generated text detection using machine learning: A systematic review",
                    "The science of detecting llm-generated texts",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection"
                ]
            },
            {
                "name": "Detecting AI-Generated Sentences in Human-AI Collaborative Hybrid Texts: Challenges, Strategies, and Insights",
                "arxiv_id": "2403.03506",
                "reference": [
                    "Segformer: a topic segmentation model with controllable range of attention",
                    "Statistical section segmentation in free-text clinical records",
                    "Threat scenarios and best practices to detect neural fake news",
                    "A sequence-to-sequence approach with mixed pointers to topic segmentation and segment labeling",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Text segmentation by cross segment attention",
                    "Text segmentation as a supervised learning task",
                    "Transformer over pre-trained transformer for neural text segmentation with enhanced topic coherence",
                    "SynSciPass: detecting appropriate uses of scientific text generation",
                    "Improving long document topic segmentation models with enhanced coherence modeling",
                    "DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing",
                    "Towards automatic boundary detection for human-ai collaborative hybrid essay in education",
                    "Human guided exploitation of interpretable attention patterns in summarization and topic segmentation",
                    "Improving context modeling in neural topic segmentation",
                    "SeqXGPT: Sentence-level AI-generated text detection",
                    "Bad actor, good advisor: Exploring the role of large language models in fake news detection",
                    "Dialogue topic segmentation via parallel extraction network with neighbor smoothing",
                    "Is this abstract generated by ai? a research for the gap between ai-generated scientific text and human-written scientific text",
                    "Tipster: A topic-guided language model for topic-aware text segmentation",
                    "Real or fake text?: Investigating human ability to detect boundaries between human-written and machine-generated text",
                    "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities",
                    "Lessons learnt from linear text segmentation: a fair comparison of architectural and sentence encoding strategies for successful segmentation",
                    "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
                    "Two-level transformer and auxiliary coherence modeling for improved text segmentation",
                    "The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers"
                ]
            },
            {
                "name": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
                "arxiv_id": "2402.11175",
                "reference": [
                    "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
                    "Gpt-who: An information density-based machine-generated text detector",
                    "Red teaming language model detectors with language models",
                    "Provable robust watermarking for ai-generated text",
                    "Gltr: Statistical detection and visualization of generated text",
                    "A watermark for large language models",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                    "Adversarial glue: A multi-task benchmark for robustness evaluation of language models",
                    "Radar: Robust ai-text detection via adversarial learning",
                    "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
                    "Release strategies and the social impacts of language models",
                    "Authorship authentication using short messages from social networking sites",
                    "Adversarial robustness of neural-statistical features in detection of generative transformers",
                    "Authorship obfuscation in multilingual machine-generated text detection",
                    "Authorship attribution for neural text generation",
                    "Sparks: Inspiration for science writing using language models",
                    "Defending against neural fake news",
                    "Stylometric detection of ai-generated text in twitter timelines",
                    "M4: Multi-generator, multi-domain, and multi-lingual black-box machine-generated text detection",
                    "Real or fake text?: Investigating human ability to detect boundaries between human-written and machine-generated text",
                    "Llm-as-a-coauthor: The challenges of detecting llm-human mixcase",
                    "Through the looking glass: Learning to attribute synthetic text generated by language models",
                    "Rewritelm: An instruction-tuned large language model for text rewriting",
                    "GLTR: Statistical detection and visualization of generated text",
                    "Automatic detection of generated text is easiest when humans are fooled",
                    "Coco: Coherence-enhanced machine-generated text detection under data limitation with contrastive learning",
                    "Fine-tuning large language models for multigenerator, multidomain, and multilingual machine-generated text detection",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "The next chapter: A study of large language models in storytelling",
                    "Turingbench: A benchmark environment for turing test in the age of neural text generation",
                    "Unsupervised cross-lingual representation learning at scale",
                    "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
                    "Neural deepfake detection with factual structure of text",
                    "Few-shot detection of machine-generated text using style representations",
                    "Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples",
                    "Mgtbench: Benchmarking machine-generated text detection",
                    "RoFT: A tool for evaluating human detection of machine-generated text",
                    "Protecting language generation models via invisible watermarking",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Robust fake news detection over time and attack"
                ]
            },
            {
                "name": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
                "arxiv_id": "2402.11167",
                "reference": [
                    "Can ai-generated text be reliably detected",
                    "The science of detecting llm-generated texts",
                    "Is chatgpt a general-purpose natural language processing task solver",
                    "A watermark for large language models",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature"
                ]
            },
            {
                "name": "EAGLE: A Domain Generalization Framework for AI-generated Text Detection",
                "arxiv_id": "2403.15690",
                "reference": [
                    "Spotting llms with binoculars: Zero-shot detection of machine-generated text",
                    "On the zero-shot generalization of machine-generated text detectors",
                    "Release strategies and the social impacts of language models",
                    "Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text",
                    "Real or fake? learning to discriminate machine from human generated text",
                    "Zero-shot detection of machine-generated codes",
                    "Gltr: Statistical detection and visualization of generated text",
                    "Conda: Contrastive domain adaptation for ai-generated text detection",
                    "Fighting fire with fire: Can chatgpt detect ai-generated text? arXiv preprint arXiv",
                    "Automatic detection of generated text is easiest when humans are fooled",
                    "Ghostbuster: Detecting text ghostwritten by large language models",
                    "Mgtbench: Benchmarking machine-generated text detection",
                    "Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text",
                    "Smaller language models are better black-box machine-generated text detectors",
                    "Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Detectgpt: Zero-shot machine-generated text detection using probability curvature"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
            "arxiv_id": "2303.14524",
            "isAPA": false,
            "abstract": "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks.However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually alsohinder their broad deployment in real-world systems. To address theselimitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augmentsLLMs for building conversational recommender systems by convertinguser profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, whichalso makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferencescan transfer to different products for cross-domain recommendations,and prompt-based injection of information into LLMs can also handlethe cold-start scenarios with new items. In our experiments, Chat-Receffectively improve the results of top-k recommendations and performsbetter in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practicalscenarios for the implementation of AIGC (AI generated content) in recommender system studies",
            "reference": [
                "Li, L., Zhang, Y., Chen, L.: Personalized transformer for explainable recommendation. arXiv preprint arXiv",
                "Yuan, F., Zhang, G., Karatzoglou, A., Jose, J., Kong, B., Li, Y.: One person, one model, one world: Learning continual user representation without forgetting. In: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp",
                "Shi, S., Zhang, M., Yu, X., Zhang, Y., Hao, B., Liu, Y., Ma, S.: Adaptive feature sampling for recommendation with missing content feature values. In: Proceedings of the 28th ACM International Conference on Information and Knowledge Management. pp",
                "Geng, S., Liu, S., Fu, Z., Ge, Y., Zhang, Y.: Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p5) . In: Proceedings of the 16th ACM Conference on Recommender Systems. pp",
                "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. arXiv preprint arXiv",
                "Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al.: Evaluating large language models trained on code. arXiv preprint arXiv",
                "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv",
                "Zhu, F., Wang, Y., Chen, C., Zhou, J., Li, L., Liu, G.: Cross-domain recommendation: challenges, progress, and prospects. arXiv preprint arXiv",
                "Wei, J., Wang, X., Schuurmans, D., Bosma, M., hsin Chi, E.H., Le, Q., Zhou, D.: Chain of thought prompting elicits reasoning in large language models. ArXiv abs",
                "Xu, Y., Zhu, C., Xu, R., Liu, Y., Zeng, M., Huang, X.: Fusing context into knowledge graph for commonsense question answering",
                "Liu, P., Zhang, L., Gulla, J.A.: Pre-train, prompt and recommendation: A comprehensive survey of language modelling paradigm adaptations in recommender systems. arXiv preprint arXiv",
                "Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., hsin Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent abilities of large language models. ArXiv abs",
                "Zhang, Y., Ding, H., Shui, Z., Ma, Y., Zou, J., Deoras, A., Wang, H.: Language models as recommender systems: Evaluations and limitations",
                "Sun, C., Liu, H., Liu, M., Ren, Z., Gan, T., Nie, L.: Lara: Attribute-to-feature adversarial learning for new-item recommendation. In: Proceedings of the 13th international conference on web search and data mining. pp",
                "Schick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language models can teach themselves to use tools",
                "Li, L., Zhang, Y., Chen, L.: Generate neural template explanations for recommendation. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management. pp",
                "Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., Hajishirzi, H.: Unifiedqa: Crossing format boundaries with a single qa system",
                "LeCun, Y.: A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review",
                "Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu, C., Zeng, M., Jiang, M.: Generate rather than retrieve: Large language models are strong context generators",
                "Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv",
                "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems",
                "Chen, X., Chen, H., Xu, H., Zhang, Y., Cao, Y., Qin, Z., Zha, H.: Personalized fashion recommendation with visual explanations based on multimodal attention network: Towards visually explainable recommendation. In: Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp",
                "Petroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A.H., Riedel, S.: Language models as knowledge bases? arXiv preprint arXiv",
                "Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learners. ArXiv abs",
                "Fu, Yao; Peng, H., Khot, T.: How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion (Dec 2022) , https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc",
                "Parisi, A., Zhao, Y., Fiedel, N.: Talm: Tool augmented language models"
            ],
            "related work": "2Related Work2.1 Augmented Language Models Augmented Language Models (ALMs) are a new research direction that aims to overcome the limitations of traditional Language Models (LMs) [5,1,4]by equipping them with reasoning skills and the ability to use external tools, which has served millions of users, such as the coding assistant Copilot[2], or more recently ChatGPT based on GPT3.5 and GPT4. Reasoning is defined as breaking down complex tasks into simpler subtasks that the LM can solve more easily by itself or with the help of tools[9,15,13], while tools are external modules that the LM can call to augment its context. ALMs can use these augmentations separately or in combination to expand their context processing ability and outperform most regular LMs on several benchmarks. ALMs can learn to reason, use tools, and even act, while still performing standard natural language tasks. This new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues. By jointly discussing reasoning and tools, and tools and actions, ALMs can solve a broad range of complex tasks without heuristics, thus offering better generalization capabilities.2.2NLP for RecommendationThe field of recommender systems has had a long-standing relationship with natural language processing (NLP) techniques, especially when pre-trained language models (PLMs) comes out, which improve the performance of recommender systems and explainability[3,10,11]. PLMs are language models that have learned universal representations on large corpora in a self-supervised manner, and the learned representations can be beneficial to a series of downstream NLP tasks. In the recommendation domain, PLMs can help alleviate the data sparsity issue, which is a major performance bottleneck of current deep recommendation models. By extracting and transferring knowledge from pre-trained models learned by different PLM-related training paradigms, researchers aim to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency, and effectiveness. In this vibrant field, there are open issues and future research directions that need to be explored, including the connection between PLM-based training paradigms and different input data types for recommender systems. Overall, adapting language modelling paradigms for recommendation is seen as a promising direction in both academia and industry.2.3Cold-start RecommendationCold start recommendation is a problem that arises in recommender systems when users or items have no prior interaction records with the system. This means that there is no data available for the system to make personalized recommendations. To address this issue, solutions have been proposed that either learn to model content features[16]or transfer representations from auxiliary domains[24,26]. The former approach focuses on learning about the characteristics of the items or users based on their content, such as text, images, or metadata. The latter approach involves leveraging information from other domains, such as social networks or product descriptions, to infer user preferences. Additionally, there are approaches that aim to quickly adapt to new domains instead of only providing recommendations for cold-start cases. A good generalization ability of recommendation models on startup cases is essential to ensure a better user experience and increased engagement. In our work, we use the reasoning and background knowledge of LLMs to enhance the performance of recommender systems for cold start scenarios.",
            "date": "2023"
        },
        "topic": "Explainability for LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation",
                "arxiv_id": "2401.08217",
                "subtitles": [
                    "Hypergraph and Recommendation",
                    "LLMs and Recommendation"
                ],
                "reference": [
                    "U-bert: Pre-training user representations for improved recommendation",
                    "Next basket recommendation with intent-aware hypergraph adversarial network",
                    "Palr: Personalization aware llms for recommendation",
                    "Empowering news recommendation with pre-trained language models",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Reprbert: Distilling bert to an efficient representation-based relevance model for e-commerce",
                    "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                    "Session-based recommendations with recurrent neural networks",
                    "How can recommender systems benefit from large language models: A survey",
                    "Uncovering chatgpt's capabilities in recommender systems",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Leveraging large language models in conversational recommender systems",
                    "Ctr-bert: Cost-effective knowledge distillation for billion-parameter teacher models",
                    "Towards unified conversational recommender systems via knowledge-enhanced prompt learning",
                    "A first look at llm-powered generative news recommendation",
                    "Do llms understand user preferences? evaluating llms on user rating prediction",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Session-based recommendation with graph neural networks",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Session-based recommendation with hypergraph attention networks",
                    "Training large-scale news recommenders with pretrained language models in the loop",
                    "Language models as agent models",
                    "Generative recommendation: Towards next-generation recommender paradigm"
                ],
                "related_work": "2 Related Work Hypergraph and Recommendation.The relations between items under certain intents represent higher-order information. To accurately model these complex relationships,hypergraphscan be incorporated into recommendation systems. Hypergraphs are a generalization of graphs in which a hyperedge is an arbitrary non-empty subset of the vertex set. Currently, there are three main types of hypergraphs.Transition hyperedges(Hidasi et al.,2015; Wu et al.,2019) directly encodes the sequential order of item transitions.Context hyperedges(Wang et al.,2021) capture local interests by applying a sliding window to item sequences.Intent hyperedges(Li et al.,2023) identify intent-specific associations by calculating similarities between prototype vectors for intents and item embeddings. These existing techniques rely on  \"algorithmically \" extractive operations on behavioral data to construct hypergraphs. While surface-level statistics are uncovered, the higher-order semantics between items and intents are not truly obtained. Our work pioneers the use of LLMs to generate hypergraph structures. Rather than purely algorithmic computations, rich latent connections are synthesized through the expansive knowledge encoded within the parameters of the language model. This allows our method to produce a more holistic hypergraph containing nuanced semantic representations of the relationships. LLMs and Recommendation.The integration of LLMs into recommendation systems is an emerging area of research that has shown considerable promise, according to pioneering studies. The potential of these models can be categorized into three distinct approaches, each harnessing the power of LLMs in innovative ways.Using LLMs Directly for Recommendations.A novel aspect of LLMs is that they can be used in recommendation systems without constructing new models from scratch. These methods depend on crafting specific prompts for the LLMs(Liu et al.,2023a; Gao et al.,2023; Dai et al.,2023; Chen,2023) or applying minimal fine-tuning to adapt the model to the task of recommendations(Zhang et al.,2023; Kang et al.,2023; Bao et al.,2023) .LLMs as Sources of Supplementary Information.LLMs act as sophisticated feature extractors that process information about items and users, subsequently producing context-rich embeddings(Wu et al.,2021; Qiu et al.,2021; Yao et al.,2022; Muhamed et al.,2021; Xiao et al.,2022; Liu et al.,2023b; Wang et al.,2022,2023a) . These embeddings can then be seamlessly integrated into traditional recommendation models, thereby enriching them with the LLM's extensive knowledge base.LLMs as Interactive Agents in Recommendation Systems.LLMs take a more active role by managing the entire recommendation process. These advanced models are adapted for use in recommendation contexts, where they can take charge of gathering user data, engineering features, encoding this information, and even directing the scoring and ranking mechanisms(Andreas,2022; Bao et al.,2023; Hou et al.,2023; Lin et al.,2023; Gao et al.,2023; Friedman et al.,2023) . Together, these forward-looking approaches demonstrate the transformative potential of LLMs in revolutionizing recommendation systems.",
                "abstract": "As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests. To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks. By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability. We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets. The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications."
            },
            {
                "name": "Dynamic and Adaptive Feature Generation with LLM",
                "arxiv_id": "2406.03505",
                "subtitles": [
                    "Feature Generation",
                    "Large Language Models (LLMs) and Tree of Thoughts"
                ],
                "reference": [
                    "Emergent abilities of large language models",
                    "Learning feature engineering for classification",
                    "Representation learning: A review and new perspectives",
                    "Autoint: Automatic feature interaction learning via self-attentive neural networks",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification",
                    "Cognito: Automated feature engineering for supervised learning",
                    "Group-wise reinforcement feature generation for optimal and explainable representation space reconstruction",
                    "A survey of chain of thought reasoning: Advances, frontiers and future",
                    "Toward causal representation learning",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Generating efficient training data via llm-based attribute manipulation",
                    "Feature engineering for machine learning: principles and techniques for data scientists",
                    "A survey on large language model based autonomous agents",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Prompt engineering with chatgpt: a guide for academic writers",
                    "Gpt-4 technical report",
                    "Automatic feature engineering for answer selection and extraction",
                    "Physics-constrained automatic feature engineering for predictive modeling in materials science",
                    "Towards better chain-of-thought prompting strategies: A survey",
                    "Reasoning with language model is planning with world model",
                    "Active prompting with chain-of-thought for large language models",
                    "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                    "An overview on data representation learning: From traditional feature learning to recent deep learning",
                    "Deep feature generating network: A new method for intelligent fault detection of mechanical systems under class imbalance",
                    "Explorekit: Automatic feature generation and selection",
                    "Language models are few-shot learners",
                    "Techniques for automated machine learning",
                    "Aligning large language models with human: A survey",
                    "Summary of chatgpt-related research and perspective towards the future of large language models",
                    "Deepfm: a factorization-machine based neural network for ctr prediction"
                ],
                "related_work": "2Preliminary and Related Works 2.1Feature GenerationFeature Engineering. Feature engineering is the process of selecting, modifying or creating new features from raw data to improve the performance of machine learning modelsSeveryn and Moschitti(2013) . The goal is to reconstruct an optimal and explainable feature representation space for a certain machine learning task(Zheng and Casari,2018;Chen et al.,2021) . This process can be described as a function \\phi:\\mathcal{F}\\rightarrow\\mathcal{F}^{{}^{\\prime}} that transforms the original feature set \\mathcal{F} into a new feature set \\mathcal{F}^{{}^{\\prime}} with certain operations. Feature generation is a decisive category in the field of feature engineering. Feature generation means generating new attributes from existing features in a dataset through various mathematical or logical transformation operations. Feature generation methods can primarily be categorized into two main classes: (1) latent representation learning based methods, such as deep factorization machines methods(Guo et al.,2017;Song et al.,2019) and deep representation learning methods(Zhong et al.,2016;Bengio et al.,2013) . These methods can create complex latent feature spaces; however, their generation processes often lack transparency and are difficult to trace and explain(Sch\u00f6lkopf et al.,2021) . (2) feature transformation-based methods, which generate new features by performing arithmetic or aggregation operations(Nargesian et al.,2017) . These methods often require a large amount of manual operations and highly rely on domain knowledge to select appropriate transformations and adjust parameters.Automated Feature Generation.With the widespread adoption of large model techniques and various deep learning methods in feature engineering field, automated feature generation has significant development and application. Automated feature generation enhances the feature space by systematically creating and integrating new features that improve the model performance(Xiang et al.,2021;Wang et al.,2022;Pan et al.,2020;Wang et al.,2022) . For example,(Katz et al.,2016) developed ExploreKit, which generates a large set of candidate features by combining information in the original features.(Khurana et al.,2016) explores the feature space using manually crafted heuristic traversal strategies, while(Shi et al.,2018) proposed a feature optimization approach using deep learning and feature selection to enhance traffic classification performance. These methods are more efficient than traditional manual feature engineering and enable faster processing of large datasets. However, despite their efficiency and ease of operation, these automated feature generation methods often ignore the semantic aspects of data, and their  \"black box \" operation process make the results even harder to explain.2.2Large Language Models (LLMs) and Tree of ThoughtsLLMs Capabilities.LLMs have revolutionized numerous fields with their extraordinary capabilities of in-context learning and step-by-step reasoning(Brown et al.,2020;Achiam et al.,2023;Wei et al.,2022a,b) . With these capabilities, LLMs can effectively understand and navigate complex tasks with appropriate guidance and precise prompts. Thus they can exhibit outstanding professional competence across various specialized domains(Wang et al.,2023;White et al.,2023) . Recent advancements in prompt engineering and reasoning structures have further enabled these models to tackle complex data analysis tasks and multi-step feature engineering processes(Peng et al.,2023;Liu et al.,2023) .LLM Prompting.LLM prompting leverages pre-train models to recall existing knowledge and generate output based on specific guideline prompts(White et al.,2023) . LLM prompting interacts closely with LLMs' capabilities of in-context learning and step-by-step reasoning(Giray,2023;Chu et al.,2023) . Prompts with context details guide the models in thinking more coherently when handling long tasks. Moreover, step-by-step reasoning prompts can guide the model through a logical sequence of thought steps by mimicking the human-like reasoning process of solving complicated problems(Yu et al.,2023;Wei et al.,2022b;Diao et al.,2023) .Tree of Thoughts.In prompting methods, one category is \"logical reasoning prompting,\" which employs structured prompts to guide LLMs through logical analysis and problem-solving tasks.Input-Output (IO) Prompting(Wang et al.,2024;Hao et al.,2023) involves wrapping the input x with task-specific instructions or examples to guide the model to produce the desired outputy y. This method is straightforward but its guiding capability is limited in complex tasks.Chain-of-Thought (CoT) Prompting(Wei et al.,2022b;Besta et al.,2024) involves a sequence of intermediate, coherent language expressions z_{1},\\ldots,z_{n} that logically bridge the input x to the output y. This method enhances the model's logical capabilities for complex problem-solving but it is limited to linear, single-strategy reasoning, which may only capture some possible solutions.Tree of Thoughts (ToT) Prompting(Yao et al.,2024) extends the CoT prompting by exploring multiple reasoning paths over thoughts. ToT works as a search over a tree structure where each node represents a partial solution within the input and contextual thoughts. ToT advantages over other methods by exploring multiple reasoning paths with different strategies. Thus it performs better in deep data analysis and multi-step feature engineering.",
                "abstract": "The representation of feature space is a crucial environment where data points get vectorized and embedded for upcoming modeling. Thus the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and draws advantages over strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods."
            },
            {
                "name": "LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations",
                "arxiv_id": "2401.12576",
                "subtitles": [
                    "Interfaces for interactive explanations",
                    "Dialogue-based systems for interpretability"
                ],
                "reference": [
                    "XMD: An end-to-end framework for interactive explanation-based debugging of NLP models",
                    "A conversational interface for interacting with machine learning models",
                    "iSee: Intelligent sharing of explanation experience by users for users",
                    "Explaining machine learning models with interactive natural language conversations using TalkToModel",
                    "May i ask a follow-up question? understanding the benefits of conversations in neural network explainability",
                    "The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models",
                    "IFAN: An explainability-focused interaction framework for humans and NLP models",
                    "CrossCheck: Rapid, reproducible, and interpretable model evaluation",
                    "ConvXAI: Delivering heterogeneous AI explanations via conversations to support human-AI scientific writing",
                    "InterroLang: Exploring NLP models and datasets through dialogue-based explanations",
                    "Diagnosing AI explanation methods with folk concepts of behavior",
                    "Follow the successful herd: Towards explanations for improved use and mental models of natural language systems"
                ],
                "related_work": "7Related workInterfaces for interactive explanationsLITTenney et al. (2020) is a GUI-based tool available for analyzing model behaviors across entire datasets. However,LIThas less functionalities in terms of prompting and lower accessibility, e.g. no tutorial and a lower level of integration withHuggingFace.CrossCheckArendt et al. (2021) exhibits the capability to facilitate quick cross-model comparison and error analysis across various data types, but adapting it for other use cases needs substantial code modification and customization.XMD'sLee et al. (2023) primary purpose is model debugging, but it shares similarities in the focus on feature attributions, visualization of single instances and user feedback options. It is, however, limited to feature attribution explanations and smaller, efficiently retrainable models.IFANMosca et al. (2023) enables real-time explanation-based interaction with NLP models, but is limited to the sequence-to-class format, restricting its applicability to other tasks and it offers only a limited set of explainability methods.Dialogue-based systems for interpretabilityCarneiro et al. (2021) point out that conversational interfaces have the potential to greatly enhance the transparency and the level of trust that human decision-makers place in them. According toZhang et al.'s (2023) user studies, delivering explanations in a conversational manner can improve users' understanding, satisfaction, and acceptance.Jacovi et al. (2023) emphasizes the necessity of interactive interrogation in order to build understandable explanation narratives.ConvXAIShen et al. (2023) ,TalkToModelSlack et al. (2023) ,InterroLangFeldhus et al. (2023) andBrachman et al. (2023) share some similarities with our framework, but are more complex in their setup and consider fewer explainability methods. Additionally, they might overrely on external LMs to explain the deployed LM's behavior, whereasLLMCheckupplaces a strong emphasis on self-explanation, which is crucial for faithfulness. Finally,LLMCheckupuses auto-regressive models, as they have become increasingly dominant in various NLP applications nowadays. IniSeeWijekoon et al. (2023) , a chatbot adapts explanations to the user's persona, but they do not consider LLMs.",
                "abstract": "Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding (Slack et al., 2023; Shen et al., 2023), as one-off explanations may fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, often require external tools and modules and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate explanations and perform user intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) methods, including white-box explainability tools such as feature attributions, and self-explanations (e.g., for rationale generation). LLM-based (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckupprovides tutorials for operations available in the system, catering to individuals with varying levels of expertise in XAI and supporting multiple input modalities. We introduce a new parsing strategy that substantially enhances the user intent recognition accuracy of the LLM. Finally, we showcase LLMCheckup for the tasks of fact checking and commonsense question answering."
            },
            {
                "name": "A Concept-Based Explainability Framework for Large Multimodal Models",
                "arxiv_id": "2406.08074",
                "subtitles": [
                    "Large Multimodal Models (LMMs) ",
                    "Concept activation vector based approaches",
                    "Understanding VLM/LMM representations"
                ],
                "reference": [
                    "Flamingo: a visual language model for few-shot learning",
                    "Interpreting clip's image representation via text-based decomposition",
                    "Invertible concept-based explanations for cnn models with non-negative concept activation vectors",
                    "Linearly mapping from image to text space",
                    "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav",
                    "Towards vision-language mechanistic interpretability: A causal tracing tool for blip",
                    "Towards automatic concept-based explanations",
                    "A holistic approach to unifying automatic concept extraction and concept importance estimation",
                    "OpenFlamingo: An open-source framework for training large autoregressive vision-language models",
                    "Finding and editing multi-modal neurons in pre-trained transformer",
                    "Grounding language models to images for multimodal generation",
                    "Craft: Concept recursive activation factorization for explainability",
                    "Multimodal neurons in artificial neural networks",
                    "Gpt-4 technical report",
                    "Magma-multimodal augmentation of generative models through adapter-based finetuning",
                    "On concept-based explanations in deep neural networks",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Learning transferable visual models from natural language supervision",
                    "Multi-dimensional concept discovery (mcd) : A unifying framework with completeness guarantees",
                    "Training compute-optimal large language models",
                    "Interpreting clip with sparse linear concept embeddings (splice",
                    "Language models are few-shot learners",
                    "OBELICS: An open web-scale filtered dataset of interleaved image-text documents",
                    "ep-alm: Efficient perceptual augmentation of language models",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Multimodal neurons in pretrained text-only transformers",
                    "What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods",
                    "Improved baselines for data-efficient perceptual augmentation of llms"
                ],
                "related_work": "2Related workLarge Multimodal Models (LMMs) Large language models (LLMs) [8,20,41,31]have emerged as the cornerstone of contemporary multimodal models. Typical large multimodal models (LMMs) [1,4,24,25]comprise three components: LLMs, visual encoders, and light-weight connector modules to glue the two models. Remarkably, recent works have demonstrated that by keeping all pretrained models frozen and training only a few million parameters in the connector (e.g., a linear layer) , LLMs can be adapted to understand images, videos, and audios[40,42,22,29,12], thus paving the way for solving multi-modal tasks. However, there is still a lack of effort aimed at understanding why such frozen LLMs can generalize to multimodal inputs. In this study, we try to decode the internal representation of LLMs when exposed to multimodal inputs.Concept activation vector based approachesConcept based interpretability aim to extract the semantic content relevant for a model[9]. For post-hoc interpretation of pretrained models, concept activation vector (CAV) based approaches[21,17,44,43,45,15]have been most widely used. The idea of CAV was first proposed byKim et al. [21]. They define a concept as a set of user-specified examples. The concept is represented in the activation space of deep layer of a CNN by a hyperplane that separates these examples from a set of random examples. This direction in the activation space is referred to as the concept activation vector. Built upon CAV, ACE[17]automate the concept extraction process. CRAFT[15]proposed to learn a set of concepts for a class by decomposing activations of image crops via non-negative matrix factorization (NMF) . Recently,Fel et al. [14]proposed a unified view of CAV-based approaches as variants of a dictionary learning problem. However, these methods have only been applied for interpretation of CNNs on classification tasks. LMMs on the contrary exhibit a different architecture. We propose a dictionary learning based concept extraction method, designed for LMMs. We also propose a Semi-NMF variant of the dictionary learning problem, which has not been previously considered for concept extraction.Understanding VLM/LMM representationsThere has been an increasing interest in understanding internal representations of visual-language models (VLM) like CLIP through the lens of multimodality.Goh et al. [18]for instance discover neurons termedmultimodal, that activate for certain conceptual information given images as input. Recently proposed TEXTSPAN[16]and SpLiCE[7], aim to understand representations in CLIP[36]by decomposing its visual representations on textual representations. For LMMs,Palit et al. [32]extend the causal tracing used for LLMs to analyze information across different layers in an LMM.Schwettmann et al. [39]first proposed the notion ofmultimodal neuronsexisting within the LLM part of an LMM. They term the neurons  \"multimodal \" as they translate high-level visual information to corresponding information in text modality. The neurons are discovered by ranking them by a gradient based attribution score.Pan et al. [33]recently proposed a more refined algorithm to identify such neurons based on a different neuron importance measure that leverages architectural information of transformer MLP blocks. Instead, we propose to discover a concept structure in the token representations by learning a small dictionary of multimodally grounded concepts. Limiting the analysis to a specific token of interest allows our method to discover fine details about the token in the learnt concepts.",
                "abstract": "Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as \"multi-modal concepts\". We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. We will publicly release our code."
            },
            {
                "name": "Evaluating the Reliability of Self-Explanations in Large Language Models",
                "arxiv_id": "2407.14487",
                "subtitles": [
                    "Local Explainability for Transformers",
                    "Attention-Based Explanations",
                    "Gradient-Based Explanations",
                    "Counterfactual Explanations",
                    "Rationale-Based Explanations"
                ],
                "reference": [
                    "Understanding neural networks through representation erasure",
                    "On exploring attention-based explanation for transformer models in text classification",
                    "\"why should i trust you?\": Explaining the predictions of any classifier",
                    "Transformer interpretability beyond attention visualization",
                    "Mistral 7b",
                    "Explaining nonlinear classification decisions with deep taylor decomposition",
                    "Counterfactual explanations and algorithmic recourses for machine learning: A review",
                    "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Interpretation of prediction models using the input gradient",
                    "Axiomatic attribution for deep networks",
                    "Rationalization for explainable nlp: a survey",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "The falcon series of open language models",
                    "A unified approach to interpreting model predictions",
                    "Gemma: Open models based on gemini research and technology",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Llama 3 model card",
                    "Language models are few-shot learners",
                    "Are self-explanations from large language models faithful",
                    "Attention is all you need",
                    "Explainability for large language models: A survey 15(2) (feb",
                    "Quantifying attention flow in transformers",
                    "Recursive deep models for semantic compositionality over a sentiment treebank",
                    "Towards understanding in-context learning with contrastive demonstrations and saliency maps",
                    "Learning important features through propagating activation differences",
                    "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                    "Can large language models explain themselves? a study of llm-generated self-explanations"
                ],
                "related_work": "2 Related Work 2.0.1Local Explainability for Transformers:In the scope of this work, we define LLMs as pre-trained text-to-text processing systems, based on the Transformer architecture[31]. As such systems usually complete an input text by iteratively predicting the next token, we use the following notation throughout this paper: t_{n+1}=\\mathrm{LLM}(t_{0},...,t_{n}) Specifically, LLMs consist of an embedding layer h^{(0) }_{i}=f^{\\mathrm{In}}(t_{i},i),computing the input embedding \\textbf{h}^{(0) }=[h^{(0) }_{0},...,h^{(0) }_{n}], followed by L transformer \\textbf{h}^{(l+1) }=f^{(l) }(\\textbf{h}^{(l) }), and a head t_{n+1}=f^{\\mathrm{out}}(\\textbf{h}^{(L) }). Each of the transformer blocks uses multi-head attention which we will explain in more detail later.Modern Transformers can be divided into three sub-architectures: encoder-only[7,17], encoder-decoder[23,12], and decoder-only[29,2]. Decoder-only LLMs have demonstrated good classification abilities, even without additional fine-tuning, using  \"in-context learning \": By simply asking the LLM to classify a sample text provided in the input text or  \"prompt \" along with a list of possible classes, LLMs can successfully solve many reasoning tasks. This method called  \"zero-shot prompting \" can be extended to  \"few-shot prompting \"[5]for more difficult tasks by additionally including a small number of labeled samples in the prompt. Recently,instruction-tuningimproves further the performance[29,2].In this paper, we focus on local explainability. This means we want to explain specific predictions of the Transformer rather than explain how the model works in general. Since the first publication in 2017, different methods for generating such explanations for the classification output of Transformers, and therefore LLMs, have been proposed. These are heavily dependent on the classification paradigm[36]: in general deep learning, which is often extendable to Transformers, the literature shows feature attribution approaches (e.g. relevance-propagation-based[4,20]or gradient-based[9,28]) . For traditional applications where the Transformer is fine-tuned to produce class probabilities via a task-specific output layer, we are aware of attention-based[1]or mixed[6,16]approaches. In prompting, specifically for instruction-tuned models, the literature mainly contains methods for generating textual explanations such as Chain-of-Thought (CoT) [33,36]. Independent of the applied paradigm, surrogate-based model-agnostic approaches, such as LIME[25]and SHAP[18], can be found in literature[14,36]. Next, we introduce the most important types of explanations.2.0.2Attention-Based Explanationsleverage the Transformer's scaled dot product attention weightsA(l) superscript A^{(l) }, generated during the forward pass, to explain the impact of each input token to each output token. Given an input vector \\mathbf{h}^{(l) }\\in\\mathbb{R}^{n}, the self-attention variant, which is applied in all Transformers used in this paper, is computed by feeding \\mathbf{h}^{(l) } through three linear layers, computing the vectors \\mathbf{q}^{(l) }, \\mathbf{k}^{(l) }, and \\mathbf{v}^{(l) }, and then: A^{(l) }=\\mathrm{softmax}\\left(\\frac{\\mathbf{q}^{(l) }\\cdot{\\mathbf{k}^{(l) }}^{T% }}{\\sqrt{n}}\\right)  \\mathbf{h}^{\\prime(l) }=A^{(l) }\\cdot\\mathbf{v}^{(l) } This makes A^{(l) }, with all elements \\in[0,1] a weight matrix connecting \\mathbf{h}^{(l) } and \\mathbf{h}^{\\prime(l) }. As Transformers produce one matrix A attention head (e.g.12 \\times 12 heads in BERTbase[7],28 \\times 16 heads in Gemma-7B[29]) extracting meaningful explanations is not trivial: while naive approaches simply use the mean attention weights of the last layer, methods that follow the attention through the whole Transformer have been shown to outperform them[1]. Attention-based explanations can be improved by combining them with gradient-based methods to estimate the importance of each head towards the prediction[6], the crucial step lies in connecting attention weights in the last attention layer to the output, as the residual connections within the Transformer keep input to output association stable over multiple layers[16]. As there is a debate in the literature about whether attention weights can be used as explanations, we also employ gradient-based explanations.2.0.3Gradient-Based Explanationscreate saliency maps of the input by computing the gradient \\frac{\\partial\\mathrm{LLM}(\\cdot) }{\\partial h^{(0) }_{i}},0\\leq i\\leq n of the n+1^{th} of the LLM with regard to a specific input embedding h^{(0) }_{i}. In the simplest case, this gradient itself can be the explanation[9], but the literature shows that computing the Hadamard product with the input improves on it[26]. An often discussed problem of gradient-based approaches is the so-called saturation problem[26,28]: as neural networks minimize the absolute gradient during training, gradients of a well-fitted network will be close to zero. We argue, however, that the ambiguous nature of natural language prevents overfitting and therefore also, to a certain degree, gradient saturation of pre-trained multi-purpose LLMs. To support this theory, we provide statistics on the gradients for each of our experiments.2.0.4Counterfactual Explanationsare - simply put - versions of the model input that alter the model's output. A good counterfactual should fulfill at least the following two criteria[32]:(i) validity:the model output between the counterfactual and the original input should differ at inference time.(ii) similaritythe changes made to the original to produce the counterfactual should be minimal: the more that is changed from the original, the less specific the counterfactual becomes, eventually making it irrelevant as a local explanation. However, defining a good distance measure for comparing two texts is non-trivial, and may well require the combination of both semantic and syntactic similarity. Therefore, this second point is sometimes overlooked in similar studies[19].2.0.5Rationale-Based Explanationscan be described as textual excerpts or abstractions of the model input that contribute to the model's predictions[8]. In contrast to feature attribution methods, these do not provide a measure for the importance of each token, but rather a text that describes the influences. While traditional methods rely on extraction or abstraction methods for generating those texts[8], LLMs can be prompted to provide explanations. CoT generates textual rationales at inference time and even boosts reasoning performance[33], while additional prompting forself-explanationshas recently received increased attention in the research community:Huang et al.[10]prompt ChatGPT to yield feature importance scores for a sentiment classification task based on 100 random texts taken from the Stanford Sentiment Treebank dataset[27]. Then, they evaluate the faithfulness of the generated scores compared to LIME[25]and occlusion[13]. They conclude that none of the three methods has a clear advantage over the others and that the methods often do not agree on feature importance. However, this ambiguity may be owed to their relatively small sample size. Furthermore, prompting an LLM to produce numerical importance scores for each token is not what these models are designed to produce.Madsen et al.[19]instead prompt for the most important words, counterfactuals, and redactions (i.e. asking the model to mask important tokens) for classification tasks on different datasets using Llama2[30], Falcon[3], and Mistral[11]. They conclude that faithfulness of explanations is highly dependent both on the choice of the LLM and on the data used. The authors, however, do not compare the extracted self-explanations to established explainability methods or human annotations.We extendthe previously discussed work by addressing the following questions: RQ1:Do LLMself-explanationscorrelate well with human judgment? RQ2:Do LLMself-explanationscorrelate well with internal model dynamics, represented by attention- and gradient-based explainability methods?",
                "abstract": "This paper investigates the reliability of explanations generated by large language models (LLMs) when prompted to explain their previous output. We evaluate two kinds of such self-explanations - extractive and counterfactual - using three state-of-the-art LLMs (2B to 8B parameters) on two different classification tasks (objective and subjective). Our findings reveal, that, while these self-explanations can correlate with human judgement, they do not fully and accurately follow the model's decision process, indicating a gap between perceived and actual model reasoning. We show that this gap can be bridged because prompting LLMs for counterfactual explanations can produce faithful, informative, and easy-to-verify results. These counterfactuals offer a promising alternative to traditional explainability methods (e.g. SHAP, LIME), provided that prompts are tailored to specific tasks and checked for validity."
            },
            {
                "name": "Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations",
                "arxiv_id": "2407.08983",
                "subtitles": [
                    "Interpretability",
                    "Related Work on Interpretability in NLP",
                    "Related Work on Interpretability in SE",
                    "Trustworthiness"
                ],
                "reference": [
                    "A Survey on Open Source Software Trustworthiness",
                    "Explaining Transformer-based Code Models: What Do They Learn? When They Do Not Work",
                    "Explainable ai for android malware detection: Towards understanding why the models perform so well",
                    "PyExplainer: Explaining the Predictions of Just-In-Time Defect Models",
                    "Beyond word importance: Contextual decomposition to extract interactions from lstms",
                    "Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps",
                    " \" Why should i trust you? \" Explaining the predictions of any classifier",
                    "Probing Pretrained Models of Source Code",
                    "A Unified Approach to Interpreting Model Predictions",
                    "Quality and Trust in LLM-generated Code",
                    "On the reliability and explainability of language models for program generation",
                    "VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types",
                    "TrustLLM: Trustworthiness in Large Language Models",
                    "Axiomatic attribution for deep networks",
                    "Explainable AI for SE: Challenges and Future Directions",
                    "Counterfactual explanations for models of code",
                    "Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement",
                    "Towards A Rigorous Science of Interpretable Machine Learning",
                    "Trust in automation: Designing for appropriate Reliance"
                ],
                "related_work": "2.Background & Related WorkIn this section, we present background on interpretability and trustworthiness as complementary terms for generating syntax-grounded post hoc (e.g.,generated after training) explanations for LLMs of code.Interpretability.The brittleness of LLMs can be formulated as anincompletenessin problem formalization(Doshi-Velez and Kim,2017) , which means that it is insufficient that models only infer predictions for certain tasks (thewhat?) . The models must also explain how they arrive at such predictions (thewhy?) . To mitigate such incompleteness in problem formalization, the field ofinterpretabilityhas risen to encompass techniques and methods that aim to solve thewhyquestion. Although authors in this field generally use the termsexplainabilityandinterpretabilityinterchangeably, these definitions are inconsistent throughout the literature(Flora et al.,[n. d.]) . We distinguish between the terms to avoid confusion with the purposes of our approach. We will useexplainabilityfor methods whose goal is to understand how a LLM operates and comes to a decision by exploring inner mechanisms or layers. Conversely, we will useinterpretabilityfor methods that defineconceptual mapping mechanismswhose goal is to contextualize models' predictions by associating them with an understandable concept, which in this paper is the syntax of programming languages.Related Work on Interpretability in NLP.There are existing techniques in both natural language processing (NLP) and SE literature focused on interpretability, including LIME(Ribeiro et al.,2016a) , Kernel SHAP(Lundberg and Lee,2017) , Integrated Gradient(Sundararajan et al.,2017) and Contextual Decomposition(Murdoch et al.,2018) . These techniques generally try to approximate an interpretable model that either attempts to attribute meaning to hidden representations of neural networks, or illustrate the relationship between input features and model performance. However, we argue that such techniques are difficult to make practical in the context of LLMs for code, given the lack of conceptual mappings explained earlier. However, the most closely related interpretability technique to ASTrust, and one of the only to have adapted to LLMs of code is that ofprobingwhich is a supervised analysis to determine which type of parameters (e.g.,input code snippets, tokenization process, number of hidden layers, and model size) influence the learning process in ML models(Troshin and Chirkova,2022) . Probing aims to assess whether hidden representations of LLMs encode specific linguistic properties such as syntactic structures of programming languages. Given our generated visualizations, there may be an inclination to characterize ASTrustas aprobing technique. However, it is important to note that ASTrustis focused on estimating thecorrectnessof predicted syntactic code elements rather than mapping meaning to internal model representations of data.Related Work on Interpretability in SE.In the realm of SE research, prior work has taken two major directions: (i) techniques for task-specific explanations(Fu et al.,2023; Liu et al.,2022; Pornprasit et al.,2021) , and (ii) empirical interpretability studies using existing NLP techniques(Liu et al.,2024; Tantithamthavorn et al.,2023; Mohammadkhani et al.,2023a) . Previous authors have proposed techniques for explaining specific tasks including vulnerability explanation(Fu et al.,2023) , vulnerability prediction for Android(Liu et al.,2022) , and defect prediction models(Pornprasit et al.,2021) . More recently Liu et al. conducted large empirical study using existing explainability techniques for global explanations of code to better understand generative language models of code(Liu et al.,2024) . Mohammadkhani et al. conducted a study using LLM's attention mechanism to interpret their performance on generating code. Finally, one paper that proposed a code-specific interpretability technique is that of Cito et al.(Cito et al.,2022) who formulated a method to generate explanations using counterfactual reasoning of models. Our work on ASTrustcomplements this body of past work by developing anew, generally applicable interpretability methodthat can be applied tobothlocal and global explanations of code, which no prior study or technique has done.Trustworthiness.This research is inspired by definitions oftrustfrom automated systems, SE, and NLP. In automated systems, trust is defined as \"the attitude that an agent will help achieve an individual's goal in a situation characterized by uncertainty and vulnerability \"(Lee and See,2004) . Bianco et al. define software trust as the degree of confidence when the software meets certain requirements(del Bianco et al.,2011) . In NLP, Sun et al. argue that LLMs must appropriately reflect truthfulness, safety, fairness, robustness, privacy, machine ethics, transparency, and accountability for them to be trustworthy(Sun et al.,2024) . We define trust as the confidence that practitioners and researchers have in LLMs' code prediction, anticipating that these predictions will effectively align with their intended goals. Trustworthiness in LLMs implies a sense of interpretability in a given LLM's performance, instilling confidence among practitioners in their abilities to perform code-related tasks. To the best of our knowledge, no paper proposes a concrete definition of trust based on interpretability within the SE research community. Yet, several researchers have called for the importance of trustworthiness in LLMs for code(Lo,[n. d.]; Spiess et al.,2024) . In our work we present a concrete definition of trustworthiness, highlight its importance, and show how syntax-grounded explanations such as ASTrustcontribute to more trustworthy LLMs.",
                "abstract": "Trustworthiness and interpretability are inextricably linked concepts for LLMs. The more interpretable an LLM is, the more trustworthy it becomes. However, current techniques for interpreting LLMs when applied to code-related tasks largely focus on accuracy measurements, measures of how models react to change, or individual task performance instead of the fine-grained explanations needed at prediction time for greater interpretability, and hence trust. To improve upon this status quo, this paper introduces ASTrust, an interpretability method for LLMs of code that generates explanations grounded in the relationship between model confidence and syntactic structures of programming languages. ASTrust explains generated code in the context of syntax categories based on Abstract Syntax Trees and aids practitioners in understanding model predictions at both local (individual code snippets) and global (larger datasets of code) levels. By distributing and assigning model confidence scores to well-known syntactic structures that exist within ASTs, our approach moves beyond prior techniques that perform token-level confidence mapping by offering a view of model confidence that directly aligns with programming language concepts with which developers are familiar. To put ASTrust into practice, we developed an automated visualization that illustrates the aggregated model confidence scores superimposed on sequence, heat-map, and graph-based visuals of syntactic structures from ASTs. We examine both the practical benefit that ASTrust can provide through a data science study on 12 popular LLMs on a curated set of GitHub repos and the usefulness of ASTrust through a human study."
            },
            {
                "name": "From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI",
                "arxiv_id": "2407.03778",
                "subtitles": [
                    "Commonsense Reasoning Approaches",
                    "LLMs",
                    "Combining LLMs and Reasoning"
                ],
                "reference": [
                    "Commonsense reasoning in and over natural language",
                    "Atomic: An atlas of machine commonsense for if-then reasoning",
                    "Logical formalizations of commonsense reasoning: A survey",
                    "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
                    "Large language models are zero-shot reasoners",
                    "CYC: A large-scale investment in knowledge infrastructure",
                    "Mathematical capabilities of chatgpt",
                    "Long short-term memory",
                    "Qualitative process theory",
                    "Introducing meta llama 3: The most capable openly available llm to date",
                    "Neural-Symbolic Cognitive Reasoning",
                    "Cyc",
                    "Gemma: Open models based on gemini research and technology",
                    "ConceptNet",
                    "Mathematics, word problems, common sense, and artificial intelligence",
                    "Differentiable logic machines",
                    "Symbolic knowledge extraction from trained neural networks: A sound approach",
                    "Adimen-SUMO: Reengineering an ontology for first-order reasoning",
                    "Language models are few-shot learners",
                    "Llama: Open and efficient foundation language models",
                    "Programs with common sense",
                    "Large language models still can't plan (a benchmark for llms on planning and reasoning about change",
                    "Language models are unsupervised multitask learners",
                    "Commonsense reasoning using theorem proving and machine learning",
                    "Dissociating language and thought in large language models",
                    "Gpt"
                ],
                "related_work": "2Related Work 2.1Commonsense Reasoning Approaches Commonsense reasoning is a difficult task for a computer to handle [41]. To address this problem, various approaches have been followed in the past. McCarthy [30] was the first who outlined the basic approach of representing commonsense knowledge with predicate logic. Symbolic logic approaches were the main representation type, see e.g. [13, 25]. While still in use today [7] for this extremely complex task to work well it requires a large amount of additional logical scaffolding to precisely define the terms used in the statement and their interrelationships [27]. There is a big gap between the logical approach with deductive reasoning and human reasoning, which is largely inductive, associative, and empirical, i.e., based on former experience. Human reasoning, in contrast to formal logical reasoning, does not strictly follow the rules of classical logic. There have been efforts to utilize an approach which uses an automatic theorem prover (that allows to derive new knowledge in an explainable way), large existing ontologies with background knowledge, and recurrent networks with long short-term memory (LSTM) [20] but still did not stand out much from the baseline [41]. Recent efforts to acquire and represent commonsense knowledge resulted in large knowledge graphs, acquired through extractive methods [43] or crowdsourcing [39]. Some approaches use supervised training on a particular knowledge base, e.g., ConceptNet for commonsense knowledge. ConceptNet is a crowd-sourced database that represents commonsense knowledge as a graph of concepts connected by relations [43]. Interestingly, LLMs (cf. Section 2.2) do not contain any explicit semantic knowledge or grammatical let alone logical rules that would allow an explicit reasoning process, not even the large ontologies from the logical knowledge representation like Cyc [26] or Adimen-SUMO [1]. A way out might be to have neural networks learn reasoning explicitly, possibly by focusing on certain sentence forms as in syllogistic reasoning maybe implemented with neural-symbolic cognitive reasoning by specifically structured neural networks [16, 17, 55]. In contrast to simple deep learning, information from different places and/or documents must be merged here in any case. It does not suffice to investigate any local text properties, e.g., determining the text form. There are different types of commonsense reasoning, e.g., causal, temporal, physical, social etc. (see Section 3.1), each with its unique characteristics and applications. Understanding how these different reasoning tasks can be solved with LLMs is essential. 2.2LLMs In the past, most deep learning methods used supervised learning and therefore required substantial amounts of manually labelled data. Recent research has shown that learning good representations in an unsupervised fashion can provide a significant performance boost. The capacity of LLMs is essential to the success of zero-shot task transfer [37]. An example of a premier LLM that can handle a wide range of natural language processing tasks is OpenAI's GPT-3 [4]. GPT-3 (Generative Pre-trained Transformer) is a third-generation, autoregressive language model that uses unsupervised learning to produce human-like text. In our further investigation, we will focus on version GPT-3.5 with 175B parameters, as this chatbot based model is regarded as one of the most groundbreaking LLMs. We further utilise the open-source model Meta-Llama-3-70B-Instruct by Meta. This is a instruction-tuned version with 70 billion parameters [48, 32]. Nevertheless, we use the very lightweight model with only 7 billion parameters \"Gemma-1.1-7b-it\" by Google. This model is open-source and built for responsible AI development based on the same research and technology used to create Gemini models [47]. We carefully chose these 3 LLMs, as they are state-of-the-art, differ in size (see Table 1) and are mostly open-source. 2.3Combining LLMs and Reasoning Reasoning is one of the most actively discussed and debated capabilities of LLMs. However, it is important to note that proficiency in language does not necessarily equate to strong reasoning abilities in models [29]. Experts continue to debate the ability of LLMs to reason effectively in zero-shot scenarios [12]. While some researchers argue that LLMs demonstrate satisfactory zero-shot reasoning capabilities [22], others contend that the models struggle with planning and reasoning tasks [49]. ChatGPT, for example, is often seen as an unreliable reasoner due to issues such as hallucinations, a problem common to many LLMs [2]. Specifically, ChatGPT-3.5 faces significant challenges in certain areas, such as mathematics [12]. The mathematical performance of both GPT-3.5 and GPT-4 falls well below the level of a graduate student [14]. Attempts to combine elementary mathematics with commonsense reasoning have shown that no existing AI systems can reliably solve these problems [8]. Thus, we investigate the reasoning ability of different current LLMs in a fine-grained manner, which includes causal, temporal, comparative, physical, social, numerical reasoning, etc., via question-answering tasks.",
                "abstract": "Commonsense reasoning is a difficult task for a computer, but a critical skill for an artificial intelligence (AI). It can enhance the explainability of AI models by enabling them to provide intuitive and human-like explanations for their decisions. This is necessary in many areas especially in question answering (QA), which is one of the most important tasks of natural language processing (NLP). Over time, a multitude of methods have emerged for solving commonsense reasoning problems such as knowledge-based approaches using formal logic or linguistic analysis. In this paper, we investigate the effectiveness of large language models (LLMs) on different QA tasks with a focus on their abilities in reasoning and explainability. We study three LLMs: GPT-3.5, Gemma and Llama 3. We further evaluate the LLM results by means of a questionnaire. We demonstrate the ability of LLMs to reason with commonsense as the models outperform humans on different datasets. While GPT-3.5's accuracy ranges from 56% to 93% on various QA benchmarks, Llama 3 achieved a mean accuracy of 90% on all eleven datasets. Thereby Llama 3 is outperforming humans on all datasets with an average 21% higher accuracy over ten datasets. Furthermore, we can appraise that, in the sense of explainable artificial intelligence (XAI), GPT-3.5 provides good explanations for its decisions. Our questionnaire revealed that 66% of participants rated GPT-3.5's explanations as either \"good\" or \"excellent\". Taken together, these findings enrich our understanding of current LLMs and pave the way for future investigations of reasoning and explainability."
            },
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "subtitles": [
                    "LLM Benchmarks",
                    "Risks of Static Benchmarks",
                    "Ranking System",
                    "Human Preference Dataset"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related WorkLLM Benchmarks.We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU(Hendrycks et al.,2020) , HellaSwag(Zellers et al.,2019) , GSM-8K(Cobbe et al.,2021) , BigBench(Srivastava et al.,2023) , AGIEval(Zhong et al.,2023) , and HumanEval(Chen et al.,2021) . Benchmarks focusing on safety, such as ToxicChat(Lin et al.,2023) , and comprehensive suites like HELM(Liang et al.,2022) , also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk(Karpinska et al.,2021; Geng et al.,2023; Wang et al.,2023) . The recent trend includes utilizing GPT-4 for approximating human judgment(Chiang & Lee,2023) , with notable instances being MT-Bench(Zheng et al.,2023b) and AlpacaEval(Li et al.,2023) . In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces(Li et al.,2022; Huang et al.,2023) . They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference(Bai et al.,2022; Ouyang et al.,2022; Touvron et al.,2023) . However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.Risks of Static Benchmarks.Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment(Yang et al.,2023; Oren et al.,2023) . DynaBench(Kiela et al.,2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.Ranking System.Ranking systems have been a well-studied topic in statistics. Related topics include probability models(Hunter,2004; Rao & Kupper,1967) , rank elicitation(Sz\u00f6r\u00e9nyi et al.,2015; Busa-Fekete et al.,2014a,b) , and online experiment design(Chernoff,1992; Karimi et al.,2021) . The Elo rating system has also been used for LLMs(Bai et al.,2022; Boubdir et al.,2023) . Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.Human Preference Dataset.Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant(K\u00f6pf et al.,2023) , HH-RLHF(Bai et al.,2022) , LMSYS-Chat-1M(Zheng et al.,2023a) , and synthetic approximations of human preferences like UltraFeedback(Cui et al.,2023) and Nectar(Zhu et al.,2023) . Our prior data release, LMSYS-Chat-1M(Zheng et al.,2023a) , is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.",
                "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{this https URL}."
            },
            {
                "name": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
                "arxiv_id": "2401.06373",
                "subtitles": [
                    "Optimization",
                    "Side-channel Communication",
                    "Distribution",
                    "Ours: Challenging AI safety by Humanizing LLMs"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Model card and evaluations for claude models",
                    "When consumers and brands talk: Storytelling theory and research in psychology and marketing",
                    "Large language models respond to influence like humans",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Persuasion: Social influence and compliance gaining",
                    "Narrative persuasion",
                    "Persuasion",
                    "Evidence-based advertising using persuasion principles: Predictive validity and proof of concept",
                    "Certifying llm safety against adversarial prompting",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Persuasion and coercion: a critical review of philosophical and empirical approaches",
                    "Frame analysis: An essay on the organization of experience",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "The Dynamics of Persuasion: Communication and Attitudes in the 21st Century",
                    "Social influence: Compliance and conformity",
                    "Rain: Your language models can align themselves without finetuning",
                    "Self-persuasion via self-reflection",
                    "Perspectives on ethics in persuasion",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Scarcity messages",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Low-resource languages jailbreak gpt",
                    "Stanford alpaca: An instruction-following llama model",
                    "What's in my big data",
                    "Multilingual jailbreak challenges in large language models",
                    "The science of persuasion",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                    "Adaptation in dyadic interaction: Defining and operationalizing patterns of reciprocity and compensation",
                    "The earth is flat because ...: Investigating llms' belief towards misinformation via persuasive conversation",
                    "Dark patterns: Past, present, and future: The evolution of tricky user interfaces",
                    "Shining a light on dark patterns",
                    "Self-inference processes: The ontario symposium, vol",
                    "The neural basis of social influence and attitude change",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Logic, emotion, and the paradigm of persuasion",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "The effects of expert and consumer endorsements on audience response",
                    "Credibility: A multidisciplinary framework",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Rumors influence: Toward a dynamic social impact theory of rumor",
                    "Susceptibility to influence of large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Detecting language model attacks with perplexity",
                    "Dark patterns at scale: Findings from a crawl of 11k shopping websites",
                    "Emotional factors in attitudes and persuasion",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
                    " \"he would still be here \": Man dies by suicide after talking with ai chatbot, widow says",
                    "Adversarial demonstration attacks on large language models",
                    "Automatically auditing large language models via discrete optimization",
                    "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
                    "Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests",
                    "Striking a responsive chord: How political ads motivate and persuade voters by appealing to emotions",
                    "The persuasiveness of source credibility: A critical review of five decades' evidence",
                    "Interpersonal influence"
                ],
                "related_work": "2Related WorkAs LLMs become more widely used in real-world applications, jailbreak research efforts have diversified and can be broadly classified into 3 main categories: Optimization, Side-channel Communication, and Distribution-based methods. Figure 2 shows concrete examples of different methods.Optimization-based techniques are at the forefront of jailbreak research and involve three main types: (1) Gradient-Based methods Zou et al. (2023) ; Jones et al. (2023) manipulate model inputs based on gradients to elicit compliant responses to harmful commands; (2) Genetic algorithms-based methods Liu et al. (2023a) ; Lapid et al. (2023) use mutation and selection to explore effective prompts; and (3) Edit-based methods Chao et al. (2023) asks a pre-trained LLM to edit and improve the adversarial prompt to subvert alignment.Side-channel Communication exploits long-tailed distribution to increase jailbreak success rates, such as ciphers Yuan et al. (2023) and translating harmful instructions into low-resource languages Deng et al. (2023b) ; Yong et al. (2023) . Other studies Mozes et al. (2023) ; Kang et al. (2023) use programmatic behaviors, such as code injection and virtualization, to expose LLM vulnerabilities.Distribution-based methods include learning from successful manually-crafted jailbreak templates Deng et al. (2023a) ; Yu et al. (2023) and in-context examples Wei et al. (2023) ; Wang et al. (2023) . Notably, Shah et al. (2023) employs in-context persona to increase LLMs' susceptibility to harmful instructions. While this approach shares some similarities with ours in eliciting harmful outputs via priming and framing, it only represents a small subset of the persuasive techniques we explore.Ours: Challenging AI safety by Humanizing LLMs. Figure 2 compares existing jailbreaking methods and PAP in this study, organized by their degree of humanizing. One line of research treats LLMs as traditional algorithmic systems (i.e., without attributing intelligence or human-like qualities) that take in less interpretable adversarial prompts, while another line views them as simple instruction followers who understand human commands. However, they both ignore the fact that LLMs can understand and conduct complex natural communication Griffin et al. (2023a, b) . Our approach innovatively treats LLMs as human-like communicators and grounds on a taxonomy informed by decades of social science research on human communication. Such an interdisciplinary approach allows us to uncover and address distinct risks related to human-AI interactions, particularly human-driven persuasion-based jailbreak. Moreover, humanizing AI presents other unique risks that can occur unintentionally: for instance, as highlighted by Xiang (2023) , a user's suicide was related to involved conversations with an AI Chatbot. This points out important future directions to further explore the inherent risks associated with AI humanization.",
                "abstract": "Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs"
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "subtitles": [
                    "Multimodal LLMs",
                    "Evaluating Multimodal LLMs",
                    "Visual Encoders",
                    "Ambiguities in Embedding Models"
                ],
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ],
                "related_work": "5Related WorksMultimodal LLMs.We study the limitations of Multimodal LLMs[40,13,30,31,8]and explore possible ways to improve these models. Multimodal LLMs build from pretrained Large Language Models[41,3,58,59,69]and CLIP vision encoder[43,54]. These systems then use an adapter, such as MLPs[30,31], Q-Former[26,8], and gated attention[2,25], to integrate the pretrained CLIP vision encoder into LLMs. More recently, instructBLIP[8], LLaVA-1.5[30]highlight the importance of high-quality training data. Yet, there is a scarcity of research focusing on the impact of visual encoders, which is an important gap our work aims to address through a systematic study.Evaluating Multimodal LLMs.MMVP assesses MLLMs using a set of simple yet critical Visual Question Answering (VQA) questions constructed from CLIP-blind pairs. Previous benchmarks such as TextVQA[52], VQAv2[15], and GQA[21]have centered on traditional VQA queries. Recently, there are works like MM-Vet[64], POPE[27], and MM-Bench[32]designed to specifically evaluate multimodal LLMs including hallucination, reasoning, and robustness. The previous benchmarks and evaluations have shown that Multimodal LLMs can suffer from hallucination[29,28], catastrophic forgetting[67]and lack of robustness[11]. In taking a step back to the fundamentals, our work uncovers that even the most advanced multimodal LLMs, such as GPT-4V[40], Gemini[14], Bard[30], and LLaVA-1.5[30], are not immune to stumbling over elementary visual questions. We also identified part of the problem as being the incapable visual encoder.Visual Encoders.MMVP-VLM provides a detailed analysis of the visual capabilities of various CLIP variants[43,54,62,66]. These models mostly follow the method proposed inRadford et al.[43]that uses contrastive loss to train on large volumes of image-text pairs. They differ in training data[62], training recipes[54], and objective functions[66]. Nonetheless, our studies show that all of these CLIP variants struggle with simple visual patterns such as  \"orientation \",  \"count \",  \"presence of specific features \",etc. Another line of research focuses on vision-only self-supervised learning (SSL) . This category includes contrastive SSL[7,16,5,17]and mask-based SSL[70,18,4]. SLIP[39]explores the synergy between CLIP and contrastive SSL, but focusing primarily on standard classification tasks. In fact, a common practice to evaluate the quality of these vision models is through linear probing or fine-tuning on ImageNet[47,45]. Although current evaluation methods provide a basic level of assessment on representation quality, our findings indicate a growing detachment from the needs of recent use cases. As demonstrated in the MoF experiments in Section4, the CLIP vision model and the vision-only SSL models learn complementary features. However, the linear probing accuracy on ImageNet alone provides a limited understanding of feature utility in MLLMs. This observation suggests the need for more diverse evaluations[61]in visual representation learning, to better align with current and emerging applications.Ambiguities in Embedding Models.Our work exploits CLIP-blind pairs within the CLIP vision embedding space to generate examples of failures in CLIP models and subsequently MLLMs. This concept has ties to previous research focused on documenting failure modes in text embedding models[12,36,55]. More recently,Thrush et al.[56],Yuksekgonul et al.[65]andHsieh et al.[19]study the binding problems CLIP faces in processing text queries, noting that CLIP models treat text input as a bag of words.Tong et al.[57]examines the implications for downstream text-guided generative models.Tschannen et al.[60]suggests image captioners as promising alternatives to CLIP for improving attribute binding. Our work focuses on the visual patterns.",
                "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems."
            },
            {
                "name": "TOFU: A Task of Fictitious Unlearning for LLMs",
                "arxiv_id": "2401.06121",
                "subtitles": [
                    "Question answering",
                    "Realistic goals",
                    "Principled evaluation",
                    "Connection to differential privacy (DP) "
                ],
                "reference": [
                    "Towards adversarial evaluations for inexact machine unlearning",
                    "Propile: Probing privacy leakage in large language models",
                    "The eu general data protection regulation (gdpr",
                    "Remember what you want to forget: Algorithms for machine unlearning",
                    "Detecting pretraining data from large language models",
                    "The brainy student: Scalable unlearning by selectively disobeying the teacher",
                    "Privacy auditing with one (1) training run",
                    "Locating and editing factual associations in gpt",
                    "Ccpa regulations: Final regulation text",
                    "Membership inference attacks against machine learning models",
                    "Certified data removal from machine learning models",
                    "On the necessity of auditable algorithmic definitions for machine unlearning",
                    "Right to be forgotten in the era of large language models: Implications, challenges, and solutions",
                    "Unlearn what you want to forget: Efficient unlearning for llms",
                    "Who's harry potter? approximate unlearning in llms",
                    "Regulation (eu) 2016/679 of the european parliament and of the council",
                    "Evaluating differentially private machine learning in practice",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Auditing differentially private machine learning: How private is private sgd",
                    "Knowledge unlearning for mitigating privacy risks in language models",
                    "Adversary instantiation: Lower bounds for differentially private machine learning",
                    "Kga: A general machine unlearning framework based on knowledge gap alignment",
                    "Editing factual knowledge in language models",
                    "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
                    "Towards unbounded machine unlearning",
                    "A comprehensive study of knowledge editing for large language models",
                    "Machine unlearning",
                    "Can sensitive information be deleted from llms? objectives for defending against extraction attacks"
                ],
                "related_work": "1.1Motivation and Related Work To contextualize our work, it is helpful to consider a private individual who is mentioned in a single article on Wikipedia. LLMs trained on Common Crawl data may be able to correctly answer factual questions about this person and they may wish to have their data removed from an LLM. In fact, regulations around theRight to be Forgottenthat focus on this situation exactly are emerging(Union,2016; OAG,2021; Voigt & Von dem Bussche,2017; Zhang et al.,2023) .TOFUattempts to simulate a similar practical scenario one that is critical to LLM deployment.Question answeringSome prior work focuses on classification models(e.g Guo et al.,2019; Golatkar et al.,2020; Kurmanji et al.,2023a; Wang et al.,2023; Chen & Yang,2023; Pawelczyk et al.,2023) , but with recent advancements in chatbots and instruction-tuned LLMs, we need to shift our attention to question and answer tasks that reflect the way most people interact with LLMs. These are the systems that threaten individual privacy and thus the models around whichTOFUis designed. Recent works that do consider text generation(Chen & Yang,2023; Jang et al.,2022; Kim et al.,2023) are evaluated with limited metrics like perplexity or ROUGE, which do not entirely capture the behaviors of unlearning. Another related line of work is knowledge/model editing(De Cao et al.,2021; Meng et al.,2022; Zhang et al.,2024) , although the aim of this direction is at understanding and manipulating models, rather than preserving privacy.Realistic goalsFor some people like former presidents of the United States, superheroes, or global pop stars, who occur frequently in various documents in the pretraining data, what does it even mean to forget them? Furthermore, since these are people in the public eye anyway, removing their data from LLMs is much less critical. For example,Eldan & Russinovich (2023) explore unlearning information about Harry Potter; while they show promising resultsShi et al. (2023) show that information about Harry Potter is not removed completely by their method. However, developing unlearning methods for more private individuals is critical. Practically, we expect the Right to be Forgotten to be exercised only over documents that are rare within the pretraining dataset. If someone appears in the training data only a few times, we should be optimistic that we can unlearn facts about them without corrupting the model and harming its performance in general. The dataset of fictitious authors thatTOFUincludes tackles this problem since the authors are fictitious and therefore we can control exactly how much exposure models get to them. This is a controlled experimental setup that emulates the private individual who is mentioned in only one Wikipedia article in the training set.Principled evaluationHow can we measure unlearning? Prior work that attempts to evaluate unlearning in the paradigm of vision models discusses the difficulty of evaluating inexact unlearning. In particular, these works consider a combination of forget quality and model utility, each using methods applicable in the classification context(Goel et al.,2022; Thudi et al.,2022; Kurmanji et al.,2023b) . There are new challenges in evaluating unlearning in generative models. (i) There is no single correct answer. Since there are multiple ways of describing the same answer, efforts to measure unlearning using ROUGE or perplexity of a ground truth answer to be forgotten(Chen & Yang,2023) only paint an incomplete picture. AsPatil et al. (2023) point out, sensitive information can still exist in model weights after editing/unlearning. (ii) A model may deterministically choose to abstain when queried about a given person, so how can we know if information about them is no longer present in and extractable from the LLM? (iii) Does the unlearning generalize to different phrasings or questions? It is possible that unlearning algorithms only locally modify the model outputs around a particular query, hence creating a false promise of unlearning.Connection to differential privacy (DP) A principled approach with theoretical backing is to formulate an \\epsilon-\\delta condition that limits how different a model that has undergone unlearning to forget some forget set is from a model trained from scratch on almost the same data but without the forget set(Bourtoule et al.,2021; Sekhari et al.,2021) . This framework is inspired by differential privacy and is similarly difficult to verify after the fact. Many works attempt empirical audits to verify lower bounds on privacy parameters(Shokri et al.,2017; Steinke et al.,2023; Jayaraman & Evans,2019; Jagielski et al.,2020; Nasr et al.,2021) . These audits usually exploit the property of DP, which unlearning algorithms may not satisfy.",
                "abstract": "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."
            },
            {
                "name": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
                "arxiv_id": "2401.01335",
                "subtitles": [
                    "Self-Play",
                    "Synthetic Data for LLMs"
                ],
                "reference": [
                    "Emergent complexity via multi-agent competition",
                    "Rephrase and respond: Let large language models ask better questions for themselves",
                    "Some studies in machine learning using the game of checkers",
                    "Is multiagent deep reinforcement learning the answer or the question? a brief survey",
                    "Rephrase, augment, reason: Visual grounding of questions for vision-language models",
                    "Metamath: Bootstrap your own mathematical questions for large language models",
                    "A generalized training approach for multiagent learning",
                    "Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data",
                    "Stanford alpaca: An instruction-following llama model",
                    "A unified game-theoretic approach to multiagent reinforcement learning",
                    "Gpt-4 technical report",
                    "Scaling relationship on learning mathematical reasoning with large language models",
                    "Tinygsm: achieving> 80% on gsm8k with small language models",
                    "Code llama: Open foundation models for code",
                    "Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction",
                    "Mastering the game of go without human knowledge",
                    "Language models are few-shot learners",
                    "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
                    "Temporal difference learning and td-gammon",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March",
                    "AlphaStar: Mastering the Real-Time Strategy Game StarCraft II",
                    "Language models are unsupervised multitask learners",
                    "Thinking fast and slow with deep learning and tree search",
                    "Textbooks are all you need ii: phi-1.5 technical report"
                ],
                "related_work": "2Related WorkSelf-Play.Self-play(Samuel,1959; Tesauro et al.,1995) , where the algorithm learns by playing against itself, has gained notable attention due to its effectiveness in multi-agent reinforcement learning (MARL) . This method involves agents engaging in interactions with copies of themselves, enabling an increasing level of challenge and complexity within the learning environment. A fundamental work in the field of self-play is AlphaGo Zero(Silver et al.,2017b) , which demonstrated exceptional performance against human players using a self-play learning scheme. Subsequent research has expanded upon the concept of self-play, exploring various adaptations and implementations(Anthony et al.,2017; Lanctot et al.,2017; Bansal et al.,2018; Hernandez-Leal et al.,2018; Muller et al.,2019; Vinyals et al.,2019) . Our method takes the self-play approach akin to AlphaGo Zero, which can convert a weak model to a strong one without additional human-annotated data. While the effectiveness of self-play in MARL is well-established, to our knowledge, our work is the first to apply this approach to the enhancement of LLMs.Synthetic Data for LLMs.In the context of supervised fine-tuning (SFT) of LLMs, human-crafted data has proven to be a remarkably effective source that enhances the performance of LLMs on tasks such as code generation(Roziere et al.,2023; Yang et al.,2023) and mathematical reasoning(Yuan et al.,2023; Luo et al.,2023) . While human data typically exhibits high quality, acquiring sufficient amount of such data poses a challenge in cost. In light of this consideration, the use of synthetic data has become increasingly popular and considered as a proxy for human data. This approach primarily leverages advanced LLMs such as the GPT series(Radford et al.,2019; Brown et al.,2020; OpenAI,2023) as the guidance to generate high-quality data(Josifoski et al.,2023; Taori et al.,2023; Chiang et al.,2023; Li et al.,2023) . Recent research has also highlighted the rephrasing capability of LLMs in prompting for better LLM response(Deng et al.,2023; Prasad et al.,2023) as well as augmenting synthetic data for more effective SFT(Yu et al.,2023; Liu et al.,2023) . In contrast to prior studies that utilized more advanced models for synthetic data generation when pre-training or fine-tuning a target model, our approach directly generates synthetic data from the target model itself.",
                "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available atthis https URL."
            }
        ],
        "survey": {
            "name": "XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models",
            "arxiv_id": "2407.15248",
            "subtitles": [
                {
                    "name": "The Need for Explanations in LLMs",
                    "key_history": [
                        {
                            "reference_title": "exdil: A tool for classifying and explaining hospital discharge letters",
                            "key_word": "Trust and Transparency"
                        },
                        {
                            "reference_title": "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                            "key_word": "Misuse and Critical Thinking Impacts"
                        },
                        {
                            "reference_title": "Ethical and Privacy Concerns",
                            "key_word": "Ethical and social risks of harm from language models"
                        },
                        {
                            "reference_title": "Inaccuracies and Hallucinations",
                            "key_word": "A survey of hallucination in large foundation models"
                        }
                    ],
                    "references_in_this_section": [
                        "The good, the bad, and the explainer: a tool for contrastive explanations of text classifiers",
                        "Financial sentiment analysis: an investigation into common mistakes and silver bullets",
                        "Practical and ethical challenges of large language models in education: A systematic scoping review",
                        "Large language models in ophthalmology scientific writing: Ethical considerations blurred lines or not at all",
                        "exdil: A tool for classifying and explaining hospital discharge letters",
                        "Contrastive explanations of text classifiers as a service",
                        "A survey on xai for cyber physical systems in medicine",
                        "Large language models in medical education: Opportunities, challenges, and future directions",
                        "A comprehensive review on financial explainable ai",
                        "A survey of hallucination in large foundation models",
                        "Ethical and social risks of harm from language models",
                        "A survey on the explainability of supervised machine learning",
                        "Leveraging group contrastive explanations for handling fairness",
                        "ContrXT: Generating contrastive explanations from any text classifier",
                        "Model-contrastive explanations through symbolic reasoning",
                        "Xai for myo-controlled prosthesis: Explaining emg data for hand gesture classification",
                        "A survey on xai and natural language explanations",
                        "Generative ai in eu law: Liability, privacy, intellectual property, and cybersecurity",
                        "The internal state of an llm knows when its lying",
                        "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models"
                    ]
                },
                {
                    "name": "Methodology",
                    "key_history": [
                        {
                            "reference_title": "Conducting systematic literature reviews and systematic mapping studies",
                            "key_word": "Systematic Literature Review (SLR)"
                        }
                    ],
                    "references_in_this_section": [
                        "Changes in evidence for studies assessing interventions for covid-19 reported in preprints: meta-research study",
                        "Model-driven approaches for conversational agents development: A systematic mapping study",
                        "Conducting systematic literature reviews and systematic mapping studies"
                    ]
                },
                {
                    "name": "Application Papers",
                    "key_history": [
                        {
                            "reference_title": "A multiscale visualization of attention in the transformer model",
                            "key_word": "visualisation tool"
                        },
                        {
                            "reference_title": "Techs: Temporal logical graph networks for explainable extrapolation reasoning",
                            "key_word": "Knowledge graph"
                        },
                        {
                            "reference_title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
                            "key_word": "Counterfactual generator"
                        },
                        {
                            "reference_title": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
                            "key_word": "mechanistic interpretability"
                        },
                        {
                            "reference_title": "Visual classification via description from large language models",
                            "key_word": "classification by description"
                        },
                        {
                            "reference_title": "Rethinking with retrieval: Faithful large language model inference",
                            "key_word": "external knowledge"
                        },
                        {
                            "reference_title": "Tbexplain: A text-based explanation method for scene classification models with the statistical prediction correction",
                            "key_word": "fair and reproducible evaluations"
                        },
                        {
                            "reference_title": "Towards understanding in-context learning with contrastive demonstrations and saliency maps",
                            "key_word": "input-label demonstration pairs"
                        },
                        {
                            "reference_title": "Lmexplainer: a knowledge-enhanced explainer for language models",
                            "key_word": "decision-making processes"
                        },
                        {
                            "reference_title": "The unreliability of explanations in few-shot prompting for textual reasoning",
                            "key_word": "few-shot learning"
                        },
                        {
                            "reference_title": "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
                            "key_word": "CoT reasoning"
                        },
                        {
                            "reference_title": "Language in a bottle: Language model guided concept bottlenecks for interpretable image classification",
                            "key_word": "Language Guided Bottlenecks (LaBo)"
                        },
                        {
                            "reference_title": "Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images",
                            "key_word": "visual commonsense reasoning abilities"
                        }
                    ],
                    "references_in_this_section": [
                        "Post hoc explanations of language models can improve language models",
                        "Interpreting language models through knowledge graph extraction",
                        "Chatgraph: Interpretable text classification by converting chatgpt knowledge to graphs",
                        "Visual classification via description from large language models",
                        "Is chatgpt a good causal reasoner? a comprehensive evaluation",
                        "Lmexplainer: a knowledge-enhanced explainer for language models",
                        "Improved logical reasoning of language models via differentiable symbolic programming",
                        "Learning transferable visual models from natural language supervision",
                        "Inseq: An interpretability toolkit for sequence generation models",
                        "Explaining black box text modules in natural language with language models",
                        "Towards understanding in-context learning with contrastive demonstrations and saliency maps",
                        "Explainable automated debugging via large language model-driven scientific debugging",
                        "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
                        "Unifying large language models and knowledge graphs: A roadmap",
                        "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
                        "Interpretability at scale: Identifying causal mechanisms in alpaca",
                        "Language in a bottle: Language model guided concept bottlenecks for interpretable image classification",
                        "The unreliability of explanations in few-shot prompting for textual reasoning",
                        "Techs: Temporal logical graph networks for explainable extrapolation reasoning",
                        "Answering questions by meta-reasoning over multiple chains of thought",
                        "Rethinking with retrieval: Faithful large language model inference",
                        "A multiscale visualization of attention in the transformer model",
                        "Explanations from large language models make small reasoners better",
                        "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
                        "Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images",
                        "Towards automated circuit discovery for mechanistic interpretability",
                        "Tbexplain: A text-based explanation method for scene classification models with the statistical prediction correction",
                        "Chat-rec: Towards interactive and explainable llms-augmented recommender system"
                    ]
                },
                {
                    "name": "Discussion Papers",
                    "key_history": [
                        {
                            "reference_title": "A multiscale visualization of attention in the transformer model",
                            "key_word": "visualisation tool"
                        },
                        {
                            "reference_title": "Trustworthy llms: a survey and guideline for evaluating large language models\u2019 alignment",
                            "key_word": "alignment of LLMs with human values"
                        },
                        {
                            "reference_title": "Ai transparency in the age of llms: A human-centered research roadmap",
                            "key_word": "human-centred perspective"
                        },
                        {
                            "reference_title": "The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges",
                            "key_word": "zero-shot analysis"
                        },
                        {
                            "reference_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                            "key_word": "multimodal science question answering"
                        },
                        {
                            "reference_title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
                            "key_word": "step-by-step reasoning"
                        },
                        {
                            "reference_title": "Explainability for large language models: A survey",
                            "key_word": "Transformer-based models"
                        }
                    ],
                    "references_in_this_section": [
                        "Roscoe: A suite of metrics for scoring step-by-step reasoning",
                        "Ai transparency in the age of llms: A human-centered research roadmap",
                        "The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges",
                        "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                        "Trustworthy llms: a survey and guideline for evaluating large language models\u2019 alignment",
                        "Explainability for large language models: A survey",
                        "Eight things to know about large language models"
                    ]
                }
            ],
            "all_references": [
                "Post hoc explanations of language models can improve language models",
                "Interpreting language models through knowledge graph extraction",
                "Changes in evidence for studies assessing interventions for covid-19 reported in preprints: meta-research study",
                "Is chatgpt a good causal reasoner? a comprehensive evaluation",
                "A survey of large language models",
                "Explainable ai: a narrative review at the crossroad of knowledge discovery, knowledge representation and representation learning",
                "The good, the bad, and the explainer: a tool for contrastive explanations of text classifiers",
                "Seven pillars for the future of artificial intelligence",
                "Visual classification via description from large language models",
                "Chatgraph: Interpretable text classification by converting chatgpt knowledge to graphs",
                "Lmexplainer: a knowledge-enhanced explainer for language models",
                "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                "Bloomberggpt: A large language model for finance",
                "Improved logical reasoning of language models via differentiable symbolic programming",
                "Financial sentiment analysis: an investigation into common mistakes and silver bullets",
                "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
                "Learning transferable visual models from natural language supervision",
                "Ai transparency in the age of llms: A human-centered research roadmap",
                "Conducting systematic literature reviews and systematic mapping studies",
                "Inseq: An interpretability toolkit for sequence generation models",
                "Towards understanding in-context learning with contrastive demonstrations and saliency maps",
                "Explaining black box text modules in natural language with language models",
                "Practical and ethical challenges of large language models in education: A systematic scoping review",
                "Large language models in ophthalmology scientific writing: Ethical considerations blurred lines or not at all",
                "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting",
                "Explainable automated debugging via large language model-driven scientific debugging",
                "Unifying large language models and knowledge graphs: A roadmap",
                "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
                "Interpretability at scale: Identifying causal mechanisms in alpaca",
                "exdil: A tool for classifying and explaining hospital discharge letters",
                "The unreliability of explanations in few-shot prompting for textual reasoning",
                "Contrastive explanations of text classifiers as a service",
                "A survey on xai for cyber physical systems in medicine",
                "Language in a bottle: Language model guided concept bottlenecks for interpretable image classification",
                "Large language models in medical education: Opportunities, challenges, and future directions",
                "Techs: Temporal logical graph networks for explainable extrapolation reasoning",
                "Eight things to know about large language models",
                "Roscoe: A suite of metrics for scoring step-by-step reasoning",
                "Answering questions by meta-reasoning over multiple chains of thought",
                "Beware the rationalization trap! when language model explainability diverges from our mental models of language",
                "A comprehensive review on financial explainable ai",
                "Rethinking with retrieval: Faithful large language model inference",
                "A survey of hallucination in large foundation models",
                "Ethical and social risks of harm from language models",
                "Large language models in medicine",
                "Can ChatGPT's responses boost traditional natural language processing",
                "A multiscale visualization of attention in the transformer model",
                "A survey on the explainability of supervised machine learning",
                "Leveraging group contrastive explanations for handling fairness",
                "Explanations from large language models make small reasoners better",
                "ContrXT: Generating contrastive explanations from any text classifier",
                "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
                "Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images",
                "Towards automated circuit discovery for mechanistic interpretability",
                "The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges",
                "Tbexplain: A text-based explanation method for scene classification models with the statistical prediction correction",
                "Model-contrastive explanations through symbolic reasoning",
                "Xai for myo-controlled prosthesis: Explaining emg data for hand gesture classification",
                "A survey on xai and natural language explanations",
                "Generative ai in eu law: Liability, privacy, intellectual property, and cybersecurity",
                "The internal state of an llm knows when its lying",
                "Model-driven approaches for conversational agents development: A systematic mapping study",
                "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                "The importance of human-labeled data in the era of llms",
                "Explainability for large language models: A survey"
            ]
        },
        "topic_history": [
            {
                "name": "LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation",
                "arxiv_id": "2401.08217",
                "reference": [
                    "U-bert: Pre-training user representations for improved recommendation",
                    "Next basket recommendation with intent-aware hypergraph adversarial network",
                    "Palr: Personalization aware llms for recommendation",
                    "Empowering news recommendation with pre-trained language models",
                    "Large language models are zero-shot rankers for recommender systems",
                    "Reprbert: Distilling bert to an efficient representation-based relevance model for e-commerce",
                    "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                    "Session-based recommendations with recurrent neural networks",
                    "How can recommender systems benefit from large language models: A survey",
                    "Uncovering chatgpt's capabilities in recommender systems",
                    "Recommendation as instruction following: A large language model empowered recommendation approach",
                    "Leveraging large language models in conversational recommender systems",
                    "Ctr-bert: Cost-effective knowledge distillation for billion-parameter teacher models",
                    "Towards unified conversational recommender systems via knowledge-enhanced prompt learning",
                    "A first look at llm-powered generative news recommendation",
                    "Do llms understand user preferences? evaluating llms on user rating prediction",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Session-based recommendation with graph neural networks",
                    "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
                    "Session-based recommendation with hypergraph attention networks",
                    "Training large-scale news recommenders with pretrained language models in the loop",
                    "Language models as agent models",
                    "Generative recommendation: Towards next-generation recommender paradigm"
                ]
            },
            {
                "name": "Dynamic and Adaptive Feature Generation with LLM",
                "arxiv_id": "2406.03505",
                "reference": [
                    "Emergent abilities of large language models",
                    "Learning feature engineering for classification",
                    "Representation learning: A review and new perspectives",
                    "Autoint: Automatic feature interaction learning via self-attentive neural networks",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification",
                    "Cognito: Automated feature engineering for supervised learning",
                    "Group-wise reinforcement feature generation for optimal and explainable representation space reconstruction",
                    "A survey of chain of thought reasoning: Advances, frontiers and future",
                    "Toward causal representation learning",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Generating efficient training data via llm-based attribute manipulation",
                    "Feature engineering for machine learning: principles and techniques for data scientists",
                    "A survey on large language model based autonomous agents",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Prompt engineering with chatgpt: a guide for academic writers",
                    "Gpt-4 technical report",
                    "Automatic feature engineering for answer selection and extraction",
                    "Physics-constrained automatic feature engineering for predictive modeling in materials science",
                    "Towards better chain-of-thought prompting strategies: A survey",
                    "Reasoning with language model is planning with world model",
                    "Active prompting with chain-of-thought for large language models",
                    "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                    "An overview on data representation learning: From traditional feature learning to recent deep learning",
                    "Deep feature generating network: A new method for intelligent fault detection of mechanical systems under class imbalance",
                    "Explorekit: Automatic feature generation and selection",
                    "Language models are few-shot learners",
                    "Techniques for automated machine learning",
                    "Aligning large language models with human: A survey",
                    "Summary of chatgpt-related research and perspective towards the future of large language models",
                    "Deepfm: a factorization-machine based neural network for ctr prediction"
                ]
            },
            {
                "name": "LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools and Self-Explanations",
                "arxiv_id": "2401.12576",
                "reference": [
                    "XMD: An end-to-end framework for interactive explanation-based debugging of NLP models",
                    "A conversational interface for interacting with machine learning models",
                    "iSee: Intelligent sharing of explanation experience by users for users",
                    "Explaining machine learning models with interactive natural language conversations using TalkToModel",
                    "May i ask a follow-up question? understanding the benefits of conversations in neural network explainability",
                    "The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models",
                    "IFAN: An explainability-focused interaction framework for humans and NLP models",
                    "CrossCheck: Rapid, reproducible, and interpretable model evaluation",
                    "ConvXAI: Delivering heterogeneous AI explanations via conversations to support human-AI scientific writing",
                    "InterroLang: Exploring NLP models and datasets through dialogue-based explanations",
                    "Diagnosing AI explanation methods with folk concepts of behavior",
                    "Follow the successful herd: Towards explanations for improved use and mental models of natural language systems"
                ]
            },
            {
                "name": "A Concept-Based Explainability Framework for Large Multimodal Models",
                "arxiv_id": "2406.08074",
                "reference": [
                    "Flamingo: a visual language model for few-shot learning",
                    "Interpreting clip's image representation via text-based decomposition",
                    "Invertible concept-based explanations for cnn models with non-negative concept activation vectors",
                    "Linearly mapping from image to text space",
                    "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav",
                    "Towards vision-language mechanistic interpretability: A causal tracing tool for blip",
                    "Towards automatic concept-based explanations",
                    "A holistic approach to unifying automatic concept extraction and concept importance estimation",
                    "OpenFlamingo: An open-source framework for training large autoregressive vision-language models",
                    "Finding and editing multi-modal neurons in pre-trained transformer",
                    "Grounding language models to images for multimodal generation",
                    "Craft: Concept recursive activation factorization for explainability",
                    "Multimodal neurons in artificial neural networks",
                    "Gpt-4 technical report",
                    "Magma-multimodal augmentation of generative models through adapter-based finetuning",
                    "On concept-based explanations in deep neural networks",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Learning transferable visual models from natural language supervision",
                    "Multi-dimensional concept discovery (mcd) : A unifying framework with completeness guarantees",
                    "Training compute-optimal large language models",
                    "Interpreting clip with sparse linear concept embeddings (splice",
                    "Language models are few-shot learners",
                    "OBELICS: An open web-scale filtered dataset of interleaved image-text documents",
                    "ep-alm: Efficient perceptual augmentation of language models",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Multimodal neurons in pretrained text-only transformers",
                    "What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods",
                    "Improved baselines for data-efficient perceptual augmentation of llms"
                ]
            },
            {
                "name": "Evaluating the Reliability of Self-Explanations in Large Language Models",
                "arxiv_id": "2407.14487",
                "reference": [
                    "Understanding neural networks through representation erasure",
                    "On exploring attention-based explanation for transformer models in text classification",
                    "\"why should i trust you?\": Explaining the predictions of any classifier",
                    "Transformer interpretability beyond attention visualization",
                    "Mistral 7b",
                    "Explaining nonlinear classification decisions with deep taylor decomposition",
                    "Counterfactual explanations and algorithmic recourses for machine learning: A review",
                    "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Interpretation of prediction models using the input gradient",
                    "Axiomatic attribution for deep networks",
                    "Rationalization for explainable nlp: a survey",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "The falcon series of open language models",
                    "A unified approach to interpreting model predictions",
                    "Gemma: Open models based on gemini research and technology",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Llama 3 model card",
                    "Language models are few-shot learners",
                    "Are self-explanations from large language models faithful",
                    "Attention is all you need",
                    "Explainability for large language models: A survey 15(2) (feb",
                    "Quantifying attention flow in transformers",
                    "Recursive deep models for semantic compositionality over a sentiment treebank",
                    "Towards understanding in-context learning with contrastive demonstrations and saliency maps",
                    "Learning important features through propagating activation differences",
                    "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                    "Can large language models explain themselves? a study of llm-generated self-explanations"
                ]
            },
            {
                "name": "Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations",
                "arxiv_id": "2407.08983",
                "reference": [
                    "A Survey on Open Source Software Trustworthiness",
                    "Explaining Transformer-based Code Models: What Do They Learn? When They Do Not Work",
                    "Explainable ai for android malware detection: Towards understanding why the models perform so well",
                    "PyExplainer: Explaining the Predictions of Just-In-Time Defect Models",
                    "Beyond word importance: Contextual decomposition to extract interactions from lstms",
                    "Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps",
                    " \" Why should i trust you? \" Explaining the predictions of any classifier",
                    "Probing Pretrained Models of Source Code",
                    "A Unified Approach to Interpreting Model Predictions",
                    "Quality and Trust in LLM-generated Code",
                    "On the reliability and explainability of language models for program generation",
                    "VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types",
                    "TrustLLM: Trustworthiness in Large Language Models",
                    "Axiomatic attribution for deep networks",
                    "Explainable AI for SE: Challenges and Future Directions",
                    "Counterfactual explanations for models of code",
                    "Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement",
                    "Towards A Rigorous Science of Interpretable Machine Learning",
                    "Trust in automation: Designing for appropriate Reliance"
                ]
            },
            {
                "name": "From Data to Commonsense Reasoning: The Use of Large Language Models for Explainable AI",
                "arxiv_id": "2407.03778",
                "reference": [
                    "Commonsense reasoning in and over natural language",
                    "Atomic: An atlas of machine commonsense for if-then reasoning",
                    "Logical formalizations of commonsense reasoning: A survey",
                    "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
                    "Large language models are zero-shot reasoners",
                    "CYC: A large-scale investment in knowledge infrastructure",
                    "Mathematical capabilities of chatgpt",
                    "Long short-term memory",
                    "Qualitative process theory",
                    "Introducing meta llama 3: The most capable openly available llm to date",
                    "Neural-Symbolic Cognitive Reasoning",
                    "Cyc",
                    "Gemma: Open models based on gemini research and technology",
                    "ConceptNet",
                    "Mathematics, word problems, common sense, and artificial intelligence",
                    "Differentiable logic machines",
                    "Symbolic knowledge extraction from trained neural networks: A sound approach",
                    "Adimen-SUMO: Reengineering an ontology for first-order reasoning",
                    "Language models are few-shot learners",
                    "Llama: Open and efficient foundation language models",
                    "Programs with common sense",
                    "Large language models still can't plan (a benchmark for llms on planning and reasoning about change",
                    "Language models are unsupervised multitask learners",
                    "Commonsense reasoning using theorem proving and machine learning",
                    "Dissociating language and thought in large language models",
                    "Gpt"
                ]
            },
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
                "arxiv_id": "2401.06373",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Model card and evaluations for claude models",
                    "When consumers and brands talk: Storytelling theory and research in psychology and marketing",
                    "Large language models respond to influence like humans",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Persuasion: Social influence and compliance gaining",
                    "Narrative persuasion",
                    "Persuasion",
                    "Evidence-based advertising using persuasion principles: Predictive validity and proof of concept",
                    "Certifying llm safety against adversarial prompting",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Persuasion and coercion: a critical review of philosophical and empirical approaches",
                    "Frame analysis: An essay on the organization of experience",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "The Dynamics of Persuasion: Communication and Attitudes in the 21st Century",
                    "Social influence: Compliance and conformity",
                    "Rain: Your language models can align themselves without finetuning",
                    "Self-persuasion via self-reflection",
                    "Perspectives on ethics in persuasion",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Scarcity messages",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Low-resource languages jailbreak gpt",
                    "Stanford alpaca: An instruction-following llama model",
                    "What's in my big data",
                    "Multilingual jailbreak challenges in large language models",
                    "The science of persuasion",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                    "Adaptation in dyadic interaction: Defining and operationalizing patterns of reciprocity and compensation",
                    "The earth is flat because ...: Investigating llms' belief towards misinformation via persuasive conversation",
                    "Dark patterns: Past, present, and future: The evolution of tricky user interfaces",
                    "Shining a light on dark patterns",
                    "Self-inference processes: The ontario symposium, vol",
                    "The neural basis of social influence and attitude change",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Logic, emotion, and the paradigm of persuasion",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "The effects of expert and consumer endorsements on audience response",
                    "Credibility: A multidisciplinary framework",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Rumors influence: Toward a dynamic social impact theory of rumor",
                    "Susceptibility to influence of large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Detecting language model attacks with perplexity",
                    "Dark patterns at scale: Findings from a crawl of 11k shopping websites",
                    "Emotional factors in attitudes and persuasion",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
                    " \"he would still be here \": Man dies by suicide after talking with ai chatbot, widow says",
                    "Adversarial demonstration attacks on large language models",
                    "Automatically auditing large language models via discrete optimization",
                    "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
                    "Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests",
                    "Striking a responsive chord: How political ads motivate and persuade voters by appealing to emotions",
                    "The persuasiveness of source credibility: A critical review of five decades' evidence",
                    "Interpersonal influence"
                ]
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ]
            },
            {
                "name": "TOFU: A Task of Fictitious Unlearning for LLMs",
                "arxiv_id": "2401.06121",
                "reference": [
                    "Towards adversarial evaluations for inexact machine unlearning",
                    "Propile: Probing privacy leakage in large language models",
                    "The eu general data protection regulation (gdpr",
                    "Remember what you want to forget: Algorithms for machine unlearning",
                    "Detecting pretraining data from large language models",
                    "The brainy student: Scalable unlearning by selectively disobeying the teacher",
                    "Privacy auditing with one (1) training run",
                    "Locating and editing factual associations in gpt",
                    "Ccpa regulations: Final regulation text",
                    "Membership inference attacks against machine learning models",
                    "Certified data removal from machine learning models",
                    "On the necessity of auditable algorithmic definitions for machine unlearning",
                    "Right to be forgotten in the era of large language models: Implications, challenges, and solutions",
                    "Unlearn what you want to forget: Efficient unlearning for llms",
                    "Who's harry potter? approximate unlearning in llms",
                    "Regulation (eu) 2016/679 of the european parliament and of the council",
                    "Evaluating differentially private machine learning in practice",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Auditing differentially private machine learning: How private is private sgd",
                    "Knowledge unlearning for mitigating privacy risks in language models",
                    "Adversary instantiation: Lower bounds for differentially private machine learning",
                    "Kga: A general machine unlearning framework based on knowledge gap alignment",
                    "Editing factual knowledge in language models",
                    "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
                    "Towards unbounded machine unlearning",
                    "A comprehensive study of knowledge editing for large language models",
                    "Machine unlearning",
                    "Can sensitive information be deleted from llms? objectives for defending against extraction attacks"
                ]
            },
            {
                "name": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
                "arxiv_id": "2401.01335",
                "reference": [
                    "Emergent complexity via multi-agent competition",
                    "Rephrase and respond: Let large language models ask better questions for themselves",
                    "Some studies in machine learning using the game of checkers",
                    "Is multiagent deep reinforcement learning the answer or the question? a brief survey",
                    "Rephrase, augment, reason: Visual grounding of questions for vision-language models",
                    "Metamath: Bootstrap your own mathematical questions for large language models",
                    "A generalized training approach for multiagent learning",
                    "Decoding data quality via synthetic corruptions: Embedding-guided pruning of code data",
                    "Stanford alpaca: An instruction-following llama model",
                    "A unified game-theoretic approach to multiagent reinforcement learning",
                    "Gpt-4 technical report",
                    "Scaling relationship on learning mathematical reasoning with large language models",
                    "Tinygsm: achieving> 80% on gsm8k with small language models",
                    "Code llama: Open foundation models for code",
                    "Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction",
                    "Mastering the game of go without human knowledge",
                    "Language models are few-shot learners",
                    "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
                    "Temporal difference learning and td-gammon",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March",
                    "AlphaStar: Mastering the Real-Time Strategy Game StarCraft II",
                    "Language models are unsupervised multitask learners",
                    "Thinking fast and slow with deep learning and tree search",
                    "Textbooks are all you need ii: phi-1.5 technical report"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Prompting PaLM for Translation: Assessing Strategies and Performance",
            "arxiv_id": "2211.09102",
            "isAPA": true,
            "abstract": "Large language models (LLMs) that have beentrained on multilingual but not parallel text exhibit a remarkable ability to translate betweenlanguages. We probe this ability in an in-depthstudy of the pathways language model (PaLM) ,which has demonstrated the strongest machinetranslation (MT) performance among similarlytrained LLMs to date. We investigate variousstrategies for choosing translation examples forfew-shot prompting, concluding that examplequality is the most important factor. Using optimized prompts, we revisit previous assessmentsof PaLM's MT capabilities with more recenttest sets, modern MT metrics, and human evaluation, and find that its performance, whileimpressive, still lags that of state-of-the-art supervised systems. We conclude by providing ananalysis of PaLM's MT output which revealssome interesting properties and prospects forfuture work.",
            "reference": [
                "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022b. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022) : The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics",
                "Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners",
                "Yasmin Moslem, Rejwanul Haque, and Andy Way. 2023. Adaptive machine translation with large language models. arXiv preprint arXiv",
                "Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics",
                "Hyung Won Chung, Thibault F\u00e9vry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. 2020. Rethinking embedding coupling in pre-trained language models. arXiv preprint",
                "Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ond\u0159ej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain",
                "Matt Post. 2018. A call for clarity in reporting BLEU scores",
                "Xavier Garcia and Orhan Firat. 2022. Using natural language prompts for machine translation. arXiv preprint arXiv",
                "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv",
                "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems",
                "Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. 2022. What can transformers learn in-context? a case study of simple function classes. arXiv preprint arXiv",
                "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning",
                "Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. 2022. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv",
                "Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. arXiv preprint arXiv",
                "Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming",
                "Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. 2020. Accelerating large-scale inference with anisotropic vector quantization",
                "Marta R. Costa-juss\u00e0, Eric Smith, Christophe Ropers, Daniel Licht, Javier Ferrando, and Carlos Escolano. 2022. Toxicity in multilingual machine translation at scale. arXiv preprint arXiv",
                "Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023. Chain-of-dictionary prompting elicits translation in large language models. arXiv preprint arXiv",
                "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer",
                "Eleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a haystack: On the role of incidental bilingualism in palm's translation capability. arXiv preprint arXiv",
                "Josef Valvoda, Yimai Fang, and David Vandyke. 2022. Prompting for a conversation: How to control a dialog model? arXiv preprint arXiv",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding",
                "Yutai Hou, Hongyuan Dong, Xinghao Wang, Bohan Li, and Wanxiang Che. 2022. Metaprompting: Learning to learn better prompts. arXiv preprint arXiv",
                "Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022a. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. arXiv preprint arXiv",
                "Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference",
                "Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                "Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, and Jianfeng Gao. 2022. Explaining patterns in data with language models via interpretable autoprompting. arXiv preprint arXiv",
                "Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch. 2022. Bidirectional language models are also few-shot learners. arXiv preprint arXiv",
                "Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-level machine translation with large language models. arXiv preprint arXiv",
                "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022. In-context examples selection for machine translation. arXiv preprint arXiv",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv",
                "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog",
                "Markus Freitag, Isaac Caswell, and Scott Roy. 2019. APE at scale and its implications on MT evaluation biases",
                "Jonathan Pilault, Xavier Garcia, Arthur Bra\u017einskas, and Orhan Firat. 2023. Interactive-chain-prompting: Ambiguity resolution for crosslingual conditional generation with interaction. arXiv preprint arXiv",
                "Daniel Deutsch, Rotem Dror, and Dan Roth. 2021. A statistical analysis of summarization evaluation metrics using resampling methods. Transactions of the Association for Computational Linguistics",
                "Chau Tran, Shruti Bhosale, James Cross, Philipp Koehn, Sergey Edunov, and Angela Fan. 2021. Facebook AI's WMT21 news translation task submission",
                "Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer. 2023. Dictionary-based phrase-level prompting of large language models for machine translation. arXiv preprint arXiv",
                "Longyue Wang, Mu Li, Fangxu Liu, Shuming Shi, Zhaopeng Tu, Xing Wang, Shuangzhi Wu, Jiali Zeng, and Wen Zhang. 2021. Tencent translation system for the WMT21 news translation task",
                "Junyi Li, Tianyi Tang, Jian-Yun Nie, Ji-Rong Wen, and Xin Zhao. 2022b. Learning to transfer prompts for text generation",
                "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv",
                "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv",
                "Sawan Kumar and Partha Talukdar. 2021. Reordering examples helps during priming-based few-shot learning",
                "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation",
                "Suchin Gururangan, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. 2022. Whose language counts as high quality? measuring language ideologies in text data selection. arXiv preprint arXiv",
                "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv",
                "Anonymous. 2023. Does gpt-3 produces less literal translations? Anonymous preprint under review",
                "Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional Quality Metrics (MQM) : A Framework for Declaring and Describing Translation Quality Metrics. Tradum\u00e0tica, pages",
                "Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond\u0159ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espa\u00f1a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT",
                "Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation",
                "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation",
                "Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2022a. Probing via prompting. arXiv preprint arXiv",
                "Andrea Schioppa, Xavier Garcia, and Orhan Firat. 2023. Cross-lingual supervision improves large language models pre-training. arXiv preprint arXiv",
                "Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation",
                "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation",
                "Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of few-shot learning for machine translation. arXiv preprint arXiv",
                "Biao Zhang, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: A case study. arXiv preprint arXiv",
                "Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023. Exploring human-like translation strategy with large language models. arXiv preprint arXiv",
                "Rachel Bawden and Fran\u00e7ois Yvon. 2023. Investigating the translation performance of a large multilingual language model: the case of bloom. arXiv preprint arXiv",
                "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
                "Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv",
                "Alex Jones, Isaac Caswell, Ishank Saxena, and Orhan Firat. 2023. Bilex rx: Lexical data augmentation for massively multilingual machine translation. arXiv preprint arXiv",
                "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv",
                "Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm",
                "Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M Rush. 2022. Interactive and visual prompt engineering for ad-hoc task adaptation with large language models. IEEE transactions on visualization and computer graphics",
                "Xianfeng Zeng, Yijin Liu, Ernan Li, Qiu Ran, Fandong Meng, Peng Li, Jinan Xu, and Jie Zhou. 2021. WeChat neural machine translation systems for WMT",
                "Nuno M Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and Andr\u00e9 FT Martins. 2023. Hallucinations in large multilingual translation models. arXiv preprint arXiv",
                "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv"
            ],
            "related work": "2Related Work Inspired by the findings of Radford et al. (2019) ; Brown et al. (2020) , prompting strategies for LLMs have become a topic of intense interest, generating work across a broad spectrum of methods and applicationsLiu et al. (2021) . A basic distinction can be made betweenhard(explicit text) prompting such as we use, andsoftprompting that seeks to learn embeddings Lester et al. (2021) , activations Li and Liang (2021) ; Hambardzumyan et al. (2021) , or attention weights Liu et al. (2022a) that condition the model to perform a desired task. The latter approach is more expressive and more efficient at inference time, but performance can be sensitive to initialization Hou et al. (2022) , and some techniques require modifications to the model.Hard prompts have the advantage of being easy to interpret and modify. Work in this area includes tools to facilitate development of handcrafted promptsStrobelt et al. (2022) ; Bach et al. (2022) ; algorithms to find optimal prompts through gradient-guided searchShin et al. (2020) or exhaustive search through labelsSchick and Sch\u00fctze (2021) or both labels and templatesGao et al. (2021) ; as well as studies on the effect of example orderKumar and Talukdar (2021) ; Lu et al. (2022) . Hard prompts have also been used to analyze model capabilities Garg et al. (2022) ; Li et al. (2022a) , the role of data Singh et al. (2022) , and the nature of prompting itselfMin et al. (2022) ; Wei et al. (2022) .With few exceptions, e.g.Li et al. (2022b) ; Liu et al. (2022b) ; Valvoda et al. (2022) , early approaches to hard prompting tended to condition on the task rather than the specific input. Our k\\text{NN} approach for conditioning on the input was pioneered byLiu et al. (2022b) , who used RoBERTa embeddings to identify relevant GPT-3 prompts for sentiment, table-to-text, and QA tasks. They found that k\\text{NN} works better than a random-selection baseline, and that the advantage grows as the size of the (domain-controlled) example pool increases.Work on prompting LLMs for MT began with the GPT-3 and PaLM papers Brown et al. (2020) ; Chowdhery et al. (2022) , which adopted similar approaches, comparing 0, 1, and n-shot(Wheren n is 64 for GPT-3 and 5 for PaLM.) random selection of independent sentence pairs from WMT training corpora, and testing on older French, German, and Romanian WMT test sets traditionally used in ML, augmented in PaLM with French \\rightarrow German and Kazakh. For both models, performance increased with number of shots, and n-shotbleuscores were found to be competitive with previous unsupervised SOTA, and in some settings particularly into English supervised SOTA as well.In other early MT work, Reynolds and McDonell (2021) experimented with prompt templates for GPT-3, and found that 0-shot prompts with carefully-chosen templates can outperform n-shot prompts with sub-optimal templates. Garcia and Firat (2022) explored using prompts with mT5 Xue et al. (2021) to control output attributes such as formality, and also examine the effect of using prompt-like natural-language tags during fine-tuning. Patel et al. (2022) proposed autoregressive prompting: concatenating only the first predicted word to a prompt and output prefix at each step.Apr\u00e8s nous, le d\u00e9lugeSince our paper appeared on arXiv in November 2022, there has been a flood of work on using LLMs for MT, which we summarize briefly for completeness. A number of papers Agrawal et al. (2022) ; Zhang et al. (2023) ; Jiao et al. (2023) ; Hendy et al. (2023) investigate prompt quality and source proximity using methods similar to ours but with different LLMs, notably GPT-3.5, GPT-4 and their instruction-tuned counterparts. Their findings are in line with ours, with the exception of Agrawal et al. (2022) , who achieve significant gains using lexical matching augmented with a diversity mechanism to select prompts. Apart from differences in model and setting, a potentially salient discrepancy is their emphasis on BLEU rather than neural metrics to measure performance. Other interesting work that conditions prompts on source segments uses dictionaries to supply translations in low-resource settings Ghazvininejad et al. (2023) ; Lu et al. (2023) , or chain-of-thought inspired prompts that elicit keywords, topic, and related examples from the model itself He et al. (2023) .Further recent work looks at the role of data, attributing LLM MT capabilities to the presence of incidental bilingual examples Briakou et al. (2023) , or showing that parallel data Schioppa et al. (2023) , dictionaries Jones et al. (2023) , or restriction to bilingual settings Garcia et al. (2023) can boost performance in smaller LMs. Another popular line aims at controlling various properties of translations such as formality or use of specified terminology, either statically Garcia et al. (2023) ; Moslem et al. (2023) or with human interaction Pilault et al. (2023) . Finally, there is extensive work on analyzing the translation output of LLMs, generally finding that it is more fluent than accurateHendy et al. (2023) ; Anonymous (2023) , good at handling document context Wang et al. (2023) ; Karpinska and Iyyer (2023) but also prone to problems such as hallucination Zhang et al. (2023) ; Guerreiro et al. (2023) , and frequently sub-par in low-resource settings Zhu et al. (2023) ; Bawden and Yvon (2023) ",
            "date": "2022"
        },
        "topic": "Evaluation of LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Efficient multi-prompt evaluation of LLMs",
                "arxiv_id": "2405.17202",
                "subtitles": [
                    "LLMs' sensitivity to prompt templates",
                    "Efficient evaluation of LLMs",
                    "Item response theory (IRT) "
                ],
                "reference": [
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Label-efficient model selection for text generation",
                    "Efficient benchmarking (of language models",
                    "Statistical theories of mental test scores",
                    "Holistic evaluation of language models",
                    "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
                    "Item response theory",
                    "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning",
                    "Building an evaluation scale using item response theory",
                    "State of what art? a call for multi-prompt llm evaluation",
                    "Handbook of item response theory: Three volume set",
                    "How predictable are large language model capabilities? a case study on big-bench",
                    "tinybenchmarks: evaluating llms with fewer examples",
                    "Anchor points: Benchmarking models with much fewer examples",
                    "The icl consistency test",
                    "Mind your format: Towards consistent evaluation of in-context learning improvements",
                    "Evaluation examples are not equally informative: How should that change NLP leaderboards",
                    "Lmentry: A language model benchmark of elementary language tasks",
                    "Item response theory models in the measurement theory",
                    "Best arm identification for prompt learning under a limited budget",
                    "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
                    "Comparing test sets with item response theory"
                ],
                "related_work": "1.1Related workLLMs' sensitivity to prompt templatesThe sensitivity of Large Language Models (LLMs) to the prompts is well-documented. For example,Sclar et al. (2023) revealed that subtle variations in prompt templates in few-shot settings can lead to significant performance discrepancies among several open-source LLMs, with differences as large as 76 accuracy points in tasks from the SuperNaturalInstructions dataset(Wang et al.,2022) . Additionally, they report that the performance of different prompt templates tends to correlate weakly between models. This finding challenges the reliability of evaluation methods that depend on a single prompt template. To measure LLMs sensitivity, the researchers suggested calculating a  \"performance spread, \" which represents the difference between the best and worst performances observed.Mizrahi et al. (2023) conducted a complementary analysis using state-of-the-art models and subsets of BigBench and LMentry(Srivastava et al.,2022; Efrat et al.,2022) . The authors arrive at similar conclusions with respect to LLMs' sensitivity to the used prompt templates and empirically showed that the LLM ranking considering different formats are usually weakly or intermediately correlated with each other. As a solution to the lack of robustness in LLM evaluation, the authors propose the use of summary statistics, as the average performance, for LLM evaluation. Some other works,e.g.,Voronov et al. (2024) ; Weber et al. (2023b,a) , show that even when in-context examples are given to the models, the prompt templates can have a big impact on the final numbers, sometimes reducing the performance of the strongest model in their analyses to a random guess level(Voronov et al.,2024) . In a different direction,Shi et al. (2024) acknowledges that different prompt templates have different performances and proposes using best-arm-identification to efficiently select the best template for an application at hand. One major bottleneck is still on how to efficiently compute the performance distribution for LLMs over many prompt templates; we tackle this problem.Efficient evaluation of LLMsThe escalating size of models and datasets has led to increased evaluation costs. To streamline evaluations,Ye et al. (2023b) considered minimizing the number oftaskswithin Big-bench(Srivastava et al.,2022) . Additionally,Perlitz et al. (2023) observed that evaluations on HELM(Liang et al.,2022) rely on diversity across datasets, though the quantity of examples currently utilized is unnecessarily large.Perlitz et al. (2023) also highlighted the problems in evaluating with insufficient prompts and called to evaluate on more, suggesting evaluating the typical behavior by sampling prompts and examples together by employing stratified sampling, where subscenarios give the strata; in our work, we also apply stratification but consider prompt templates and examples to give the strata. To accelerate evaluations for classification tasks,Vivek et al. (2023) suggested clustering evaluation examples based on model confidence in the correct class. More recently,Polo et al. (2024) empirically showed that it is possible to shrink the size of modern LLM benchmarks and still retain good estimates for LLMs' performances. Similarly (and in parallel to this work) Ashury-Tahan et al. (2024) recognized unlabeled examples that better distinguish between models or prompts, by analyzing model outputs on them, hence saving costly annotation for them. Despite these advancements in streamlining LLM evaluations, there are no other works that propose a general and efficient method to estimate the benchmark performance of LLMs across prompt templates to the best of our knowledge.Item response theory (IRT) IRT(Cai et al.,2016; Van der Linden,2018; Brzezi\u0144ska,2020; Lord et al.,1968) is a collection of statistical models initially developed in psychometrics to assess individuals' latent abilities through standardized tests but with increasing importance in the fields of artificial intelligence and natural language processing (NLP) . For example,Lalor et al. (2016) used IRT's latent variables to measure language model abilities,Vania et al. (2021) applied IRT to benchmark language models and examine the saturation of benchmarks, andRodriguez et al. (2021) explored various uses of IRT with language models, including predicting responses to unseen items, categorizing items by difficulty, and ranking models. Recently,Polo et al. (2024) suggested using IRT for efficient LLM performance evaluation, introducing the Performance-IRT (pIRT) estimator to evaluate LLMs. Our quantile estimation methodology is built upon pIRT.",
                "abstract": "Most popular benchmarks for comparing LLMs rely on a limited set of prompt templates, which may not fully capture the LLMs' abilities and can affect the reproducibility of results on leaderboards. Many recent works empirically verify prompt sensitivity and advocate for changes in LLM evaluation. In this paper, we consider the problem of estimating the performance distribution across many prompt variants instead of finding a single prompt to evaluate with. We introduce PromptEval, a method for estimating performance across a large set of prompts borrowing strength across prompts and examples to produce accurate estimates under practical evaluation budgets. The resulting distribution can be used to obtain performance quantiles to construct various robust performance metrics (e.g., top 95% quantile or median). We prove that PromptEval consistently estimates the performance distribution and demonstrate its efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench Hard, and LMentry. For example, PromptEval can accurately estimate performance quantiles across 100 prompt templates on MMLU with a budget equivalent to two single-prompt evaluations. Our code and data can be found atthis https URL."
            },
            {
                "name": "MoralBench: Moral Evaluation of LLMs",
                "arxiv_id": "2406.04428",
                "subtitles": [
                    "Moral Foundations Theory",
                    "Large Language Models",
                    "LLMs in Moral Evaluation"
                ],
                "reference": [
                    "Liberals and conservatives rely on different sets of moral foundations",
                    "Moral foundations theory: The pragmatic validity of moral pluralism",
                    "Will morality or political ideology determine attitudes to climate change",
                    "Moral foundations of large language models",
                    "The ghost in the machine has an american accent: value conflict in gpt",
                    "Scaling laws for neural language models",
                    "Aligning ai with shared human values",
                    "The moral foundations of trust",
                    "Moral foundations and political orientation: Systematic review and meta-analysis",
                    "Gpt-3: Its nature, scope, limits, and consequences",
                    "Moral intuitions and political orientation: Similarities and differences between south korea and the united states",
                    "Gpt-3: What's it good for",
                    "Introducing chatgpt",
                    "How words do the work of politics: Moral foundations theory and the debate over stem cell research",
                    "Persistent anti-muslim bias in large language models",
                    "The moral roots of socio-political attitudes: How moral foundations theory can help to understand contested scientific issues and political ideology",
                    "Can machines learn morality? the delphi experiment",
                    "On the dangers of stochastic parrots: Can language models be too big",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Shifting liberal and conservative attitudes using moral foundations theory",
                    "Does moral code have a moral code? probing delphi's moral philosophy",
                    "Language models are unsupervised multitask learners",
                    "Who is gpt-3? an exploration of personality, values and demographics"
                ],
                "related_work": "2Related WorkIn this section, we introduce the most related background and scientific investigations to this work, which are roughly divided into three categories: 1) Moral Foundations Theory, 2) Large Language Models, and 3) LLMs in Moral Evaluation.2.1Moral Foundations TheoryAccording to proposals by social and cultural psychologists, every individual is instinctively equipped with an intuitive ethical sense that guides our feelings of approval or disapproval regarding certain behavioral patterns in humans. Moral Foundations Theory, as posited by Graham et al.[19,17], suggests that a variety of innate moral foundations underpin the rich tapestry of moral judgments and values that vary across cultures, providing a pluralistic framework to comprehend the intricacies of human morality. Since then, the concept of moral foundations has been extensively employed in a variety of research studies, particularly in the examination of political cultures[19,20,17]and the measurement of cooperation[23]arising from differences in values. These studies utilized moral foundations as a robust metric for assessing differences in moral identity among groups and explored whether these differences contribute to divergent viewpoints on topics such as healthcare, climate change, and stem cell research[24,25,26,27,28]. For instance, by evaluating the moral foundations of political groups, researchers can collect insights into how values influence political behaviors and decisions, ultimately affecting societal dynamics[29].2.2Large Language ModelsPioneering language models such as GPT-2[30]and BERT[31], trained on expansive web-text datasets, have led to significant advancements in the field of Natural Language Processing (NLP) . Informed by scaling laws[32], Large Language Models (LLMs) with greater capacity and more extensive training data have been developed, extending the frontier of language processing capabilities. More recent iterations like ChatGPT[33]showcase effective interaction with human guidance and feedback, exhibiting robust proficiency in diverse language related tasks from responding to a wide array of questions and sustaining conversations with users to performing intricate functions such as text polishing and coding assistance. Despite these achievements, there remain critical concerns with LLMs stemming from the voluminous, yet noisy, training datasets; these can lead to the inadvertent generation of biased or harmful content, such as gender and religious prejudices as well as aggressive language[34,35,36,37,38], thereby undermining their reliability and trustworthiness.2.3LLMs in Moral EvaluationOur research focuses on developing a robust metric for assessing moral reasoning within LLMs. A number of studies have attempted to understand whether LLMs can truly discern differences in various moralities and personalities[39], as well as their potential to learn and embody moral values[40]. Meanwhile, Fraser et al.[41]investigated the capacity of machine learning models, particularly the Allen AI Delphi model, to adopt consistent, higher-level ethical principles from datasets annotated with human moral judgments. Their findings suggest that model often aligns with the moral standards of the demographics involved in its training, prompting important reflections on the implications for ethical AI development. More recently, Abdulhai et al.[21]examined the propensity of popular LLMs to display biases toward certain moral questions, using Moral Foundations Theory as a backdrop. Their study provides insights into the similarity between human and LLM moral identity. However, there is no quantitative analysis to evaluate the LLM's moral identity. In this paper, we introduce a novel benchmark to offer a fair evaluation of the LLM's moral identity.",
                "abstract": "In the rapidly evolving field of artificial intelligence, large language models (LLMs) have emerged as powerful tools for a myriad of applications, from natural language processing to decision-making support systems. However, as these models become increasingly integrated into societal frameworks, the imperative to ensure they operate within ethical and moral boundaries has never been more critical. This paper introduces a novel benchmark designed to measure and compare the moral reasoning capabilities of LLMs. We present the first comprehensive dataset specifically curated to probe the moral dimensions of LLM outputs, addressing a wide range of ethical dilemmas and scenarios reflective of real-world complexities.The main contribution of this work lies in the development of benchmark datasets and metrics for assessing the moral identity of LLMs, which accounts for nuance, contextual sensitivity, and alignment with human ethical standards. Our methodology involves a multi-faceted approach, combining quantitative analysis with qualitative insights from ethics scholars to ensure a thorough evaluation of model performance. By applying our benchmark across several leading LLMs, we uncover significant variations in moral reasoning capabilities of different models. These findings highlight the importance of considering moral reasoning in the development and evaluation of LLMs, as well as the need for ongoing research to address the biases and limitations uncovered in our study. We publicly release the benchmark atthis https URLand also open-source the code of the project atthis https URL."
            },
            {
                "name": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "arxiv_id": "2407.10457",
                "subtitles": [
                    "LLM Evaluation",
                    "Decoding Strategy"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Program synthesis with large language models",
                    "Ctrl: A conditional transformer language model for controllable generation",
                    "Measuring massive multitask language understanding",
                    "Measuring mathematical problem solving with the math dataset",
                    "Think you have solved question answering? try arc, the ai2 reasoning challenge",
                    "The curious case of neural text degeneration",
                    "Wildbench: Benchmarking llms with challenging tasks from real users in the wild",
                    "Hierarchical neural story generation",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "Evaluating large language models trained on code",
                    "From live data to high-quality benchmarks: The arena-hard pipeline",
                    "Beam search strategies for neural machine translation",
                    "A learning algorithm for boltzmann machines",
                    "HellaSwag: Can a machine really finish your sentence"
                ],
                "related_work": "6Related WorkLLM EvaluationIn recent years, the development of various benchmarks has significantly advanced the evaluation of LLMs. Benchmarks like MMLU(Hendrycks et al.,2021a) , HellaSwag(Zellers et al.,2019) , and ARC(Clark et al.,2018) have expanded the scope by assessing capabilities across knowledge understanding, and complex reasoning. AlpacaEval(alpaca_eval) , MT-Bench(Zheng et al.,2023) , ArenaHard(Li et al.,2024b) , and WildBench(Lin et al.,2024) , leveraging frontier models as judges, evaluate open-ended instruction-following capabilities. Moreover, GSM8K(Cobbe et al.,2021) , MATH(Hendrycks et al.,2021b) , HumanEval(Chen et al.,2021) , and MBPP(Austin et al.,2021) focus on evaluating math reasoning and code generation capabilities.Due to the costly nature of LLM inference and evaluation process, most evaluations of LLMs rely on a single output per example. In this paper, we aim to explore the impact of various generation configurations, particularly non-deterministic generations, on the performance of LLMs.Decoding StrategyGiven a prompt, LLMs rely on a decoding strategy to auto-regressively generate response. The simplest decoding method, greedy decoding, selects the next token with the highest probability. Beam search(Freitag and Al-Onaizan,2017) , an improved version of greedy search, retains the top-B tokens with the highest probability at each time step. In order to generate diverse responses, non-determinism generation methods, such as Top-k(Fan et al.,2018) and Top-psampling(Holtzman et al.,2020) , randomly picks the next token based on the probability distribution. The temperature parameter serves to balance response quality and diversity(Ackley et al.,1985) . Other decoding parameters, like length and repetition penalties(Keskar et al.,2019) , are also available to further control the generation process.",
                "abstract": "Current evaluations of large language models (LLMs) often overlook non-determinism, typically focusing on a single output per example. This limits our understanding of LLM performance variability in real-world applications. Our study addresses this issue by exploring key questions about the performance differences between greedy decoding and sampling, identifying benchmarks' consistency regarding non-determinism, and examining unique model behaviors. Through extensive experiments, we observe that greedy decoding generally outperforms sampling methods for most evaluated tasks. We also observe consistent performance across different LLM sizes and alignment methods, noting that alignment can reduce sampling variance. Moreover, our best-of-N sampling approach demonstrates that smaller LLMs can match or surpass larger models such as GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This research shows the importance of considering non-determinism in LLM evaluations and provides insights for future LLM development and evaluation."
            },
            {
                "name": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
                "arxiv_id": "2402.08699",
                "subtitles": [
                    "Supervised Code Evaluation Metrics",
                    "Self-Consistency",
                    "Back Translation",
                    "Code Synthesis Benchmarks",
                    "Faithfulness"
                ],
                "reference": [
                    "Improving neural machine translation models with monolingual data",
                    "On multi-modal learning of editing source code",
                    "Evaluating large language models trained on code",
                    "Understanding back-translation at scale",
                    "Automating code review activities by large-scale pre-training",
                    "Competition-level code generation with alphacode",
                    "Code generation as a dual task of code summarization",
                    "OctoPack: Instruction tuning code large language models",
                    "Measuring coding challenge competence with apps",
                    "Faithfulness tests for natural language explanations",
                    "Program synthesis with large language models",
                    "CodeBERTScore: Evaluating code generation with pretrained models of code",
                    "Beyond accuracy: Evaluating self-consistency of code large language models with IdentityChain",
                    "CodeBLEU: a method for automatic evaluation of code synthesis",
                    "Data augmentation using back-translation for context-aware neural machine translation",
                    "A syntactic neural model for general-purpose code generation",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "JuICe: A large scale distantly supervised dataset for open domain context-based code generation",
                    "BLEU: a method for automatic evaluation of machine translation"
                ],
                "related_work": "5Related WorkSupervised Code Evaluation MetricsAccuracy, or how often model-generated code exactly matches reference code, is a metric for evaluating code generation(Agashe et al.,2019; Li et al.,2022b; Yin & Neubig,2017; Chakraborty & Ray,2021) . However, it is overly conservative and significantly underestimates the actual performance, as it is possible to generate semantically correct code without exactly matching a reference (e.g.,different variable names, ordering of statements, or logic) .Agashe et al. (2019) ; Li et al. (2022b) ; Yin & Neubig (2017) ; Wei et al. (2019) among others adopted BLEU(Papineni et al.,2002) . However,Ren et al. (2020) showed that BLEU is not well-suited for evaluating code correctness since it fails to capture the syntax, semantics, and functionality of code. CodeBLEU(Ren et al.,2020) addresses this by augmenting BLEU with syntactic and data flow information. However, CodeBLEU requires the model-generated code to follow the same structure of the reference code, which is still overly conservative penalizing correct code that follows different ordering of statements or logic while still partly relying on BLEU's token overlap score. More recently,Zhou et al. (2023) proposed CodeBERTScore which measures the similarity of the model-generated code and the reference code based on the dot-product similarity of their contextualized vector representations from a pretrained LLM. However, CodeBERTScore does not explicitly evaluate semantic similarity.To capture code's functional correctness, research has moved towards test-based oracles: generated code is executed against predefined test cases, and if the generated code passes these test cases, it is considered correct. This does not require the model-generated code to match the naming, structure, or logic of a reference, but checks if aspects of the functionality are correct. It should be noted that unit tests are partial indicators of functional correctness and cannot guarantee semantic equivalence which in the general case is undecidable.Self-ConsistencySelf-consistency is often used to describe how consistent a model's generated samples are with one another. If a model generates consistent predictions multiple times for the same input, the model is likely more confident that those predictions are correct. Based on this intuition,Wang et al. (2022) designed a decoding strategy using self-consistency to identify the most likely correct answer from a set of samples, which they showed improved the model's ability to do chain-of-thought reasoning. While self-consistency can be used as an uncertainty estimator, it is not always well-suited to evaluate accuracy, as a model can be consistently wrong. In contrast,RTCcan be a more reliable metric since it requires functional correctness which is a stronger signal than consistency.RTCalso relates to IdentityChain ofMin et al. (2023) who propose to measure the self-consistency of a Code LLM via multiple efforts to make a round-trip. In contrast toRTC, IdentityChain still requires an annotated human corpus andMin et al. (2023) argue that IdentityChain measures a distinct quality from conventional accuracy. Instead, we have shown thatRTCis strongly correlated with conventional accuracy for a given benchmark, does not require human annotated examples, and covers multiple tasks.Back TranslationAt a high-level, the forward and backward samples inRTCresemble back translation - a data augmentation technique for machine translation(Sennrich et al.,2016; Edunov et al.,2018; Sugiyama & Yoshinaga,2019) . Back translation commonly does not enforce semantic equivalence and may result in noisy data, when back translation is used to perform data augmentation at scale, it can still be very useful for training since models are robust to some level of noise. Instead our focus is model evaluation.Code Synthesis BenchmarksThe most common benchmarks for code synthesis include HumanEval(Chen et al.,2021) , MBPP(Austin et al.,2021) , APPS(Hendrycks et al.,2021) , and CodeContests(Li et al.,2022a) . These benchmarks have a curated set of input (natural language) specifications and test cases for each example, and this is non-trivial to expand. In contrast to these benchmarks,RTCcan fairly easily be expanded to include new domains (Sec. 4.2) .Finally, the HumanEvalExplain task of HumanEvalPack(Muennighoff et al.,2023) is a special case ofSynthesisRtcwithout forward/backward sampling for HumanEval. In contrast to that benchmark, we acknowledge that tasks like HumanEvalExplain evaluate both synthesis and code description, make them more robust through sampling, and introduce new tasks. Furthermore, we show thatSynthesisRtctasks do actually correlate with widely accepted metrics and thus are worth measuring.FaithfulnessAtanasova et al. (2023) proposed evaluating the faithfulness of natural language explanations based on a model's ability to generate the same output if the explanation is included in the input. The consistency between the outputs are evaluated, since a faithful explanation should guide the model in generating the same output. However, the output does not necessarily have to be correct, and the explanation does not have to be descriptive. The main requirement is that the explanation does not include any information that might lead the model to generate a different output. In contrast,RTCevaluates the correctness between the input and the prediction from the backward pass, and necessitates good predictions in both directions.",
                "abstract": "To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader set of domains and tasks which was not previously possible without costly human annotations."
            },
            {
                "name": "Does Biomedical Training Lead to Better Medical Performance?",
                "arxiv_id": "2404.04067",
                "subtitles": [
                    "Large Language Models in the clinical domain",
                    "Medical evaluation of LLMs"
                ],
                "reference": [
                    "Benchmarking large language models on answering and explaining challenging medical questions",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Dr. bench: Diagnostic reasoning benchmark for clinical natural language processing",
                    "Med42-v2: A suite of clinical llms",
                    "Capabilities of gemini models in medicine",
                    "Meditron3 model card",
                    "Med42 - evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches",
                    "Impact of high-quality, mixed-domain data on the performance of medical language models",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Measuring massive multitask language understanding",
                    "Large language models encode clinical knowledge",
                    "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
                    "Large language models in the clinic: A comprehensive benchmark",
                    "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
                    "LongHealth: A Question Answering Benchmark with Long Clinical Documents",
                    "PubMedQA: A dataset for biomedical research question answering",
                    "Ehrnoteqa: A patient-specific question answering benchmark for evaluating large language models in clinical settings"
                ],
                "related_work": "2Related Work 2.1Large Language Models in the clinical domain With the transformer architecture (Vaswani et al., 2017), automatic language processing gained noticeable traction (Hossain et al., 2019; Wolf et al., 2019), which has led to the development of initial foundation models for not only general language processing (Devlin et al., 2018; Conneau & Lample, 2019; Raffel et al., 2020) but also the specialized medical context (Liu et al., 2021; Yan et al., 2022; Rasmy et al., 2021). This progression has only been accelerated since the reveal of ChatGPT in late 2022 as the potential use for dynamically usable LLMs has been spread through the public. Commercial models such as Palm Chowdhery et al. (2022) and GPT4 (Achiam et al., 2023) seem to deliver great performances for various tasks that at first glance might rival specialists (Yunxiang et al., 2023; Nori et al., 2023; Lee et al., 2023; Thirunavukarasu et al., 2023), but on more thorough investigation show noticeable discrepancies (Lee et al., 2023). Shortly after the release of commercial models, a surge of open-source alternatives like Llama (Touvron et al., 2023; Wu et al., 2023a), Alpaca (Taori et al., 2023) and Mistral (Jiang et al., 2023) followed them. To deliver better performance on medical tasks, these models were pre-trained on medical data (Han et al., 2023; Peng et al., 2023a; b; Labrak et al., 2024). Commercial models can often not be integrated into the clinical routine in many countries due to data privacy concerns Liu et al. (2023); Wang et al. (2023). Thus it is important to gauge which open source models have the best potential to be hosted on-site. In this work, we provide the Clinical Language Understanding Evaluation for LLMs (CLUE), a benchmark that provides insights into the capabilities of LLMs for various tasks in the medical routine, including reasoning, summarization, and question answering. 2.2Medical evaluation of LLMs Evaluation of medical LLMs has predominantly focused on datasets comprising multiple choice exam questions, such as MMLU (Hendrycks et al., 2021), MEDQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), and synthetic questions generated from PubMed articles (Jin et al., 2019). This evaluation paradigm highlights a notable gap in that none of the models were assessed using datasets containing clinical text. Despite the requirement for extensive medical knowledge and reasoning in addressing these quizzes, the nature of the texts vastly differs from that of clinical documents. Clinical documents are characterized by their irregular structure, frequent jargon and abbreviations that vary across medical specialties and regions, and their typically greater length than quiz questions, leading to a less homogeneous domain of texts. Furthermore, recent work has reported severe issues with data contamination of LLM evaluation (Magar & Schwartz, 2022; Shi et al., 2024), specifically with regards to MMLU (Deng et al., 2023), and proposed solutions to mitigate these issues (Li et al., 2024; Deng et al., 2023). In a recent development, Kweon et al. (2024) introduced a multiple-choice question-answering (MCQA) dataset derived from MIMIC-IV Electronic Health Records (EHR), and Adams et al. (2024) released another MCQA dataset based on fictive reports concerning constructed patient cases. However, both studies refrained from evaluating biomedical LLMs on their datasets. Furthermore, DR.BENCH (Gao et al., 2023), a benchmark focusing on clinical documents, predominantly consists of traditional encoder model tasks, such as extractive question-answering, rendering it less suitable for a comprehensive evaluation of LLMs in the context of clinical documents.",
                "abstract": "Large Language Models (LLMs) are expected to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs aim to address healthcare-specific challenges, including privacy demands and computational constraints. Assessing the models' suitability for this sensitive application area is of the utmost importance. However, biomedical training has not been systematically evaluated on medical tasks. This study investigates the effect of biomedical training in the context of six practical medical tasks evaluating $25$ models. In contrast to previous evaluations, our results reveal a performance decline in nine out of twelve biomedical models after fine-tuning, particularly on tasks involving hallucinations, ICD10 coding, and instruction adherence. General-domain models like Meta-Llama-3.1-70B-Instruct outperformed their biomedical counterparts, indicating a trade-off between domain-specific fine-tuning and general medical task performance. We open-source all evaluation scripts and datasets atthis https URLto support further research in this critical area."
            },
            {
                "name": "MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge",
                "arxiv_id": "2406.02919",
                "subtitles": [
                    "Large Language Models on Medical Tasks",
                    "Medical Evaluation Benchmarks"
                ],
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Large language models encode clinical knowledge",
                    "Measuring massive multitask language understanding",
                    "On the generation of medical dialogues for covid",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding",
                    "Towards expert-level medical question answering with large language models",
                    "Cblue: A chinese biomedical language understanding evaluation benchmark",
                    "Overview of the medical question answering task at trec 2017 liveqa",
                    "Med42 - a clinical large language model",
                    "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets",
                    "PubMedQA: A dataset for biomedical research question answering",
                    "Bridging the gap between consumers' medication questions and trusted answers",
                    "Meddialog: Large-scale medical dialogue datasets"
                ],
                "related_work": "2Related WorkLarge Language Models on Medical TasksRecently, some famous LLMs are reported to encode medical knowledge and achieve considerable performance on existing medical benchmarks. General LLMs such as Flan-PaLM and GPT-4 are reported to achieve state-of-the-art performance on multiple datasetsSinghalet al.(2023a) ; Noriet al.(2023b) . For example, they achieve accuracies of 67.6 and 90.2 on a medical exam benchmark MedQAJinet al.(2021) , largely surpassing the prior SOTA models. Several LLMs specially pretrained or finetuned on the medical corpora, such as Med-PaLM, Med-PaLM2Singhalet al.(2023b) , ClinicalCamelTomaet al.(2023) , and Med42Christopheet al.(2023) , are also proposed to address problems in the medical domain and achieve high performance on various medical benchmarks. However, these models cannot tackle problems in real medical scenarios. Our study aims to investigate the gap between the high evaluation performance and the limited practical effectiveness of existing LLMs in the medical domain.Medical Evaluation BenchmarksCurrent medical evaluation benchmarks can be classified into three classes: (1) Question-answering datasets with problems collected from different sources, including medical examsJinet al.(2021) ; Palet al.(2022) ; Hendryckset al.(2020) , scientific literatureJinet al.(2019) , and consumer health questionsBen Abachaet al.(2017,2019) ; Singhalet al.(2023a) ; (2) medical dialogue datasetsZenget al.(2020) ; Yanget al.(2020) ; (3) datasetsPenget al.(2019) ; Zhanget al.(2022) involving conventional NLP tasks (NER, relation extraction, NLI) on medical corpora. Some of these datasets assess LLMs from a single facet. Others evaluate LLMs with multiple tasks, while the tasks are constructed on different groups of knowledge points. In this paper, we design a new evaluation method to evaluate LLMs' mastery of the same knowledge point from multiple facets.",
                "abstract": "Large language models (LLMs) have excelled across domains, also delivering notable performance on the medical evaluation benchmarks, such as MedQA. However, there still exists a significant gap between the reported performance and the practical effectiveness in real-world medical scenarios. In this paper, we aim to explore the causes of this gap by employing a multifaceted examination schema to systematically probe the actual mastery of medical knowledge by current LLMs. Specifically, we develop a novel evaluation framework MultifacetEval to examine the degree and coverage of LLMs in encoding and mastering medical knowledge at multiple facets (comparison, rectification, discrimination, and verification) concurrently. Based on the MultifacetEval framework, we construct two multifaceted evaluation datasets: MultiDiseK (by producing questions from a clinical disease knowledge base) and MultiMedQA (by rephrasing each question from a medical benchmark MedQA into multifaceted questions). The experimental results on these multifaceted datasets demonstrate that the extent of current LLMs in mastering medical knowledge is far below their performance on existing medical benchmarks, suggesting that they lack depth, precision, and comprehensiveness in mastering medical knowledge. Consequently, current LLMs are not yet ready for application in real-world medical tasks. The codes and datasets are available atthis https URL."
            },
            {
                "name": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
                "arxiv_id": "2402.13249",
                "subtitles": [
                    "Factual Consistency Evaluation Benchmarks",
                    "Detecting Hallucinations"
                ],
                "reference": [
                    "Evaluating the factual consistency of abstractive text summarization",
                    "BERTScore: Evaluating text generation with BERT",
                    "Neural text summarization: A critical evaluation",
                    "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
                    "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
                    "ChatGPT as a factual inconsistency evaluator for abstractive text summarization",
                    "DialSummEval: Revisiting summarization evaluation for dialogues",
                    "On faithfulness and factuality in abstractive summarization",
                    "Evaluating large language models on medical evidence summarization",
                    "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
                    "SummEdits: Measuring LLM ability at factual reasoning through the lens of summarization",
                    "AlignScore: Evaluating factual consistency with a unified alignment function",
                    "SummEval: Re-evaluating summarization evaluation",
                    "ROUGE: A package for automatic evaluation of summaries",
                    "Is ChatGPT a good NLG evaluator? a preliminary study",
                    "Bleu: a method for automatic evaluation of machine translation",
                    "SummaC: Re-visiting NLI-based models for inconsistency detection in summarization",
                    "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
                    "CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
                    "Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors",
                    "Annotating and modeling fine-grained factuality in summarization"
                ],
                "related_work": "2Related WorkFactual Consistency Evaluation BenchmarksIn text summarization, there have been significant efforts to collect human-annotated data for assessing the effectiveness and correlation of different evaluation metrics with human judgments in detecting hallucianted contents in generated summariesFabbri et al. (2021) ; Cao and Wang (2021) ; Maynez et al. (2020) .222We use the termsfactual inconsistency,factual errorsandhallucinationsinterchangeably in this work.Our proposed benchmarkTofuEvalaligns with these efforts but differs from prior work as follows (summarized in Table1) : (1) unlike existing evaluation benchmarks that contains non-LLM-generated summaries,TofuEvalfocuses on LLM-generated summaries. Contrasting withSummEditsLaban et al. (2023) , which produces factually inconsistent summaries by editing correct LLM outputs, we directly identify factual errors in LLM-generated summaries. (2) TofuEvalfocuses on dialogue summarization. Even thoughDialSummEvalGao and Wan (2022) shares this focus, source documents inTofuEvalare considerably longer than those inDialSummEval, which are based on short conversations in the SAMSum corpusGliwa et al. (2019) . (3) Human evaluation from prior work comes from diverse sources, such as crowd-workers inSummEvalFabbri et al. (2021) andFrankPagnoni et al. (2021) , and trained college students fromDialSummEvalGao and Wan (2022) .TofuEvalconsists of annotations from professional linguistic data annotators.Detecting HallucinationsCommon automatic metrics for text summarization such as ROUGELin (2004) , BLEUPapineni et al. (2002) , and BERTScoreZhang* et al. (2020) have poor correlations with human judgement on factual consistencyKryscinski et al. (2019) ; Falke et al. (2019) ; Gao and Wan (2022) ; Tang et al. (2023b) . Therefore, a few non-LLM-based metrics have been developed to detect factuality errorsKryscinski et al. (2020) ; Goyal and Durrett (2021) ; Laban et al. (2022) ; Fabbri et al. (2022) ; Zha et al. (2023) . More recently, LLMs have been shown to have superior zero-shot performance at factual consistency evaluation under certain evaluation settings, highlighting their potential as state-of-the-art factual consistency evaluatorsLuo et al. (2023) ; Wang et al. (2023) .As hallucinations from more recent models are harder to detectTang et al. (2023a) , we re-evaluate non-LLM-based and LLM-based factuality metrics using LLM-generated summaries within the context of our dialogue summarization benchmarkTofuEval. We find that non-LLM-based metrics can surpass most LLM-based evaluators. Nevertheless, all automated factuality metrics still perform quite poorly, underlining the challenging nature of the problem and the substantial room for improvement in automated factual inconsistency detection.",
                "abstract": "Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators."
            },
            {
                "name": "Multitask-based Evaluation of Open-Source LLM on Software Vulnerability",
                "arxiv_id": "2404.02056",
                "subtitles": [
                    "Large Language Model",
                    "Software Vulnerability"
                ],
                "reference": [
                    "The effect of common vulnerability scoring system metrics on vulnerability exploit delay",
                    "Detecting and augmenting missing key aspects in vulnerability descriptions",
                    "Deep reinforcement learning from human preferences",
                    "Linevd: Statement-level vulnerability detection using graph neural networks",
                    "Mvd: Memory-related vulnerability detection based on flow-sensitive graph neural networks",
                    "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Path-sensitive code embedding via contrastive learning for software vulnerability detection",
                    "Cvefixes: automated collection of vulnerabilities and their fixes from open-source software",
                    "A syntax-guided edit decoder for neural program repair",
                    "Atvhunter: Reliable version detection of third-party libraries for vulnerability identification in android applications",
                    "A multi-target approach to estimate software vulnerability characteristics and severity scores",
                    "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
                    "Distinguishing look-alike innocent and vulnerable code by subtle semantic representation learning and explanation",
                    "Graphcodebert: Pre-training code representations with data flow",
                    "A survey on data-driven software vulnerability assessment and prioritization",
                    "Linevul: A transformer-based line-level vulnerability prediction",
                    "Unified pre-training for program understanding and generation",
                    "Vuldeepecker: A deep learning-based system for vulnerability detection",
                    "Predicting missing information of key aspects in vulnerability reports",
                    "Fva: Assessing function-level vulnerability by integrating flow-sensitive structure and code statement semantic",
                    "Vulcnn: An image-inspired scalable vulnerability detection system",
                    "Vulnerability detection with fine-grained interpretations",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "Generating informative cve description from exploitdb posts by extractive summarization",
                    "Key aspects augmentation of vulnerability description based on multiple security databases",
                    "Proximal policy optimization algorithms",
                    "Improving language understanding by generative pre-training",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "An empirical study of deep learning models for vulnerability detection",
                    "The Best of Both Worlds: Integrating Semantic Features with Expert Features for Defect Prediction and Localization",
                    "Fine-tuning language models from human preferences",
                    "Pca: memory leak detection using partial call-path analysis",
                    "Smoke: scalable path-sensitive memory leak detection for millions of lines of code",
                    "Defect identification, categorization, and repair: Better together",
                    "Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks",
                    "Vuldeelocator: a deep learning-based fine-grained vulnerability detector",
                    "Common vulnerabilities and exposures (cve",
                    "Deepcva: Automated commit-level vulnerability assessment with deep multi-task learning",
                    "Large language model for vulnerability detection: Emerging results and future directions",
                    "Deep learning based vulnerability detection: Are we there yet",
                    "Language models are few-shot learners",
                    "Attention is all you need",
                    "Program repair: Automated vs. manual",
                    "Codet5+: Open code large language models for code understanding and generation",
                    "Sequencer: Sequence-to-sequence learning for end-to-end program repair",
                    "Dataflow analysis-inspired deep learning for efficient vulnerability detection",
                    "Unixcoder: Unified cross-modal pre-training for code representation",
                    "securityfocus",
                    "Sysevr: A framework for using deep learning to detect software vulnerabilities",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Background and Related Work2.1Large Language ModelSince the advancements in Natural Language Processing, Large Language Models (LLMs) [1]have seen widespread adoption due to their capacity to be effectively trained with billions of parameters and training samples, resulting in significant performance enhancements. LLMs can readily be applied to downstream tasks through either fine-tuning[2]or prompting[3]. Their versatility stems from being trained to possess a broad understanding, enabling them to capture diverse knowledge across various domains. Fine-tuning involves updating the model parameters specifically for a given downstream task through iterative training on a specific dataset. In contrast, prompting allows for direct utilization by providing natural language descriptions or a few examples of the downstream task. Compared to prompting, fine-tuning is resource-intensive as it necessitates additional model training and is applicable in limited scenarios, particularly when adequate training datasets are unavailable.LLMs are usually built on the transformer architecture[23]and can be classified into three types of architectures: encoder-only, encoder-decoder, and decoder-only. Encoder-only (e.g., CodeBERT[24], GraphCodeBERT[25], and UniXcoder[26]) and Encoder-Decoder (e.g., PLBART[27], CodeT5[7], and CodeT5+[8]) models are trained using Masked Language Modeling (MLM) or Masked Span Prediction (MSP) objective, respectively, where a small portion (e.g., 15%) of the tokens are replaced with either masked tokens or masked span tokens, LLMs are trained to recover the masked tokens. These models are trained as general ones on the code-related data and then are fine-tuned for the downstream tasks to achieve superior performance. Decoder-only models also attract a small portion of people's attention and they are trained by using Causal Language Modeling objectives to predict the probability of the next token given all previous tokens. GPT[2]and its variants are the most representative models, which bring the large language models into practical usage.Recently, the ChatGPT model attracts the widest attention from the world, which is the successor of the large language model InstructGPT[28]with a dialog interface that is fine-tuned using the Reinforcement Learning with Human Feedback (RLHF) approach[29,28,30]. RLHF initially fine-tunes the base model using a small dataset of prompts as input and the desired output, typically human-written, to refine its performance. Subsequently, a reward model is trained on a larger set of prompts by sampling outputs generated by the fine-tuned model. These outputs are then reordered by human labelers to provide feedback for training the reward model. Reinforcement learning[31]is then used to calculate rewards for each output generated based on the reward model, updating LLM parameters accordingly. With fine-tuning and alignment with human preferences, LLMs better understand input prompts and instructions, enhancing performance across various tasks[32,28].The application of LLMs in software engineering has seen a surge, with models like ChatGPT being employed for various tasks (e.g., code review, code generation, and vulnerability detection) . Although some works use LLMs for vulnerability tasks[33,34], our work differs from these previous studies in the following aspects.(1) Closed-source ChatGPT vs. Open-source LLMs:They only explore the capabilities of the closed-source ChatGPT in vulnerability tasks, whereas we investigate the abilities of both open-source code-related LLMs and general LLMs in these tasks.(2) Prompts vs. Few-shot and Fine-tuning Settings:They focus solely on the performance of LLMs using prompts, which introduces randomness and hinders the reproducibility of their findings. In contrast, we examine the capabilities of LLMs under both few-shot and fine-tuning settings, providing the source code and corresponding model files to ensure the reproducibility of our experimental results.2.2Software VulnerabilitySoftware Vulnerabilities (SVs) can expose software systems to risk situations and consequently make the software under cyber-attacks, eventually causing huge economic losses and even threatening people's lives. Therefore, vulnerability databases have been created to document and analyze publicly known security vulnerabilities. For example, Common Vulnerabilities and Exposures (CVE) [35,36]and SecurityFocus[37]are two well-known vulnerability databases. Besides, Common Weakness Enumeration (CWE) defines the common software weaknesses of individual vulnerabilities, which are often referred to as vulnerability types of CVEs. To better address these vulnerabilities, researchers have proposed many approaches for understanding the effects of software vulnerabilities, including SV detection[38,39,40,41,42,43,44,45,46,47,48,49,50], SV assessment[51,52,20,53,54], SV location[55,56,57], SV repair[58,59,60,61]as well as SV description[62,63,64,65]. Many novel technologies are adopted to promote the progress of software vulnerability management, including software analysis[66,67], machine learning[38,45], and deep learning[51,56], especially LLMs[63,64].",
                "abstract": "This paper proposes a pipeline for quantitatively evaluating interactive Large Language Models (LLMs) using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. This evaluation assesses the multi-tasking capabilities of LLMs based on this dataset. We find that the existing state-of-the-art approaches and pre-trained Language Models (LMs) are generally superior to LLMs in software vulnerability detection. However, in software vulnerability assessment and location, certain LLMs (e.g., CodeLlama and WizardCoder) have demonstrated superior performance compared to pre-trained LMs, and providing more contextual information can enhance the vulnerability assessment capabilities of LLMs. Moreover, LLMs exhibit strong vulnerability description capabilities, but their tendency to produce excessive output significantly weakens their performance compared to pre-trained LMs. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights into the capabilities of LLMs in handling software vulnerabilities."
            },
            {
                "name": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
                "arxiv_id": "2402.16775",
                "subtitles": [
                    "LLMs Quantization",
                    "LLMs Evaluation"
                ],
                "reference": [
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "CMMLU: measuring massive multitask language understanding in chinese",
                    "Emergent abilities of large language models",
                    "Instruction-following evaluation for large language models",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "M3KE: A massive multi-level multi-subject knowledge evaluation benchmark for chinese large language models",
                    "BBQ: A hand-built bias benchmark for question answering",
                    "Do emergent abilities exist in quantized large language models: An empirical study",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Finemath: A fine-grained mathematical evaluation benchmark for chinese large language models",
                    "Unlock predictable scaling from emergent abilities",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "LHMKE: A large-scale holistic multi-subject knowledge evaluation benchmark for chinese large language models",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Measuring massive multitask chinese understanding",
                    "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
                    "Do large language models know what they don't know",
                    "Alignbench: Benchmarking chinese alignment of large language models",
                    "Quantization and training of neural networks for efficient integer-arithmetic-only inference",
                    "Roleeval: A bilingual role evaluation benchmark for large language models",
                    "Gpt3.int8() : 8-bit matrix multiplication for transformers at scale",
                    "CORECODE: A common sense annotated dialogue dataset with benchmark tasks for chinese large language models",
                    "Gpteval: A survey on assessments of chatgpt and GPT",
                    "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
                    "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models",
                    "OPTQ: accurate quantization for generative pre-trained transformers",
                    "Outlier suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
                    "A survey on evaluation of large language models",
                    "Spqr: A sparse-quantized representation for near-lossless LLM weight compression",
                    "Through the lens of core competency: Survey on evaluation of large language models",
                    "LFED: A literary fiction evaluation dataset for large language models",
                    "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
                    "Are emergent abilities of large language models a mirage",
                    "Are emergent abilities in large language models just in-context learning",
                    "CBBQ: A chinese bias benchmark dataset curated with human-ai collaboration for large language models",
                    "Measuring massive multitask language understanding",
                    "Qlora: Efficient finetuning of quantized llms"
                ],
                "related_work": "2Related WorkLLMs QuantizationThere are currently two main formalisms of model quantization: QAT(Jacob et al.,2018) and PTQ.PTQ applies quantization after model training, while QAT considers the effects of quantization during the training process, necessitating considerable resources and expertise, thereby restricting its broader application. Consequently, our research primarily concentrates on PTQ.Concerning the identification and protection of outlier values, GPT3.int8() (Dettmers et al.,2022) (also known as LLM.int8() ) identifies outliers by magnitude while SpQR(Dettmers et al.,2023b) employs Hessian matrix to identify outlier values. By equivalently scaling weights and activation values, SmoothQuant(Xiao et al.,2023) greatly reduces quantization error of activation and thus results in a great reduction in the quantization loss of the model. Outlier Suppression+(Wei et al.,2023) suppresses the outlier of weights by performing channel-wise shift and scale. QLoRA(Dettmers et al.,2023a) proposes to use the NF4 data format to reduce quantization rounding errors further. OPTQ(Frantar et al.,2023) (generally known as GPTQ) adjusting the weights during the quantization process to reduce quantization errors.LLMs EvaluationAs the technology behind LLMs continues to advance, these models have shown remarkable performance in many tasks(Bang et al.,2023; Mao et al.,2023) , sometimes surpassing human proficiency(Srivastava et al.,2022; Laskar et al.,2023) . Additionally, as the number of parameters in these models increases, they exhibit emergent abilities(Wei et al.,2022; Schaeffer et al.,2023; Liu et al.,2023b; Lu et al.,2023; Hu et al.,2023) , making it challenging to compare their performance to that of other models and understand their behavior. As a result, numerous benchmarks have been curated to rigorously assess the performance of LLMs(Chang et al.,2024; Ziyu et al.,2023; Liu et al.,2023d; Yu et al.,2024) . These benchmarks can be divided into two primary categories: (1) knowledge & capacity evaluation(Hendrycks et al.,2021; Li et al.,2023; Zeng,2023; Huang et al.,2023; Qin et al.,2023; Liu et al.,2024b,a; Shen et al.,2023; Liu et al.,2023a; Shi et al.,2024) , which examines the model's ability to understand and generate correct responses; and (2) alignment evaluation, which measures how well the model's outputs align with human preference and values(Gehman et al.,2020; Lin et al.,2022; Parrish et al.,2022; Huang and Xiong,2023; Liu et al.,2023c; Yin et al.,2023; Zhou et al.,2023) . Although these benchmarks are commonly employed to assess LLMs, their quantized counterparts are often excluded from these evaluations. As a result, it can be challenging to comprehend the behavior of quantized LLMs and determine the extent of the performance gap between them and their non-quantized counterparts.In addressing these challenges, our research primarily focuses on evaluating quantized LLMs. Our goal is to conduct a thorough examination of the performance of LLMs that have been quantized using various methods. In doing so, we hope to yield valuable insights that will inform and enhance future advancements in quantization methodologies.",
                "abstract": "Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse benchmarks. Our experimental results indicate that LLMs with 4-bit quantization can retain performance comparable to their non-quantized counterparts, and perplexity can serve as a proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs with larger parameter scales can outperform smaller LLMs. Despite the memory savings achieved through quantization, it can also slow down the inference speed of LLMs. Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of quantized LLMs."
            },
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "subtitles": [
                    "LLM Benchmarks",
                    "Risks of Static Benchmarks",
                    "Ranking System",
                    "Human Preference Dataset"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work LLM Benchmarks. We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU(Hendrycks et al.,2020) , HellaSwag(Zellers et al.,2019) , GSM-8K(Cobbe et al.,2021) , BigBench(Srivastava et al.,2023) , AGIEval(Zhong et al.,2023) , and HumanEval(Chen et al.,2021) . Benchmarks focusing on safety, such as ToxicChat(Lin et al.,2023) , and comprehensive suites like HELM(Liang et al.,2022) , also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk(Karpinska et al.,2021; Geng et al.,2023; Wang et al.,2023) . The recent trend includes utilizing GPT-4 for approximating human judgment(Chiang & Lee,2023) , with notable instances being MT-Bench(Zheng et al.,2023b) and AlpacaEval(Li et al.,2023) . In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces(Li et al.,2022; Huang et al.,2023) . They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference(Bai et al.,2022; Ouyang et al.,2022; Touvron et al.,2023) . However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.Risks of Static Benchmarks.Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment(Yang et al.,2023; Oren et al.,2023) . DynaBench(Kiela et al.,2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.Ranking System.Ranking systems have been a well-studied topic in statistics. Related topics include probability models(Hunter,2004; Rao & Kupper,1967) , rank elicitation(Sz\u00f6r\u00e9nyi et al.,2015; Busa-Fekete et al.,2014a,b) , and online experiment design(Chernoff,1992; Karimi et al.,2021) . The Elo rating system has also been used for LLMs(Bai et al.,2022; Boubdir et al.,2023) . Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.Human Preference Dataset.Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant(K\u00f6pf et al.,2023) , HH-RLHF(Bai et al.,2022) , LMSYS-Chat-1M(Zheng et al.,2023a) , and synthetic approximations of human preferences like UltraFeedback(Cui et al.,2023) and Nectar(Zhu et al.,2023) . Our prior data release, LMSYS-Chat-1M(Zheng et al.,2023a) , is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.",
                "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{this https URL}."
            },
            {
                "name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
                "arxiv_id": "2402.04249",
                "subtitles": [
                    "Red Teaming LLMs",
                    "Defenses"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    " \"do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
                    "Robust prompt optimization for defending language models against jailbreaking attacks",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models, 2023b",
                    "Mistral 7b",
                    "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
                    "Rain: Your language models can align themselves without finetuning",
                    "Llm censorship: A machine learning challenge or a computer security problem",
                    "A holistic approach to undesired content detection in the real world",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Masterkey: Automated jailbreak across multiple large language model chatbots",
                    "Red teaming language models with language models",
                    "Are aligned neural networks adversarially aligned",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Universal adversarial triggers for attacking and analyzing NLP",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Gpt-4 technical report",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Mart: Improving llm safety with multi-round automatic red-teaming",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Gradient-based adversarial attacks against text transformers",
                    "Explore, establish, exploit: Red teaming language models from scratch",
                    "Jailbroken: How does llm safety training fail",
                    "Image hijacks: Adversarial images can control generative models at runtime",
                    "Automatically auditing large language models via discrete optimization",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work 2.1Red Teaming LLMs Manual red teaming. Several large-scale manual red teaming efforts have been conducted on LLMs as part of pre-deployment testing (Bai et al., 2022a; Ganguli et al., 2022; Achiam et al., 2023; Touvron et al., 2023). Shen et al. (2023a) characterize the performance of a wide variety of human jailbreaks discovered for closed-source models post-deployment, and (Wei et al., 2023) identify successful high-level attack strategies. These studies and others can serve as a baseline for developing more scalable automated red teaming methods. Automated red teaming. A wide variety of automated red teaming methods have been proposed for LLMs. These include text optimization methods (Wallace et al., 2019; Guo et al., 2021; Shin et al., 2020; Wen et al., 2023; Jones et al., 2023; Zou et al., 2023), LLM optimizers (Perez et al., 2022; Chao et al., 2023; Mehrotra et al., 2023), and custom jailbreaking templates or pipelines (Liu et al., 2023b; Shah et al., 2023; Casper et al., 2023; Deng et al., 2023; Zeng et al., 2024). Most of these methods can be directly compared with each other for eliciting specific harmful behaviors from LLMs. Several papers have also explored image attacks on multimodal LLMs (Bagdasaryan et al., 2023; Shayegani et al., 2023; Qi et al., 2023a; Bailey et al., 2023). In some instances, multimodal attacks have been observed to be stronger than text attacks (Carlini et al., 2023), motivating their inclusion in a standardized evaluation for attacks and defenses. The literature on automated red teaming has grown rapidly, and many attacks are now available for comparison. However, the lack of a standardized evaluation has prevented easy comparisons across papers, such that the relative performance of these methods is unclear. Evaluating red teaming. Due to the rapid growth of the area, many papers on automated red teaming have developed their own evaluation setups to compare their methods against baselines. Among prior work, we find at least  9  distinct evaluation setups, which we show in Table 1. We find that existing comparisons rarely overlap, and in Section 3.2 we demonstrate that prior evaluations are essentially incomparable across papers due to a lack of standardization. 2.2Defenses Several complimentary approaches have been studied for defending LLMs against malicious use. These can be categorized into system-level defenses and model-level defenses. System-level defenses System-level defenses do not alter the LLM itself, but rather add external safety measures on top of the LLM. These include input and output filtering (Markov et al., 2023; Inan et al., 2023; Computer, 2023; Li et al., 2023; Cao et al., 2023; Jain et al., 2023), input sanitization (Jain et al., 2023) and modification (Zhou et al., 2024), and constrained inference (Rebedea et al., 2023). The most widely-used defense in production is filtering, but Glukhov et al. (2023) note that output filtering can be foiled if jailbroken LLMs assist malicious users with bypassing detection, e.g., by generating encoded outputs. This motivates a defense in depth approach where system-level defenses like filtering are combined with defenses built into LLMs. Model-level defenses Model-level defenses alter the LLM itself to reduce the risk of malicious use and improve robustness to adversarial prompting. These include safety training, refusal mechanisms, system prompts and context distillation, and adversarial training. Safety training is commonly approached via fine-tuning methods such as RLHF (Ouyang et al., 2022), DPO (Rafailov et al., 2023), and RLAIF (Bai et al., 2022b). Combined with safety datasets and manual red teaming, these approaches can yield substantial improvements to safety and robustness (Bai et al., 2022a; Achiam et al., 2023; Touvron et al., 2023). These training procedures often instill models with \"refusal mechanisms\" whereby models identify a user request as harmful and refuse to carry out the request. Several works have explored adversarial training with automated red teaming methods. This differs in important ways from training against perturbation attacks, which has been extensively explored in prior work. We discuss these differences in Section A.1. Jain et al. (2023) note that current attacks can be extremely computationally expensive, which makes them challenging to integrate into an LLM fine-tuning loop. They conduct an adversarial training experiment with a static dataset of harmful prompts, in which the adversary does not optimize against the model during fine-tuning. Concurrently with our work, Ge et al. (2023) propose multi-round adversarial training with automated red teaming methods, generating new test cases  4  times throughout training. In Section 5 we introduce a novel adversarial training method for robust refusal, demonstrating how HarmBench can facilitate the codevelopment of attacks and defenses. Other factors that may affect the inherent robustness of a model to jailbreaks include its training set, architecture, system prompt (Touvron et al., 2023; Jiang et al., 2023), and size (Ganguli et al., 2022). Our large-scale comparison enables thorough examinations of the effect of these factors.",
                "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench atthis https URL."
            },
            {
                "name": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
                "arxiv_id": "2401.01614",
                "subtitles": [
                    "Large Multimodal Models",
                    "Web Agent",
                    "Visual Grounding"
                ],
                "reference": [
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Fine-grained visual prompting",
                    "Referitgame: Referring to objects in photographs of natural scenes",
                    "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
                    "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v",
                    "Language models can solve computer tasks",
                    "Multimodal web navigation with instruction-finetuned foundation models",
                    "Ferret: Refer and ground anything anywhere at any granularity",
                    "The dawn of lmms: Preliminary explorations with gpt-4v(ision",
                    "An in-depth look at gemini's language abilities",
                    "A real-world webagent with planning, long context understanding, and program synthesis",
                    "What does clip know about a red circle? visual prompt engineering for vlms",
                    "Reinforcement learning on web interfaces using workflow-guided exploration",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Gpt-4v(ision) as a generalist evaluator for vision-language tasks",
                    "Kosmos-2: Grounding multimodal large language models to the world",
                    "Flin: A flexible natural language interface for web navigation",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Gpt-4 technical report",
                    "Bubogpt: Enabling visual grounding in multi-modal llms",
                    "Gemini: A family of highly capable multimodal models",
                    "Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation",
                    "Mind2web: Towards a generalist agent for the web",
                    "Cogagent: A visual language model for gui agents",
                    "Measuring massive multitask language understanding",
                    "Scienceqa: a novel resource for question answering on scholarly articles",
                    "Understanding html with large language models",
                    "Webshop: Towards scalable real-world web interaction with grounded language agents",
                    "World of bits: An open-domain platform for web-based agents",
                    "Hierarchical prompting assists large language model on web navigation",
                    "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                    "From pixels to ui actions: Learning to follow instructions via graphical user interfaces",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
                ],
                "related_work": "5Related WorkWeb Agent.Many works have focused on improving web agents relying on the HTML document(Deng et al.,2023; Gur et al.,2023,2022,2023; Kim et al.,2023; Sridhar et al.,2023) . However, a raw HTML document is often massive making it infeasible or cost-prohibitively to feed into LLMs directly. MindAct(Deng et al.,2023) instead employs a small language model to rank each HTML element and selectively consider top elements as the context. WebAgent(Gur et al.,2023) proposes an enhanced planning strategy by summarizing the HTML documents and decomposing the instruction into multiple sub-instructions. Another stream considers visual information for web agents(Shaw et al.,2023; Furuta et al.,2023; Hong et al.,2023) . Pix2Act(Shaw et al.,2023) leverages Pix2Struct(Lee et al.,2022) to parse screenshot images into simplified HTML to complete GUI-based tasks(Shaw et al.,2023; Liu et al.,2018; Shi et al.,2017; Mazumder & Riva,2020; Yao et al.,2022) . WebGUM(Furuta et al.,2023) and CogAgent(Hong et al.,2023) pre-train an LMM with massive screenshot-HTML data to enhance its decision-making on real-world web navigation like Mind2Web. While all these prior works show promise, generalizing to various web environments remains a challenge for existing models. Thus,SeeActexplores recently released, more powerful LMMs such as GPT-4V and Gemini, to demonstrate their potential as generalist web agents with comprehensive online and offline evaluation and analysis. In a concurrent work(Yan et al.,2023) ,GPT-4Vexhibits strong performance on mobile UI understanding, which is less complex than the desktop websites we study.Large Multimodal Models.GPT-4V(OpenAI,2023) and Gemini(Anil et al.,2023) represent significant progress in LMMs. Several studies(Akter et al.,2023; OpenAI,2023; Yang et al.,2023c; Zhang et al.,2023; Yang et al.,2023a; Yan et al.,2023) have highlighted their remarkable multimodal capabilities, emphasizing the advanced and versatile integration of visual and language reasoning abilities. Their performance on a series of benchmarks(Kazemzadeh et al.,2014; Goyal et al.,2016; Hendrycks et al.,2020; Saikh et al.,2022; Lu et al.,2022; Zhong et al.,2023; Yue et al.,2023) also showcases remarkable capabilities on vision-and-language understanding and reasoning. Although open-sourced models still exhibit a performance gap withGPT-4V, they have the advantages of controllability and ease of fine-tuning for various applications. For example, in CogAgent(Hong et al.,2023) , LMMs are fine-tuned on HTML and screenshot image pairs to enhance webpage understanding ability and further enhanced with an image encoder for high-resolution image details. Ferret(You et al.,2023) is finetuned to allow visual referring and grounding.Visual Grounding.Despite LMMs having achieved remarkable vision-language understanding capabilities, they still face challenges in fine-grained visual grounding. Various visual prompting(Shtedritski et al.,2023; Yang et al.,2023b,c; Yan et al.,2023) methods have been proposed to augmentGPT-4V's image detail grounding ability by overlaying visual marks onto the image. SoM(Yang et al.,2023a) involves segmenting the image into semantically meaningful regions and overlaying an array of visual marks like numbers, alphabets, masks, or bounding boxes. Fine-tuning vision-language models with image-annotated data is effective. Kosmos-2(Peng et al.,2023) represents bounding box locations through textual location tokens. BuboGPT(Zhao et al.,2023) extract entities and find corresponding masks for objects in the image. Shikra(Chen et al.,2023) handles image detail referring and grounding by applying spatial coordinates as text tokens in inputs and outputs, respectively. Ferret(You et al.,2023) represents regions with both discrete coordinates and continuous features along with a spatial-aware visual sampler to handle diverse spatial characteristics across various shapes.",
                "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents -- it can successfully complete 51.1 of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out to be not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML structure and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement. All code, data, and evaluation tools are available atthis https URL."
            },
            {
                "name": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "arxiv_id": "2403.20330",
                "subtitles": [
                    "Large Vision-Language Models",
                    "Evaluations of LVLMs"
                ],
                "reference": [
                    "Phi2: The surprising power of small language models",
                    "A-okvqa: A benchmark for visual question answering using world knowledge",
                    "A diagram is worth a dozen images",
                    "Palm: Scaling language modeling with pathways",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Internlm: A multilingual language model with progressively enhanced capabilities",
                    "Mistral 7b",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Deepseek llm: Scaling open-source language models with longtermism",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "To see is to believe: Prompting gpt-4v for better visual instruction tuning",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Improved baselines with visual instruction tuning",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Multilayer perceptron (mlp",
                    "Can vision-language models think from a first-person perspective",
                    "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Yi: Open foundation models by 01. ai",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Cheap and quick: Efficient vision-language instruction tuning for large language models",
                    "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Mixtral of experts",
                    "Deepseek-vl: Towards real-world vision-language understanding",
                    "Visual instruction tuning",
                    "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Scaling up visual and vision-language representation learning with noisy text supervision",
                    "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                    "Learning transferable visual models from natural language supervision",
                    "Baichuan 2: Open large-scale language models",
                    "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Chatgpt",
                    "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
                    "Qwen technical report",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Gemini: a family of highly capable multimodal models",
                    "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
                    "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work Large Vision-Language Models. As large language models (LLMs) [8, 43, 43, 47, 42, 34, 36, 9] rapidly advance, a growing fraction of the research community is focusing on integrating visual content into LLMs to build a powerful intelligent assistant with more interactive ways. Central to these large vision-language models (LVLMs) are the seminal works in modality alignment within the vision-language learning area [37, 17]. The foundation work CLIP [37] exemplifies the alignment of vision and language modalities through contrastive learning on extensive image-text pairs. Built upon the CLIP image encoder which is somewhat aligned with the language modality, current LVLMs typically utilize vast image-text pairs to connect the vision encoder and LLM, enabling LLM to receive and understand visual content [54, 26, 24, 11, 52, 2, 48, 31, 5]. For example, MiniGPT4 [54] and LLaVA [26] directly connect the vision encoder and LLM with QFormer [22] and MLP [40], showing proficiency in multi-modal dialogues. Subsequent works have further enhanced LVLMs by improving the multi-modal instruction data [24, 48, 5, 44] and designing novel modules [2, 23, 45, 28, 15, 12] for more sufficient modality alignment. Evaluations of LVLMs. To probe the true capabilities of the emerging LVLMs, the research community has developed many multi-modal benchmarks encompassing a wide range of evaluation axes [27, 14, 38, 51, 39, 21, 26, 50, 46]. Early single-task benchmarks, such as VQA [16], MS-COCO [39], and OK-VQA [38], fail to holistically assess LVLMs' general multi-modal perception and reasoning capabilities. To address this issue, comprehensive multi-modal benchmarks have been constructed [26, 21, 51, 14, 27, 7, 46]. For example, SEED [21] and MMBench [27] cover 12 and 20 evaluation dimensions respectively, while MMMU [51] spans 30 college-level subjects, providing some competitive arenas for a comprehensive comparison of cutting-edge LVLMs. However, existing evaluations of LVLMs overlook some critical issues. On the one hand, they do not guarantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multi-modal capabilities brought by multi-modal training.",
                "abstract": "Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain."
            }
        ],
        "survey": {
            "name": "A Survey on Evaluation of Multimodal Large Language Models",
            "arxiv_id": "2408.15769",
            "subtitles": [
                {
                    "name": "Background",
                    "key_history": [
                        {
                            "reference_title": "Attention is all you need",
                            "key_word": "Transformer Architecture"
                        },
                        {
                            "reference_title": "Llama: Open and efficient foundation language models",
                            "key_word": "LLaMA"
                        },
                        {
                            "reference_title": "Visual spatial reasoning",
                            "key_word": "Vision Transformer"
                        },
                        {
                            "reference_title": "Valley: Video assistant with large language model enhanced ability",
                            "key_word": "Model incorporating temporal modeling for video data"
                        },
                        {
                            "reference_title": "Aligning large multimodal models with factually augmented rlhf",
                            "key_word": "Reinforcement learning with human feedback"
                        }
                    ],
                    "references_in_this_section": [
                        "Mobile-agent: Autonomous multi-modal mobile device agent with visual perception",
                        "Visualagentbench: Towards large multimodal models as visual foundation agents",
                        "A-okvqa: A benchmark for visual question answering using world knowledge",
                        "Eyes can deceive: Benchmarking counterfactual reasoning abilities of multi-modal large language models",
                        "Mdas: A new multimodal benchmark dataset for remote sensing",
                        "Mm-spubench: Towards better understanding of spurious biases in multimodal llms",
                        "Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs",
                        "Seed-bench: Benchmarking multimodal large language models",
                        "Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
                        "M3dbench: Let's instruct large models with multi-modal 3d prompts",
                        "Shield: An evaluation benchmark for face spoofing and forgery detection with multimodal large language models",
                        "Learning the user's deeper preferences for multi-modal recommendation systems",
                        "Implicitave: An open-source dataset and multimodal llms benchmark for implicit attribute value extraction",
                        "Synthesize diagnose and optimize: Towards fine-grained vision-language understanding",
                        "M3cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought",
                        "Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark",
                        "Mmc: Advancing multimodal chart understanding with large-scale instruction tuning",
                        "What's\" up\" with vision-language models? investigating their struggle with spatial reasoning",
                        "Equivariant similarity for vision-language foundation models",
                        "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
                        "M3gia: A cognition inspired multilingual and multimodal general intelligence ability benchmark",
                        "Evaluating object hallucination in large vision-language models",
                        "Asclepius: A spectrum evaluation benchmark for medical multi-modal large language models",
                        "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
                        "Visually dehallucinative instruction generation",
                        "Gsr-bench: A benchmark for grounded spatial reasoning evaluation via multimodal llms",
                        "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                        "Milebench: Benchmarking mllms in long context",
                        "What matters in training a gpt4-style language model with multimodal inputs",
                        "Plug-and-play grounding of reasoning in multimodal large language models",
                        "Attention is all you need",
                        "Benchlmm: Benchmarking cross-style visual capability of large multimodal models",
                        "Vlkeb: A large vision-language model knowledge editing benchmark",
                        "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark",
                        "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                        "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                        "Mc-mke: A fine-grained multimodal knowledge editing benchmark emphasizing modality consistency",
                        "Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations",
                        "Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models",
                        "When and why vision-language models behave like bags-of-words, and what to do about it",
                        "Seeing clearly, answering incorrectly: A multimodal robustness benchmark for evaluating mllms on leading questions",
                        "Measuring multimodal mathematical reasoning with math-vision dataset",
                        "Raven: A dataset for relational and analogical visual reasoning",
                        "Charxiv: Charting gaps in realistic chart understanding in multimodal llms",
                        "Judging llm-as-a-judge with mt-bench and chatbot arena",
                        "Multi: Multimodal understanding leaderboard with text and images",
                        "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
                        "Muchomusic: Evaluating music understanding in multimodal audio-language models",
                        "Evaluation and analysis of hallucination in large vision-language models",
                        "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
                        "Mm-sap: A comprehensive benchmark for assessing self-awareness of multimodal large language models in perception",
                        "Rsgpt: A remote sensing vision language model and benchmark",
                        "Unified hallucination detection for multimodal large language models",
                        "The instinctive bias: Spurious images lead to hallucination in mllms",
                        "Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges",
                        "What is the visual cognition gap between humans and multimodal llms",
                        "Mmiu: Multimodal multi-image understanding for evaluating large vision-language models",
                        "Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models",
                        "Scanqa: 3d question answering for spatial scene understanding",
                        "Robust speech recognition via large-scale weak supervision",
                        "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
                        "Muirbench: A comprehensive benchmark for robust multi-image understanding",
                        "V?: Guided visual search as a core mechanism in multimodal llms",
                        "Towards vqa models that can read",
                        "How easy is it to fool your multimodal llms? an empirical analysis on deceptive prompts",
                        "Uniaa: A unified multi-modal image aesthetic assessment baseline and benchmark",
                        "Transportationgames: Benchmarking transportation knowledge of (multimodal) large language models",
                        "Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning",
                        "Is your model really a good math reasoner? evaluating mathematical reasoning with checklist",
                        "Are we on the right way for evaluating large vision-language models",
                        "Red teaming visual language models",
                        "Cross-city matters: A multimodal remote sensing benchmark dataset for cross-city semantic segmentation using high-resolution domain adaptation networks",
                        "An llm-free multi-dimensional benchmark for mllms hallucination evaluation",
                        "Mmbench-video: A long-form multi-shot benchmark for holistic video understanding",
                        "Cvqa: Culturally-diverse multilingual visual question answering benchmark",
                        "Designqa: A multimodal benchmark for evaluating large language models' understanding of engineering documentation",
                        "Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception",
                        "Textcaps: a dataset for image captioning with reading comprehension",
                        "Mm-soc: Benchmarking multimodal large language models in social media platforms",
                        "Pointnet: Deep learning on point sets for 3d classification and segmentation",
                        "Mmbench: Is your multi-modal model an all-around player",
                        "Sok-bench: A situated video reasoning benchmark with aligned open-world knowledge",
                        "Aligning large multimodal models with factually augmented rlhf",
                        "Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want",
                        "Valley: Video assistant with large language model enhanced ability",
                        "Egoplan-bench: Benchmarking multimodal large language models for human-level planning",
                        "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
                        "Crab: Cross-environment agent benchmark for multimodal language model agents",
                        "Peacock: A family of arabic multimodal large language models and benchmarks",
                        "The all-seeing project v2: Towards general relation comprehension of the open world",
                        "Openeqa: Embodied question answering in the era of foundation models",
                        "Lavy: Vietnamese multimodal large language model",
                        "Compbench: A comparative reasoning benchmark for multimodal llms",
                        "Visual spatial reasoning",
                        "A benchmark for multi-modal foundation models on low-level vision: from single images to pairs",
                        "Mtvqa: Benchmarking multilingual text-centric visual question answering",
                        "Hierarchical multimodal transformers for multipage docvqa",
                        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
                        "Single image unlearning: Efficient machine unlearning in multimodal large language models",
                        "Logicvista: Multimodal llm logical reasoning benchmark in visual contexts",
                        "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
                        "Visual instruction tuning",
                        "Mmrel: A relation understanding dataset and benchmark in the mllm era",
                        "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                        "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems",
                        "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                        "Visual hallucinations of multi-modal large language models",
                        "Mvbench: A comprehensive multi-modal video understanding benchmark",
                        "Qlora: Efficient finetuning of quantized llms",
                        "Touchstone: Evaluating vision-language models by language models",
                        "Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models",
                        "Fvqa: Fact-based visual question answering",
                        "Document visual question answering challenge",
                        "Spatialrgpt: Grounded spatial reasoning in vision language model",
                        "Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning",
                        "Detecting and preventing hallucinations in large vision language models",
                        "Ok-vqa: A visual question answering benchmark requiring external knowledge",
                        "M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
                        "Scemqa: A scientific college entrance level multimodal question answering benchmark",
                        "Ferret-ui: Grounded mobile ui understanding with multimodal llms",
                        "Mitigating hallucination in large multi-modal models via robust instruction tuning",
                        "Eyes wide shut? exploring the visual shortcomings of multimodal llms",
                        "Chartx & chartvlm: A versatile benchmark and foundation model for complicated chart reasoning",
                        "Chartbench: A benchmark for complex visual reasoning in charts",
                        "Needle in a multimodal haystack",
                        "Ii-bench: An image implication understanding benchmark for multimodal large language models",
                        "Codis: Benchmarking context-dependent visual comprehension for multimodal large language models",
                        "Improved baselines with visual instruction tuning",
                        "Vizwiz grand challenge: Answering visual questions from blind people",
                        "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
                        "Scifibench: Benchmarking large multimodal models for scientific figure interpretation",
                        "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                        "Contextual object detection with multimodal large language models",
                        "Air-bench: Benchmarking large audio-language models via generative comprehension",
                        "Bubogpt: Enabling visual grounding in multi-modal llms",
                        "Nphardeval4v: A dynamic reasoning benchmark of multimodal large language models",
                        "Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai",
                        "Mike: A new benchmark for fine-grained multimodal entity knowledge editing",
                        "Learning transferable visual models from natural language supervision",
                        "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
                        "On the hidden mystery of ocr in large multimodal models",
                        "M3d: Advancing 3d medical image analysis with multi-modal large language models",
                        "Llama: Open and efficient foundation language models",
                        "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
                        "Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences",
                        "Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models",
                        "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
                        "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
                        "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
                    ]
                },
                {
                    "name": "What to evaluation",
                    "key_history": [
                        {
                            "reference_title": "Mmbench: Is your multi-modal model an all-around player",
                            "key_word": "Multi-modal Recognition"
                        },
                        {
                            "reference_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                            "key_word": "Attribute Recognition"
                        },
                        {
                            "reference_title": "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                            "key_word": "Concept Recognition"
                        },
                        {
                            "reference_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                            "key_word": "Multi-modal Perception"
                        },
                        {
                            "reference_title": "Visual instruction tuning",
                            "key_word": "Text Recognition"
                        }
                    ],
                    "references_in_this_section": [
                        "Microsoft coco: Common objects in context",
                        "Visualagentbench: Towards large multimodal models as visual foundation agents",
                        "Infmllm: A unified framework for visual-language tasks",
                        "Mdas: A new multimodal benchmark dataset for remote sensing",
                        "Mm-spubench: Towards better understanding of spurious biases in multimodal llms",
                        "Xtuner: A toolkit for efficiently fine-tuning llm",
                        "Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
                        "M3dbench: Let's instruct large models with multi-modal 3d prompts",
                        "M3cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought",
                        "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
                        "Chain-of-thought prompting elicits reasoning in large language models",
                        "Evaluating object hallucination in large vision-language models",
                        "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
                        "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                        "Osprey: Pixel understanding with visual instruction tuning",
                        "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                        "What matters in training a gpt4-style language model with multimodal inputs",
                        "Plug-and-play grounding of reasoning in multimodal large language models",
                        "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark",
                        "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                        "Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models",
                        "Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations",
                        "When and why vision-language models behave like bags-of-words, and what to do about it",
                        "Seeing clearly, answering incorrectly: A multimodal robustness benchmark for evaluating mllms on leading questions",
                        "Lxmert: Learning cross-modality encoder representations from transformers",
                        "Pandagpt: One model to instruction-follow them all",
                        "Ferret: Refer and ground anything anywhere at any granularity",
                        "Muchomusic: Evaluating music understanding in multimodal audio-language models",
                        "Rsgpt: A remote sensing vision language model and benchmark",
                        "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                        "Scanqa: 3d question answering for spatial scene understanding",
                        "Muirbench: A comprehensive benchmark for robust multi-image understanding",
                        "V?: Guided visual search as a core mechanism in multimodal llms",
                        "Towards vqa models that can read",
                        "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
                        "Transportationgames: Benchmarking transportation knowledge of (multimodal) large language models",
                        "Are we on the right way for evaluating large vision-language models",
                        "Red teaming visual language models",
                        "Cross-city matters: A multimodal remote sensing benchmark dataset for cross-city semantic segmentation using high-resolution domain adaptation networks",
                        "Mmbench-video: A long-form multi-shot benchmark for holistic video understanding",
                        "Llama-adapter v2: Parameter-efficient visual instruction model",
                        "Cvqa: Culturally-diverse multilingual visual question answering benchmark",
                        "Mm-react: Prompting chatgpt for multimodal reasoning and action",
                        "Designqa: A multimodal benchmark for evaluating large language models' understanding of engineering documentation",
                        "Mm-soc: Benchmarking multimodal large language models in social media platforms",
                        "Vilt: Vision-and-language transformer without convolution or region supervision",
                        "Mmbench: Is your multi-modal model an all-around player",
                        "Sok-bench: A situated video reasoning benchmark with aligned open-world knowledge",
                        "Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want",
                        "Egoplan-bench: Benchmarking multimodal large language models for human-level planning",
                        "Single image unlearning: Efficient machine unlearning in multimodal large language models",
                        "Gpt-4 technical report",
                        "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
                        "Visual instruction tuning",
                        "Llava-next: Improved reasoning, ocr, and world knowledge, January",
                        "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                        "Cider: Consensus-based image description evaluation",
                        "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                        "Mvbench: A comprehensive multi-modal video understanding benchmark",
                        "Touchstone: Evaluating vision-language models by language models",
                        "Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models",
                        "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
                        "Empirical study towards building an effective multi-modal large language model",
                        "M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
                        "Scemqa: A scientific college entrance level multimodal question answering benchmark",
                        "Mitigating hallucination in large multi-modal models via robust instruction tuning",
                        "Wemm",
                        "Ii-bench: An image implication understanding benchmark for multimodal large language models",
                        "Codis: Benchmarking context-dependent visual comprehension for multimodal large language models",
                        "Improved baselines with visual instruction tuning",
                        "Multi-grained vision language pre-training: Aligning texts with visual concepts",
                        "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                        "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
                        "Air-bench: Benchmarking large audio-language models via generative comprehension",
                        "Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai",
                        "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                        "Learning transferable visual models from natural language supervision",
                        "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                        "M3d: Advancing 3d medical image analysis with multi-modal large language models",
                        "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
                        "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                        "Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models",
                        "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
                    ]
                },
                {
                    "name": "Where to evaluate",
                    "key_history": [
                        {
                            "reference_title": "Mmbench: Is your multi-modal model an all-around player",
                            "key_word": "basic recognition tasks"
                        },
                        {
                            "reference_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                            "key_word": "contextually relevant and coherent outputs"
                        },
                        {
                            "reference_title": "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                            "key_word": "wide-ranging evaluation"
                        },
                        {
                            "reference_title": "Evaluating object hallucination in large vision-language models",
                            "key_word": "object hallucination"
                        },
                        {
                            "reference_title": "Cvqa: Culturally-diverse multilingual visual question answering benchmark",
                            "key_word": "cross-cultural visual question answering"
                        },
                        {
                            "reference_title": "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems",
                            "key_word": "visual math benchmark"
                        },
                        {
                            "reference_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                            "key_word": "science question"
                        },
                        {
                            "reference_title": "Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai",
                            "key_word": "MLLMs in the medical"
                        }
                    ],
                    "references_in_this_section": [
                        "Evaluating object hallucination in large vision-language models",
                        "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems",
                        "Transportationgames: Benchmarking transportation knowledge of (multimodal) large language models",
                        "Are we on the right way for evaluating large vision-language models",
                        "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                        "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                        "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
                        "Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai",
                        "Touchstone: Evaluating vision-language models by language models",
                        "Logicvista: Multimodal llm logical reasoning benchmark in visual contexts",
                        "Mmbench: Is your multi-modal model an all-around player",
                        "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                        "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                        "Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models",
                        "Cvqa: Culturally-diverse multilingual visual question answering benchmark"
                    ]
                },
                {
                    "name": "How to Evaluate",
                    "key_history": [
                        {
                            "reference_title": "Visual instruction tuning",
                            "key_word": "Human Evaluation"
                        },
                        {
                            "reference_title": "Gpt-4 technical report",
                            "key_word": "GPT-4 Evaluation"
                        },
                        {
                            "reference_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                            "key_word": "Metric Evaluation"
                        }
                    ],
                    "references_in_this_section": [
                        "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                        "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
                        "Gpt-4 technical report",
                        "Microsoft coco captions: Data collection and evaluation server",
                        "A dataset of clinically generated visual questions and answers about radiology images",
                        "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
                        "Audiocaps: Generating captions for audios in the wild",
                        "Visual instruction tuning"
                    ]
                }
            ],
            "all_references": [
                "Mdas: A new multimodal benchmark dataset for remote sensing",
                "Spatialrgpt: Grounded spatial reasoning in vision language model",
                "Shield: An evaluation benchmark for face spoofing and forgery detection with multimodal large language models",
                "Air-bench: Benchmarking large audio-language models via generative comprehension",
                "Osprey: Pixel understanding with visual instruction tuning",
                "Mvbench: A comprehensive multi-modal video understanding benchmark",
                "Robust speech recognition via large-scale weak supervision",
                "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
                "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                "Mm-sap: A comprehensive benchmark for assessing self-awareness of multimodal large language models in perception",
                "Detecting and preventing hallucinations in large vision language models",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                "Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences",
                "Charxiv: Charting gaps in realistic chart understanding in multimodal llms",
                "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                "Visual hallucinations of multi-modal large language models",
                "Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning",
                "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                "Equivariant similarity for vision-language foundation models",
                "Attention is all you need",
                "Raven: A dataset for relational and analogical visual reasoning",
                "Seeing clearly, answering incorrectly: A multimodal robustness benchmark for evaluating mllms on leading questions",
                "Asclepius: A spectrum evaluation benchmark for medical multi-modal large language models",
                "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
                "How easy is it to fool your multimodal llms? an empirical analysis on deceptive prompts",
                "Xtuner: A toolkit for efficiently fine-tuning llm",
                "When and why vision-language models behave like bags-of-words, and what to do about it",
                "Muchomusic: Evaluating music understanding in multimodal audio-language models",
                "Egoplan-bench: Benchmarking multimodal large language models for human-level planning",
                "Are we on the right way for evaluating large vision-language models",
                "What matters in training a gpt4-style language model with multimodal inputs",
                "Learning transferable visual models from natural language supervision",
                "Eyes can deceive: Benchmarking counterfactual reasoning abilities of multi-modal large language models",
                "Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models",
                "Mmrel: A relation understanding dataset and benchmark in the mllm era",
                "Mtvqa: Benchmarking multilingual text-centric visual question answering",
                "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
                "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
                "Chartx & chartvlm: A versatile benchmark and foundation model for complicated chart reasoning",
                "Textcaps: a dataset for image captioning with reading comprehension",
                "The instinctive bias: Spurious images lead to hallucination in mllms",
                "Scifibench: Benchmarking large multimodal models for scientific figure interpretation",
                "Mmbench-video: A long-form multi-shot benchmark for holistic video understanding",
                "Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech",
                "Pandagpt: One model to instruction-follow them all",
                "Vlkeb: A large vision-language model knowledge editing benchmark",
                "Mmiu: Multimodal multi-image understanding for evaluating large vision-language models",
                "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",
                "Multi-grained vision language pre-training: Aligning texts with visual concepts",
                "Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models",
                "Evaluation and analysis of hallucination in large vision-language models",
                "Mitigating hallucination in large multi-modal models via robust instruction tuning",
                "Seed-bench: Benchmarking multimodal large language models",
                "Plug-and-play grounding of reasoning in multimodal large language models",
                "Towards vqa models that can read",
                "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
                "Cambrian-1: A fully open, vision-centric exploration of multimodal llms",
                "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
                "Milebench: Benchmarking mllms in long context",
                "Pointnet: Deep learning on point sets for 3d classification and segmentation",
                "Bubogpt: Enabling visual grounding in multi-modal llms",
                "Openeqa: Embodied question answering in the era of foundation models",
                "Holistic analysis of hallucination in gpt-4v (ision) : Bias and interference challenges",
                "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                "Touchstone: Evaluating vision-language models by language models",
                "Visual spatial reasoning",
                "Hierarchical multimodal transformers for multipage docvqa",
                "M3d: Advancing 3d medical image analysis with multi-modal large language models",
                "Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai",
                "Single image unlearning: Efficient machine unlearning in multimodal large language models",
                "Needle in a multimodal haystack",
                "M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models",
                "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
                "What is the visual cognition gap between humans and multimodal llms",
                "Muirbench: A comprehensive benchmark for robust multi-image understanding",
                "A dataset of clinically generated visual questions and answers about radiology images",
                "Crab: Cross-environment agent benchmark for multimodal language model agents",
                "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems",
                "Peacock: A family of arabic multimodal large language models and benchmarks",
                "Is your model really a good math reasoner? evaluating mathematical reasoning with checklist",
                "Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want",
                "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
                "Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks",
                "Vizwiz grand challenge: Answering visual questions from blind people",
                "On the hidden mystery of ocr in large multimodal models",
                "Multi: Multimodal understanding leaderboard with text and images",
                "Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models",
                "Mike: A new benchmark for fine-grained multimodal entity knowledge editing",
                "A benchmark for multi-modal foundation models on low-level vision: from single images to pairs",
                "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                "Vilt: Vision-and-language transformer without convolution or region supervision",
                "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                "Transportationgames: Benchmarking transportation knowledge of (multimodal) large language models",
                "Mc-mke: A fine-grained multimodal knowledge editing benchmark emphasizing modality consistency",
                "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
                "Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception",
                "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
                "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
                "Ok-vqa: A visual question answering benchmark requiring external knowledge",
                "Mmc: Advancing multimodal chart understanding with large-scale instruction tuning",
                "Large language models: A survey",
                "Uniaa: A unified multi-modal image aesthetic assessment baseline and benchmark",
                "M3cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought",
                "Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs",
                "Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models",
                "A-okvqa: A benchmark for visual question answering using world knowledge",
                "Synthesize diagnose and optimize: Towards fine-grained vision-language understanding",
                "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                "Empirical study towards building an effective multi-modal large language model",
                "Visual instruction tuning",
                "Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations",
                "Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning",
                "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark",
                "Wemm",
                "Compbench: A comparative reasoning benchmark for multimodal llms",
                "Microsoft coco captions: Data collection and evaluation server",
                "Mm-react: Prompting chatgpt for multimodal reasoning and action",
                "Ferret: Refer and ground anything anywhere at any granularity",
                "Document visual question answering challenge",
                "Judging llm-as-a-judge with mt-bench and chatbot arena",
                "Improved baselines with visual instruction tuning",
                "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
                "Infmllm: A unified framework for visual-language tasks",
                "M3dbench: Let's instruct large models with multi-modal 3d prompts",
                "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
                "Benchlmm: Benchmarking cross-style visual capability of large multimodal models",
                "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                "Visually dehallucinative instruction generation",
                "Lavy: Vietnamese multimodal large language model",
                "Lxmert: Learning cross-modality encoder representations from transformers",
                "Unified hallucination detection for multimodal large language models",
                "Rsgpt: A remote sensing vision language model and benchmark",
                "Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark",
                "Audiocaps: Generating captions for audios in the wild",
                "Llava-next: Improved reasoning, ocr, and world knowledge, January",
                "Mm-soc: Benchmarking multimodal large language models in social media platforms",
                "Gsr-bench: A benchmark for grounded spatial reasoning evaluation via multimodal llms",
                "Microsoft coco: Common objects in context",
                "A survey on large language models: Applications, challenges, limitations, and practical usage",
                "M3gia: A cognition inspired multilingual and multimodal general intelligence ability benchmark",
                "Cvqa: Culturally-diverse multilingual visual question answering benchmark",
                "Contextual object detection with multimodal large language models",
                "Ii-bench: An image implication understanding benchmark for multimodal large language models",
                "Codis: Benchmarking context-dependent visual comprehension for multimodal large language models",
                "Cross-city matters: A multimodal remote sensing benchmark dataset for cross-city semantic segmentation using high-resolution domain adaptation networks",
                "Fvqa: Fact-based visual question answering",
                "The all-seeing project v2: Towards general relation comprehension of the open world",
                "Llama: Open and efficient foundation language models",
                "Mobile-agent: Autonomous multi-modal mobile device agent with visual perception",
                "Sok-bench: A situated video reasoning benchmark with aligned open-world knowledge",
                "Ferret-ui: Grounded mobile ui understanding with multimodal llms",
                "Nphardeval4v: A dynamic reasoning benchmark of multimodal large language models",
                "Eyes wide shut? exploring the visual shortcomings of multimodal llms",
                "Designqa: A multimodal benchmark for evaluating large language models' understanding of engineering documentation",
                "Valley: Video assistant with large language model enhanced ability",
                "Gpt-4 technical report",
                "What's \" up \" with vision-language models? investigating their struggle with spatial reasoning",
                "Measuring multimodal mathematical reasoning with math-vision dataset",
                "Learning the user's deeper preferences for multi-modal recommendation systems",
                "Scemqa: A scientific college entrance level multimodal question answering benchmark",
                "Cider: Consensus-based image description evaluation",
                "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "Llama-adapter v2: Parameter-efficient visual instruction model",
                "Qlora: Efficient finetuning of quantized llms",
                "Implicitave: An open-source dataset and multimodal llms benchmark for implicit attribute value extraction",
                "V?: Guided visual search as a core mechanism in multimodal llms",
                "Red teaming visual language models",
                "Chartbench: A benchmark for complex visual reasoning in charts",
                "Evaluating object hallucination in large vision-language models",
                "An llm-free multi-dimensional benchmark for mllms hallucination evaluation",
                "Vision-language models for vision tasks: A survey",
                "Scanqa: 3d question answering for spatial scene understanding",
                "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                "Mmbench: Is your multi-modal model an all-around player",
                "Aligning large multimodal models with factually augmented rlhf",
                "Benchmarking trustworthiness of multimodal large language models: A comprehensive study",
                "Mm-spubench: Towards better understanding of spurious biases in multimodal llms",
                "Mm-safetybench: A benchmark for safety evaluation of multimodal large language models",
                "Logicvista: Multimodal llm logical reasoning benchmark in visual contexts",
                "Visualagentbench: Towards large multimodal models as visual foundation agents"
            ]
        },
        "topic_history": [
            {
                "name": "Efficient multi-prompt evaluation of LLMs",
                "arxiv_id": "2405.17202",
                "reference": [
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Label-efficient model selection for text generation",
                    "Efficient benchmarking (of language models",
                    "Statistical theories of mental test scores",
                    "Holistic evaluation of language models",
                    "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
                    "Item response theory",
                    "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning",
                    "Building an evaluation scale using item response theory",
                    "State of what art? a call for multi-prompt llm evaluation",
                    "Handbook of item response theory: Three volume set",
                    "How predictable are large language model capabilities? a case study on big-bench",
                    "tinybenchmarks: evaluating llms with fewer examples",
                    "Anchor points: Benchmarking models with much fewer examples",
                    "The icl consistency test",
                    "Mind your format: Towards consistent evaluation of in-context learning improvements",
                    "Evaluation examples are not equally informative: How should that change NLP leaderboards",
                    "Lmentry: A language model benchmark of elementary language tasks",
                    "Item response theory models in the measurement theory",
                    "Best arm identification for prompt learning under a limited budget",
                    "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
                    "Comparing test sets with item response theory"
                ]
            },
            {
                "name": "MoralBench: Moral Evaluation of LLMs",
                "arxiv_id": "2406.04428",
                "reference": [
                    "Liberals and conservatives rely on different sets of moral foundations",
                    "Moral foundations theory: The pragmatic validity of moral pluralism",
                    "Will morality or political ideology determine attitudes to climate change",
                    "Moral foundations of large language models",
                    "The ghost in the machine has an american accent: value conflict in gpt",
                    "Scaling laws for neural language models",
                    "Aligning ai with shared human values",
                    "The moral foundations of trust",
                    "Moral foundations and political orientation: Systematic review and meta-analysis",
                    "Gpt-3: Its nature, scope, limits, and consequences",
                    "Moral intuitions and political orientation: Similarities and differences between south korea and the united states",
                    "Gpt-3: What's it good for",
                    "Introducing chatgpt",
                    "How words do the work of politics: Moral foundations theory and the debate over stem cell research",
                    "Persistent anti-muslim bias in large language models",
                    "The moral roots of socio-political attitudes: How moral foundations theory can help to understand contested scientific issues and political ideology",
                    "Can machines learn morality? the delphi experiment",
                    "On the dangers of stochastic parrots: Can language models be too big",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Shifting liberal and conservative attitudes using moral foundations theory",
                    "Does moral code have a moral code? probing delphi's moral philosophy",
                    "Language models are unsupervised multitask learners",
                    "Who is gpt-3? an exploration of personality, values and demographics"
                ]
            },
            {
                "name": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
                "arxiv_id": "2407.10457",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Program synthesis with large language models",
                    "Ctrl: A conditional transformer language model for controllable generation",
                    "Measuring massive multitask language understanding",
                    "Measuring mathematical problem solving with the math dataset",
                    "Think you have solved question answering? try arc, the ai2 reasoning challenge",
                    "The curious case of neural text degeneration",
                    "Wildbench: Benchmarking llms with challenging tasks from real users in the wild",
                    "Hierarchical neural story generation",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "Evaluating large language models trained on code",
                    "From live data to high-quality benchmarks: The arena-hard pipeline",
                    "Beam search strategies for neural machine translation",
                    "A learning algorithm for boltzmann machines",
                    "HellaSwag: Can a machine really finish your sentence"
                ]
            },
            {
                "name": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
                "arxiv_id": "2402.08699",
                "reference": [
                    "Improving neural machine translation models with monolingual data",
                    "On multi-modal learning of editing source code",
                    "Evaluating large language models trained on code",
                    "Understanding back-translation at scale",
                    "Automating code review activities by large-scale pre-training",
                    "Competition-level code generation with alphacode",
                    "Code generation as a dual task of code summarization",
                    "OctoPack: Instruction tuning code large language models",
                    "Measuring coding challenge competence with apps",
                    "Faithfulness tests for natural language explanations",
                    "Program synthesis with large language models",
                    "CodeBERTScore: Evaluating code generation with pretrained models of code",
                    "Beyond accuracy: Evaluating self-consistency of code large language models with IdentityChain",
                    "CodeBLEU: a method for automatic evaluation of code synthesis",
                    "Data augmentation using back-translation for context-aware neural machine translation",
                    "A syntactic neural model for general-purpose code generation",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "JuICe: A large scale distantly supervised dataset for open domain context-based code generation",
                    "BLEU: a method for automatic evaluation of machine translation"
                ]
            },
            {
                "name": "Does Biomedical Training Lead to Better Medical Performance?",
                "arxiv_id": "2404.04067",
                "reference": [
                    "Benchmarking large language models on answering and explaining challenging medical questions",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Dr. bench: Diagnostic reasoning benchmark for clinical natural language processing",
                    "Med42-v2: A suite of clinical llms",
                    "Capabilities of gemini models in medicine",
                    "Meditron3 model card",
                    "Med42 - evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches",
                    "Impact of high-quality, mixed-domain data on the performance of medical language models",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Measuring massive multitask language understanding",
                    "Large language models encode clinical knowledge",
                    "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
                    "Large language models in the clinic: A comprehensive benchmark",
                    "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
                    "LongHealth: A Question Answering Benchmark with Long Clinical Documents",
                    "PubMedQA: A dataset for biomedical research question answering",
                    "Ehrnoteqa: A patient-specific question answering benchmark for evaluating large language models in clinical settings"
                ]
            },
            {
                "name": "MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge",
                "arxiv_id": "2406.02919",
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Large language models encode clinical knowledge",
                    "Measuring massive multitask language understanding",
                    "On the generation of medical dialogues for covid",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding",
                    "Towards expert-level medical question answering with large language models",
                    "Cblue: A chinese biomedical language understanding evaluation benchmark",
                    "Overview of the medical question answering task at trec 2017 liveqa",
                    "Med42 - a clinical large language model",
                    "Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets",
                    "PubMedQA: A dataset for biomedical research question answering",
                    "Bridging the gap between consumers' medication questions and trusted answers",
                    "Meddialog: Large-scale medical dialogue datasets"
                ]
            },
            {
                "name": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
                "arxiv_id": "2402.13249",
                "reference": [
                    "Evaluating the factual consistency of abstractive text summarization",
                    "BERTScore: Evaluating text generation with BERT",
                    "Neural text summarization: A critical evaluation",
                    "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
                    "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
                    "ChatGPT as a factual inconsistency evaluator for abstractive text summarization",
                    "DialSummEval: Revisiting summarization evaluation for dialogues",
                    "On faithfulness and factuality in abstractive summarization",
                    "Evaluating large language models on medical evidence summarization",
                    "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
                    "SummEdits: Measuring LLM ability at factual reasoning through the lens of summarization",
                    "AlignScore: Evaluating factual consistency with a unified alignment function",
                    "SummEval: Re-evaluating summarization evaluation",
                    "ROUGE: A package for automatic evaluation of summaries",
                    "Is ChatGPT a good NLG evaluator? a preliminary study",
                    "Bleu: a method for automatic evaluation of machine translation",
                    "SummaC: Re-visiting NLI-based models for inconsistency detection in summarization",
                    "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
                    "CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization",
                    "Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors",
                    "Annotating and modeling fine-grained factuality in summarization"
                ]
            },
            {
                "name": "Multitask-based Evaluation of Open-Source LLM on Software Vulnerability",
                "arxiv_id": "2404.02056",
                "reference": [
                    "The effect of common vulnerability scoring system metrics on vulnerability exploit delay",
                    "Detecting and augmenting missing key aspects in vulnerability descriptions",
                    "Deep reinforcement learning from human preferences",
                    "Linevd: Statement-level vulnerability detection using graph neural networks",
                    "Mvd: Memory-related vulnerability detection based on flow-sensitive graph neural networks",
                    "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Path-sensitive code embedding via contrastive learning for software vulnerability detection",
                    "Cvefixes: automated collection of vulnerabilities and their fixes from open-source software",
                    "A syntax-guided edit decoder for neural program repair",
                    "Atvhunter: Reliable version detection of third-party libraries for vulnerability identification in android applications",
                    "A multi-target approach to estimate software vulnerability characteristics and severity scores",
                    "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
                    "Distinguishing look-alike innocent and vulnerable code by subtle semantic representation learning and explanation",
                    "Graphcodebert: Pre-training code representations with data flow",
                    "A survey on data-driven software vulnerability assessment and prioritization",
                    "Linevul: A transformer-based line-level vulnerability prediction",
                    "Unified pre-training for program understanding and generation",
                    "Vuldeepecker: A deep learning-based system for vulnerability detection",
                    "Predicting missing information of key aspects in vulnerability reports",
                    "Fva: Assessing function-level vulnerability by integrating flow-sensitive structure and code statement semantic",
                    "Vulcnn: An image-inspired scalable vulnerability detection system",
                    "Vulnerability detection with fine-grained interpretations",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "Generating informative cve description from exploitdb posts by extractive summarization",
                    "Key aspects augmentation of vulnerability description based on multiple security databases",
                    "Proximal policy optimization algorithms",
                    "Improving language understanding by generative pre-training",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "An empirical study of deep learning models for vulnerability detection",
                    "The Best of Both Worlds: Integrating Semantic Features with Expert Features for Defect Prediction and Localization",
                    "Fine-tuning language models from human preferences",
                    "Pca: memory leak detection using partial call-path analysis",
                    "Smoke: scalable path-sensitive memory leak detection for millions of lines of code",
                    "Defect identification, categorization, and repair: Better together",
                    "Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks",
                    "Vuldeelocator: a deep learning-based fine-grained vulnerability detector",
                    "Common vulnerabilities and exposures (cve",
                    "Deepcva: Automated commit-level vulnerability assessment with deep multi-task learning",
                    "Large language model for vulnerability detection: Emerging results and future directions",
                    "Deep learning based vulnerability detection: Are we there yet",
                    "Language models are few-shot learners",
                    "Attention is all you need",
                    "Program repair: Automated vs. manual",
                    "Codet5+: Open code large language models for code understanding and generation",
                    "Sequencer: Sequence-to-sequence learning for end-to-end program repair",
                    "Dataflow analysis-inspired deep learning for efficient vulnerability detection",
                    "Unixcoder: Unified cross-modal pre-training for code representation",
                    "securityfocus",
                    "Sysevr: A framework for using deep learning to detect software vulnerabilities",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
                "arxiv_id": "2402.16775",
                "reference": [
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "CMMLU: measuring massive multitask language understanding in chinese",
                    "Emergent abilities of large language models",
                    "Instruction-following evaluation for large language models",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "M3KE: A massive multi-level multi-subject knowledge evaluation benchmark for chinese large language models",
                    "BBQ: A hand-built bias benchmark for question answering",
                    "Do emergent abilities exist in quantized large language models: An empirical study",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Finemath: A fine-grained mathematical evaluation benchmark for chinese large language models",
                    "Unlock predictable scaling from emergent abilities",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "LHMKE: A large-scale holistic multi-subject knowledge evaluation benchmark for chinese large language models",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Measuring massive multitask chinese understanding",
                    "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
                    "Do large language models know what they don't know",
                    "Alignbench: Benchmarking chinese alignment of large language models",
                    "Quantization and training of neural networks for efficient integer-arithmetic-only inference",
                    "Roleeval: A bilingual role evaluation benchmark for large language models",
                    "Gpt3.int8() : 8-bit matrix multiplication for transformers at scale",
                    "CORECODE: A common sense annotated dialogue dataset with benchmark tasks for chinese large language models",
                    "Gpteval: A survey on assessments of chatgpt and GPT",
                    "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
                    "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models",
                    "OPTQ: accurate quantization for generative pre-trained transformers",
                    "Outlier suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
                    "A survey on evaluation of large language models",
                    "Spqr: A sparse-quantized representation for near-lossless LLM weight compression",
                    "Through the lens of core competency: Survey on evaluation of large language models",
                    "LFED: A literary fiction evaluation dataset for large language models",
                    "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
                    "Are emergent abilities of large language models a mirage",
                    "Are emergent abilities in large language models just in-context learning",
                    "CBBQ: A chinese bias benchmark dataset curated with human-ai collaboration for large language models",
                    "Measuring massive multitask language understanding",
                    "Qlora: Efficient finetuning of quantized llms"
                ]
            },
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
                "arxiv_id": "2402.04249",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    " \"do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
                    "Robust prompt optimization for defending language models against jailbreaking attacks",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models, 2023b",
                    "Mistral 7b",
                    "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
                    "Rain: Your language models can align themselves without finetuning",
                    "Llm censorship: A machine learning challenge or a computer security problem",
                    "A holistic approach to undesired content detection in the real world",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Masterkey: Automated jailbreak across multiple large language model chatbots",
                    "Red teaming language models with language models",
                    "Are aligned neural networks adversarially aligned",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Universal adversarial triggers for attacking and analyzing NLP",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Gpt-4 technical report",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Mart: Improving llm safety with multi-round automatic red-teaming",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Gradient-based adversarial attacks against text transformers",
                    "Explore, establish, exploit: Red teaming language models from scratch",
                    "Jailbroken: How does llm safety training fail",
                    "Image hijacks: Adversarial images can control generative models at runtime",
                    "Automatically auditing large language models via discrete optimization",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
                "arxiv_id": "2401.01614",
                "reference": [
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Fine-grained visual prompting",
                    "Referitgame: Referring to objects in photographs of natural scenes",
                    "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
                    "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v",
                    "Language models can solve computer tasks",
                    "Multimodal web navigation with instruction-finetuned foundation models",
                    "Ferret: Refer and ground anything anywhere at any granularity",
                    "The dawn of lmms: Preliminary explorations with gpt-4v(ision",
                    "An in-depth look at gemini's language abilities",
                    "A real-world webagent with planning, long context understanding, and program synthesis",
                    "What does clip know about a red circle? visual prompt engineering for vlms",
                    "Reinforcement learning on web interfaces using workflow-guided exploration",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Gpt-4v(ision) as a generalist evaluator for vision-language tasks",
                    "Kosmos-2: Grounding multimodal large language models to the world",
                    "Flin: A flexible natural language interface for web navigation",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Gpt-4 technical report",
                    "Bubogpt: Enabling visual grounding in multi-modal llms",
                    "Gemini: A family of highly capable multimodal models",
                    "Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation",
                    "Mind2web: Towards a generalist agent for the web",
                    "Cogagent: A visual language model for gui agents",
                    "Measuring massive multitask language understanding",
                    "Scienceqa: a novel resource for question answering on scholarly articles",
                    "Understanding html with large language models",
                    "Webshop: Towards scalable real-world web interaction with grounded language agents",
                    "World of bits: An open-domain platform for web-based agents",
                    "Hierarchical prompting assists large language model on web navigation",
                    "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                    "From pixels to ui actions: Learning to follow instructions via graphical user interfaces",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
                ]
            },
            {
                "name": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "arxiv_id": "2403.20330",
                "reference": [
                    "Phi2: The surprising power of small language models",
                    "A-okvqa: A benchmark for visual question answering using world knowledge",
                    "A diagram is worth a dozen images",
                    "Palm: Scaling language modeling with pathways",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Internlm: A multilingual language model with progressively enhanced capabilities",
                    "Mistral 7b",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Deepseek llm: Scaling open-source language models with longtermism",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "To see is to believe: Prompting gpt-4v for better visual instruction tuning",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Improved baselines with visual instruction tuning",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Multilayer perceptron (mlp",
                    "Can vision-language models think from a first-person perspective",
                    "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Yi: Open foundation models by 01. ai",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Cheap and quick: Efficient vision-language instruction tuning for large language models",
                    "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Mixtral of experts",
                    "Deepseek-vl: Towards real-world vision-language understanding",
                    "Visual instruction tuning",
                    "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Scaling up visual and vision-language representation learning with noisy text supervision",
                    "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                    "Learning transferable visual models from natural language supervision",
                    "Baichuan 2: Open large-scale language models",
                    "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Chatgpt",
                    "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
                    "Qwen technical report",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Gemini: a family of highly capable multimodal models",
                    "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
                    "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                    "Training language models to follow instructions with human feedback"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "AgentBench: Evaluating LLMs as Agents",
            "arxiv_id": "2308.03688",
            "isAPA": true,
            "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous,targeting real-world pragmatic missions beyond traditional NLP tasks. As a result,there has been an urgent need to evaluate LLMs as agents on challenging tasksin interactive environments. We present AGENTBENCH, a multi-dimensionalevolving benchmark that currently consists of 8 distinct environments to assessLLM-as-Agent's reasoning and decision-making abilities in a multi-turn openended generation setting. Our extensive test over 27 API-based and open-sourced(OSS) LLMs shows that, while top commercial LLMs present a strong abilityof acting as agents in complex environments, there is a significant disparity inperformance between them and OSS competitors. We identify the typical reasonsof failures in environments and LLMs, showing that poor long-term reasoning,decision-making, and instruction following abilities are the main obstacles fordeveloping usable LLM agents. Training on code and high quality multi-turnalignment data could improve agent performance. Datasets, environments, andan integrated evaluation package for AGENTBENCH are released at https://github.com/THUDM/AgentBench.",
            "reference": [
                "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners",
                "Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv",
                "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim\u00e9nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. Transactions on Machine Learning Research",
                "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv",
                "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
                "Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents",
                "Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April",
                "Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kry\u015bci\u0144ski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir Radev. Fetaqa: Free-form table question answering",
                "Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems",
                "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April",
                "Jack Edmonds and Richard M Karp. Theoretical improvements in algorithmic efficiency for network flow problems. Journal of the ACM (JACM",
                "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling",
                "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm",
                "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv",
                "Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023d",
                "Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021a",
                "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv",
                "Amy K Hoover, Julian Togelius, Scott Lee, and Fernando de Mesentier Silva. The many ai challenges of hearthstone. KI-K\u00fcnstliche Intelligenz",
                "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv",
                "Heinrich K\u00fcttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rockt\u00e4schel. The nethack learning environment. Advances in Neural Information Processing Systems",
                "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023a",
                "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge",
                "LAION. Open-assistant. https://github.com/LAION-AI/Open-Assistant",
                "Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv",
                "Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system",
                "Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00e4schel, Douwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a fantasy text adventure game",
                "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models",
                "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research",
                "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv",
                "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv",
                "Anonymous. Knowledge base question answering as tool learning. under review",
                "Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning",
                "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv",
                "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science",
                "Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems",
                "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding",
                "Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. Beyond i.i.d.: Three levels of generalization for question answering on knowledge bases",
                "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv",
                "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. ArXiv, abs/2305.16291, 2023b",
                "Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00edn-Mart\u00edn, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments",
                "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv",
                "Dirk Merkel et al. Docker: lightweight linux containers for consistent development and deployment. Linux j",
                "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv",
                "Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review",
                "Anthropic. Claude 2, 2023b. URL https://www.anthropic.com/index/claude",
                "Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. arXiv preprint arXiv",
                "Yu Su, Huan Sun, Brian M. Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan. On generating characteristic-rich question sets for QA evaluation",
                "Toran Bruce Richards. Auto-gpt: An autonomous gpt-4 experiment",
                "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models",
                "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv",
                "Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data, 2023a",
                "John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. arXiv preprint arXiv",
                "Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u00f4t\u00e9, and Xingdi Yuan. Interactive fiction games: A colossal adventure",
                "Paul Sloane. Lateral thinking puzzlers. Sterling Publishing Company, Inc",
                "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00e8, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv",
                "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization",
                "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv",
                "Pattie Maes. Agents that reduce work and information overload. Commun. ACM",
                "Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive. IEEE transactions on knowledge and data engineering",
                "Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions",
                "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv",
                "Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaboratively created graph database for structuring human knowledge",
                "LR Ford Jr and DR F\u0173lkerson. Flows in networks",
                "R OpenAI. Gpt-4 technical report. arXiv, pp",
                "Anthropic. Introducing claude, 2023a. URL https://www.anthropic.com/index/introducing-claude",
                "Yohei Nakajima. Babyagi. Python. https://github. com/yoheinakajima/babyagi",
                "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis",
                "Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. ArXiv, abs",
                "Edward De Bono. Lateral thinking. New York, pp",
                "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. HybridQA: A dataset of multi-hop question answering over tabular and textual data",
                "Philip Osborne, Heido N\u00f5mm, and Andr\u00e9 Freitas. A survey of text games for reinforcement learning informed by natural language. Transactions of the Association for Computational Linguistics",
                "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems",
                "Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential question answering",
                "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "Marc-Alexandre C\u00f4t\u00e9, Akos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games",
                "Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv",
                "Yu Gu, Xiang Deng, and Yu Su. Don't generate, discriminate: A proposal for grounding language models to real-world environments",
                "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b",
                "Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
                "Yu Gu and Yu Su. ArcaneQA: Dynamic program induction and contextualized encoding for knowledge base question answering",
                "Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models",
                "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs",
                "Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart\u00edn-Mart\u00edn, Linxi Fan, Guanzhi Wang, Claudia P\u00e9rez-D'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simulation environment for interactive tasks in large realistic scenes",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding",
                "OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt",
                "John R. Searle. Speech acts: An essay in the philosophy of language. Language",
                "Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, et al. The gem benchmark: Natural language generation, its evaluation and metrics",
                "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners",
                "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems",
                "Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables",
                "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
                "Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv",
                "Agentgpt. Python. https://github.com/reworkd/AgentGPT",
                "Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyuan Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. ArXiv, abs",
                "Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv"
            ],
            "related work": "Evaluation of LLMs. The general capabilities of self-supervised (Liu et al., 2021) LLMs (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022; Scao et al., 2022; Zeng et al., 2022; Touvron et al., 2023) , especially those chat-aligned ones (Ouyang et al., 2022; Anthropic, 2023a; OpenAI, 2023) , have refreshed people's impression on deep learning systems and significantly transcended the conventional scope of NLP evaluation. It thus makes the evaluation of LLMs an urgent and challenging problem. Compared to previous efforts focusing on a subset of specified tasks (Wang et al., 2019; ; Gehrmann et al., 2021) , an increasing number of benchmarks are including broader spectra of tasks and datasets (Hendrycks et al., 2021b; Liang et al., 2022; Srivastava et al., 2023) in the evaluation. However, most of them are still limited to traditional tasks and thus fail to evaluate LLMs' open-ended generation, multi-turn interaction, and ability to act as agents.LLM-as-Agent. In pre-LLM era, text game environments such as TextWorld (C\u00f4t\u00e9 et al., 2019) , Jericho (Hausknecht et al., 2020) , and LIGHT (Urbanek et al., 2019) are dominant in language agent study which bases on BERT (Devlin et al., 2019) and reinforcement learning. With the advent of LLMs, the study of LLM agents begins to thrive (Huang et al., 2022) , especially after Chain-of-Thought (Wei et al., 2022b) came out. ReAct (Yao et al., 2023b) is a pioneer work to combine CoT reasoning and actions in agent tasks. Later, a bunch of advanced reasoning strategies (Kim et al., 2023; Shinn et al., 2023; Wang et al., 2023d; Liu et al., 2023; Yao et al., 2023a; Gu et al., 2023) and applications (Park et al., 2023; Richards, 2023; Nakajima, 2023; age, 2023) for LLM-as-Agent have emerged and arouse much public interest. Nevertheless, limited datasets and models and available on the topic, without a standard and comprehensive benchmark. AgentBench presents the first systematic benchmark for evaluating LLM-as-Agent with a broad coverage of tasks and available LLMs. Additionally, it also initiates the idea of adopting agent tasks to measure LLM performance.Evaluating LLMs in Executive Environments. As LLMs become increasingly capable of real-world challenges, there is also a trend to evaluate them in executive environments rather than static datasets. Besides text games (e.g., ALFWorld (Shridhar et al., 2020b) ) , another main stream of works lies in code execution. APPS (Hendrycks et al., 2021a) , HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) pioneer the effort to evaluate code LLMs for functional correctness instead of text similarity. The paradigm has been later widely recognized and adopted in following works (Li et al., 2022; Zheng et al., 2023; Nijkamp et al., 2023) . However, few previous code evaluation frameworks consider multi-turn interactions. A concurrent work InterCode (Yang et al., 2023) releases a framework that allows evaluation of interaction between models and Bash and SQL environments, which are similar to OS and DB tasks in AgentBench.",
            "date": "2023"
        },
        "topic": "LLMs-based Agents",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
                "arxiv_id": "2403.05307",
                "subtitles": [
                    "Large Language Models for Data Analysis",
                    "Data Analysis Benchmarks",
                    "Multi-Agent Environments for Data Generation"
                ],
                "reference": [
                    "Do lvlms understand charts? analyzing and correcting factual errors in chart captioning",
                    "Universal self-consistency for large language model generation",
                    "Iterative forward tuning boosts in-context learning in language models",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Towards ecologically valid research on language user interfaces",
                    "Beyond generating code: Evaluating gpt on a data visualization course",
                    "A survey for in-context learning",
                    "Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers",
                    "Text-to-sql empowered by large language models: A benchmark evaluation",
                    "Natural language to code generation in interactive data science notebooks",
                    "Evaluating large language models trained on code",
                    "Can LLM already serve as a database interface? a BIg bench for large-scale database grounded text-to-SQLs",
                    "Reactable: Enhancing react for table question answering",
                    "Learning to simulate natural language feedback for interactive semantic parsing",
                    "MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback",
                    "Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases",
                    "Llm-assisted code cleaning for training accurate code generators",
                    "Infiagent-dabench: Evaluating agents on data analysis tasks",
                    "Codes: Towards building open-source language models for text-to-sql",
                    "Data-copilot: Bridging billions of data and humans with autonomous workflow",
                    "Program synthesis with large language models",
                    "Middleware for llms: Tools are instrumental for language agents in complex environments",
                    "Mac-sql: Multi-agent collaboration for text-to-sql",
                    "Dts-sql: Decomposed text-to-sql with small large language models",
                    "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                    "Din-sql: Decomposed in-context learning of text-to-sql with self-correction",
                    "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Api-bank: A comprehensive benchmark for tool-augmented llms",
                    "Opencodeinterpreter: Integrating code generation with execution and refinement",
                    "Sheetcopilot: Bringing software productivity to the next level through large language models",
                    "Teaching large language models to self-debug",
                    "Text2analysis: A benchmark of table question answering with advanced data analysis and unclear queries",
                    "Dialgen: collaborative human-lm generated dialogues for improved understanding of human-human conversations",
                    "S3eval: A synthetic, scalable, systematic evaluation suite for large language models",
                    "DS-1000: A natural and reliable benchmark for data science code generation"
                ],
                "related_work": "7Related WorkLarge Language Models for Data Analysis.The use of LLMs for data analysis has been a topic of interest in recent years. LLMs powered by In-Context Learning(Yang et al.,2023; Dai et al.,2023; Dong et al.,2023) have been employed in various data analysis tasks, such as SQL query generation(Pourreza and Rafiei,2024a; Gao et al.,2023; Lei et al.,2023; Zhang et al.,2023b; Gu et al.,2024; Wang et al.,2023a; Pourreza and Rafiei,2024b; Li et al.,2024b) , pandas or python code generation(Jain et al.,2023; Chen et al.,2024,2023a; Li et al.,2024c; Zha et al.,2023; Zhang et al.,2023a; Zheng et al.,2024b) , and data visualization(Chen et al.,2023b; Huang et al.,2023a) . However, most of these works focus on single-turn setting, where the user's query is explicit and does not require any interaction or clarification. Recently, there has been a growing interest in interactive data analysis, where the user intents may need to be clarified or refined through interactive communication(De Vries et al.,2020; Yan et al.,2023; Wang et al.,2024) .Data Analysis Benchmarks.The development of benchmarks for data analysis tasks has been a crucial factor in driving the progress of LLMs in data science. Existing benchmarks can be broadly categorized into single-turn and multi-turn benchmarks. Single-turn benchmarks, such as HumanEval(Chen et al.,2021) , MBPP(Austin et al.,2021) , Spider(Yu et al.,2018) , BIRD(Li et al.,2023a) , Text2Analysis(He et al.,2023) , DABench(Hu et al.,2024) and DS-1000(Lai et al.,2023) , focus on generating code snippets or closed-form insight summaries for data analysis given a single user query. To explore interactive nature of real-world data analysis scenarios, where the user's intent may need to be clarified or refined through interactive communication, several multi-turn benchmarks have been proposed, including CoSQL(Yu et al.,2019a) , and ARCADE(Yin et al.,2023) . However, these benchmarks are primarily focused on code generation and do not cover other aspects of data analysis, such as data visualization and understanding based on intermediate results. Our work extends the existing literature by introducing a new benchmark,Tapilot-Crossing, for evaluating LLM agents in interactive data analysis tasks across wide range of data analysis settings.Multi-Agent Environments for Data Generation.LLMs have proven to be effective in constructing multi-agent environments for automatic data generation. For instance,Lu et al. (2023) andDing et al. (2023) simulate dialogs for QA and text generation tasks. AlsoLi et al. (2023b) generates data about API calls using multi-agent environments. This is because LLM agents can simulate believable human actions when placed in an environment with dynamically updating knowledge and memory(Park et al.,2023) . Inspired by this, we also createdDecision Companyto generate interaction log data for data analysis with more believable behaviors. Unlike most of previous work on training dataset generation, our research pioneers the construction of the interactive benchmark with a specific focus on interactive data analysis agent evaluation.",
                "abstract": "Interactive Data Analysis, the collaboration between humans and LLM agents, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic interactive logs for data analysis hinder the quantitative evaluation of Large Language Model (LLM) agents in this task. To mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering 4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing is constructed by an economical multi-agent environment, Decision Company, with few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing, which underscores the challenges of interactive data analysis. Furthermore, we propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy that guides LLM agents to learn from successful history. Experiments demonstrate that Air can evolve LLMs into effective interactive data analysis agents, achieving a relative performance improvement of up to 44.5%."
            },
            {
                "name": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
                "arxiv_id": "2402.14672",
                "subtitles": [
                    "Interface Complex Environments with LLMs",
                    "Tool Learning"
                ],
                "reference": [
                    "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
                    "Grounding 'grounding' in NLP",
                    "AgentBench: Evaluating llms as agents",
                    "Can LLM already serve as A database interface? A big bench for large-scale database grounded text-to-sqls",
                    "Few-shot in-context learning on knowledge base question answering",
                    "Toolformer: Language models can teach themselves to use tools",
                    "DecAF: Joint decoding of answers and logical forms for question answering over knowledge bases",
                    "ToolLLM: Facilitating large language models to master 16000+ real-world apis",
                    "API-Bank: A benchmark for tool-augmented llms",
                    "Don't generate, discriminate: A proposal for grounding language models to real-world environments",
                    "Exploring chain of thought style prompting for text-to-sql",
                    "Code-style in-context learning for knowledge-based question answering",
                    "Tool learning with foundation models",
                    "Mind2web: Towards a generalist agent for the web",
                    "Augmented language models: a survey",
                    "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
                    "Chameleon: Plug-and-play compositional reasoning with large language models",
                    "A comprehensive exploration on wikisql with table-aware word contextualization",
                    "StructGPT: A general framework for large language model to reason over structured data",
                    "Alfworld: Aligning text and embodied environments for interactive learning"
                ],
                "related_work": "2Related WorkInterface Complex Environments with LLMs.Existing methods that feed the environment directly into the LLM for groundingChandu et al. (2021) would fail in complex environments due to scalability issues. Specifically, these methods process the environment by linearizing it into discrete tokensHwang et al. (2019) ; Shridhar et al. (2021) ; Yu et al. (2023) ; Liu et al. (2023) ; Tai et al. (2023) ; Song et al. (2023) . However, linearizing expansive environments like databases with millions of entriesLi et al. (2023a) or lengthy webpage HTML codeDeng et al. (2023) can often exceed an LLM's input length constraints. Alternative studies bypass the LLM's direct interaction with complex environments by generating ungrounded draft plans for post-processing groundingLi et al. (2023c) ; Nie et al. (2023) or by using the LLM to assess grounded plans created via predefined rulesGu et al. (2023) . Such strategies do not fully utilize the LLMs' innate reasoning potential in actively navigating complex environments. In this paper, we explore a new paradigm where we can bypass these issues by equipping LLMs with a suite of comprehensive tools to actively gather necessary information about the environment upon demand, leveraging the LLMs' inherent reasoning capabilities. The most closely related work to ours is StructGPTJiang et al. (2023b) . However, the narrow tool selection of StructGPT (i.e., only two tools for KBs and three schema-level tools for databases) largely constrains its flexibility in perceiving the complex environment when handling diverse tasks.Tool Learning.Tools are essential for enhancing the capabilities of LLMsSchick et al. (2023) ; Qin et al. (2023a) ; Mialon et al. (2023) ; Hao et al. (2023) . Existing research, such as ToolLLMQin et al. (2023b) and API-BankLi et al. (2023b) , focuses on open-domain applications with a wide array of readily available RESTful APIs. In contrast, this paper specifically aims to study the potential of tools in augmenting LLMs to effectively execute tasks within complex environments, where we carefully craft the specialized tools for different environments by ourselves. In addition, research focusing on RESTful APIs typically displays shallow reasoning, while practical tasks within a complex environment typically entail a long sequence of actions (e.g., querying a KB or browsing a webpage) . To enable tool use in more intricate settings within a more specific complex environment, StructGPTJiang et al. (2023b) employs a predefined sequence of tool invocations; ChameleonLu et al. (2023) functions in an open-loop setting where the LLM directly produces a sequence for tool usage before any execution occurs. Both of them fail to seamlessly integrate the reasoning capacity of the LLM with the use of tools. In this paper, we build on ReAct to tightly synergize the generation of a reasoning step and corresponding tool use. Additionally, we introduce two simple yet effective strategies aimed at improving the accuracy of action prediction.",
                "abstract": "The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with these tools, GPT-4 achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks. Our findings illuminate the path for advancing language agents in complex real-world applications."
            },
            {
                "name": "A Learnable Agent Collaboration Network Framework for Personalized Multimodal AI Search Engine",
                "arxiv_id": "2409.00636",
                "subtitles": [
                    "AI Search Engine",
                    "Personalized Generation of LLM",
                    "AI Agent"
                ],
                "reference": [
                    "Symbolic Learning Enables Self-Evolving Agents",
                    "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
                    "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "Simulating Financial Market via Large Language Model based Agents",
                    "A large annotated corpus for learning natural language inference",
                    "Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models",
                    "Expel: Llm agents are experiential learners",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "RARR: Researching and Revising What Language Models Say, Using Language Models",
                    "Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism",
                    "Gpt4tools: Teaching large language model to use tools via self-instruction",
                    "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering",
                    "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
                    "Knowledge-augmented large language models for personalized contextual query suggestion",
                    "Query Rewriting for Retrieval-Augmented Large Language Models",
                    "Lift yourself up: Retrieval-augmented text generation with self-memory",
                    "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
                    "Making Retrieval-Augmented Language Models Robust to Irrelevant Context"
                ],
                "related_work": "2.Related Works 2.1.AI Search Engine AI search engines represent the convergence of large language models (LLMs), retrieval-augmented generation (RAG), and intelligent agent technologies, heralding a new era of search engine innovation. Given the nascent stage of AI search engine technology, we categorize the information retrieval process into six distinct phases (Gao et al., 2024b): identification of information retrieval requirements (Kim et al., 2023), retrieval augmentation (Ma et al., 2023), information retrieval and knowledge gathering (Zhang et al., 2024), knowledge caching (Zhao et al., 2023), knowledge filtering and ranking (Bowman et al., 2015; Yoran et al., 2023), and LLM-based content generation, followed by verification and refinement (Gao et al., 2023). Certain AI search engines do not adopt a conversational interface, requiring users to input search queries into a search bar, thereby omitting the initial phase of identifying information retrieval needs. Retrieval enhancement can be selectively integrated before the information retrieval and knowledge gathering phase, generating multiple search keywords to ensure a higher recall rate of relevant knowledge. Knowledge caching post-retrieval mitigates resource consumption associated with the retrieval and gathering phase, enhancing system responsiveness and efficiency. Knowledge filtering and ranking technique is employed before the generation phase, eliminate irrelevant information, thereby enhancing the precision of the retrieved knowledge, improving the robustness of LLM responses, and ensuring content quality. The verification and refinement phase post-generation ensures the factual accuracy of the AI-generated content and optimizes its presentation format for the user. Therefore, among these six phases, the steps of information retrieval and knowledge gathering, as well as LLM-based content generation, are necessary, corresponding to the retrieve-then-read pipeline of RAG. The other steps are optional according to practical application scenarios. The advanced studies mentioned above focus primarily on ensuring the precision and efficiency of information generation in AI search engines. Beyond these critical metrics, our study more emphasizes content richness, personalization, and interactivity. These factors are crucial for maintaining the attractiveness of the content and enhancing the overall user experience with AI search engines. 2.2.Personalized Generation of LLM Recent studies (Kirk et al., 2023) emphasize the importance of personalizing large language models (LLMs) beyond aggregate fine-tuning methods like RLHF, as these may not fully capture diverse user preferences and values. Micro-level preference learning can better align models with individual users. Current personalization techniques mainly involve prompt tuning, which models user profiles based on historical search data to prompt LLM generating tailored outputs. Basic approaches use the entire user action history for prompting, while more advanced methods selectively retrieve relevant user data using memory mechanisms (Zhou et al., 2024b). To address potential information loss, (Richardson et al., 2023) proposes a task-aware user profile summarization for prompting. Another approach (Baek et al., 2024) constructs knowledge graphs from user search and browsing activities to enhance prompt relevance. While existing work focuses on prompt construction and user profiling, our Agent Collaboration Network (ACN) architecture shifts the emphasis to tuning agents across each step of the AI search engine workflow, aiming for a more integrated and efficient personalization strategy. 2.3.AI Agent AI agents can interact with environments, dynamically selecting optimal actions to achieve predetermined objectives. With the advent of LLMs capable of function calls (Yang et al., 2024; Qin et al., 2023), AI agents driven by such models exhibit exceptional intelligence and flexibility. This advancement paves the way for the evolution of RAG technology towards an Agentic RAG paradigm1 1 https://github.com/infiniflow/ragflow . Recent studies have introduced frameworks where multiple interconnected agents collaborate to support a wide array of tasks (Li et al., 2024; Gao et al., 2024a). Research indicates that agents can enhance their capabilities through reflective thinking (Cheng et al., 2024) or by leveraging optimization techniques analogous to neural networks (Zhou et al., 2024a), highlighting significant potential for online, training-free self-adjustment of Agent-based applications. Our work pioneers a universal framework for AI search engines utilizing multi-agent collaboration named ACN. We also introduce an optimization method enabling real-time learning and adaptation based on user feedback. Such adaptability enhances the personalized and interactive capabilities of AI search engines, offering a more tailored and responsive user experience.",
                "abstract": "Large language models (LLMs) and retrieval-augmented generation (RAG) techniques have revolutionized traditional information access, enabling AI agent to search and summarize information on behalf of users during dynamic dialogues. Despite their potential, current AI search engines exhibit considerable room for improvement in several critical areas. These areas include the support for multimodal information, the delivery of personalized responses, the capability to logically answer complex questions, and the facilitation of more flexible interactions. This paper proposes a novel AI Search Engine framework called the Agent Collaboration Network (ACN). The ACN framework consists of multiple specialized agents working collaboratively, each with distinct roles such as Account Manager, Solution Strategist, Information Manager, and Content Creator. This framework integrates mechanisms for picture content understanding, user profile tracking, and online evolution, enhancing the AI search engine's response quality, personalization, and interactivity. A highlight of the ACN is the introduction of a Reflective Forward Optimization method (RFO), which supports the online synergistic adjustment among agents. This feature endows the ACN with online learning capabilities, ensuring that the system has strong interactive flexibility and can promptly adapt to user feedback. This learning method may also serve as an optimization approach for agent-based systems, potentially influencing other domains of agent applications."
            },
            {
                "name": "Executable Code Actions Elicit Better LLM Agents",
                "arxiv_id": "2402.01030",
                "subtitles": [
                    "Action Module in LLM Agents",
                    "Improving LLM Agents"
                ],
                "reference": [
                    "Retroformer: Retrospective large language agents with policy gradient optimization",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "Prefer: Prompt ensemble learning via feedback-reflect-refine",
                    "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                    "Taskweaver: A code-first agent framework",
                    "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
                    "Training socially aligned language models in simulated human society",
                    "Agenttuning: Enabling generalized agent abilities for llms",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Fireact: Toward language agent fine-tuning",
                    "Communicative agents for software development",
                    "Measuring and improving chain-of-thought reasoning in vision-language models",
                    "Mint: Evaluating llms in multi-turn interaction with tools and language feedback",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "A survey on large language model based autonomous agents",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities",
                    "Reflective linguistic programming (rlp) : A stepping stone in socially-aware agi (socialagi",
                    "Webgpt: Browser-assisted question-answering with human feedback",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Tool learning with foundation models",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Scaling instruction-finetuned language models",
                    "Webshop: Towards scalable real-world web interaction with grounded language agents",
                    "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
                    "Intercode: Standardizing and benchmarking interactive coding with execution feedback"
                ],
                "related_work": "4Related Work4.1Action Module in LLM AgentsAs detailed in(Wang et al.,2023b) , LLM-based autonomous agents are typically structured around four components: customized profiles(Park et al.,2023; Qian et al.,2023) , long-term memory capabilities(Zhu et al.,2023; Fischer,2023) , reasoning and planning algorithms(Wei et al.,2022; Chen et al.,2023d) , and, most crucially, action modules. The action modules are key to facilitating LLM agents to effectively interact with external entities, including humans(Lee et al.,2022) and tools(Qin et al.,2023a) in the environment(Wang et al.,2023e; Yang et al.,2024a) . In this study, we address the critical problem of standardizing the action space for LLM agents. We further discuss the difference betweenCodeActand the line of work that uses code generation for problem-solving in \u00a7A. We notice a concurrent study TaskWeaver(Qiao et al.,2023) similarly endorses the use of code. We discuss the principal distinctions in \u00a7B.4.2Improving LLM AgentsTwo primary methods for enhancing LLM agents are prompt engineering and instruction tuning, as surveyed by(Wang et al.,2023b) . Forprompt engineering(Liu et al.,2023a) , numerous strategies have been introduced to improve the chain-of-thought reasoning(Wei et al.,2022) , including self-consistency-based reasoning(Wang et al.,2022b; Chen et al.,2023d) and tree-based approaches(Yao et al.,2023a) . Moreover, LLMs can be strategically prompted to reflect on previous plans(Yao et al.,2023b; Wang et al.,2023f; Zhang et al.,2023) , enabling them to refine initial actions through trial and error. Contrast to prompt engineering,instruction tuningintrinsically enhances LLMs(Chung et al.,2022) , particularly in their agent capabilities(Zeng et al.,2023; Chen et al.,2023a) . For effective training, human annotators can curate expert demonstrations for specific agent tasks, such as web browsing(Yao et al.,2022a; Nakano et al.,2021) . To minimize human annotation efforts, prior work creates synthetic datasets using stronger LLMs to distill agent capabilities into local models, focusing on tool usage(Qin et al.,2023b) , interaction(Chen et al.,2023c) , and social skills(Liu et al.,2023b) . CodeActInstruct aligns with the latter approach and creates datasets using stronger LLMs.",
                "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug."
            },
            {
                "name": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "arxiv_id": "2402.01622",
                "subtitles": [
                    "Large Language Model based Agents",
                    "Planning",
                    "Evaluation of Language Agents"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
                    "Are NLP models really able to solve simple math word problems",
                    "Llm-powered autonomous agents",
                    "Solving general arithmetic word problems",
                    "Robot planning in the real world: Research challenges and opportunities",
                    "Autogpt",
                    "Agentbench: Evaluating llms as agents",
                    "Walking down the memory maze: Beyond context limit through interactive reading",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "React: Synergizing reasoning and acting in language models",
                    "Planning and the brain",
                    "Reflexion: Language agents with verbal reinforcement learning",
                    "Planning and scheduling in manufacturing and services",
                    "Llm+ p: Empowering large language models with optimal planning proficiency",
                    "Dart: an example of accelerated evolutionary development",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Language agents: a critical evolutionary step of artificial intelligence",
                    "Openagents: An open platform for language agents in the wild",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Openagi: When llm meets domain experts",
                    "Robot planning",
                    "Recurrentgpt: Interactive generation of (arbitrarily) long text",
                    "HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face",
                    "Webgpt: Browser-assisted question-answering with human feedback",
                    "Gpt-4v(ision) is a generalist web agent, if grounded",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Memorybank: Enhancing large language models with long-term memory",
                    "On the tool manipulation capability of open-source large language models",
                    "Distilling script knowledge from large language models for constrained language planning",
                    "Mind2web: Towards a generalist agent for the web",
                    "Api-bank: A comprehensive benchmark for tool-augmented llms",
                    "ToolQA: A dataset for LLM question answering with external tools",
                    "Language models are few-shot learners",
                    "Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications",
                    "A cognitive model of planning",
                    "Chameleon: Plug-and-play compositional reasoning with large language models",
                    "Webarena: A realistic web environment for building autonomous agents",
                    "Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system",
                    "Language models as agent models"
                ],
                "related_work": "2Related Work2.1Large Language Model based AgentsEmpowered by large language models (LLMs) , language agents have the capability to decompose complex tasks and arrive at solutions through a series of reasoned actions. Notable examples such as AutoGPT(AutoGPT,2023) , BabyAGI(Nakajima,2023) , and HuggingGPT(Shen et al.,2023) have illuminated the community with their impressive abilities. Current LLM-powered language agents, equipped with Memory, Tool-use, and Planning modules, have seen a substantial improvement in their general abilities(Weng,2023) . Memory in language agents refers to their ability to acquire and process information. It is divided into two types: long-term memory, which is the parametric memory inherent in LLMs, and short-term memory, also known as in-context learning(Brown et al.,2020) or working memory. Techniques like memory summarization(Chen et al.,2023; Zhou et al.,2023; Liang et al.,2023) and retrieval(Andreas,2022; Park et al.,2023; Zhong et al.,2023) are widely employed to enhance the memory capabilities of language agents. Moreover, by interacting with external tools, language agents expand their potential capabilities significantly. This tool-augmentation paradigm has been validated as effective in previous work(Nakano et al.,2021; Lu et al.,2023; Ge et al.,2023; Xie et al.,2023) . We further discuss the planning module in Section2.2.2.2PlanningPlanning, a hallmark of human intelligence, entails a sequence of actions that involve decomposing tasks, searching for solutions, and making final decisions(Hayes-Roth & Hayes-Roth,1979; Grafman et al.,2004; Su,2023) . This skill is crucial for achieving human-level intelligence and has been widely studied in areas such as robotics(McDermott,1992; Alterovitz et al.,2016) and transportation scheduling(Cross & Estrada,1994; Pinedo,2005) . The emergence of language agents powered by LLMs has further intensified discussions around their planning capabilities(Liu et al.,2023a; Valmeekam et al.,2023) . Previous research has demonstrated that language agents can effectively decompose tasks and engage in step-by-step reasoning, leading to significant improvements(Wei et al.,2022; Yuan et al.,2023; Zheng et al.,2024) . Furthermore, to optimize solution searches in fewer steps, classical data structures like trees and graphs have been employed in prior studies(Yao et al.,2023; Besta et al.,2023) , enhancing the planning capabilities of language agents. In addition, methods involving feedback from the environment(Yao et al.,2022; Shinn et al.,2023) have also been shown to be beneficial. However, while these planning abilities have shown promise in specific tasks, the effectiveness of these planning strategies in scenarios with multiple constraints remains uncertain.2.3Evaluation of Language AgentsPrevious studies typically assess LLM-powered language agents in focused domains: arithmetic reasoning targeting correct solutions(Roy & Roth,2015; Cobbe et al.,2021; Patel et al.,2021) ; tool-use evaluating agents' proficiency in employing tools and reporting results(Li et al.,2023; Xu et al.,2023; Zhuang et al.,2023) ; and web navigation, testing agents' ability to locate specific websites(Deng et al.,2023; Zhou et al.,2024; Liu et al.,2024) . However, the complexity of the real-world implies that previous evaluation methods, which focus on single objective and fixed ground truths, may fall short of capturing the full scope of agents' capabilities. To address this, we introduce TravelPlanner for comprehensive evaluations, assessing whether language agents can generate feasible solutions facing various objectives, referred to as constraints in this paper.",
                "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents."
            },
            {
                "name": "LLM Agents can Autonomously Exploit One-day Vulnerabilities",
                "arxiv_id": "2404.08144",
                "subtitles": [
                    "Cybersecurity and AI",
                    "Cybersecurity",
                    "Security of LLM agents"
                ],
                "reference": [
                    "More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models",
                    "Burp Suite Essentials",
                    "Metasploit: the penetration tester's guide",
                    "Benchmarking and defending against indirect prompt injection attacks on large language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "Getting pwn'd by ai: Penetration testing with large language models",
                    "Evaluating frontier models for dangerous capabilities",
                    "Machine learning in cybersecurity: A review",
                    "Will ai make cyber swords or shields",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Generative ai for pentesting: the good, the bad, the ugly",
                    "The social and psychological impact of cyberattacks",
                    "Removing rlhf protections in gpt-4 via fine-tuning",
                    "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
                    "A classification of sql-injection attacks and countermeasures",
                    "Automated vulnerability detection in source code using deep representation learning",
                    "A research agenda acknowledging the persistence of passwords",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Owasp zed attack proxy",
                    "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Llm agents can autonomously hack websites"
                ],
                "related_work": "7Related WorkCybersecurity and AI.The most related work to ours is a recent study that showed that LLM agents can hack websites(Fang et al.,2024) . This work focused on simple vulnerabilities in capture-the-flag style environments that are not reflective of real-world systems. Work contemporaneous to ours also evaluates the ability of LLM agents in a cybersecurity context(Phuong et al.,2024) , but appears to perform substantially worse than our agent and an agent in the CTF setting(Fang et al.,2024) . Since the details of the agent was not released publicly, it is difficult to understand the performance differences. We hypothesize that it is largely due to the prompt. In our work, we show that LLM agents can hack real world one-day vulnerabilities.Other recent work has shown the ability of LLMs to aid in penetration testing or malware generation(Happe & Cito,2023; Hilario et al.,2024) . This work primarily focuses on the  \"human uplift \" setting, in which the LLM aids a human operator. Other work focuses on the societal implications the intersection of AI and cybersecurity(Lohn & Jackson,2022; Handa et al.,2019) . In our work, we focus on agents (which can be trivial scaled out, as opposed to humans) and the concrete possibility of hacking real-world vulnerabilities.Cybersecurity.Cybersecurity is a incredibly wide research area, ranging from best practices for passwords(Herley & Van Oorschot,2011) , studying the societal implications of cyber attacks(Bada & Nurse,2020) , to understanding web vulnerabilities(Halfond et al.,2006) . The subarea of cybersecurity closest to ours is automatic vulnerability detection and exploitation(Russell et al.,2018; Bennetts,2013; Kennedy et al.,2011; Mahajan,2014) .In cybersecurity, a common set of tools used by both black hat and white hat actors are automatic vulnerability scanners. These include ZAP(Bennetts,2013) , Metasploit(Kennedy et al.,2011) , and Burp Suite(Mahajan,2014) . Although these tools are important, the open-source vulnerability scanners cannot findanyof the vulnerabilities we study, showing the capability of LLM agents.Security of LLM agents.A related, but orthogonal line of work is the security of LLM agents(Greshake et al.,2023a; Kang et al.,2023; Zou et al.,2023; Zhan et al.,2023; Qi et al.,2023; Yang et al.,2023) . For example, an attacker can use an indirect prompt injection attack to misdirect an LLM agent(Greshake et al.,2023b; Yi et al.,2023; Zhan et al.,2024) . Attackers can also fine-tune away protections from models, enabling highly capable models to perform actions or tasks that the creators of the models did not intent(Zhan et al.,2023; Yang et al.,2023; Qi et al.,2023) . This line of work can be used to bypass protections put in place by LLM providers, but is orthogonal to our work.",
                "abstract": "LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents."
            },
            {
                "name": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
                "arxiv_id": "2402.18667",
                "subtitles": [
                    "CONTENT-FOLLOWING benchmarks",
                    "FORMAT-FOLLOWING benchmarks"
                ],
                "reference": [
                    "Benchmarking large language models on controllable generation under diversified instructions",
                    "Measuring massive multitask language understanding",
                    "Alpacafarm: A simulation framework for methods that learn from human feedback",
                    "Instruction-following evaluation for large language models",
                    "Safetybench: Evaluating the safety of large language models with multiple choice questions",
                    "Trafficsafetygpt: Tuning a pre-trained large language model to a domain-specific expert in transportation safety",
                    "Red-teaming large language models using chain of utterances for safety-alignment",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Large language models help humans verify truthfulness-except when they are convincingly wrong"
                ],
                "related_work": "2Related Work Content-Following  benchmarks. There are various existing efforts that build evaluation data sets to try to assess LLMs' general problem-solving capability through conversations. Among them, MMLU dataset (Hendrycks et al., 2020) is collected to measure knowledge and problem solving capabilities of LLMs in different knowledge domains such as elementary mathematics and US history. The performance of LLMs on MMLU is measured by the accuracy of their selected answers from multiple options. AlpacaEval (Dubois et al., 2023) and MT-Bench (Zheng et al., 2023a), on the other hand, collect open-ended questions accross different domains without providing concrete reference answers. They rely on LLMs such as GPT-4 to conduct automatic evaluations on the target LLM's answers. Except for these general benchmarks, there are also evaluation data sets specifically focusing on assessing LLM answers' truthfulness (Si et al., 2023; Lin et al., 2022) and safety (Bhardwaj and Poria, 2023; Zhang et al., 2023; Zheng et al., 2023b). Format-Following  benchmarks. In the past few months, several instruction-following benchmarks are recently curated to contain a small sub-set of test cases relevant to format following (Zhou et al., 2023a; Chen et al., 2024), such as generating data following JSON format, or following format requirements such as numbers of bullet points/paragraphs (Zhou et al., 2023a). Compared to such format-following sub-sets that only covers a handful of generic formats such as JSON, our benchmark covers more diverse and domain-specific format requirements and each test example in our benchmark comes with complicated combined requirements and domain-specific context. Therefore, we empirically find that our format-following benchmark is harder for existing LLMs and can unveil performance discrepancy across different domains, compared to the format-following sub sets in these existing benchmarks.",
                "abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's role in guiding the selection of domain-specific AI agents. FoFo is released here atthis https URL."
            },
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "subtitles": [
                    "LLM Benchmarks",
                    "Risks of Static Benchmarks",
                    "Ranking System",
                    "Human Preference Dataset"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work LLM Benchmarks. We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU(Hendrycks et al.,2020) , HellaSwag(Zellers et al.,2019) , GSM-8K(Cobbe et al.,2021) , BigBench(Srivastava et al.,2023) , AGIEval(Zhong et al.,2023) , and HumanEval(Chen et al.,2021) . Benchmarks focusing on safety, such as ToxicChat(Lin et al.,2023) , and comprehensive suites like HELM(Liang et al.,2022) , also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk(Karpinska et al.,2021; Geng et al.,2023; Wang et al.,2023) . The recent trend includes utilizing GPT-4 for approximating human judgment(Chiang & Lee,2023) , with notable instances being MT-Bench(Zheng et al.,2023b) and AlpacaEval(Li et al.,2023) . In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces(Li et al.,2022; Huang et al.,2023) . They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference(Bai et al.,2022; Ouyang et al.,2022; Touvron et al.,2023) . However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.Risks of Static Benchmarks.Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment(Yang et al.,2023; Oren et al.,2023) . DynaBench(Kiela et al.,2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.Ranking System.Ranking systems have been a well-studied topic in statistics. Related topics include probability models(Hunter,2004; Rao & Kupper,1967) , rank elicitation(Sz\u00f6r\u00e9nyi et al.,2015; Busa-Fekete et al.,2014a,b) , and online experiment design(Chernoff,1992; Karimi et al.,2021) . The Elo rating system has also been used for LLMs(Bai et al.,2022; Boubdir et al.,2023) . Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.Human Preference Dataset.Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant(K\u00f6pf et al.,2023) , HH-RLHF(Bai et al.,2022) , LMSYS-Chat-1M(Zheng et al.,2023a) , and synthetic approximations of human preferences like UltraFeedback(Cui et al.,2023) and Nectar(Zhu et al.,2023) . Our prior data release, LMSYS-Chat-1M(Zheng et al.,2023a) , is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.",
                "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{this https URL}."
            },
            {
                "name": "More Agents Is All You Need",
                "arxiv_id": "2402.05120",
                "subtitles": [
                    "LLM Self-Ensemble",
                    "Heterogeneous LLM Ensemble",
                    "Multiple LLM Agents Collaboration"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Metagpt: Meta programming for multi-agent collaborative framework",
                    "Knowledge fusion of large language models",
                    "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework",
                    "Making language models better reasoners with step-aware verifier",
                    "Complexity-based prompting for multi-step reasoning",
                    "Improving factuality and reasoning in language models through multiagent debate",
                    "Examining the inter-consistency of large language models: An in-depth analysis via debate",
                    "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
                    "Reflexion: Language agents with verbal reinforcement learning",
                    "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Lamda: Language models for dialog applications",
                    "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
                    "Fusing models with complementary expertise",
                    "Encouraging divergent thinking in large language models through multi-agent debate",
                    "Frugalgpt: How to use large language models while reducing cost and improving performance",
                    "Routing to the expert: Efficient reward-guided ensemble of large language models",
                    "Large language model routing with benchmark datasets",
                    "Ask one more time: Self-agreement improves reasoning of language models in (almost) all scenarios",
                    "CAMEL: communicative agents for  \"mind \" exploration of large scale language model society",
                    "Blending is all you need: Cheaper, better alternative to trillion-parameters llm",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Autoagents: A framework for automatic agent generation"
                ],
                "related_work": "2Related WorkRelated works can be categorized into three parts: 1) LLM self-ensemble(Wang et al.,2023b) , which attempts to harness multiple outputs from homogeneous LLMs to assemble the final answer; 2) heterogeneous LLM ensemble, which focuses on combining heterogeneous LLMs through supervised learning to improve performance across various downstream applications; and 3) multiple LLM agents collaboration, which improves performance through interactions among LLM agents. We discuss these works below.LLM Self-Ensemble.CoT-SC(Wang et al.,2023b) harnesses diverse chain-of-thought(Wei et al.,2022) prompts to elicit a variety of reasoning processes from a single LLM and select the final answer through majority voting.Fu et al. (2023) ; Li et al. (2023b) ; Cobbe et al. (2021b) ; Thoppilan et al. (2022) ; Lin et al. (2023) can be considered as the extensions of CoT-SC. These methods mainly focus on reasoning tasks and exclusively investigate the compatibility with CoT. In contrast, our method not only validates effectiveness in reasoning tasks but also in generation tasks. Moreover, our method is compatible with a broader range of methods, such as prompt engineering (including CoT) and multiple LLM agents collaboration. Very recently,Lu et al. (2024) proposes a method named Blended that utilizes multiple LLMs for chat scenarios. In contrast, Blended focuses on utilizing the power of multiple LLMs, whereas our focus is on the scaling trend of adding more LLMs. Also, Blended is only for limited chat scenarios evaluated via human annotations. Furthermore, we explore orthogonality with other methods.Heterogeneous LLM Ensemble.Wan et al. (2024) conducts a supervised LLM fusion framework to distill multiple heterogeneous LLMs into a single model and surpasses each of these LLMs.Jiang et al. (2023) introduces a supervised ensembling framework based on multiple heterogeneous LLMs.Chen et al. (2023b) proposes a sequential inference method for LLMs that halts when the output quality is deemed adequate.Wang et al. (2023a) addresses the fusion-of-experts problem by integrating outputs from models with distinct knowledge domains through supervised learning.Shnitzer et al. (2023) andLu et al. (2023) select the most suitable LLM for new tasks by training a reward-guided router. These approaches primarily employ supervised learning, necessitating task-specific annotated data, and exhibit limited generalizability. In contrast, our method is unsupervised, without the need for additional training data.Multiple LLM Agents Collaboration. Studies explore various multiple LLM agents interaction architectures, with employing static debate-style engagements among LLMs for enhanced reasoning(Du et al.,2023; Liang et al.,2023; Xiong et al.,2023) .Liu et al. (2023) enables agents to interact for multiple rounds in a dynamic architecture.Li et al. (2023a) ; Hong et al. (2023) ; Wu et al. (2023) ; Chen et al. (2023c,a) offer several multi-agent frameworks that enable the development of LLM applications or enhance task-solving capabilities. However, these methods primarily focus on the interaction structures between LLM agents, rather than the relationship between the number of agents and performance. We also select representative methods(Du et al.,2023; Shinn et al.,2023) to combine with our method, achieving further enhancements.",
                "abstract": "We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \\url{this https URL}."
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "subtitles": [
                    "Multimodal LLMs",
                    "Evaluating Multimodal LLMs",
                    "Visual Encoders",
                    "Ambiguities in Embedding Models"
                ],
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ],
                "related_work": "5Related WorksMultimodal LLMs.We study the limitations of Multimodal LLMs[40,13,30,31,8]and explore possible ways to improve these models. Multimodal LLMs build from pretrained Large Language Models[41,3,58,59,69]and CLIP vision encoder[43,54]. These systems then use an adapter, such as MLPs[30,31], Q-Former[26,8], and gated attention[2,25], to integrate the pretrained CLIP vision encoder into LLMs. More recently, instructBLIP[8], LLaVA-1.5[30]highlight the importance of high-quality training data. Yet, there is a scarcity of research focusing on the impact of visual encoders, which is an important gap our work aims to address through a systematic study.Evaluating Multimodal LLMs.MMVP assesses MLLMs using a set of simple yet critical Visual Question Answering (VQA) questions constructed from CLIP-blind pairs. Previous benchmarks such as TextVQA[52], VQAv2[15], and GQA[21]have centered on traditional VQA queries. Recently, there are works like MM-Vet[64], POPE[27], and MM-Bench[32]designed to specifically evaluate multimodal LLMs including hallucination, reasoning, and robustness. The previous benchmarks and evaluations have shown that Multimodal LLMs can suffer from hallucination[29,28], catastrophic forgetting[67]and lack of robustness[11]. In taking a step back to the fundamentals, our work uncovers that even the most advanced multimodal LLMs, such as GPT-4V[40], Gemini[14], Bard[30], and LLaVA-1.5[30], are not immune to stumbling over elementary visual questions. We also identified part of the problem as being the incapable visual encoder.Visual Encoders.MMVP-VLM provides a detailed analysis of the visual capabilities of various CLIP variants[43,54,62,66]. These models mostly follow the method proposed inRadford et al.[43]that uses contrastive loss to train on large volumes of image-text pairs. They differ in training data[62], training recipes[54], and objective functions[66]. Nonetheless, our studies show that all of these CLIP variants struggle with simple visual patterns such as  \"orientation \",  \"count \",  \"presence of specific features \",etc. Another line of research focuses on vision-only self-supervised learning (SSL) . This category includes contrastive SSL[7,16,5,17]and mask-based SSL[70,18,4]. SLIP[39]explores the synergy between CLIP and contrastive SSL, but focusing primarily on standard classification tasks. In fact, a common practice to evaluate the quality of these vision models is through linear probing or fine-tuning on ImageNet[47,45]. Although current evaluation methods provide a basic level of assessment on representation quality, our findings indicate a growing detachment from the needs of recent use cases. As demonstrated in the MoF experiments in Section4, the CLIP vision model and the vision-only SSL models learn complementary features. However, the linear probing accuracy on ImageNet alone provides a limited understanding of feature utility in MLLMs. This observation suggests the need for more diverse evaluations[61]in visual representation learning, to better align with current and emerging applications.Ambiguities in Embedding Models.Our work exploits CLIP-blind pairs within the CLIP vision embedding space to generate examples of failures in CLIP models and subsequently MLLMs. This concept has ties to previous research focused on documenting failure modes in text embedding models[12,36,55]. More recently,Thrush et al.[56],Yuksekgonul et al.[65]andHsieh et al.[19]study the binding problems CLIP faces in processing text queries, noting that CLIP models treat text input as a bag of words.Tong et al.[57]examines the implications for downstream text-guided generative models.Tschannen et al.[60]suggests image captioners as promising alternatives to CLIP for improving attribute binding. Our work focuses on the visual patterns.",
                "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems."
            }
        ],
        "survey": {
            "name": "Understanding the planning of LLM agents: A survey",
            "arxiv_id": "2402.02716",
            "subtitles": [
                {
                    "name": "Taxonomy",
                    "key_history": [
                        {
                            "reference_title": "Chain-of-thought prompting elicits reasoning in large language models",
                            "key_word": "Task Decomposition"
                        },
                        {
                            "reference_title": "Tree of thoughts: Deliberate problem solving with large language models",
                            "key_word": "Multi-plan Selection"
                        },
                        {
                            "reference_title": "Llm+ p: Empowering large language models with optimal planning proficiency",
                            "key_word": "External Planner-aided"
                        },
                        {
                            "reference_title": "Reflexion: Language agents with verbal reinforcement learning",
                            "key_word": "Reflection and Refinement"
                        },
                        {
                            "reference_title": "Large language model is semi-parametric reinforcement learning agent",
                            "key_word": "Memory-aided Planning"
                        }
                    ],
                    "references_in_this_section": [
                        "Reflexion: Language agents with verbal reinforcement learning",
                        "Self-refine: Iterative refinement with self-feedback",
                        "Llm+ p: Empowering large language models with optimal planning proficiency",
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Large language models as commonsense knowledge for large-scale task planning",
                        "Large language model is semi-parametric reinforcement learning agent",
                        "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                        "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning",
                        "Tree of thoughts: Deliberate problem solving with large language models",
                        "Graph of thoughts: Solving elaborate problems with large language models",
                        "React: Synergizing reasoning and acting in language models",
                        "Memorybank: Enhancing large language models with long-term memory",
                        "Critic: Large language models can self-correct with tool-interactive critiquing",
                        "Chain-of-thought prompting elicits reasoning in large language models"
                    ]
                },
                {
                    "name": "Task Decomposition",
                    "key_history": [
                        {
                            "reference_title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                            "key_word": "Multimodal Task Decomposition"
                        },
                        {
                            "reference_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
                            "key_word": "Two-step Prompting"
                        },
                        {
                            "reference_title": "Progprompt: Generating situated robot task plans using large language models",
                            "key_word": "Code-based Task Planning"
                        },
                        {
                            "reference_title": "Chain-of-thought prompting elicits reasoning in large language models",
                            "key_word": "Few-shot Reasoning"
                        },
                        {
                            "reference_title": "Large language models are zero-shot reasoners",
                            "key_word": "Think Step-by-Step"
                        },
                        {
                            "reference_title": "React: Synergizing reasoning and acting in language models",
                            "key_word": "Alternating Reasoning and Action Steps"
                        },
                        {
                            "reference_title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
                            "key_word": "Visual ChatGPT with ReAct"
                        },
                        {
                            "reference_title": "Pal: Program-aided language models",
                            "key_word": "Code Generation in Task Decomposition"
                        },
                        {
                            "reference_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
                            "key_word": "Program-of-Thought (PoT) for Formalized Reasoning"
                        }
                    ],
                    "references_in_this_section": [
                        "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models",
                        "Pal: Program-aided language models",
                        "Llama 2: Open foundation and fine-tuned chat models",
                        "Large language models are zero-shot reasoners",
                        "Cognitive task analysis",
                        "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                        "Evaluating large language models trained on code",
                        "Progprompt: Generating situated robot task plans using large language models",
                        "Visual chatgpt: Talking, drawing and editing with visual foundation models",
                        "React: Synergizing reasoning and acting in language models",
                        "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
                        "Chain-of-thought prompting elicits reasoning in large language models"
                    ]
                },
                {
                    "name": "Multi-Plan Selection",
                    "key_history": [
                        {
                            "reference_title": "Self-consistency improves chain of thought reasoning in language models",
                            "key_word": "Self-consistency"
                        },
                        {
                            "reference_title": "Tree of thoughts: Deliberate problem solving with large language models",
                            "key_word": "Tree-of-Thought (ToT)"
                        },
                        {
                            "reference_title": "Graph of thoughts: Solving elaborate problems with large language models",
                            "key_word": "Graph-of-Thought (GoT)"
                        },
                        {
                            "reference_title": "Large language models as commonsense knowledge for large-scale task planning",
                            "key_word": "Monte Carlo Tree Search"
                        },
                        {
                            "reference_title": "Reasoning with language model is planning with world model",
                            "key_word": "Heuristic Policy Function"
                        },
                        {
                            "reference_title": "Self-consistency improves chain of thought reasoning in language models",
                            "key_word": "Majority Vote Strategy"
                        },
                        {
                            "reference_title": "Tree of thoughts: Deliberate problem solving with large language models",
                            "key_word": "BFS and DFS for Tree Search"
                        },
                        {
                            "reference_title": "Large language models as commonsense knowledge for large-scale task planning",
                            "key_word": "Monte Carlo Tree Search for Optimal Plan"
                        },
                        {
                            "reference_title": "Reasoning with language model is planning with world model",
                            "key_word": "MCTS Algorithm for Plan Search"
                        },
                        {
                            "reference_title": "Llm a*: Human in the loop large language models enabled a* search for robotics",
                            "key_word": "Heuristic Cost Function"
                        }
                    ],
                    "references_in_this_section": [
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Large language models as commonsense knowledge for large-scale task planning",
                        "Llm a*: Human in the loop large language models enabled a* search for robotics",
                        "Tree of thoughts: Deliberate problem solving with large language models",
                        "Graph of thoughts: Solving elaborate problems with large language models",
                        "Reasoning with language model is planning with world model"
                    ]
                },
                {
                    "name": "External Planner-Aided Planning",
                    "key_history": [
                        {
                            "reference_title": "Llm+ p: Empowering large language models with optimal planning proficiency",
                            "key_word": "PDDL-based Symbolic Planner"
                        },
                        {
                            "reference_title": "Dynamic planning with a llm",
                            "key_word": "Dynamic Interactive Environments"
                        },
                        {
                            "reference_title": "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning",
                            "key_word": "Manual Verification"
                        },
                        {
                            "reference_title": "Coupling large language models with logic programming for robust and general reasoning from text",
                            "key_word": "Atomic Facts to ASP Problems"
                        },
                        {
                            "reference_title": "Deep reinforcement learning with a natural language action space",
                            "key_word": "Reinforcement Learning"
                        },
                        {
                            "reference_title": "Decision transformer: Reinforcement learning via sequence modeling",
                            "key_word": "Imitation Learning for Planning"
                        },
                        {
                            "reference_title": "Keep calm and explore: Language models for action generation in text-based games",
                            "key_word": "RL-based Neural Planner"
                        },
                        {
                            "reference_title": "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks",
                            "key_word": "Dual-Process Theory, Fast and Slow Thinking"
                        }
                    ],
                    "references_in_this_section": [
                        "Dynamic planning with a llm",
                        "Deep reinforcement learning with a natural language action space",
                        "Keep calm and explore: Language models for action generation in text-based games",
                        "Llm+ p: Empowering large language models with optimal planning proficiency",
                        "Coupling large language models with logic programming for robust and general reasoning from text",
                        "Lpg: A planner based on local search for planning graphs with action costs",
                        "An introduction to the planning domain definition language, volume",
                        "Decision transformer: Reinforcement learning via sequence modeling",
                        "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning",
                        "Pddl  the planning domain definition language",
                        "Width and inference based planners: Siw, bfs (f), and probe",
                        "Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks"
                    ]
                },
                {
                    "name": "Reflection and Refinement",
                    "key_history": [
                        {
                            "reference_title": "Self-refine: Iterative refinement with self-feedback",
                            "key_word": "Self-refine iterative Process"
                        },
                        {
                            "reference_title": "Reflexion: Language agents with verbal reinforcement learning",
                            "key_word": "Error Detection"
                        },
                        {
                            "reference_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
                            "key_word": "External Tools"
                        },
                        {
                            "reference_title": "Recommender ai agent: Integrating large language models for interactive recommendations",
                            "key_word": "self-correction"
                        },
                        {
                            "reference_title": "Learning from mistakes makes llm better reasoner",
                            "key_word": "mistaken planning samples"
                        }
                    ],
                    "references_in_this_section": [
                        "Reflexion: Language agents with verbal reinforcement learning",
                        "Learning from mistakes makes llm better reasoner",
                        "Self-refine: Iterative refinement with self-feedback",
                        "Critic: Large language models can self-correct with tool-interactive critiquing",
                        "Recommender ai agent: Integrating large language models for interactive recommendations"
                    ]
                },
                {
                    "name": "Memory-Augumented Planning",
                    "key_history": [
                        {
                            "reference_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                            "key_word": "Retrieval-augmented generation"
                        },
                        {
                            "reference_title": "Unifying large language models and knowledge graphs: A roadmap",
                            "key_word": "knowledge graph"
                        },
                        {
                            "reference_title": "Recmind: Large language model powered agent for recommendation",
                            "key_word": "indexing structure"
                        },
                        {
                            "reference_title": "Large language model is semi-parametric reinforcement learning agent",
                            "key_word": "Q-value table"
                        },
                        {
                            "reference_title": "Keep calm and explore: Language models for action generation in text-based games",
                            "key_word": "ground-truth action trajectorie"
                        },
                        {
                            "reference_title": "Scienceworld: Is your agent smarter than a 5th grader",
                            "key_word": "Markov Decision Process Data"
                        },
                        {
                            "reference_title": "Agenttuning: Enabling generalized agent abilities for llms",
                            "key_word": "Plan Trajectories"
                        }
                    ],
                    "references_in_this_section": [
                        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                        "Generation-augmented retrieval for open-domain question answering",
                        "Memgpt: Towards llms as operating systems",
                        "Keep calm and explore: Language models for action generation in text-based games",
                        "Billion-scale similarity search with GPUs",
                        "Large language model is semi-parametric reinforcement learning agent",
                        "Agenttuning: Enabling generalized agent abilities for llms",
                        "Recent advances in retrieval-augmented text generation",
                        "Think-in-memory: Recalling and post-thinking enable llms with long-term memory",
                        "Scienceworld: Is your agent smarter than a 5th grader",
                        "Generative agents: Interactive simulacra of human behavior",
                        "Memorybank: Enhancing large language models with long-term memory",
                        "Recmind: Large language model powered agent for recommendation",
                        "Unifying large language models and knowledge graphs: A roadmap"
                    ]
                },
                {
                    "name": "Evaluation",
                    "key_history": [
                        {
                            "reference_title": "Alfworld: Aligning text and embodied environments for interactive learning",
                            "key_word": "Interactive Gaming Environments"
                        },
                        {
                            "reference_title": "React: Synergizing reasoning and acting in language models",
                            "key_word": "Interactive Retrieval Environments"
                        },
                        {
                            "reference_title": "Agentbench: Evaluating llms as agents",
                            "key_word": "Interactive Programming Environments"
                        },
                        {
                            "reference_title": "C-pack: Packaged resources to advance general chinese embedding",
                            "key_word": "Expenses and Tokens"
                        }
                    ],
                    "references_in_this_section": [
                        "Mind2web: Towards a generalist agent for the web",
                        "Fever: a large-scale dataset for fact extraction and verification",
                        "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
                        "Agentbench: Evaluating llms as agents",
                        "Language models can solve computer tasks",
                        "Webarena: A realistic web environment for building autonomous agents",
                        "C-pack: Packaged resources to advance general chinese embedding",
                        "Scienceworld: Is your agent smarter than a 5th grader",
                        "React: Synergizing reasoning and acting in language models",
                        "Alfworld: Aligning text and embodied environments for interactive learning"
                    ]
                }
            ],
            "all_references": [
                "In-context learning for few-shot dialogue state tracking",
                "Lora: Low-rank adaptation of large language models",
                "Lambada: Backward chaining for automated reasoning in natural language",
                "The flan collection: Designing data and methods for effective instruction tuning",
                "Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system",
                "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
                "Finding supporting examples for in-context learning",
                "Improving in-context few-shot learning via self-supervised training",
                "Determinantal point processes for machine learning",
                "Large language models are zero-shot reasoners",
                "Roberta: A robustly optimized bert pretraining approach",
                "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                "What can transformers learn in-context? a case study of simple function classes",
                "Complexity-based prompting for multi-step reasoning",
                "Lora: Low-rank adaptation of large language models",
                "Finding supporting examples for in-context learning",
                "Qlora: Efficient finetuning of quantized llms",
                "Dense passage retrieval for open-domain question answering",
                "Language models are few-shot learners",
                "Selective annotation makes language models better few-shot learners",
                "Large language models are zero-shot reasoners",
                "Determinantal point processes for machine learning",
                "Piqa: Reasoning about physical commonsense in natural language",
                "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
                "Semantic parsing on freebase from question-answer pairs",
                "How can we know what language models know",
                "Lambada: Backward chaining for automated reasoning in natural language",
                "Mot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts",
                "Palm: Scaling language modeling with pathways",
                "Determinantal point processes for machine learning",
                "Roberta: A robustly optimized bert pretraining approach",
                "Measuring faithfulness in chain-of-thought reasoning",
                "Semantic noise matters for neural natural language generation",
                "Scaling instruction-finetuned language models",
                "Solving quantitative reasoning problems with language models",
                "From ranknet to lambdarank to lambdamart: An overview",
                "Boolq: Exploring the surprising difficulty of natural yes/no questions",
                "ComQA: A community-sourced dataset for complex factoid question answering with paraphrase clusters",
                "A neural probabilistic language model",
                "Mtop: A comprehensive multilingual task-oriented semantic parsing benchmark",
                "Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources",
                "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                "Complexity-based prompting for multi-step reasoning",
                "A large annotated corpus for learning natural language inference",
                "Improving in-context few-shot learning via self-supervised training",
                "Training verifiers to solve math word problems",
                "In-context learning for few-shot dialogue state tracking",
                "What can transformers learn in-context? a case study of simple function classes",
                "In-context examples selection for machine translation",
                "The power of scale for parameter-efficient prompt tuning",
                "Can machines learn morality? the delphi experiment",
                "Finding supporting examples for in-context learning",
                "Understanding finetuning for factual knowledge extraction from language models",
                "Finding contradictions in text",
                "SimCSE: Simple contrastive learning of sentence embeddings",
                "Natural questions: a benchmark for question answering research",
                "N-gram language models",
                "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
                "A large annotated corpus for learning natural language inference"
            ]
        },
        "topic_history": [
            {
                "name": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
                "arxiv_id": "2403.05307",
                "reference": [
                    "Do lvlms understand charts? analyzing and correcting factual errors in chart captioning",
                    "Universal self-consistency for large language model generation",
                    "Iterative forward tuning boosts in-context learning in language models",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Towards ecologically valid research on language user interfaces",
                    "Beyond generating code: Evaluating gpt on a data visualization course",
                    "A survey for in-context learning",
                    "Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers",
                    "Text-to-sql empowered by large language models: A benchmark evaluation",
                    "Natural language to code generation in interactive data science notebooks",
                    "Evaluating large language models trained on code",
                    "Can LLM already serve as a database interface? a BIg bench for large-scale database grounded text-to-SQLs",
                    "Reactable: Enhancing react for table question answering",
                    "Learning to simulate natural language feedback for interactive semantic parsing",
                    "MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback",
                    "Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases",
                    "Llm-assisted code cleaning for training accurate code generators",
                    "Infiagent-dabench: Evaluating agents on data analysis tasks",
                    "Codes: Towards building open-source language models for text-to-sql",
                    "Data-copilot: Bridging billions of data and humans with autonomous workflow",
                    "Program synthesis with large language models",
                    "Middleware for llms: Tools are instrumental for language agents in complex environments",
                    "Mac-sql: Multi-agent collaboration for text-to-sql",
                    "Dts-sql: Decomposed text-to-sql with small large language models",
                    "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                    "Din-sql: Decomposed in-context learning of text-to-sql with self-correction",
                    "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Api-bank: A comprehensive benchmark for tool-augmented llms",
                    "Opencodeinterpreter: Integrating code generation with execution and refinement",
                    "Sheetcopilot: Bringing software productivity to the next level through large language models",
                    "Teaching large language models to self-debug",
                    "Text2analysis: A benchmark of table question answering with advanced data analysis and unclear queries",
                    "Dialgen: collaborative human-lm generated dialogues for improved understanding of human-human conversations",
                    "S3eval: A synthetic, scalable, systematic evaluation suite for large language models",
                    "DS-1000: A natural and reliable benchmark for data science code generation"
                ]
            },
            {
                "name": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
                "arxiv_id": "2402.14672",
                "reference": [
                    "Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings",
                    "Grounding 'grounding' in NLP",
                    "AgentBench: Evaluating llms as agents",
                    "Can LLM already serve as A database interface? A big bench for large-scale database grounded text-to-sqls",
                    "Few-shot in-context learning on knowledge base question answering",
                    "Toolformer: Language models can teach themselves to use tools",
                    "DecAF: Joint decoding of answers and logical forms for question answering over knowledge bases",
                    "ToolLLM: Facilitating large language models to master 16000+ real-world apis",
                    "API-Bank: A benchmark for tool-augmented llms",
                    "Don't generate, discriminate: A proposal for grounding language models to real-world environments",
                    "Exploring chain of thought style prompting for text-to-sql",
                    "Code-style in-context learning for knowledge-based question answering",
                    "Tool learning with foundation models",
                    "Mind2web: Towards a generalist agent for the web",
                    "Augmented language models: a survey",
                    "LLM-Planner: Few-shot grounded planning for embodied agents with large language models",
                    "Chameleon: Plug-and-play compositional reasoning with large language models",
                    "A comprehensive exploration on wikisql with table-aware word contextualization",
                    "StructGPT: A general framework for large language model to reason over structured data",
                    "Alfworld: Aligning text and embodied environments for interactive learning"
                ]
            },
            {
                "name": "A Learnable Agent Collaboration Network Framework for Personalized Multimodal AI Search Engine",
                "arxiv_id": "2409.00636",
                "reference": [
                    "Symbolic Learning Enables Self-Evolving Agents",
                    "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent",
                    "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "Simulating Financial Market via Large Language Model based Agents",
                    "A large annotated corpus for learning natural language inference",
                    "Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models",
                    "Expel: Llm agents are experiential learners",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "RARR: Researching and Revising What Language Models Say, Using Language Models",
                    "Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism",
                    "Gpt4tools: Teaching large language model to use tools via self-instruction",
                    "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering",
                    "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
                    "Knowledge-augmented large language models for personalized contextual query suggestion",
                    "Query Rewriting for Retrieval-Augmented Large Language Models",
                    "Lift yourself up: Retrieval-augmented text generation with self-memory",
                    "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
                    "Making Retrieval-Augmented Language Models Robust to Irrelevant Context"
                ]
            },
            {
                "name": "Executable Code Actions Elicit Better LLM Agents",
                "arxiv_id": "2402.01030",
                "reference": [
                    "Retroformer: Retrospective large language agents with policy gradient optimization",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "Prefer: Prompt ensemble learning via feedback-reflect-refine",
                    "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                    "Taskweaver: A code-first agent framework",
                    "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
                    "Training socially aligned language models in simulated human society",
                    "Agenttuning: Enabling generalized agent abilities for llms",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Fireact: Toward language agent fine-tuning",
                    "Communicative agents for software development",
                    "Measuring and improving chain-of-thought reasoning in vision-language models",
                    "Mint: Evaluating llms in multi-turn interaction with tools and language feedback",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "A survey on large language model based autonomous agents",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities",
                    "Reflective linguistic programming (rlp) : A stepping stone in socially-aware agi (socialagi",
                    "Webgpt: Browser-assisted question-answering with human feedback",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Tool learning with foundation models",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Scaling instruction-finetuned language models",
                    "Webshop: Towards scalable real-world web interaction with grounded language agents",
                    "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
                    "Intercode: Standardizing and benchmarking interactive coding with execution feedback"
                ]
            },
            {
                "name": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "arxiv_id": "2402.01622",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
                    "Are NLP models really able to solve simple math word problems",
                    "Llm-powered autonomous agents",
                    "Solving general arithmetic word problems",
                    "Robot planning in the real world: Research challenges and opportunities",
                    "Autogpt",
                    "Agentbench: Evaluating llms as agents",
                    "Walking down the memory maze: Beyond context limit through interactive reading",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "React: Synergizing reasoning and acting in language models",
                    "Planning and the brain",
                    "Reflexion: Language agents with verbal reinforcement learning",
                    "Planning and scheduling in manufacturing and services",
                    "Llm+ p: Empowering large language models with optimal planning proficiency",
                    "Dart: an example of accelerated evolutionary development",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Language agents: a critical evolutionary step of artificial intelligence",
                    "Openagents: An open platform for language agents in the wild",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Openagi: When llm meets domain experts",
                    "Robot planning",
                    "Recurrentgpt: Interactive generation of (arbitrarily) long text",
                    "HuggingGPT: Solving AI tasks with chatGPT and its friends in hugging face",
                    "Webgpt: Browser-assisted question-answering with human feedback",
                    "Gpt-4v(ision) is a generalist web agent, if grounded",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Memorybank: Enhancing large language models with long-term memory",
                    "On the tool manipulation capability of open-source large language models",
                    "Distilling script knowledge from large language models for constrained language planning",
                    "Mind2web: Towards a generalist agent for the web",
                    "Api-bank: A comprehensive benchmark for tool-augmented llms",
                    "ToolQA: A dataset for LLM question answering with external tools",
                    "Language models are few-shot learners",
                    "Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications",
                    "A cognitive model of planning",
                    "Chameleon: Plug-and-play compositional reasoning with large language models",
                    "Webarena: A realistic web environment for building autonomous agents",
                    "Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system",
                    "Language models as agent models"
                ]
            },
            {
                "name": "LLM Agents can Autonomously Exploit One-day Vulnerabilities",
                "arxiv_id": "2404.08144",
                "reference": [
                    "More than you've asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models",
                    "Burp Suite Essentials",
                    "Metasploit: the penetration tester's guide",
                    "Benchmarking and defending against indirect prompt injection attacks on large language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "Getting pwn'd by ai: Penetration testing with large language models",
                    "Evaluating frontier models for dangerous capabilities",
                    "Machine learning in cybersecurity: A review",
                    "Will ai make cyber swords or shields",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Generative ai for pentesting: the good, the bad, the ugly",
                    "The social and psychological impact of cyberattacks",
                    "Removing rlhf protections in gpt-4 via fine-tuning",
                    "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
                    "A classification of sql-injection attacks and countermeasures",
                    "Automated vulnerability detection in source code using deep representation learning",
                    "A research agenda acknowledging the persistence of passwords",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Owasp zed attack proxy",
                    "Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Llm agents can autonomously hack websites"
                ]
            },
            {
                "name": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
                "arxiv_id": "2402.18667",
                "reference": [
                    "Benchmarking large language models on controllable generation under diversified instructions",
                    "Measuring massive multitask language understanding",
                    "Alpacafarm: A simulation framework for methods that learn from human feedback",
                    "Instruction-following evaluation for large language models",
                    "Safetybench: Evaluating the safety of large language models with multiple choice questions",
                    "Trafficsafetygpt: Tuning a pre-trained large language model to a domain-specific expert in transportation safety",
                    "Red-teaming large language models using chain of utterances for safety-alignment",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Large language models help humans verify truthfulness-except when they are convincingly wrong"
                ]
            },
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "More Agents Is All You Need",
                "arxiv_id": "2402.05120",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Metagpt: Meta programming for multi-agent collaborative framework",
                    "Knowledge fusion of large language models",
                    "Autogen: Enabling next-gen LLM applications via multi-agent conversation framework",
                    "Making language models better reasoners with step-aware verifier",
                    "Complexity-based prompting for multi-step reasoning",
                    "Improving factuality and reasoning in language models through multiagent debate",
                    "Examining the inter-consistency of large language models: An in-depth analysis via debate",
                    "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
                    "Reflexion: Language agents with verbal reinforcement learning",
                    "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Lamda: Language models for dialog applications",
                    "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
                    "Fusing models with complementary expertise",
                    "Encouraging divergent thinking in large language models through multi-agent debate",
                    "Frugalgpt: How to use large language models while reducing cost and improving performance",
                    "Routing to the expert: Efficient reward-guided ensemble of large language models",
                    "Large language model routing with benchmark datasets",
                    "Ask one more time: Self-agreement improves reasoning of language models in (almost) all scenarios",
                    "CAMEL: communicative agents for  \"mind \" exploration of large scale language model society",
                    "Blending is all you need: Cheaper, better alternative to trillion-parameters llm",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Autoagents: A framework for automatic agent generation"
                ]
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Large Language Models Encode Clinical Knowledge",
            "arxiv_id": "2212.13138",
            "isAPA": false,
            "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias.In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics) , including 67.6% accuracy on MedQA (US Medical License Exam questions) , surpassing prior state-of-the-art by over 17%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians.We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.",
            "reference": [
                "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma and Eli Tran-Johnson  \"Language models (mostly) know what they know \" In arXiv preprint arXiv",
                "Yuxian Gu, Xu Han, Zhiyuan Liu and Minlie Huang  \"Ppt: Pre-trained prompt tuning for few-shot learning \" In arXiv preprint arXiv",
                "Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu  \"BioGPT: generative pre-trained transformer for biomedical text generation and mining \" In Briefings in Bioinformatics 23.6 Oxford Academic",
                "Irene Y Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman and Marzyeh Ghassemi  \"Ethical machine learning in healthcare \" In Annual review of biomedical data science 4 Annual Reviews, 2021, pp",
                "Kathleen Creel and Deborah Hellman  \"The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in Algorithmic Decision-Making Systems \" In Canadian Journal of Philosophy Cambridge University Press, 2022, pp",
                "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang and Jie Tang  \"GPT understands, too \" In arXiv preprint arXiv",
                "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00e8, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon and Matthias Gall\u00e9  \"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model \" In arXiv preprint arXiv",
                "Darshali A Vyas, Leo G Eisenstein and David S Jones  \"Hidden in plain sight reconsidering the use of race correction in clinical algorithms \" In New England Journal of Medicine 383.9 Mass Medical Soc, 2020, pp",
                "Haoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew McDermott and Marzyeh Ghassemi  \"Hurtful words: quantifying biases in clinical contextual word embeddings \" In proceedings of the ACM Conference on Health, Inference, and Learning, 2020, pp",
                "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu and Orhan Firat  \"Glam: Efficient scaling of language models with mixture-of-experts \" In International Conference on Machine Learning, 2022, pp. 5547-5569 PMLR",
                "Negar Rostamzadeh, Diana Mincu, Subhrajit Roy, Andrew Smart, Lauren Wilcox, Mahima Pushkarna, Jessica Schrouff, Razvan Amironesei, Nyalleng Moorosi and Katherine Heller  \"Healthsheet: Development of a Transparency Artifact for Health Datasets \" In arXiv preprint arXiv",
                "Andre Esteva, Katherine Chou, Serena Yeung, Nikhil Naik, Ali Madani, Ali Mottaghi, Yun Liu, Eric Topol, Jeff Dean and Richard Socher  \"Deep learning-enabled medical computer vision \" In NPJ digital medicine 4.1 Nature Publishing Group, 2021, pp",
                "Ilya Loshchilov and Frank Hutter  \"Decoupled weight decay regularization \" In arXiv preprint arXiv",
                "Ankit Pal, Logesh Kumar Umapathi and Malaikannan Sankarasubbu  \"MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering \" In Conference on Health, Inference, and Learning, 2022, pp. 248-260 PMLR",
                "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring and Susannah Young  \"Scaling language models: Methods, analysis & insights from training gopher \" In arXiv preprint arXiv",
                "Raynard S Kington, Stacey Arnesen, Wen-Ying Sylvia Chou, Susan J Curry, David Lazer and Antonia M Villarruel  \"Identifying credible sources of health information in social media: Principles and attributes \" In NAM perspectives 2021 National Academy of Medicine",
                "Patrick Lewis, Myle Ott, Jingfei Du and Veselin Stoyanov  \"Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art \" In Proceedings of the 3rd Clinical Natural Language Processing Workshop, 2020, pp",
                "Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev and Jennimaria Palomaki  \"TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages \" In Transactions of the Association for Computational Linguistics 8 MIT Press, 2020, pp",
                "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut and Emma Brunskill  \"On the opportunities and risks of foundation models \" In arXiv preprint arXiv",
                "Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang and Felix Hill  \"Can language models learn from explanations in context? \" In arXiv preprint arXiv",
                "Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi and Raghav Mani  \"BioMegatron: Larger biomedical domain language model \" In arXiv preprint arXiv",
                "Michihiro Yasunaga, Jure Leskovec and Percy Liang  \"LinkBERT: Pretraining Language Models with Document Links \" In arXiv preprint arXiv",
                "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song and Jacob Steinhardt  \"Measuring massive multitask language understanding \" In arXiv preprint arXiv",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le and Denny Zhou  \"Chain of thought prompting elicits reasoning in large language models \" In arXiv preprint arXiv",
                "Sarah J Shoemaker, Michael S Wolf and Cindy Brach  \"Development of the Patient Education Materials Assessment Tool (PEMAT) : a new measure of understandability and actionability for print and audiovisual patient information \" In Patient education and counseling 96.3 Elsevier, 2014, pp",
                "WHO Guidance  \"Ethics and governance of artificial intelligence for health \" In World Health Organization",
                "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez and Robert Stojnic  \"Galactica: A Large Language Model for Science \" In arXiv preprint arXiv",
                "Nenad Toma\u0161ev, Natalie Harris, Sebastien Baur, Anne Mottram, Xavier Glorot, Jack W Rae, Michal Zielinski, Harry Askham, Andre Saraiva and Valerio Magliulo  \"Use of deep learning to develop continuous-risk models for adverse event prediction from electronic health records \" In Nature Protocols 16.6 Nature Publishing Group, 2021, pp",
                "Fabiane FR Morgado, Juliana FF Meireles, Clara M Neves, Ana Amaral and Maria EC Ferreira  \"Scale development: ten main limitations and recommendations to improve future research practices \" In Psicologia: Reflexao e Critica 30 SciELO Brasil",
                "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl and Aidan Clark  \"Training Compute-Optimal Large Language Models \" In arXiv preprint arXiv",
                "Iz Beltagy, Kyle Lo and Arman Cohan  \"SciBERT: A pretrained language model for scientific text \" In arXiv preprint arXiv",
                "Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen and Xinghua Lu  \"PubMedQA: A dataset for biomedical research question answering \" In arXiv preprint arXiv",
                "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu and Ananya Kumar  \"Holistic evaluation of language models \" In arXiv preprint arXiv",
                "Stephanie Lin, Jacob Hilton and Owain Evans  \"Teaching Models to Express Their Uncertainty in Words \" In arXiv preprint arXiv",
                "Nancy D Berkman, Stacey L Sheridan, Katrina E Donahue, David J Halpern, Anthony Viera, Karen Crotty, Audrey Holland, Michelle Brasure, Kathleen N Lohr and Elizabeth Harden  \"Health literacy interventions and outcomes: an updated systematic review. \" In Evidence report/technology assessment, 2011, pp",
                "Kathleen E Walsh, Polina Harik, Kathleen M Mazor, Deborah Perfetto, Milena Anatchkova, Colleen Biggins, Joann Wagner, Pamela J Schoettker, Cassandra Firneno and Robert Klugman  \"Measuring harm in healthcare: optimizing adverse event review \" In Medical care 55.4 NIH Public Access, 2017, pp",
                "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry and Amanda Askell  \"Language models are few-shot learners \" In Advances in neural information processing systems 33, 2020, pp",
                "Sid Black, Leo Gao, Phil Wang, Connor Leahy and Stella Biderman  \"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow \" If you use this software, please cite it using these metadata. Zenodo, 2021 DOI: 10.5281/zenodo",
                "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama and Alex Ray  \"Training language models to follow instructions with human feedback \" In arXiv preprint arXiv",
                "Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning and Percy Liang  \"Stanford CRFM Introduces PubMedGPT 2.7B \", https://hai.stanford.edu/news/stanford-crfm-introduces-pubmedgpt-27b",
                "Rishi Bommasani, Percy Liang and Tony Lee  \"Language Models are Changing AI: The Need for Holistic Evaluation \", https://crfm.stanford.edu/2022/11/17/helm.html",
                "Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy Liang and Jure Leskovec  \"Deep bidirectional language-knowledge graph pretraining \" In arXiv preprint arXiv",
                "Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang and Peter Szolovits  \"What disease does this patient have? a large-scale open domain question answering dataset from medical exams \" In Applied Sciences 11.14 MDPI, 2021, pp",
                "Apoorva Mandavilli  \"Medical Journals Blind to Racism as Health Crisis, Critics Say \", https://www.nytimes.com/2021/06/02/health/jama-racism-bauchner.html",
                "Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu  \"Bleu: a method for automatic evaluation of machine translation \" In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp",
                "Valentin Li\u00e9vin, Christoffer Egeberg Hother and Ole Winther  \"Can large language models reason about medical questions? \" In arXiv preprint arXiv",
                "Mandar Joshi, Eunsol Choi, Daniel S Weld and Luke Zettlemoyer  \"TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension \" In arXiv preprint arXiv",
                "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji and Timnit Gebru  \"Model cards for model reporting \" In Proceedings of the conference on fairness, accountability, and transparency, 2019, pp",
                "Michael Matheny, Sonoo Thadaney Israni, Mahnoor Ahmed and Danielle Whicher  \"Artificial Intelligence in Health Care: The Hope, the Hype, the Promise, the Peril \" In Washington, DC: National Academy of Medicine",
                "Xiang Lisa Li and Percy Liang  \"Prefix-tuning: Optimizing continuous prompts for generation \" In arXiv preprint arXiv",
                "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai and Quoc V Le  \"Finetuned language models are zero-shot learners \" In arXiv preprint arXiv",
                "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou and Donald Metzler  \"Emergent abilities of large language models \" In arXiv preprint arXiv",
                "Inioluwa Deborah Raji, Andrew Smart, Rebecca N White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron and Parker Barnes  \"Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing \" In Proceedings of the 2020 conference on fairness, accountability, and transparency, 2020, pp",
                "Mike Schaekermann, Carrie J Cai, Abigail E Huang and Rory Sayres  \"Expert discussions improve comprehension of difficult cases in medical image assessment \" In Proceedings of the 2020 CHI conference on human factors in computing systems, 2020, pp",
                "Emre Sezgin, Joseph Sirrianni and Simon L Linwood  \"Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model \" In JMIR Medical Informatics 10.2 JMIR Publications Inc., Toronto, Canada, 2022, pp. e",
                "Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu and Maosong Sun  \"Ptr: Prompt tuning with rules for text classification \" In AI Open Elsevier",
                "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le and Ed Chi  \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models \" In arXiv preprint arXiv",
                "George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis and Dimitris Polychronopoulos  \"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition \" In BMC bioinformatics 16.1 BioMed Central, 2015, pp",
                "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi and Graham Neubig  \"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing \" In arXiv preprint arXiv",
                "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker and Yu Du  \"Lamda: Language models for dialog applications \" In arXiv preprint arXiv",
                "Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H Chi and Alex Beutel  \"Counterfactual fairness in text classification through robustness \" In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, 2019, pp",
                "Asma Ben Abacha, Eugene Agichtein, Yuval Pinter and Dina Demner-Fushman  \"Overview of the medical question answering task at TREC 2017 LiveQA. \" In TREC, 2017, pp",
                "Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao and Hoifung Poon  \"Domain-specific language model pretraining for biomedical natural language processing \" In ACM Transactions on Computing for Healthcare (HEALTH) 3.1 ACM New York, NY, 2021, pp",
                "Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim and David Sontag  \"Large Language Models are Zero-Shot Clinical Information Extractors \" In arXiv preprint arXiv",
                "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So and Jaewoo Kang  \"BioBERT: a pre-trained biomedical language representation model for biomedical text mining \" In Bioinformatics 36.4 Oxford University Press, 2020, pp",
                "White House Office Science and Technology Policy  \"The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People \", https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf",
                "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu and Dario Amodei  \"Scaling laws for neural language models \" In arXiv preprint arXiv",
                "Dustin Tran, Jeremiah Liu, Michael W Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet and Huiyi Hu  \"Plex: Towards reliability using pretrained large model extensions \" In arXiv preprint arXiv",
                "Asma Ben Abacha, Yassine Mrabet, Mark Sharp, Travis R Goodwin, Sonya E Shooshan and Dina Demner-Fushman  \"Bridging the Gap Between Consumers' Medication Questions and Trusted Answers. \" In MedInfo, 2019, pp",
                "Vinodkumar Prabhakaran, Ben Hutchinson and Margaret Mitchell  \"Perturbation sensitivity analysis to detect unintended model biases \" In arXiv preprint arXiv",
                "Jon Kleinberg and Manish Raghavan  \"Algorithmic monoculture and social welfare \" In Proceedings of the National Academy of Sciences 118.22 National Acad Sciences, 2021, pp. e",
                "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton and Sebastian Gehrmann  \"PaLM: Scaling language modeling with pathways \" In arXiv preprint arXiv",
                "Nwamaka D Eneanya, L Boulware, Jennifer Tsai, Marino A Bruce, Chandra L Ford, Christina Harris, Leo S Morales, Michael J Ryan, Peter P Reese and Roland J Thorpe  \"Health inequities and the inappropriate use of race in nephrology \" In Nature Reviews Nephrology 18.2 Nature Publishing Group, 2022, pp",
                "Jason Yim, Reena Chopra, Terry Spitz, Jim Winkens, Annette Obika, Christopher Kelly, Harry Askham, Marko Lukic, Josef Huemer and Katrin Fasler  \"Predicting conversion to wet age-related macular degeneration using deep learning \" In Nature Medicine 26.6 Nature Publishing Group, 2020, pp",
                "Seonghyeon Ye, Joel Jang, Doyoung Kim, Yongrae Jo and Minjoon Seo  \"Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization \" In arXiv preprint arXiv",
                "Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle and Atoosa Kasirzadeh  \"Ethical and social risks of harm from language models \" In arXiv preprint arXiv",
                "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li and Xi Victoria Lin  \"OPT: Open pre-trained transformer language models \" In arXiv preprint arXiv",
                "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser and Illia Polosukhin  \"Attention is all you need \" In Advances in neural information processing systems",
                "Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang and Sudip Roy  \"Pathways: Asynchronous distributed dataflow for ML \" In Proceedings of Machine Learning and Systems 4, 2022, pp",
                "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani and Siddhartha Brahma  \"Scaling instruction-finetuned language models \" In arXiv preprint arXiv",
                "Brian Lester, Rami Al-Rfou and Noah Constant  \"The power of scale for parameter-efficient prompt tuning \" In arXiv preprint arXiv",
                "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma and David Luan  \"Show your work: Scratchpads for intermediate computation with language models \" In arXiv preprint arXiv",
                "boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer and Huan Sun  \"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters \" In arXiv preprint arXiv",
                "Diane M Korngiebel and Sean D Mooney  \"Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery \" In NPJ Digital Medicine 4.1 Nature Publishing Group, 2021, pp",
                "Xiaoxuan Liu, Ben Glocker, Melissa M McCradden, Marzyeh Ghassemi, Alastair K Denniston and Lauren Oakden-Rayner  \"The medical algorithmic audit \" In The Lancet Digital Health Elsevier",
                "Yannis Papanikolaou and Andrea Pierleoni  \"DARE: Data augmented relation extraction with gpt-2 \" In arXiv preprint arXiv",
                "Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Carl Malamud, Roger Magoulas, Kyle Chard and Ian Foster  \"ScholarBERT: Bigger is Not Always Better \" In arXiv preprint arXiv",
                "Steven Y Feng, Vivek Khetan, Bogdan Sacaleanu, Anatole Gershman and Eduard Hovy  \"CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models \" In arXiv preprint arXiv",
                "Anusri Pampari, Preethi Raghavan, Jennifer Liang and Jian Peng  \"emrqa: A large corpus for question answering on electronic medical records \" In arXiv preprint arXiv",
                "Godfred O Boateng, Torsten B Neilands, Edward A Frongillo, Hugo R Melgar-Qui\u00f1onez and Sera L Young  \"Best practices for developing and validating scales for health, social, and behavioral research: a primer \" In Frontiers in public health 6 Frontiers Media SA, 2018, pp",
                "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag and Theo Gutman-Solo  \"Solving quantitative reasoning problems with language models \" In arXiv preprint arXiv",
                "Sara Hooker  \"Moving beyond  \"algorithmic bias is a data problem \" \" In Patterns 2.4 Elsevier, 2021, pp",
                "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse and John Schulman  \"Training verifiers to solve math word problems \" In arXiv preprint arXiv",
                "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 Iii and Kate Crawford  \"Datasheets for datasets \" In Communications of the ACM 64.12 ACM New York, NY, USA, 2021, pp",
                "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi and Denny Zhou  \"Self-consistency improves chain of thought reasoning in language models \" In arXiv preprint arXiv",
                "Himabindu Lakkaraju, Dylan Slack, Yuxin Chen, Chenhao Tan and Sameer Singh  \"Rethinking Explainability as a Dialogue: A Practitioner's Perspective \" In arXiv preprint arXiv",
                "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo and Yusuke Iwasawa  \"Large Language Models are Zero-Shot Reasoners \" In arXiv preprint arXiv",
                "Tamara Williams, Marilyn Szekendi, Stephen Pavkovic, Wanda Clevenger and Julie Cerese  \"The reliability of AHRQ Common Format Harm Scales in rating patient safety events \" In Journal of patient safety 11.1 JSTOR, 2015, pp",
                "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li and Peter J Liu  \"Exploring the limits of transfer learning with a unified text-to-text transformer. \" In J. Mach. Learn. Res. 21.140, 2020, pp",
                "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta and Adri\u00e0 Garriga-Alonso  \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models \" In arXiv preprint arXiv"
            ],
            "related work": "Large language models (LLMs) Over the past few years, LLMs have shown impressive performance on natural language processing (NLP) tasks  [12, 14, 15, 30, 73, 69, 70, 89, 91, 99]. They owe their success to scaling up the training of transformer-based models [84]. It has been shown that model performance and data-efficiency scales with model size and dataset size [37]. LLMs are often trained using self-supervision on large scale, using general-purpose text corpi such as Wikipedia and BooksCorpus. They have demonstrated promising results across a wide range of tasks, including tasks that require specialized scientific knowledge and reasoning [29, 17]. Perhaps the most interesting aspect of these LLMs is their in-context few-shot abilities, which adapt these models to diverse tasks without gradient-based parameter updates [12, 43, 40, 89]. This allows them to rapidly generalize to unseen tasks and even exhibit apparent reasoning abilities with appropriate prompting strategies [14, 47, 79, 91].Several studies have shown that LLMs have the capacity to act as implicit knowledge bases [35, 29, 79]. However, there is a significant risk of these models producing hallucinations, amplifying social biases present in their training data, and displaying deficiencies in their reasoning abilities. To examine the current limitations of LLMs and to quantify the large gap between human and LLM language capabilities, BIG-bench was introduced as a community-wide initiative to benchmark on tasks that were believed at time of publication to be beyond the capabilities of current language models [78].LLMs for science and biomedicineRecent studies, such as SciBERT [5], BioNLP [46], BioMegatron [76], BioBERT [44], PubMedBERT [25], DARE [66], ScholarBERT [31], and BioGPT [56], have demonstrated the effectiveness of using curated scientific and biomedical corpora for both discriminative and generative language modeling. These models, although promising, are typically small in scale and scope compared to LLMs such as GPT-3 [12] and PaLM [14]. While the medical domain is challenging, specific proposals for LLMs have already included examples as varied as augmenting non-critical clinical assessments to summarisation of complex medical communications [41, 75, 3].The closest precedents to our work are [79], who introduced a LLM for science named Galactica, and [50], who studied the reasoning capability of LLMs in the medical question answering context. In particular, [50] used Instruct GPT-3, an instruction-tuned LLM [63], and applied chain-of-thought prompting [91] on top to improve the results on the MedQA, MedMCQA, and PubMedQA datasets.",
            "date": "2022"
        },
        "topic": "LLMs in Medicine",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Aloe: A Family of Fine-tuned Open Healthcare LLMs",
                "arxiv_id": "2405.01886",
                "subtitles": [
                    "Synthetic Training Data Generation",
                    "Prompt engineering",
                    "Preference Alignment"
                ],
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Best practices and lessons learned on synthetic data for language models",
                    "A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics",
                    "Biomistral: A collection of open-source pretrained large language models for medical domains",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Magicoder: Source code is all you need",
                    "Two directions for clinical data generation with large language models: Data-to-label and label-to-data",
                    "Does synthetic data generation of llms help clinical text mining",
                    "Towards building multilingual language model for medicine",
                    "A toolbox for surfacing health equity harms and biases in large language models",
                    "Benchmarking open healthcare llms at scale",
                    "Med-halt: Medical domain hallucination test for large language models",
                    "Meditron-70b: Scaling medical pretraining for large language models",
                    "Pmc-llama: Further finetuning llama on medical papers",
                    "Chatgpt outperforms crowd workers for text-annotation tasks",
                    "Internlm2 technical report",
                    "A study of generative large language model for medical research and healthcare",
                    "Risks from language models for automated mental healthcare: Ethics and structure for implementation",
                    "Openmathinstruct-1: A 1.8 million math instruction tuning dataset",
                    "Openmedlm: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models"
                ],
                "related_work": "2Related WorkThe field of LLMs for healthcare is currently dominated by private, non-accessible models, with the two most performing models being GPT4 and MedPalm-2[20]. Meanwhile, open models have been trying to catch up.MedAlpaca, first released in April 2023, is based on LLaMA and includes 7 and 13B models. The model is instruct-tuned on a mixture of data (150K Q&A pairs) .PMC-LLaMA[51], published in May 2023, is a fine-tune on top of LLaMA. It is first trained autoregressively on a mix of books (4B tokens, 5 epochs) and papers (totalling one-third of the book tokens) , and then instruct tuned on QA (Question-Answer) pairs (202M tokens, 3 epochs) . It includes a 7B and a 13B version.Meditron[5], published in November 2023, includes a 7B and a 70B version and is also continuously pre-trained and then fine-tuned on LLaMA-2. Its data mainly includes medical papers, as well as abstracts and guidelines for continued pre-training (48B tokens) . For testing, Meditron is instruct-tuned for each specific benchmark separately.MMed-LLM 2[39], published in February 2024, is a 7B model trained on top of InternLM-2[3]using medical data extracted primarily from general purpose multilingual data sets and textbooks (25B tokens) . This data includes 6 languages and achieves state-of-the-art performance in medical QA among open models for languages such as Japanese and Chinese in their own multilingual benchmark (MMedBench) .BioMistral[24], published in February 2024, performs continuous pre-training on medical papers (3B tokens) for 1.5 epochs, on top of the instruct-tuned Mistral-7B.While increasingly competitive, these works do not yet reach the level of performance of private models and have been recently outperformed by open general-purpose models, like Llama 3. This is directly related to the large volumes of highly curated data used for training these base models and compromises the impact of continued pre-training. In contrast, the effect of leveraging synthetic data during instruct tuning, or using model merging, alignment and advanced inference schemes remains untested. These are of special relevance for open healthcare LLMs, as these techniques can have a significant impact on modelfairness, safety, reliability, and factuality. Only a few works superficially review the risks and potential harms of models[47,18,37], and this family of LLMs have been thoroughly benchmarked in that regard recently[2]. A pressing issue, considering the dangers ofbias, toxicity, sycophancy, and hallucinationsin healthcare.2.1Synthetic Training Data GenerationSynthetic data is becoming a crucial component in addressing the scarcity of high-quality data to train LLMs, particularly within specialised and private domains, such as the medical field. Synthetic data generation has been proven to be an effective way of scaling training and evaluation[15]data for LLMs for diverse domains, such as math[45], code[49]and general[7]. Current open models offer a great alternative to labour-intensive manual data curation processes, as they are easier to fit in more affordable GPUs, making data generation exponentially more scalable.However, the generation of synthetic data poses new challenges such as hallucinations and inherent model biases[28], impacting the dataset quality. For this reason, recent approaches use real medical data as a base to prompt and enhance it for a particular medical task.[44]employs ChatGPT to generate more than 10K examples based on several biomedical Named Entity Recognition and Relation Extraction datasets as seed, significantly improving the F1 score in both tasks.[25]uses a CoT style synthetic data generation strategy based on LLaMA-65B to detect Alzheimer's Disease (AD) -related signs and symptoms from electronic health records (EHRs) . Lastly, GatorTron[36]generated 20 billion words of synthetic text to train NLP models, which outperform models trained using real-world clinical text.2.2Prompt engineeringPrompt engineering has emerged as a powerful technique in natural language processing, offering an alternative approach to enhance the performance of LLMs without the need for retraining. At its core, prompt engineering involves crafting specialized input prompts or instructions that guide LLMs to generate desired outputs or responses.In-Context Learning(ICL) is a technique that involves integrating task demonstrations directly into the input prompt. This approach empowers pre-trained LLMs to tackle novel tasks without requiring fine-tuning of the model.Zero-Shot promptingpresents a task to a model without any accompanying examples, relying solely on the model's pre-existing knowledge. This concept is extended byfew-shot learning, which provides a small number of examples to guide the model in quickly learning new tasks.KNN few-shot learningbuilds on this by incorporating the most similar existing examples, often stored in vector databases like those used in Retrieval-Augmented Generation (RAG) systems.Chain of Thought(CoT) prompting involves generating intermediate reasoning steps before arriving at the final answer. By breaking down complex problems into smaller steps, CoT helps models generate more accurate responses. Integrating CoT with ICL can enhance performance further by introducing few-shot examples alongside the reasoning steps. Finally,self-consistencymethods combine outputs from different models or multiple runs of the same model. Techniques such as majority voting, averaging, or seeking consensus among outputs lead to more robust and accurate results. When employed in conjunction with other techniques like prompt engineering, self-consistency can significantly improve the reliability and effectiveness of LLMs across various tasks and domains.In regards to medical LLMs, previous works have studied the impact of integrating prompt engineering techniques to boost the performance of specialized medical models. For example, in Meditron[5], they use the Self-Consistency Chain of Though technique achieving SOTA performance within open-source models on PubmedQA, MedQA, and MedMCQA benchmarks(MultiMedQA suite) by sampling 20 generations and performing majority voting. Recent studies have also explored the use of advanced prompting methods as a cost-effective approach to optimize the performance of generalist foundation models. Microsoft developed a prompting technique that involves several strategies, combining CoT, ICL with K-NN few-shots, and self-consistency. Their technique, namedMedprompt[34], was tested on GPT-4 achieving SOTA results on all benchmarks of the MultiMedQA suite. Following this strategy, OpenMedLM[30]conducted a study of these prompting techniques using open-source models. Results obtained improved performance on three of the four most widely used medical benchmarks.2.3Preference Alignment",
                "abstract": "As the capabilities of Large Language Models (LLMs) in healthcare and medicine continue to advance, there is a growing need for competitive open-source models that can safeguard public interest. With the increasing availability of highly competitive open base models, the impact of continued pre-training is increasingly uncertain. In this work, we explore the role of instruct tuning, model merging, alignment, red teaming and advanced inference schemes, as means to improve current open models. To that end, we introduce the Aloe family, a set of open medical LLMs highly competitive within its scale range. Aloe models are trained on the current best base models (Mistral, LLaMA 3), using a new custom dataset which combines public data sources improved with synthetic Chain of Thought (CoT). Aloe models undergo an alignment phase, becoming one of the first few policy-aligned open healthcare LLM using Direct Preference Optimization, setting a new standard for ethical performance in healthcare LLMs. Model evaluation expands to include various bias and toxicity datasets, a dedicated red teaming effort, and a much-needed risk assessment for healthcare LLMs. Finally, to explore the limits of current LLMs in inference, we study several advanced prompt engineering strategies to boost performance across benchmarks, yielding state-of-the-art results for open healthcare 7B LLMs, unprecedented at this scale."
            },
            {
                "name": "Qibo: A Large Language Model for Traditional Chinese Medicine",
                "arxiv_id": "2403.16056",
                "subtitles": [
                    "Large Language Models",
                    "LLMs in Medical Domain"
                ],
                "reference": [
                    "Efficient and effective text encoding for chinese llama and alpaca",
                    "Huatuogpt, towards taming language model to be a doctor",
                    "Qilin-med: Multi-stage knowledge injection advanced medical large language model",
                    "Large language models encode clinical knowledge",
                    "Bloom: A 176b-parameter open-access multilingual language model",
                    "Chatgpt: Optimizing language models for dialogue. openai",
                    "Stanford alpaca: An instruction-following llama model",
                    "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
                    "Gpt-4 technical report",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Huatuo: Tuning llama model with chinese medical knowledge",
                    "Baichuan 2: Open large-scale language models",
                    "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue",
                    "Falcon-40b: an open large language model with state-of-the-art performance",
                    "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                    "Llama: Open and efficient foundation language models",
                    "Glm: General language model pretraining with autoregressive blank infilling",
                    "Medalpaca-an open-source collection of medical conversational ai models and training data",
                    "Towards expert-level medical question answering with large language models",
                    "Hello GPT-4o",
                    "Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence",
                    "Qwen technical report"
                ],
                "related_work": "2Related Works2.1Large Language ModelsThe remarkable achievements of Large Language Models (LLMs) , such as ChatGPT(OpenAI,2022) , GPT-4(Achiam and et al.,2023) and GPT-4o(OpenAI,2024) , have garnered substantial attention, which relies on large-scale pre-training. While OpenAI has yet to publicly disclose its training strategy or weights, the rapid emergence of open source LLMs like LLaMA(Touvron and al.,2023) , Alpaca(Taori et al.,2023) , Vicuna(Chiang et al.,2023) , Bloom(Workshop and al.,2022) , and Falcon(Almazrouei et al.,2023) . To compensate for their initially limited Chinese language proficiency, they were successfully trained to enhance their skills in Chinese through large Chinese language datasets, using next-token prediction as a key training objective to understand context and predict the next word. Chinese LLaMA and Chinese Alpaca(Cui et al.,2023) continually pre-trained and optimized with Chinese data and vocabulary. Ziya-LLaMA(Zhang et al.,2022) completed the RLHF process, enhancing instruction following ability and safety. In addition, there have been notable attempts to build efficient Chinese LLMs from scratch, such as BaiChuan(Baichuan,2023) , Qwen(Bai et al.,2023) , and GLM(Du et al.,2021) .2.2LLMs in Medical DomainLarge models generally perform best in medical contexts demanding complex knowledge and high precision. Attempts to improve this include MedAlpaca(Han et al.,2023) and ChatDoctor(Yunxiang et al.,2023) , which employe continuous training, and Med-PaLM(Singhal et al.,2022) , and Med-PaLM2(Singhal et al.,2023) , receiving favourable expert reviews for clinical responses. Chinese medical domain studies include DoctorGLM(Xiong et al.,2023) , which used extensive Chinese medical dialogue data and an external medical knowledge base, and BenTsao(Wang et al.,2023) , utilizing only a medical knowledge graph for dialogue construction.Zhang et al. (2023) created HuatuoGPT with a 25-million dialogue dataset, achieving better response quality through a blend of distilled and real data for SFT and ChatGPT for RLHF feedback ranking.Zhongjing(Yang et al.,2023) , which is a Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training, SFT, to RLHF and introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the capability for complex dialogue and proactive inquiry initiation. Qilin-Med(Ye et al.,2023) is a medical model based on Baichuan-7B, showcasing enhanced performance and more refined outputs in medical applications, owing to its innovative multi-stage training approach. It also curates a 3GB dataset of TCM, encompassing medical question answering, plain text, knowledge graph, and dialogue.",
                "abstract": "Large Language Models (LLMs) has made significant progress in a number of professional fields, including medicine, law, and finance. However, in traditional Chinese medicine (TCM), there are challenges such as the essential differences between theory and modern medicine, the lack of specialized corpus resources, and the fact that relying only on supervised fine-tuning may lead to overconfident predictions. To address these challenges, we propose a two-stage training approach that combines continuous pre-training and supervised fine-tuning. A notable contribution of our study is the processing of a 2GB corpus dedicated to TCM, constructing pre-training and instruction fine-tuning datasets for TCM, respectively. In addition, we have developed Qibo-Benchmark, a tool that evaluates the performance of LLM in the TCM on multiple dimensions, including subjective, objective, and three TCM NLP tasks. The medical LLM trained with our pipeline, named $\\textbf{Qibo}$, exhibits significant performance boosts. Compared to the baselines, the average subjective win rate is 63%, the average objective accuracy improved by 23% to 58%, and the Rouge-L scores for the three TCM NLP tasks are 0.72, 0.61, and 0.55. Finally, we propose a pipline to apply Qibo to TCM consultation and demonstrate the model performance through the case study."
            },
            {
                "name": "TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language Models",
                "arxiv_id": "2406.04941",
                "subtitles": [
                    "LLMs for Medical Application",
                    "Medical Evaluation Datasets for LLMs"
                ],
                "reference": [
                    "Large language models encode clinical knowledge",
                    "Zero-shot information extraction for clinical meta-analysis using large language models",
                    "Dense retrieval adaptation using target domain description",
                    "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
                    "Applying deep matching networks to chinese medical question answering: a study and a dataset",
                    "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
                    "Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset",
                    "Towards expert-level medical question answering with large language models",
                    "A large language model for electronic health records",
                    "Almanac: Knowledge-grounded language models for clinical medicine",
                    "Exploring the in-context learning ability of large language model for biomedical concept linking",
                    "The promise and peril of using a large language model to obtain clinical information: Chatgpt performs strongly as a fertility counseling tool with limitations",
                    "Mlec-qa: A chinese multi-choice biomedical question answering dataset"
                ],
                "related_work": "2Related Work2.1LLMs for Medical ApplicationIn recent years, there has been an increasing interest in the application of LLMs in the field of medicine. Researchers have explored various approaches to leverage LLMs for medical tasks such as biomedical information extraction[13,25,32], clinical text analysis[32,6], medical queries[33,10], and so on. Certain researchers take an additional stride by directly replacing the doctor with LLMs[30,16]. Particularly, in a double-blind trial, MedGPT, proposed by Medlinker, demonstrated a medical concordance rate of 96% with attending physicians from Three-A hospital. Also, Med-PALM 2[21]becomes the first LLM to pass the United States Medical Licensing Examination style questions in MultiMedQA[20]with an expert-level score. Through such a plethora of studies, we can envision that in the near future, LLMs will be a great help to health workers.2.2Medical Evaluation Datasets for LLMsMedical evaluation datasets for LLM are hard to construct as they require well-designed samples with diverse formats and rich medical knowledge. He[11]proposes to collect medical questions from online health consultancy websites to construct medical QA datasets. The dataset is unsatisfying in terms of professionalism and fluency because the conversations on online platforms are usually not conventional. To overcome the problem, Li[15]proposes the first large-scale open-source Chinese medical MCQA dataset, i.e. MLEC-QA, sourced from the Chinese National Medical Licensing Examination. Liu[17]collects problems from the same source and performs a more detailed categorization, making it possible to measure in multiple dimensions. To better leverage the existing datasets,[20]collects several medical datasets covering medical questions from multiple countries to assess LLMs in medical question answering.Most of these datasets collect as many problems as possible to evaluate medical abilities comprehensively. However, knowledge of different subjects holds varying significance in real-world applications. To address it, our dataset contains questions from every required medical subject in the national exam, and we also balance the distribution of questions of different subjects according to their official manual.",
                "abstract": "The recently unprecedented advancements in Large Language Models (LLMs) have propelled the medical community by establishing advanced medical-domain models. However, due to the limited collection of medical datasets, there are only a few comprehensive benchmarks available to gauge progress in this area. In this paper, we introduce a new medical question-answering (QA) dataset that contains massive manual instruction for solving Traditional Chinese Medicine examination tasks, called TCMD. Specifically, our TCMD collects massive questions across diverse domains with their annotated medical subjects and thus supports us in comprehensively assessing the capability of LLMs in the TCM domain. Extensive evaluation of various general LLMs and medical-domain-specific LLMs is conducted. Moreover, we also analyze the robustness of current LLMs in solving TCM QA tasks by introducing randomness. The inconsistency of the experimental results also reveals the shortcomings of current LLMs in solving QA tasks. We also expect that our dataset can further facilitate the development of LLMs in the TCM area."
            },
            {
                "name": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
                "arxiv_id": "2402.02392",
                "subtitles": [
                    "Decision Making with LLMs",
                    "Uncertainty in LLMs"
                ],
                "reference": [
                    "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
                    "Re-examining calibration: The case of question answering",
                    "Language models (mostly) know what they know",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "Teaching models to express their uncertainty in words",
                    "Approaching human-level forecasting with language models",
                    "Large language model as autonomous decision maker",
                    "Bird: A trustworthy bayesian inference framework for large language models",
                    "Leveraging large language models for decision support in personalized oncology",
                    "A language agent for autonomous driving",
                    "Large language models for supply chain optimization",
                    "Reflexion: Language agents with verbal reinforcement learning",
                    "Are large language models bayesian? a martingale perspective on in-context learning",
                    "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
                    "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Importance of directional feedback for llm-based optimizers",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Generating with confidence: Uncertainty quantification for black-box large language models",
                    "Reducing conversational agents' overconfidence through linguistic calibration",
                    "From word models to world models: Translating from natural language to the probabilistic language of thought",
                    "Reasoning with language model is planning with world model",
                    "Toolchain*: Efficient action space navigation in large language models with a* search",
                    "Robots that ask for help: Uncertainty alignment for large language model planners",
                    "Large language models as optimizers",
                    "Uncertainty in natural language generation: From theory to applications",
                    "Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment"
                ],
                "related_work": "2Related WorkDecision Making with LLMs.Initiated by Chain-of-Thought (CoT) prompting[44], a number of recent works have developed methods to decompose multi-step reasoning problems into modular sub-problems. For example, Tree-of-Thought (ToT) prompting[48]generalizes CoT with a tree-search procedure to optimize a reasoning path subject to external feedback. Subsequent works aim to improve ToT with better search algorithms, self-induced feedback, and tool usage[17,32,50,49]. However, these works do not focus on decision-makingunder uncertainty, which we demonstrate is challenging even for carefully-crafted chains that emulate classical decision-making procedures.Another line of work leverages LLMs for optimizing blackbox functions[47,30,38]. These settings involve methods that make a substantial number of low-cost decisions (which do not incur a high price for suboptimality) . Instead, we focus on single-stepexpensivedecisions, particularly in the prescence of uncertainty, with a focus on the optimality of decisions. Additionally, a number of applied domains that involve decision making have recently started to explore LLM-based methods, such as in supply chain optimization[23], medicine and health[5], and automated driving[28].Uncertainty in LLMs.LLMs, without proper calibration, can be overly confident in their responses[39]. Such pitfalls make them unlikely to make reliable decisions under uncertainty. Prior work has aimed to solve this issue; one line of research involves asking LLMs for their own confidence, with or without additional finetuning[18,24,29,10,41,46]. Referring to[3]for a detailed survey, many recent advances[25,13,11,45]adopt a Bayesian inference framework to quantify and reason with uncertainties in LLMs. Other works have shown that tool usage[35], retrieval augmentation[16], and model ensemble[37]can improve calibration and forecasting capabilities of LLMs. Our framework can take advantage of these advances in LLM-based forecasting for improved decision making.",
                "abstract": "The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of decision-making under uncertainty. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide a rational and human-auditable decision-making process. We validate our framework on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods."
            },
            {
                "name": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications",
                "arxiv_id": "2405.19266",
                "subtitles": [
                    "Chinese Large Language Model Evolution",
                    "LLMs in Medical Applications"
                ],
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Efficient and effective text encoding for chinese llama and alpaca",
                    "Huatuogpt, towards taming language model to be a doctor",
                    "Glm-130b: An open bilingual pre-trained model",
                    "Mistral 7b",
                    "Robust emotion recognition in context debiasing",
                    "Introducing chatgpt",
                    "Can llms' tuning methods work in medical multimodal domain",
                    "Natural instructions: Benchmarking generalization to new tasks from natural language instructions",
                    "Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt",
                    "Disentangled representation learning for multimodal emotion recognition",
                    "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Gpt-4 technical report",
                    "https://github.com/baichuan-inc/Baichuan-13B",
                    "Huatuogpt-ii, one-stage training for medical adaption of llms",
                    "Huatuo: Tuning llama model with chinese medical knowledge",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Baichuan 2: Open large-scale language models",
                    "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue",
                    "Miss: A generative pretraining and finetuning approach for med-vqa",
                    "Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences",
                    "Glm: General language model pretraining with autoregressive blank infilling",
                    "Llama: Open and efficient foundation language models",
                    "Efficiency in focus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained models",
                    "Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence",
                    "Qwen technical report",
                    "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
                    "Disc-medllm: Bridging general large language models and real-world medical consultation",
                    "Medicalgpt: Training medical gpt model",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work Chinese Large Language Model Evolution. The emergence of Large Language Models (LLMs) dominated by ChatGPT [31] and GPT-4 [5] has revolutionized the paradigm for novel human-machine interaction. Driven by learning-oriented technologies [11, 12, 13, 50, 51, 52], pragmatic instruction [30, 45] and preference optimization [7, 32] strategies enable LLMs to address complex generation tasks with aligned human intentions. Despite improvements, large-scale resources for training general LLMs [26, 41, 42] are anchored in the English corpora, limiting their abilities to respond reliably in extensive Chinese application scenarios. Recently, researchers [17, 56] have attempted to enhance the comprehension and execution of Chinese instructions in open-source LLMs by augmenting Chinese vocabulary and data (e.g., Chinese LLaMA and Alpaca [17]). To facilitate Chinese-specific demands, several LLMs trained from scratch exhibit remarkable Chinese proficiency due to multilingual data resources, such as the Baichuan [2, 49], General Language Model (GLM) [19, 54], and Qwen [6] families. In this work, the Baichuan2-Base series is utilized as the foundation model for our PediatricsGPT, given its comprehensive potential among similar contenders. LLMs in Medical Applications. Current LLMs provide unprecedented opportunities to develop resource-efficient and diagnostic-comprehensive intelligent healthcare systems. Despite universal models [5, 31] equipped with certain internal knowledge regarding biomedicine, they are incompetent in real-world medical applications due to the absence of domain-specific disciplines. In this context, several efforts [43, 47, 15, 28] attempt to construct medically tailored LLMs from multiple perspectives. For instance, ChatDoctor [28] uses patient-doctor conversation data based on LLaMA [41] to enhance the language model's accuracy in healthcare. DoctorGLM [47] proves that a healthcare-purpose LLM can be implemented with affordable overhead by fine-tuning ChatGLM-6B [19]. After that, more Chinese medical LLMs [48, 8, 55, 14, 53] are progressively presented to generate doctor-like robust responses, such as HuatuoGPT [55], DISC-MedLLM [8], and Zhongjing [53]. Despite advances in general medical knowledge, current models are suboptimal for pressing pediatric applications. In comparison, our sophisticated training procedure and high-quality instruction datasets inject new insights and prospects for developing specialized LLMs with pediatric expertise.",
                "abstract": "Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development."
            },
            {
                "name": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
                "arxiv_id": "2406.13890",
                "subtitles": [
                    "Existing Medical Benchmarks",
                    "Existing Agents for Medical Applications"
                ],
                "reference": [
                    "Large language models encode clinical knowledge",
                    "Agent hospital: A simulacrum of hospital with evolvable medical agents",
                    "Measuring massive multitask language understanding",
                    "Epidemic modeling with generative agents",
                    "What disease does this patient have? A large-scale open domain question answering dataset from medical exams",
                    "CMB: A comprehensive medical benchmark in chinese",
                    "Medagents: Large language models as collaborators for zero-shot medical reasoning",
                    "Adaptive collaboration strategy for llms in medical decision making",
                    "Towards building multilingual language model for medicine",
                    "Medbench: A large-scale chinese benchmark for evaluating medical large language models",
                    "Pubmedqa: A dataset for biomedical research question answering",
                    "Benchmarking large language models on cmexam - A comprehensive chinese medical exam dataset"
                ],
                "related_work": "2Related Work 2.1Existing Medical Benchmarks The Chinese Medical Benchmark [34] consists of CMB-Exam and CMB-Clin, with data collected from various public examination databases and multiple-choice questions from textbooks. MedBench [5] also includes multiple-choice questions from the Chinese mainland medical licensing examination and 1025 QA pairs based on electronic medical records. MMedBench [26] is a multilingual medical evaluation benchmark covering six languages, with data sourced from medical textbooks and open-source medical websites in different languages. Different from our method of collecting real cases from different departments, MMedBench and CMExam [20] utilize GPT-4 to categorize each question according to a pre-defined list of departments. However, according to the experimental results shown in Figure 4, GPT-4 exhibits significant errors in departmental classification. MedQA [15] consists of multiple-choice questions collected from professional medical board examinations. Its USMLE-style English subset offers four possible answer options for each question and is widely used to evaluate the performance of LLMs in the medical domain. PubMedQA [16] is a biomedical QA dataset collected from the PubMed website, where questions need to be answered with Yes/No/Maybe. MMLU [12] is a benchmark covering 57 tasks across multiple domains, recent work [28] extracts 6 medicine-related tasks from MMLU to evaluate the medical capabilities of LLMs. Furthermore, HealthSearchQA [28] is a dataset based on medical conditions and related symptoms frequently searched by users on search engines, aiming to evaluate a model's ability to handle real-world user's concerns about their health. Table 1 compares statistics of existing benchmarks and datasets for evaluating LLMs in the medical field and our ClinicalBench, including the number of samples, task types, language coverage, and data sources. In summary, the main shortcomings of existing evaluation benchmarks include: (1) lack of comprehensive and evenly distributed departmental coverage to prevent evaluation bias; (2) data sources often come from easily accessible online consultation platforms, medical textbooks, and professional examinations, which poses high risks of data leakage; (3) existing benchmarks primarily test medical knowledge through multiple-choice questions, which differ significantly from real-world diagnostic scenarios. 2.2Existing Agents for Medical Applications Recent works attempt to solve medical and healthcare issues through paradigms of dividing tasks and collaboration among multiple agents. MedAgent [31] is the first multi-agent framework in the medical field that improves the accuracy of solving medical multiple-choice questions by allowing the same LLM to play different roles in multi-round collaborative dialogues. However, the design of MedAgent still relies on the multiple-choice question format, which differs significantly from the diagnostic process in the real world. On the other hand, Williams et al. [35] effectively simulate real-world human behavior patterns through agents played by ChatGPT to address the challenge of incorporating human behavioral factors into epidemic models. MDAgents [17] is a multi-agent framework that employs an adaptive decision-making mechanism, addressing medical multiple-choice questions through multiple stages of checking problem complexity, dynamically recruiting experts, reasoning, and decision-making. Meanwhile, Agent Hospital [18] improves diagnostic accuracy by simulating nearly all medical processes within a hospital and trains doctor agents through doctor-patient interaction simulations. However, due to the lack of a comprehensive dataset covering the entire medical process, currently its effectiveness is only validated on the MedQA multiple-choice dataset. In summary, existing medical agents suffer from severe limitations and constraints in designs and evaluations due to the lack of benchmarks and datasets based on real diagnostic processes. Therefore, developing an evaluation benchmark based on real medical diagnostic processes is critical. Furthermore, most previously proposed medical agents perform clinical diagnosis by using prompts to make the same model play different roles. In contrast, our ClinicalAgent is driven by the evaluation results and analysis of prior models on ClinicalBench. ClinicalAgent uses different models to play different roles and implements dynamic department scheduling and doctor allocation strategies for clinical diagnosis, ensuring a strong alignment with real hospital settings.",
                "abstract": "LLMs have achieved significant performance progress in various NLP applications. However, LLMs still struggle to meet the strict requirements for accuracy and reliability in the medical field and face many challenges in clinical applications. Existing clinical diagnostic evaluation benchmarks for evaluating medical agents powered by LLMs have severe limitations. Firstly, most existing medical evaluation benchmarks face the risk of data leakage or contamination. Secondly, existing benchmarks often neglect the characteristics of multiple departments and specializations in modern medical practice. Thirdly, existing evaluation methods are limited to multiple-choice questions, which do not align with the real-world diagnostic scenarios. Lastly, existing evaluation methods lack comprehensive evaluations of end-to-end real clinical scenarios. These limitations in benchmarks in turn obstruct advancements of LLMs and agents for medicine. To address these limitations, we introduce ClinicalLab, a comprehensive clinical diagnosis agent alignment suite. ClinicalLab includes ClinicalBench, an end-to-end multi-departmental clinical diagnostic evaluation benchmark for evaluating medical agents and LLMs. ClinicalBench is based on real cases that cover 24 departments and 150 diseases. ClinicalLab also includes four novel metrics (ClinicalMetrics) for evaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate 17 LLMs and find that their performance varies significantly across different departments. Based on these findings, in ClinicalLab, we propose ClinicalAgent, an end-to-end clinical agent that aligns with real-world clinical diagnostic practices. We systematically investigate the performance and applicable scenarios of variants of ClinicalAgent on ClinicalBench. Our findings demonstrate the importance of aligning with modern medical practices in designing medical agents."
            },
            {
                "name": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
                "arxiv_id": "2406.12036",
                "subtitles": [
                    "Language Model Evaluations in Medicine",
                    "Language Model Evaluations in Mathematics",
                    "Tool Learning"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Pal: Program-aided language models",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "Pubmedqa: A dataset for biomedical research question answering",
                    "Toolformer: Language models can teach themselves to use tools",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Structured chemistry reasoning with large language models",
                    "A survey on large language model based autonomous agents",
                    "Augmenting large language models with chemistry tools",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Toolqa: A dataset for llm question answering with external tools",
                    "Tool learning with foundation models",
                    "Cognitive architectures for language agents",
                    "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
                    "Measuring massive multitask language understanding",
                    "Genegpt: Augmenting large language models with domain tools for improved access to biomedical information",
                    "Augmentation of chatgpt with clinician-informed tools improves performance on medical calculation tasks",
                    "Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning",
                    "Measuring mathematical problem solving with the MATH dataset",
                    "Bigbio: A framework for data-centric biomedical natural language processing"
                ],
                "related_work": "5Related Work 5.1Language Model Evaluations in Medicine Existing datasets for evaluating LLMs in biomedicine [10] have primarily focused on verbal reasoning through multiple choice questions such as PubMedQA [22], MedQA [21], MedMCQA [35], and the medical questions in MMLU [15]. However, these datasets are mainly focused on qualitative reasoning instead of quantitative computation. Additionally, the format of multi-choice questions does not reflect the actual clinical settings where a single answer or response must be determined without any options provided. In this work, we introduce MedCalc-Bench, the first dataset that measures the quantitative reasoning capabilities of LLMs in medicine in a realistic setting where the LLM must determine the answer by itself without the support of answer choices. 5.2Language Model Evaluations in Mathematics Many efforts have been made to evaluate the mathematical and computation capability of LLMs in various settings. GSM8k [6] and MATH [16] are two examples which focus on pure mathematical problems. However, these datasets with general settings may not reflect LLM performance in domain-specific applications. While there exist mathematical and computation-oriented datasets for specific domains such as chemistry [34], their lack of manually verified step-by-step explanations weakens their reliability for model evaluation. MedCalc-Bench not only serves as the first dataset for medical-focused calculations, but also provides explanations that are verified by human experts. A full comparison of various datasets can be found in Table 5. 5.3Tool Learning One of the key features of language agents is the capability to use tools [36, 46, 54, 62], such as code interpreters [4, 12] and external APIs [37, 41]. GeneGPT [25] and ChemCrow [29] utilize domain functionalities for scientific discovery. While OpenMedCalc [13] and AgentMD [24] use medical calculators to augment LLMs, their evaluations are based on small-scale or automatically constructed datasets. Our manually-reviewed MedCalc-Bench is much larger than their evaluation datasets and contains both natural language explanations as well as final numeric answers.",
                "abstract": "As opposed to evaluating computation and logic-based reasoning, current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks."
            },
            {
                "name": "Benchmarking Retrieval-Augmented Generation for Medicine",
                "arxiv_id": "2402.13178",
                "subtitles": [
                    "Retrieval-augmented Generation",
                    "Biomedical Question Answering"
                ],
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Retrieval-augmented generation for large language models: A survey",
                    "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                    "Literature-augmented clinical outcome prediction",
                    "Biomedical question answering: a survey of approaches and challenges",
                    "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                    "Capabilities of gpt-4 on medical challenge problems",
                    "Bioreader: a retrieval-enhanced text-to-text transformer for biomedical literature",
                    "Retrieve, summarize, and verify: How will chatgpt impact information seeking from the medical literature",
                    "In-context retrieval-augmented language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Large language models encode clinical knowledge",
                    "Active retrieval augmented generation",
                    "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
                    "Question answering in biomedicine",
                    "Almanac retrieval-augmented language models for clinical medicine",
                    "Augmenting black-box llms with medical textbooks for clinical question answering",
                    "Domain-specific language model pretraining for biomedical natural language processing",
                    "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
                    "Deep bidirectional language-knowledge graph pretraining",
                    "Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering",
                    "BERT: Pre-training of deep bidirectional transformers for language understanding",
                    "Improving language models by retrieving from trillions of tokens",
                    "Meditron-70b: Scaling medical pretraining for large language models",
                    "Pmc-llama: Further finetuning llama on medical papers",
                    "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
                    "Augmented language models: a survey",
                    "Towards expert-level medical question answering with large language models",
                    "Biomedical question answering: A survey",
                    "Paperqa: Retrieval-augmented generative agent for scientific research"
                ],
                "related_work": "2Related Work2.1Retrieval-augmented GenerationRetrieval-Augmented Generation (RAG) was proposed byLewis et al. (2020) to enhance the generation performance on knowledge-intensive tasks by integrating retrieved relevant information. RAG not only mitigates the problem of hallucinations as LLMs are grounded on given contexts, but can also provide up-to-date knowledge that might not be encoded by the LLMs. Many follow-up studies have been carried out to improve over the vanilla RAGBorgeaud et al. (2022) ; Ram et al. (2023) ; Gao et al. (2023) ; Jiang et al. (2023) ; Mialon et al. (2023) .In biomedicine, there have also been various explorations on how LLMs can improve literature information-seeking and clinical decision-making with RAGFrisoni et al. (2022) ; Naik et al. (2022) ; Jin et al. (2023b) ; L\u00e1la et al. (2023) ; Zakka et al. (2024) ; Jeong et al. (2024) ; Wang et al. (2023) , but their evaluations are not comprehensive. Nevertheless, current systematic evaluations in biomedicine typically focus on the vanilla LLMs without RAG(Chen et al.,2023a; Nori et al.,2023a) . Our study provides the first systematic evaluations of RAG systems in medicine.2.2Biomedical Question AnsweringBiomedical or medical question answering (QA) is a widely studied task since various information needs are expressed by natural language questions in biomedicineZweigenbaum (2003) ; Athenikos and Han (2010) ; Jin et al. (2022) . While BERT-basedDevlin et al. (2019) models used to be the state-of-the-art methods of medical QAAbacha et al. (2019) ; Lee et al. (2020) ; Soni and Roberts (2020) ; Gu et al. (2021) ; Yasunaga et al. (2022) , they are outperformed by LLMs with large marginsSinghal et al. (2023b) ; Chen et al. (2023b) ; Nori et al. (2023b) . Due to their knowledge-intensive nature, QA datasets are commonly used to evaluate the biomedical capabilities of both general LLMs(Nori et al.,2023a,b) and domain-specific LLMsLuo et al. (2022) ; Chen et al. (2023b) ; Wu et al. (2023) ; Singhal et al. (2023a,b) . Following these studies, we also use medical QA datasets to test if a RAG system can retrieve and leverage relevant contexts. Unlike prior efforts, our evaluation employs both RAG and question-only retrieval settings, a more realistic evaluation for medical QA.",
                "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine."
            },
            {
                "name": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models",
                "arxiv_id": "2404.17897",
                "subtitles": [
                    "Large Language Model in Medical Domain",
                    "Retrieval-Augmented Generation"
                ],
                "reference": [
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Large language models in medicine",
                    "Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt",
                    "Dense passage retrieval for open-domain question answering",
                    "Retrieval-augmented generation for large language models: A survey",
                    "Recomp: Improving retrieval-augmented lms with compression and selective augmentation",
                    "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                    "Huatuogpt, towards taming language model to be a doctor",
                    "Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases",
                    "The future landscape of large language models in medicine",
                    "Structure-aware language model pretraining improves dense retrieval on structured data",
                    "Llmlingua: Compressing prompts for accelerated inference of large language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Lift yourself up: Retrieval-augmented text generation with self-memory",
                    "Few-shot learning with retrieval augmented language models"
                ],
                "related_work": "2Related WorkLarge Language Model in Medical Domain.The impressive abilities of large language models (LLMs) across various applications have catalyzed extensive investigation into employing them in healthcare and medical domains. This surge in attention is documented through a growing body of research(Thirunavukarasu et al.,2023; Clusmann et al.,2023) . Some recent works have studied to augment LMMs with real-world data. ChatDoctor(Li et al.,2023b) , trained by fine-tuning LLaMA(Touvron et al.,2023) on a large dataset of patient-doctor dialogues, achieves high accuracy and reliability in medical scenarios with an external information retrieval module. From the other line, some adopt the synthetic data for fine-tuning.Zhang et al. (2023a) utilized real-world data from medical professionals alongside distilled data from ChatGPT to fine-tune the model. To enhance the capability in the multi-round conversation, BianQue(Chen et al.,2023) trained the model on a self-constructed dataset containing multi-round inquiries and health suggestions. Despite the remarkable performance, there is still a gap in applying LLMs in real-world scenarios due to the lack of domain-specific knowledge. To further evaluate the proficiency of LLMs in medical domains, we introduce MedicineQA, a benchmark derived from real-world medication consultation scenarios.Retrieval-Augmented Generation.LLMs require external knowledge to alleviate the factuality drawbacks. Retrieval-augmented generation (RAG) has been regarded as an effective solution to mitigate the aforementioned hallucinations and temporal misalignment issues inherent in large language models, especially for knowledge-intensive tasks. Generally, studies of RAG can be categorized into three types(Gao et al.,2023) , namely Naive RAG, Advanced RAG, and Modular RAG. Naive RAG means a straightforwardRetrieve-then-Readframework(Lewis et al.,2020; Karpukhin et al.,2020; Izacard et al.,2022) . To enhance retrieval quality, the Advanced RAG builds upon the foundation of Naive RAG by incorporating pre-retrieval(Li et al.,2023a) and post-retrieval(Jiang et al.,2023; Xu et al.,2023) strategies. Modular RAG improves the overall performance by decomposing theRetrieve-then-Readframework into fine-grained modules with distinct functionalities, such as a search module(Wang et al.,2023) , memory module(Cheng et al.,2024) .",
                "abstract": "Large-scale language models (LLMs) have achieved remarkable success across various language tasks but suffer from hallucinations and temporal misalignment. To mitigate these shortcomings, Retrieval-augmented generation (RAG) has been utilized to provide external knowledge to facilitate the answer generation. However, applying such models to the medical domain faces several challenges due to the lack of domain-specific knowledge and the intricacy of real-world scenarios. In this study, we explore LLMs with RAG framework for knowledge-intensive tasks in the medical field. To evaluate the capabilities of LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates the real-world medication consultation scenario and requires LLMs to answer with retrieved evidence from the medicine database. MedicineQA contains 300 multi-round question-answering pairs, each embedded within a detailed dialogue history, highlighting the challenge posed by this knowledge-intensive task to current LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework instead of the previous \\textit{Retrieve-then-Read}. Specifically, the distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines. With experimental results, we show that our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy. This advancement sheds light on applying RAG to the medical domain."
            },
            {
                "name": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework",
                "arxiv_id": "2408.12496",
                "subtitles": [
                    "LLMs for Medical Education",
                    "LLM-based Agents"
                ],
                "reference": [
                    "Large ai models in health informatics: Applications, challenges, and the future",
                    "Agent hospital: A simulacrum of hospital with evolvable medical agents",
                    "Simulating classroom education with llm-empowered agents",
                    "Capabilities of gemini models in medicine",
                    "Segment anything in medical images",
                    "React: Synergizing reasoning and acting in language models",
                    "Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models",
                    "Large language models encode clinical knowledge",
                    "Generative agent for teacher training: Designing educational problem-solving simulations with large language model-based agents for pre-service teacher",
                    "Almanac retrieval-augmented language models for clinical medicine",
                    "Camel: Communicative agents for\" mind\" exploration of large language model society",
                    "Mathvc: An llm-simulated multi-character virtual classroom for mathematics education",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "The rise of chatgpt: Exploring its potential in medical education",
                    "Embracing chatgpt for medical education: exploring its impact on doctors and medical students",
                    "Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis",
                    "Medagents: Large language models as collaborators for zero-shot medical reasoning",
                    "Chatcad+: Towards a universal and reliable interactive cad using llms",
                    "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
                    "Mmedagent: Learning to use medical tools with multi-modal agent",
                    "Large language models in medical education: opportunities, challenges, and future directions"
                ],
                "related_work": "2Related Work2.1LLMs for Medical EducationThe introduction of ChatGPT has accelerated the exploration of LLMs for medical education[10,33]. The early work of Kung et al.[8]revealed that ChatGPT could achieve a passing score of USMLE. Since then, new generations of LLMs[25,23]have been benchmarked on USMLE extensively, with the state-of-the-art performance achieving 91.1% accuracy[23]. While the implication of LLMs achieving such a high USMLE score is far-reaching, little research[8]has been conducted to integrate LLMs with existing medical educational systems and investigate how effective they are in improving students' learning.Furthermore, as pointed out in[20], LLMs in education can be a double-edged sword where learning can be more interactive and responsive with LLMs being assistants, but plagiarism and a decrease of a student's own creativity may also occur and proliferate. We refer readers to[20,1,33]for more comprehensive discussions about LLMs in medical education2.2LLM-based AgentsRecently, there has been a trend of shifting from a single LLM chatbot to an agentic LLM framework for task solving, as agents can access tools, plan, reflect, and form collaborations with other agents. Within an educational setting, this agentic paradigm resembles what a human student would experience during the course of learning, i.e., developing reasoning skills, knowing to leverage tools, reflect, and collaborate with others. Hence, an educational system based on multiple agents can be particularly promising. While there are some agentic frameworks, such as ReAct[34], LangChain, CAMEL[14], and AutoGen[32], they are mainly proposed for automating workflow. For example, the seminal CAMEL work shows that the workflow of a task can be automated using an agentic user and an agentic assistant in which the agentic user is a proxy of human user to instruct the assistant to fulfill tasks. Recently, agentic systems have been proposed for educational purposes such as simulated classrooms[38,36]. Lee et al.[11]have also proposed to use generative agents to train teachers to help them prepare for the actual in-class teaching.There are a few studies exploring LLM agents in medical settings, and most of them are unimodal focusing on medical text data only. Agent Hospital[15]shows that an agentic doctor can continually improve its diagnosis by merely interacting with agentic patients in a simulated hospital, and can transfer its learned knowledge to real-world cases. MEDAGENTS[27]demonstrates that multi-round discussions among agents can lead to better results than zero-/few-shot chain-of-thought[31]on medical question answering. Similarly, AI Hospital[6]shows that multi-agent discussions can enhance diagnostic accuracy. Almanac[37], a retrieval-integrated agentic system, shows consistently better performance than plain LLMs in clinical question answering. Nonetheless, the above explorations of agents within medicine are still restricted by the single text modality, whereas medicine is inherently multi-modal.Recently, MMedAgent[12]shows that the medical task-solving capabilities of LLMs can be widened, and their unimodal nature can be expanded to multi-modal, through tool access, such as invoking MedSAM[17]for medical image segmentation and ChatCAD[39]for medical report generation.",
                "abstract": "Large language models (LLMs) have had a significant impact on diverse research domains, including medicine and healthcare. However, the potential of LLMs as copilots in medical education remains underexplored. Current AI-assisted educational tools are limited by their solitary learning approach and inability to simulate the multi-disciplinary and interactive nature of actual medical training. To address these limitations, we propose MEDCO (Medical EDucation COpilots), a novel multi-agent-based copilot system specially developed to emulate real-world medical training environments. MEDCO incorporates three primary agents: an agentic patient, an expert doctor, and a radiologist, facilitating a multi-modal and interactive learning environment. Our framework emphasizes the learning of proficient question-asking skills, multi-disciplinary collaboration, and peer discussions between students. Our experiments show that simulated virtual students who underwent training with MEDCO not only achieved substantial performance enhancements comparable to those of advanced models, but also demonstrated human-like learning behaviors and improvements, coupled with an increase in the number of learning samples. This work contributes to medical education by introducing a copilot that implements an interactive and collaborative learning approach. It also provides valuable insights into the effectiveness of AI-integrated training paradigms."
            }
        ],
        "survey": {
            "name": "Large Language Models for Medicine: A Survey",
            "arxiv_id": "2405.13055",
            "subtitles": [
                {
                    "name": "Developments of LLMs",
                    "key_history": [
                        {
                            "reference_title": "Class-based N-gram models of natural language",
                            "key_word": "N-gram model"
                        },
                        {
                            "reference_title": "Hidden Markov models",
                            "key_word": "Hidden Markov Models (HMMs) "
                        },
                        {
                            "reference_title": "A review of recurrent neural networks: LSTM cells and network architectures",
                            "key_word": "Long Short-Term Memory (LSTMs) "
                        },
                        {
                            "reference_title": "Word2Vec",
                            "key_word": "Word2Vec"
                        },
                        {
                            "reference_title": "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
                            "key_word": "ELMo"
                        },
                        {
                            "reference_title": "ChatGPT and Open-AI models: A preliminary review",
                            "key_word": "GPT"
                        },
                        {
                            "reference_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                            "key_word": "BERT"
                        },
                        {
                            "reference_title": "Recurrent neural network based language model",
                            "key_word": "Recurrent Neural Network (RNN) "
                        },
                        {
                            "reference_title": "Gate-variants of gated recurrent unit (GRU) neural networks",
                            "key_word": "Gated Recurrent Unit (GRU) "
                        },
                        {
                            "reference_title": "Transformer in Transformer",
                            "key_word": "Transformer"
                        }
                    ],
                    "references_in_this_section": [
                        "Universal language model fine-tuning for text classification",
                        "Pre-trained models: Past, present and future",
                        "Tractable control for autoregressive language generation",
                        "Diversified hidden Markov models for sequential labeling",
                        "Jumping NLP curves: A review of natural language processing research",
                        "Class-based N-gram models of natural language",
                        "Recurrent neural networks",
                        "Markov chains: Structure and applications",
                        "AI-generated content (AIGC): A survey",
                        "Pre-trained multi-view word embedding using two-side neural network",
                        "Two improved continuous bag-of-word models",
                        "Recurrent neural network based language model",
                        "HumanGAN: A generative model of human images",
                        "Natural language statistical features of LSTM-generated texts",
                        "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
                        "BERT: Pre-training of deep bidirectional transformers for language understanding",
                        "ChatGPT and Open-AI models: A preliminary review",
                        "A comprehensive survey and analysis of generative models in machine learning",
                        "Gate-variants of gated recurrent unit (GRU) neural networks",
                        "Toward controlled generation of text",
                        "An HMM posterior decoder for sequence feature prediction that includes homology information",
                        "US-Rule: Discovering utility-driven sequential rules",
                        "Data mining in distributed environment: a survey",
                        "Transformers: State-of-the-art natural language processing",
                        "Transformer in Transformer",
                        "Pre-train, interact, fine-tune: A novel interaction representation for text classification",
                        "C. McCormick, Word2Vec tutorial-the skip-gram model, 2016. Available online at: http://www.mccormickml.com",
                        "Multimodal large language models: A survey",
                        "Hidden Markov models",
                        "Distributed training of large language models",
                        "FloWaveNet: A generative flow for raw audio",
                        "The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions",
                        "Named-entity recognition for a low-resource language using pre-trained language model",
                        "A review of recurrent neural networks: LSTM cells and network architectures",
                        "Word2Vec"
                    ]
                },
                {
                    "name": "Characteristics of medicine and requirements of medical LLMs",
                    "key_history": [
                        {
                            "reference_title": "Compassionate care in healthcare systems: A systematic review",
                            "key_word": "Compassionate Care"
                        },
                        {
                            "reference_title": "The importance of interpretability and visualization in machine learning for applications in medicine and health care",
                            "key_word": "Interpretability"
                        },
                        {
                            "reference_title": "Invasive techniques in emergency medicine: I. practice-oriented training concept to ensure adequately qualified emergency physicians",
                            "key_word": "Practice-Oriented"
                        },
                        {
                            "reference_title": "Emergency medicine and hospital medicine: A call for collaboration",
                            "key_word": "Team Collaboration"
                        },
                        {
                            "reference_title": "The ten commandments of ethical medical AI",
                            "key_word": "Ethical Challenges"
                        },
                        {
                            "reference_title": "Defining and measuring diagnostic uncertainty in medicine: A systematic review",
                            "key_word": "Uncertainty"
                        }
                    ],
                    "references_in_this_section": [
                        "Emergency medicine and hospital medicine: A call for collaboration",
                        "The impact and opportunities of large language models like ChatGPT in oral and maxillofacial surgery: A narrative review",
                        "The exciting potential for ChatGPT in obstetrics and gynecology",
                        "Large language models in medicine: The potentials and pitfalls: A narrative review",
                        "Invasive techniques in emergency medicine: I. practice-oriented training concept to ensure adequately qualified emergency physicians",
                        "The importance of interpretability and visualization in machine learning for applications in medicine and health care",
                        "The ten commandments of ethical medical AI",
                        "Development of a global infectious disease activity database using natural language processing, machine learning, and human expertise",
                        "Compassionate care in healthcare systems: A systematic review",
                        "Role of chat gpt in public health",
                        "Black box warning: Large language models and the future of infectious diseases consultation",
                        "Defining and measuring diagnostic uncertainty in medicine: A systematic review"
                    ]
                },
                {
                    "name": "Products of LLM for Medicine",
                    "key_history": [
                        {
                            "reference_title": "Huatuo: Tuning llama model with chinese medical knowledge",
                            "key_word": "Medical Knowledge Graph"
                        },
                        {
                            "reference_title": "Large language models encode clinical knowledge",
                            "key_word": "Prompt-Based Fine-Tuning"
                        },
                        {
                            "reference_title": "PanGu drug model: Learn a molecule like a human",
                            "key_word": "Asymmetric Conditional Variational Autoencoder"
                        },
                        {
                            "reference_title": "A method for multiple-sequence-alignment-free protein structure prediction using a protein language model",
                            "key_word": "Protein Structure Prediction"
                        },
                        {
                            "reference_title": "DSI-Net: Deep synergistic interaction network for joint classification and segmentation with endoscope images",
                            "key_word": "Deep Synergistic Interaction Network"
                        },
                        {
                            "reference_title": "MedLSAM: Localize and segment anything model for 3d medical images",
                            "key_word": "3D Localization"
                        },
                        {
                            "reference_title": "PubMed GPT: A domain-specific large language model for biomedical text",
                            "key_word": "PubMed GPT"
                        },
                        {
                            "reference_title": "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                            "key_word": "Self-Retrieval Mechanism"
                        },
                        {
                            "reference_title": "A medical multimodal large language model for future pandemics",
                            "key_word": "Multimodal Data"
                        },
                        {
                            "reference_title": "PeFoMed: Parameter efficient fine-tuning on multimodal large language models for medical visual question answering",
                            "key_word": "Medical Visual Question Answering (Med-VQA) "
                        }
                    ],
                    "references_in_this_section": [
                        "A. Koubaa, GPT-4 vs. GPT-3.5: A concise showdown, 2023. Available online at: https://www.techrxiv.org/articles/preprint/GPT-4_vs_GPT-3_5_A_Concise_Showdown",
                        "DISC-MedLLM: Bridging general large language models and real-world medical consultation",
                        "MedGPT: Medical concept prediction from clinical narratives",
                        "PeFoMed: Parameter efficient fine-tuning on multimodal large language models for medical visual question answering",
                        "GLM-130B: An open bilingual pre-trained model",
                        "DSI-Net: Deep synergistic interaction network for joint classification and segmentation with endoscope images",
                        "DoctorGLM: Fine-tuning your chinese doctor is not a herculean task",
                        "A medical multimodal large language model for future pandemics",
                        "Large language models encode clinical knowledge",
                        "PanGu drug model: Learn a molecule like a human",
                        "SMILE: Single-turn to multi-turn inclusive language expansion via ChatGPT for mental health support",
                        "MedCLIP-SAM: Bridging text and image towards universal medical image segmentation",
                        "PMC-LLaMA: Toward building open-source language models for medicine",
                        "ClinicalGPT: Large language models finetuned with diverse medical data and comprehensive evaluation",
                        "Huatuo: Tuning llama model with chinese medical knowledge",
                        "LLM-Mini-Cex: Automatic evaluation of large language model for diagnostic conversation",
                        "J. Zhou, X. He, L. Sun, J. Xu, X. Chen, Y. Chu, L. Zhou, X. Liao, B. Zhang, X. Gao, SkinGPT-4: An interactive dermatology diagnostic system with visual large language model, 2023. MedRXiv",
                        "Y. Luo, X. Y. Liu, K. Yang, K. Huang, M. Hong, J. Zhang, Y. Wu, Z. Nie, Towards unified AI drug discovery with multiple knowledge modalities",
                        "A. Venigalla, J. Frankle, M. Carbin, PubMed GPT: A domain-specific large language model for biomedical text, 2022. Available online at: https://www.mosaicml.com/blog/introducing-pubmed-gpt",
                        "MedLSAM: Localize and segment anything model for 3d medical images",
                        "BianQue: Balancing the questioning and suggestion ability of health LLMs with multi-turn health conversations polished by ChatGPT",
                        "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                        "A large language model for electronic health records",
                        "Lvit: Language meets vision transformer in medical image segmentation",
                        "A method for multiple-sequence-alignment-free protein structure prediction using a protein language model",
                        "Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT",
                        "Transformer-based molecular generative model for antiviral drug design"
                    ]
                },
                {
                    "name": "Double-edged sword of LLM for Medcine",
                    "key_history": [
                        {
                            "reference_title": "A large language model for electronic health records",
                            "key_word": "Early detection and prediction"
                        },
                        {
                            "reference_title": "Applications of large language models in cancer care: Current evidence and future perspectives",
                            "key_word": "Prediction of treatment efficacy"
                        },
                        {
                            "reference_title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
                            "key_word": "Latest medical advancements"
                        },
                        {
                            "reference_title": "Leveraging large language models for decision support in personalized oncology",
                            "key_word": "Clinical decision support"
                        },
                        {
                            "reference_title": "Large language models could change the future of behavioral healthcare: A proposal for responsible development and evaluation",
                            "key_word": "Tailored treatment plans"
                        },
                        {
                            "reference_title": "CancerGPT for few shot drug pair synergy prediction using large pretrained language models",
                            "key_word": "Drug development and precision medicine"
                        },
                        {
                            "reference_title": "The utility of language models in cardiology: A narrative review of the benefits and concerns of ChatGPT-4",
                            "key_word": "Personalized patient management"
                        },
                        {
                            "reference_title": "Performance of multimodal gpt-4v on usmle with image: Potential for imaging diagnostic support with explanations",
                            "key_word": "Optimization of healthcare processes"
                        },
                        {
                            "reference_title": "The role of large language models in medical education: Applications and implications",
                            "key_word": "Medical education and training"
                        },
                        {
                            "reference_title": "Meddialog: Large-scale medical dialogue datasets",
                            "key_word": "Dissemination of medical knowledge"
                        },
                        {
                            "reference_title": "A comprehensive capability analysis of GPT-3 and GPT-3.5 series models",
                            "key_word": "Computational resources"
                        },
                        {
                            "reference_title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
                            "key_word": "Model efficiency"
                        },
                        {
                            "reference_title": "Privacy preserving rare itemset mining",
                            "key_word": "Data privacy"
                        }
                    ],
                    "references_in_this_section": [
                        "The utility of language models in cardiology: A narrative review of the benefits and concerns of ChatGPT",
                        "Leveraging large language models for decision support in personalized oncology",
                        "Large language models could change the future of behavioral healthcare: A proposal for responsible development and evaluation",
                        "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
                        "Applications of large language models in cancer care: Current evidence and future perspectives",
                        "Annotating the human genome with disease ontology",
                        "A comprehensive capability analysis of GPT-3 and GPT-3.5 series models",
                        "C. W. Safranek, A. E. Sidamon-Eristoff, A. Gilson, D. Chartash, The role of large language models in medical education: Applications and implications",
                        "Privacy preserving rare itemset mining",
                        "CancerGPT for few shot drug pair synergy prediction using large pretrained language models",
                        "GPT-4 technical report",
                        "A large language model for electronic health records",
                        "GUARD: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
                        "Performance of multimodal gpt-4v on usmle with image: Potential for imaging diagnostic support with explanations",
                        "Exploring the potential of ChatGPT in personalized obesity treatment",
                        "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
                        "Meddialog: Large-scale medical dialogue datasets"
                    ]
                },
                {
                    "name": "Opportunities and Future Directions",
                    "key_history": [
                        {
                            "reference_title": "Integrated wearable smart sensor system for real-time multi-parameter respiration health monitoring",
                            "key_word": "Medical LLMs in smart medical devices"
                        },
                        {
                            "reference_title": "Intelligent physical robots in health care: Systematic literature review",
                            "key_word": "Medical LLMs with intelligent robots/virtual assistants"
                        },
                        {
                            "reference_title": "Metaverse: Survey, applications, security, and opportunities",
                            "key_word": "Medical LLMs in Metaverse"
                        },
                        {
                            "reference_title": "AI chatbots, health privacy, and challenges to HIPAA compliance",
                            "key_word": "Secure medical LLMs"
                        },
                        {
                            "reference_title": "Prespective chapter: Integrating large language models and blockchain in telemedicine",
                            "key_word": "Medical LLMs with blockchain"
                        },
                        {
                            "reference_title": "Recurrent neural network with attention mechanism for language model",
                            "key_word": "Medical LLMs with interpretability"
                        },
                        {
                            "reference_title": "Federated large language model: A position paper",
                            "key_word": "Multi-party collaboration of medical LLMs"
                        }
                    ],
                    "references_in_this_section": [
                        "Metaverse in education: Vision, opportunities, and challenges",
                        "AI chatbots, health privacy, and challenges to HIPAA compliance",
                        "Metaverse security and privacy: An overview",
                        "The integration of ChatGPT with the Metaverse for medical consultations",
                        "Integrated wearable smart sensor system for real-time multi-parameter respiration health monitoring",
                        "A review of cognitive assistants for healthcare: Trends, prospects, and future directions",
                        "Augmenting interpretable models with large language models during training",
                        "Recurrent neural network with attention mechanism for language model",
                        "Open Metaverse: Issues, evolution, and future",
                        "The human-centric Metaverse: A survey",
                        "Generative AI applications in the health and well-being domain: Virtual and robotic assistance and the need for niche language models (NLMs",
                        "An IoT enabled secured clinical health care framework for diagnosis of heart diseases",
                        "LLM-assisted multi-teacher continual learning for visual question answering in robotic surgery",
                        "LLM-based privacy data augmentation guided by knowledge distillation with a distribution tutor for medical text classification",
                        "AI and ethics: A systematic review of the ethical considerations of large language model use in surgery research",
                        "Controlling large language models to generate secure and vulnerable code",
                        "The impact of GDPR on data sharing for European cancer research",
                        "Metaverse for smart cities: A surveys",
                        "Voicepilot: Harnessing LLMs as speech interfaces for physically assistive robots",
                        "Internet of things-enabled real-time health monitoring system using deep learning",
                        "Towards next-generation intelligent assistants leveraging llm techniques",
                        "Use of large language model-based chatbots in managing the rehabilitation concerns and education needs of outpatient stroke survivors and caregivers",
                        "Metaverse: Survey, applications, security, and opportunities",
                        "Federated large language model: A position paper",
                        "Intelligent physical robots in health care: Systematic literature review",
                        "ShennongMGS: An LLM-based chinese medication guidance system",
                        "Bio-multifunctional smart wearable sensors for medical devices",
                        "How blockchain technology can change medicine",
                        "GPTVoiceTasker: LLM-powered virtual assistant for smartphone",
                        "T. F. Heston, Prespective chapter: Integrating large language models and blockchain in telemedicine, IntechOpen"
                    ]
                }
            ],
            "all_references": [
                "Class-based N-gram models of natural language",
                "N. J. Nilsson, Principles of artificial intelligence, Springer Science & Business Media",
                "DoctorGLM: Fine-tuning your chinese doctor is not a herculean task",
                "PMC-LLaMA: Toward building open-source language models for medicine",
                "Defining and measuring diagnostic uncertainty in medicine: A systematic review",
                "LLM-Mini-Cex: Automatic evaluation of large language model for diagnostic conversation",
                "Natural language processing",
                "Performance of multimodal gpt-4v on usmle with image: Potential for imaging diagnostic support with explanations",
                "Metaverse in education: Vision, opportunities, and challenges",
                "Large language models in law: A survey",
                "Controlling large language models to generate secure and vulnerable code",
                "T. F. Heston, Prespective chapter: Integrating large language models and blockchain in telemedicine, IntechOpen",
                "A survey of large language models in medicine: Progress, application, and challenge",
                "Pre-trained multi-view word embedding using two-side neural network",
                "GPTVoiceTasker: LLM-powered virtual assistant for smartphone",
                "The ten commandments of ethical medical AI",
                "DISC-MedLLM: Bridging general large language models and real-world medical consultation",
                "GPT-4 technical report",
                "A. Venigalla, J. Frankle, M. Carbin, PubMed GPT: A domain-specific large language model for biomedical text, 2022. Available online at: https://www.mosaicml.com/blog/introducing-pubmed-gpt",
                "MedGPT: Medical concept prediction from clinical narratives",
                "GLM-130B: An open bilingual pre-trained model",
                "Metaverse for smart cities: A surveys",
                "A large language model for electronic health records",
                "Use of large language model-based chatbots in managing the rehabilitation concerns and education needs of outpatient stroke survivors and caregivers",
                "Large language models for therapy recommendations across 3 clinical specialties: Comparative study",
                "Meddialog: Large-scale medical dialogue datasets",
                "The importance of interpretability and visualization in machine learning for applications in medicine and health care",
                "The challenges for regulating medical use of ChatGPT and other large language models",
                "MedLSAM: Localize and segment anything model for 3d medical images",
                "Model-as-a-service (MaaS) : A survey",
                "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
                "AI chatbots, health privacy, and challenges to HIPAA compliance",
                "Recurrent neural network with attention mechanism for language model",
                "AI and ethics: A systematic review of the ethical considerations of large language model use in surgery research",
                "The rise and potential of large language model based agents: A survey",
                "Gate-variants of gated recurrent unit (GRU) neural networks",
                "Medicine information needs of patients: The relationships between information needs, diagnosis and disease",
                "Distributed training of large language models",
                "Black box warning: Large language models and the future of infectious diseases consultation",
                "J. Zhou, X. He, L. Sun, J. Xu, X. Chen, Y. Chu, L. Zhou, X. Liao, B. Zhang, X. Gao, SkinGPT-4: An interactive dermatology diagnostic system with visual large language model, 2023. MedRXiv",
                "The exciting potential for ChatGPT in obstetrics and gynecology",
                "A review of cognitive assistants for healthcare: Trends, prospects, and future directions",
                "Large language models in radiology: Fundamentals, applications, ethical considerations, risks, and future directions",
                "Hidden Markov models",
                "Exploring the potential of ChatGPT in personalized obesity treatment",
                "Large language models could change the future of behavioral healthcare: A proposal for responsible development and evaluation",
                "icsBERTs: Optimizing pre-trained language models in intelligent customer service",
                "Deep learning",
                "A comprehensive capability analysis of GPT-3 and GPT-3.5 series models",
                "HumanGAN: A generative model of human images",
                "LLM-based privacy data augmentation guided by knowledge distillation with a distribution tutor for medical text classification",
                "Role of chat gpt in public health",
                "Assessing, controlling, and assuring the quality of medical information on the internet: Caveant lector et viewor let the reader and viewer beware",
                "Voicepilot: Harnessing LLMs as speech interfaces for physically assistive robots",
                "Natural language statistical features of LSTM-generated texts",
                "Open Metaverse: Issues, evolution, and future",
                "ChatGPT for good? On opportunities and challenges of large language models for education",
                "SMILE: Single-turn to multi-turn inclusive language expansion via ChatGPT for mental health support",
                "ClinicalGPT: Large language models finetuned with diverse medical data and comprehensive evaluation",
                "Internet of things-enabled real-time health monitoring system using deep learning",
                "AI-generated content (AIGC) : A survey",
                "Intelligent physical robots in health care: Systematic literature review",
                "Metaverse: Survey, applications, security, and opportunities",
                "Web 3.0: The future of Internet",
                "ChatGPT makes medicine easy to swallow: An exploratory case study on simplified radiology reports",
                "Emergency medicine and hospital medicine: A call for collaboration",
                "CancerGPT for few shot drug pair synergy prediction using large pretrained language models",
                "Internet of behaviors: A survey",
                "Y. Luo, X. Y. Liu, K. Yang, K. Huang, M. Hong, J. Zhang, Y. Wu, Z. Nie, Towards unified AI drug discovery with multiple knowledge modalities",
                "Compassionate care in healthcare systems: A systematic review",
                "Discovering high utility episodes in sequences",
                "How blockchain technology can change medicine",
                "ChatGPT for shaping the future of dentistry: The potential of multi-modal large language model",
                "Transformers: State-of-the-art natural language processing",
                "LLMs accelerate annotation for medical information extraction",
                "Integrated wearable smart sensor system for real-time multi-parameter respiration health monitoring",
                "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
                "A. Koubaa, GPT-4 vs. GPT-3.5: A concise showdown, 2023. Available online at: https://www.techrxiv.org/articles/preprint/GPT-4_vs_GPT-3_5_A_Concise_Showdown",
                "ChatGPT and large language model (LLM) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine",
                "Information giving in medical care",
                "Applications of large language models in cancer care: Current evidence and future perspectives",
                "C. W. Safranek, A. E. Sidamon-Eristoff, A. Gilson, D. Chartash, The role of large language models in medical education: Applications and implications",
                "Universal language model fine-tuning for text classification",
                "FloWaveNet: A generative flow for raw audio",
                "Large language models (LLM) and ChatGPT: What will the impact on nuclear medicine be",
                "An IoT enabled secured clinical health care framework for diagnosis of heart diseases",
                "A survey of automatic query expansion in information retrieval",
                "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
                "Toward controlled generation of text",
                "An HMM posterior decoder for sequence feature prediction that includes homology information",
                "Healthcare NER models using language model pretraining",
                "A survey of large language models",
                "Annotating the human genome with disease ontology",
                "Diversified hidden Markov models for sequential labeling",
                "Transformer in Transformer",
                "Embracing large language models for medical applications: Opportunities and challenges",
                "Named-entity recognition for a low-resource language using pre-trained language model",
                "BianQue: Balancing the questioning and suggestion ability of health LLMs with multi-turn health conversations polished by ChatGPT",
                "The future landscape of large language models in medicine",
                "ShennongMGS: An LLM-based chinese medication guidance system",
                "Large language models for robotics: A survey",
                "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "Recurrent neural networks",
                "Artificial intelligence enabled ChatGPT and large language models in drug target discovery, drug discovery, and development",
                "GUARD: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models",
                "Cloud-based intelligent self-diagnosis and department recommendation service using Chinese medical BERT",
                "Large language models encode clinical knowledge",
                "BioBERT: A pre-trained biomedical language representation model for biomedical text mining",
                "Word2Vec",
                "PanGu drug model: Learn a molecule like a human",
                "Markov chains: Structure and applications",
                "Large language models in medicine",
                "Development of a global infectious disease activity database using natural language processing, machine learning, and human expertise",
                "ChatGPT and Open-AI models: A preliminary review",
                "BloombergGPT: A large language model for finance",
                "Artificial intelligence-based clinical decision support systems using advanced medical imaging and radiomics",
                "A medical multimodal large language model for future pandemics",
                "The integration of ChatGPT with the Metaverse for medical consultations",
                "Two improved continuous bag-of-word models",
                "A review of recurrent neural networks: LSTM cells and network architectures",
                "A survey of Transformers",
                "DSI-Net: Deep synergistic interaction network for joint classification and segmentation with endoscope images",
                "Recurrent neural network based language model",
                "MedCLIP-SAM: Bridging text and image towards universal medical image segmentation",
                "Augmenting interpretable models with large language models during training",
                "Automated assistance for creative writing with an RNN language model",
                "Invasive techniques in emergency medicine: I. practice-oriented training concept to ensure adequately qualified emergency physicians",
                "A comprehensive survey and analysis of generative models in machine learning",
                "Bio-multifunctional smart wearable sensors for medical devices",
                "The scope of big data in one medicine: Unprecedented opportunities and challenges",
                "BioGPT: Generative pre-trained transformer for biomedical text generation and mining",
                "Huatuo: Tuning llama model with chinese medical knowledge",
                "The impact and opportunities of large language models like ChatGPT in oral and maxillofacial surgery: A narrative review",
                "A method for multiple-sequence-alignment-free protein structure prediction using a protein language model",
                "Revolutionizing radiology with GPT-based models: Current applications, future possibilities and limitations of ChatGPT",
                "Tractable control for autoregressive language generation",
                "LLM-assisted multi-teacher continual learning for visual question answering in robotic surgery",
                "Jumping NLP curves: A review of natural language processing research",
                "Large language models in education: Vision and opportunities",
                "Federated large language model: A position paper",
                "The human-centric Metaverse: A survey",
                "The impact of GDPR on data sharing for European cancer research",
                "TUSQ: Targeted high-utility sequence querying",
                "The utility of language models in cardiology: A narrative review of the benefits and concerns of ChatGPT",
                "Data mining in distributed environment: a survey",
                "The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions",
                "Web3: The next Internet revolution",
                "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                "PeFoMed: Parameter efficient fine-tuning on multimodal large language models for medical visual question answering",
                "Multimodal large language models: A survey",
                "US-Rule: Discovering utility-driven sequential rules",
                "Lvit: Language meets vision transformer in medical image segmentation",
                "Transformer-based molecular generative model for antiviral drug design",
                "Generative AI applications in the health and well-being domain: Virtual and robotic assistance and the need for niche language models (NLMs",
                "Leveraging large language models for decision support in personalized oncology",
                "Pre-train, interact, fine-tune: A novel interaction representation for text classification",
                "C. McCormick, Word2Vec tutorial-the skip-gram model, 2016. Available online at: http://www.mccormickml.com",
                "Metaverse security and privacy: An overview",
                "Large language models in medicine: The potentials and pitfalls: A narrative review",
                "Privacy preserving rare itemset mining",
                "Towards next-generation intelligent assistants leveraging llm techniques",
                "Pre-trained models: Past, present and future",
                "Talking about large language models",
                "GPT-InvestAR: Enhancing stock investment strategies through annual report analysis with large language models"
            ]
        },
        "topic_history": [
            {
                "name": "Aloe: A Family of Fine-tuned Open Healthcare LLMs",
                "arxiv_id": "2405.01886",
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Best practices and lessons learned on synthetic data for language models",
                    "A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics",
                    "Biomistral: A collection of open-source pretrained large language models for medical domains",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Magicoder: Source code is all you need",
                    "Two directions for clinical data generation with large language models: Data-to-label and label-to-data",
                    "Does synthetic data generation of llms help clinical text mining",
                    "Towards building multilingual language model for medicine",
                    "A toolbox for surfacing health equity harms and biases in large language models",
                    "Benchmarking open healthcare llms at scale",
                    "Med-halt: Medical domain hallucination test for large language models",
                    "Meditron-70b: Scaling medical pretraining for large language models",
                    "Pmc-llama: Further finetuning llama on medical papers",
                    "Chatgpt outperforms crowd workers for text-annotation tasks",
                    "Internlm2 technical report",
                    "A study of generative large language model for medical research and healthcare",
                    "Risks from language models for automated mental healthcare: Ethics and structure for implementation",
                    "Openmathinstruct-1: A 1.8 million math instruction tuning dataset",
                    "Openmedlm: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models"
                ]
            },
            {
                "name": "Qibo: A Large Language Model for Traditional Chinese Medicine",
                "arxiv_id": "2403.16056",
                "reference": [
                    "Efficient and effective text encoding for chinese llama and alpaca",
                    "Huatuogpt, towards taming language model to be a doctor",
                    "Qilin-med: Multi-stage knowledge injection advanced medical large language model",
                    "Large language models encode clinical knowledge",
                    "Bloom: A 176b-parameter open-access multilingual language model",
                    "Chatgpt: Optimizing language models for dialogue. openai",
                    "Stanford alpaca: An instruction-following llama model",
                    "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
                    "Gpt-4 technical report",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Huatuo: Tuning llama model with chinese medical knowledge",
                    "Baichuan 2: Open large-scale language models",
                    "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue",
                    "Falcon-40b: an open large language model with state-of-the-art performance",
                    "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                    "Llama: Open and efficient foundation language models",
                    "Glm: General language model pretraining with autoregressive blank infilling",
                    "Medalpaca-an open-source collection of medical conversational ai models and training data",
                    "Towards expert-level medical question answering with large language models",
                    "Hello GPT-4o",
                    "Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence",
                    "Qwen technical report"
                ]
            },
            {
                "name": "TCMD: A Traditional Chinese Medicine QA Dataset for Evaluating Large Language Models",
                "arxiv_id": "2406.04941",
                "reference": [
                    "Large language models encode clinical knowledge",
                    "Zero-shot information extraction for clinical meta-analysis using large language models",
                    "Dense retrieval adaptation using target domain description",
                    "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
                    "Applying deep matching networks to chinese medical question answering: a study and a dataset",
                    "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
                    "Benchmarking large language models on cmexam-a comprehensive chinese medical exam dataset",
                    "Towards expert-level medical question answering with large language models",
                    "A large language model for electronic health records",
                    "Almanac: Knowledge-grounded language models for clinical medicine",
                    "Exploring the in-context learning ability of large language model for biomedical concept linking",
                    "The promise and peril of using a large language model to obtain clinical information: Chatgpt performs strongly as a fertility counseling tool with limitations",
                    "Mlec-qa: A chinese multi-choice biomedical question answering dataset"
                ]
            },
            {
                "name": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
                "arxiv_id": "2402.02392",
                "reference": [
                    "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
                    "Re-examining calibration: The case of question answering",
                    "Language models (mostly) know what they know",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "Teaching models to express their uncertainty in words",
                    "Approaching human-level forecasting with language models",
                    "Large language model as autonomous decision maker",
                    "Bird: A trustworthy bayesian inference framework for large language models",
                    "Leveraging large language models for decision support in personalized oncology",
                    "A language agent for autonomous driving",
                    "Large language models for supply chain optimization",
                    "Reflexion: Language agents with verbal reinforcement learning",
                    "Are large language models bayesian? a martingale perspective on in-context learning",
                    "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
                    "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Importance of directional feedback for llm-based optimizers",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Generating with confidence: Uncertainty quantification for black-box large language models",
                    "Reducing conversational agents' overconfidence through linguistic calibration",
                    "From word models to world models: Translating from natural language to the probabilistic language of thought",
                    "Reasoning with language model is planning with world model",
                    "Toolchain*: Efficient action space navigation in large language models with a* search",
                    "Robots that ask for help: Uncertainty alignment for large language model planners",
                    "Large language models as optimizers",
                    "Uncertainty in natural language generation: From theory to applications",
                    "Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment"
                ]
            },
            {
                "name": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications",
                "arxiv_id": "2405.19266",
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Efficient and effective text encoding for chinese llama and alpaca",
                    "Huatuogpt, towards taming language model to be a doctor",
                    "Glm-130b: An open bilingual pre-trained model",
                    "Mistral 7b",
                    "Robust emotion recognition in context debiasing",
                    "Introducing chatgpt",
                    "Can llms' tuning methods work in medical multimodal domain",
                    "Natural instructions: Benchmarking generalization to new tasks from natural language instructions",
                    "Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt",
                    "Disentangled representation learning for multimodal emotion recognition",
                    "Doctorglm: Fine-tuning your chinese doctor is not a herculean task",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Gpt-4 technical report",
                    "https://github.com/baichuan-inc/Baichuan-13B",
                    "Huatuogpt-ii, one-stage training for medical adaption of llms",
                    "Huatuo: Tuning llama model with chinese medical knowledge",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Baichuan 2: Open large-scale language models",
                    "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue",
                    "Miss: A generative pretraining and finetuning approach for med-vqa",
                    "Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences",
                    "Glm: General language model pretraining with autoregressive blank infilling",
                    "Llama: Open and efficient foundation language models",
                    "Efficiency in focus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained models",
                    "Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence",
                    "Qwen technical report",
                    "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
                    "Disc-medllm: Bridging general large language models and real-world medical consultation",
                    "Medicalgpt: Training medical gpt model",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
                "arxiv_id": "2406.13890",
                "reference": [
                    "Large language models encode clinical knowledge",
                    "Agent hospital: A simulacrum of hospital with evolvable medical agents",
                    "Measuring massive multitask language understanding",
                    "Epidemic modeling with generative agents",
                    "What disease does this patient have? A large-scale open domain question answering dataset from medical exams",
                    "CMB: A comprehensive medical benchmark in chinese",
                    "Medagents: Large language models as collaborators for zero-shot medical reasoning",
                    "Adaptive collaboration strategy for llms in medical decision making",
                    "Towards building multilingual language model for medicine",
                    "Medbench: A large-scale chinese benchmark for evaluating medical large language models",
                    "Pubmedqa: A dataset for biomedical research question answering",
                    "Benchmarking large language models on cmexam - A comprehensive chinese medical exam dataset"
                ]
            },
            {
                "name": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
                "arxiv_id": "2406.12036",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Pal: Program-aided language models",
                    "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                    "Pubmedqa: A dataset for biomedical research question answering",
                    "Toolformer: Language models can teach themselves to use tools",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Structured chemistry reasoning with large language models",
                    "A survey on large language model based autonomous agents",
                    "Augmenting large language models with chemistry tools",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Toolqa: A dataset for llm question answering with external tools",
                    "Tool learning with foundation models",
                    "Cognitive architectures for language agents",
                    "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
                    "Measuring massive multitask language understanding",
                    "Genegpt: Augmenting large language models with domain tools for improved access to biomedical information",
                    "Augmentation of chatgpt with clinician-informed tools improves performance on medical calculation tasks",
                    "Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning",
                    "Measuring mathematical problem solving with the MATH dataset",
                    "Bigbio: A framework for data-centric biomedical natural language processing"
                ]
            },
            {
                "name": "Benchmarking Retrieval-Augmented Generation for Medicine",
                "arxiv_id": "2402.13178",
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Retrieval-augmented generation for large language models: A survey",
                    "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                    "Literature-augmented clinical outcome prediction",
                    "Biomedical question answering: a survey of approaches and challenges",
                    "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                    "Capabilities of gpt-4 on medical challenge problems",
                    "Bioreader: a retrieval-enhanced text-to-text transformer for biomedical literature",
                    "Retrieve, summarize, and verify: How will chatgpt impact information seeking from the medical literature",
                    "In-context retrieval-augmented language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Large language models encode clinical knowledge",
                    "Active retrieval augmented generation",
                    "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
                    "Question answering in biomedicine",
                    "Almanac retrieval-augmented language models for clinical medicine",
                    "Augmenting black-box llms with medical textbooks for clinical question answering",
                    "Domain-specific language model pretraining for biomedical natural language processing",
                    "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
                    "Deep bidirectional language-knowledge graph pretraining",
                    "Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering",
                    "BERT: Pre-training of deep bidirectional transformers for language understanding",
                    "Improving language models by retrieving from trillions of tokens",
                    "Meditron-70b: Scaling medical pretraining for large language models",
                    "Pmc-llama: Further finetuning llama on medical papers",
                    "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
                    "Augmented language models: a survey",
                    "Towards expert-level medical question answering with large language models",
                    "Biomedical question answering: A survey",
                    "Paperqa: Retrieval-augmented generative agent for scientific research"
                ]
            },
            {
                "name": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models",
                "arxiv_id": "2404.17897",
                "reference": [
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Large language models in medicine",
                    "Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt",
                    "Dense passage retrieval for open-domain question answering",
                    "Retrieval-augmented generation for large language models: A survey",
                    "Recomp: Improving retrieval-augmented lms with compression and selective augmentation",
                    "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
                    "Huatuogpt, towards taming language model to be a doctor",
                    "Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases",
                    "The future landscape of large language models in medicine",
                    "Structure-aware language model pretraining improves dense retrieval on structured data",
                    "Llmlingua: Compressing prompts for accelerated inference of large language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Lift yourself up: Retrieval-augmented text generation with self-memory",
                    "Few-shot learning with retrieval augmented language models"
                ]
            },
            {
                "name": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework",
                "arxiv_id": "2408.12496",
                "reference": [
                    "Large ai models in health informatics: Applications, challenges, and the future",
                    "Agent hospital: A simulacrum of hospital with evolvable medical agents",
                    "Simulating classroom education with llm-empowered agents",
                    "Capabilities of gemini models in medicine",
                    "Segment anything in medical images",
                    "React: Synergizing reasoning and acting in language models",
                    "Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models",
                    "Large language models encode clinical knowledge",
                    "Generative agent for teacher training: Designing educational problem-solving simulations with large language model-based agents for pre-service teacher",
                    "Almanac retrieval-augmented language models for clinical medicine",
                    "Camel: Communicative agents for\" mind\" exploration of large language model society",
                    "Mathvc: An llm-simulated multi-character virtual classroom for mathematics education",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "The rise of chatgpt: Exploring its potential in medical education",
                    "Embracing chatgpt for medical education: exploring its impact on doctors and medical students",
                    "Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis",
                    "Medagents: Large language models as collaborators for zero-shot medical reasoning",
                    "Chatcad+: Towards a universal and reliable interactive cad using llms",
                    "Autogen: Enabling next-gen llm applications via multi-agent conversation framework",
                    "Mmedagent: Learning to use medical tools with multi-modal agent",
                    "Large language models in medical education: opportunities, challenges, and future directions"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models",
            "arxiv_id": "2208.03306",
            "isAPA": true,
            "abstract": "We present Branch-Train-Merge (BTM) , a communication-efficient algorithm forembarrassingly parallel training of large language models (LLMs) . We show itis possible to independently train subparts of a new class of LLMs on differentsubsets of the data, eliminating the massive multi-node synchronization currentlyrequired to train LLMs. BTM learns a set of independent EXPERT LMs (ELMs) ,each specialized to a different textual domain, such as scientific or legal text. TheseELMs can be added and removed to update data coverage, ensembled to generalizeto new domains, or averaged to collapse back to a single LM for efficient inference.New ELMs are learned by branching from (mixtures of) ELMs in the currentset, further training the parameters on data for the new domain, and then mergingthe resulting model back into the set for future use. Experiments show that BTMimproves in- and out-of-domain perplexities as compared to GPT-style TransformerLMs, when controlling for training cost. Through extensive analysis, we show thatthese results are robust to different ELM initialization schemes, but require expertdomain specialization; LM ensembles with random data splits do not perform well.We also present a study of scaling BTM into a new corpus of 64 domains (192Bwhitespace-separated tokens in total) ; the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5\u00d7 more compute. Thesegains grow with the number of domains, suggesting more aggressive parallelismcould be used to efficiently train larger models in future work.",
            "reference": [
                "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling",
                "William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research",
                "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news",
                "Suchin Gururangan, Ana Marasovi\u00e8, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks",
                "Hesham Mostafa and Xin Wang. 2019. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization",
                "Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset",
                "Yoav Freund. 1995. Boosting a weak learning algorithm by majority. Information and computation",
                "Anastassia Kornilova and Vladimir Eidelman. 2019. BillSum: A corpus for automatic summarization of US legislation",
                "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP",
                "Xiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang, Zhi Yang, and Bin Cui. 2021. Dense-to-sparse gate for mixture-of-experts",
                "Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation",
                "Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2022. DEMix layers: Disentangling domains for modular language modeling",
                "Michael Matena and Colin Raffel. 2021. Merging models with fisher-weighted averaging. arXiv preprint arXiv",
                "Wikimedia Foundation. Wikimedia downloads",
                "Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation",
                "Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. 2020. Rigging the lottery: Making all tickets winners",
                "Shen Li. 2021. Getting started with distributed data parallel",
                "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis",
                "Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
                "Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tom\u00e1\u0161 Ko\u010disk\u00fd, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal generalization in neural language models",
                "Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A. Smith. 2021. Time waits for no one! analysis and challenges of temporal misalignment",
                "John R. Rickford. 1985. Ethnicity as a sociolinguistic boundary. American Speech",
                "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020. Linear mode connectivity and the lottery ticket hypothesis",
                "Jonas Pfeiffer, Ivan Vuli\u00e8, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
                "Caselaw Access Project. Caselaw access project",
                "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The pile: An 800gb dataset of diverse text for language modeling",
                "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models",
                "Jonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers",
                "Tim Dettmers and Luke Zettlemoyer. 2019. Sparse networks from scratch: Faster training without losing performance. CoRR, abs",
                "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation",
                "Lucas Caccia, Jing Xu, Myle Ott, Marc'Aurelio Ranzato, and Ludovic Denoyer. 2021. On anytime learning at macroscale. CoRR, abs",
                "Pierre Lison and J\u00f6rg Tiedemann. 2016. OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles",
                "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. {GS}hard: Scaling giant models with conditional computation and automatic sharding",
                "Tiago Fragoso, Wesley Bertoli, and Francisco Louzada. 2018. Bayesian model averaging: A systematic review and conceptual classification. International Statistical Review",
                "Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review",
                "Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021. Base layers: Simplifying training of large, sparse models",
                "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
                "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers",
                "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural text generation with unlikelihood training",
                "Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. 2022. No one representation to rule them all: Overlapping features of training methods",
                "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. arXiv preprint arXiv",
                "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One billion word benchmark for measuring progress in statistical language modeling",
                "Alexander Herzog and Slava Mikhaylov. 2017. Database of Parliamentary Speeches in Ireland",
                "Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning",
                "Li Lucy and David Bamman. 2021. Characterizing English variation across social media communities with BERT. Transactions of the Association for Computational Linguistics",
                "Huggingface. Datasets",
                "Yelp Reviews. Yelp reviews",
                "Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus",
                "Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus",
                "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex Wade, Kuansan Wang, Nancy Xin Ru Wang, Chris Wilhelm, Boya Xie, Douglas Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. Cord-19: The covid-19 open research dataset",
                "Roee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models",
                "Project Gutenberg. Project gutenberg",
                "Ou-Yang, Lucas. Newspaper3k",
                "Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019a. Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
                "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners",
                "Su Lin Blodgett, Lisa Green, and Brendan O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English",
                "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT",
                "Twitter Academic API. Twitter academic api",
                "Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. 2022. The role of permutation invariance in linear mode connectivity of neural networks",
                "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners",
                "Github Archive Project. Github archive project",
                "Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. 2022b. Robust fine-tuning of zero-shot models",
                "Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. 2021. Beyond distillation: Task-level mixture-of-experts for efficient inference",
                "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research",
                "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data",
                "Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason E Weston. 2021. Hash layers for large sparse models",
                "Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2021. Bot-adversarial dialogue for safe conversational agents",
                "Jianmo Ni, Chenguang Zhu, Weizhu Chen, and Julian McAuley. 2019b. Learning to attend on essential terms: An enhanced retriever-reader model for open-domain question answering",
                "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
                "Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. Averaging weights leads to wider optima and better generalization",
                "David H Wolpert. 1992. Stacked generalization. Neural networks",
                "David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models",
                "Daniel Blanchard, Joel R. Tetreault, Derrick Higgins, A. Cahill, and Martin Chodorow. 2013. TOEFL11: A corpus of non-native English. ETS Research Report Series",
                "Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge. 2022. Efficient hierarchical domain adaptation for pretrained language models",
                "John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning",
                "Leo Breiman. 1996. Bagging predictors. Machine learning",
                "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022a. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. 2021. Efficient large scale language modeling with mixtures of experts",
                "Douglas Biber. 1988. Variation across Speech and Writing. Cambridge University Press"
            ],
            "related work": "7Related WorkSparse Language ModelsSparsely activated language models have been considered in a few forms(Evci et al.,2020; Mostafa and Wang,2019; Dettmers and Zettlemoyer,2019) , but the Mixture-of-Experts (MoE) model is of particular note. Early versions(Jacobs et al.,1991) had independent feed-forward networks serving as experts. Recent MoE models(Shazeer et al.,2017) have been studied with token-based routing through backpropagation - notably, by Lepikhin et al. (2021) , which appplies this concept to machine translation, andFedus et al. (2022) , which simplifies the architecture to activation of only one expert per layer. Lewis et al. (2021) , find an alternative approach to routing by formulating it as a linear assignment problem, and Roller et al. (2021) use a fixed hash as the gating function.Of this line of work, ours is most closely related toGururangan et al. (2022) . In that work, DEMix layers - placed in the feedforward layers of the Transformer - contain experts which specialize on specific domains. Routing at train time is determined only by the domain label, but all experts are activated at inference time and mixed according to weights estimated from a validation set. Similarly, Pfeiffer et al. (2022) develop a multilingual expert model with language-specific routing, and Kudugunta et al. (2021) develop a multi-task expert model with task-specific routing.AdaptersPrevious work has also explored extending the capacity of a model with additional specialized parameters(e.g., adapters; Houlsby et al.,2019; Pfeiffer et al.,2020; Ben Zaken et al.,2022) . However, unlike these existing approaches, our approach is significantly simplified, as ourELMs each consist of an entire model which requires no additional parameters and no shared parameters. Future work may explore combiningELMs with adapters to scale into smaller domains.EnsemblesEnsemble methods are widely used in machine learning, for example in bagging, boosting, and stacking(Breiman,1996; Freund,1995; Wolpert,1992) . In a setting where training data is streamed, Caccia et al. (2021) define agrowing ensemble, in which new base models are trained sequentially on incoming batches. However, their growing ensemble, incrementally trained on the randomly created batches of their setting, underperforms non-incremental methods.Parameter AveragingOur averaging mechanism is inspired by the discovery that averaging many fine-tuned vision models improves out-of-domain generalization(Wortsman et al.,2022a; Izmailov et al.,2018) . InWortsman et al.2022a, the authors propose a greedy mechanism for averaging experts with uniform weights. Here, we find that uniform weighted averaging does not work for combining domain-specific models; instead we use a posterior weighted average, where the averaging weights are estimated based on the relevance of the model to the target domain. Our posterior weighted average is highly related to Bayesian model averaging techniques used in classic ensembling methods(Fragoso et al.,2018) . Model averaging has also been explored for federated learning(McMahan et al.,2017) , where different models are trained locally to fit privacy-sensitive data on different devices and merged. However, these works have found success averaging models trained from the same random initialization, which we do not find to hold in our setting. Matena and Raffel (2021) compute a parameter average of models, estimating the optimal weights via an approximation of the Fisher information. Future work may explore these (and other) variations of weighted averages withELMs.Seed trainingOur discovery of the importance of the seed training as a critical warm-up phase for BTM is in line with findings that parameter averaging only works when models share part of their optimization trajectory(Frankle et al.,2020; Entezari et al.,2022) . Future work may investigate what is learned in the seed phase that makes it so useful forELMspecialization, regardless of the corpus used for seeding. Similar to seed training,Nie et al. (2021) proposedense-to-sparsegating, where mixture-of-experts routing mechanisms are gradually sparsified during the course of training.",
            "date": "2022"
        },
        "topic": "Domain Specialization of LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "subtitles": [
                    "LLM Benchmarks",
                    "Risks of Static Benchmarks",
                    "Ranking System",
                    "Human Preference Dataset"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work LLM Benchmarks. We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU(Hendrycks et al.,2020) , HellaSwag(Zellers et al.,2019) , GSM-8K(Cobbe et al.,2021) , BigBench(Srivastava et al.,2023) , AGIEval(Zhong et al.,2023) , and HumanEval(Chen et al.,2021) . Benchmarks focusing on safety, such as ToxicChat(Lin et al.,2023) , and comprehensive suites like HELM(Liang et al.,2022) , also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk(Karpinska et al.,2021; Geng et al.,2023; Wang et al.,2023) . The recent trend includes utilizing GPT-4 for approximating human judgment(Chiang & Lee,2023) , with notable instances being MT-Bench(Zheng et al.,2023b) and AlpacaEval(Li et al.,2023) . In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces(Li et al.,2022; Huang et al.,2023) . They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference(Bai et al.,2022; Ouyang et al.,2022; Touvron et al.,2023) . However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.Risks of Static Benchmarks.Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment(Yang et al.,2023; Oren et al.,2023) . DynaBench(Kiela et al.,2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.Ranking System.Ranking systems have been a well-studied topic in statistics. Related topics include probability models(Hunter,2004; Rao & Kupper,1967) , rank elicitation(Sz\u00f6r\u00e9nyi et al.,2015; Busa-Fekete et al.,2014a,b) , and online experiment design(Chernoff,1992; Karimi et al.,2021) . The Elo rating system has also been used for LLMs(Bai et al.,2022; Boubdir et al.,2023) . Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.Human Preference Dataset.Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant(K\u00f6pf et al.,2023) , HH-RLHF(Bai et al.,2022) , LMSYS-Chat-1M(Zheng et al.,2023a) , and synthetic approximations of human preferences like UltraFeedback(Cui et al.,2023) and Nectar(Zhu et al.,2023) . Our prior data release, LMSYS-Chat-1M(Zheng et al.,2023a) , is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.",
                "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{this https URL}."
            },
            {
                "name": "GeoGalactica: A Scientific Large Language Model in Geoscience",
                "arxiv_id": "2401.00434",
                "subtitles": [
                    "Machine Learning in Geoscience",
                    "Natural Language Processing in Geoscience",
                    "Domain-specific Large Language Model"
                ],
                "reference": [
                    "Application of machine learning in carbon capture and storage: An in-depth insight from the perspective of geoscience",
                    "Machine learning for data-driven discovery in solid earth geoscience",
                    "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                    "Unsupervised pre-stack seismic facies analysis constrained by spatial continuity",
                    "Impressiongpt: an iterative optimizing framework for radiology report summarization with chatgpt",
                    "Prediction of geology condition for slurry pressure balanced shield tunnel with super-large diameter by machine learning algorithms",
                    "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
                    "Application of machine learning for lithofacies prediction and cluster analysis approach to identify rock type",
                    "A new correlation for calculating wellhead oil flow rate using artificial neural network",
                    "Domain-specific language model pretraining for biomedical natural language processing",
                    "Geo-bert pre-training model for query rewriting in poi search",
                    "Understanding geological reports based on knowledge graphs using a deep learning approach",
                    "Toward earthquake early warning: A convolutional neural network for repaid earthquake magnitude estimation",
                    "Machine learning elucidates the anatomy of buried carbonate reef from seismic reflection data",
                    "Bloomberggpt: A large language model for finance",
                    "History of artificial neural networks",
                    "Integrating the artificial intelligence and hybrid machine learning algorithms for improving the accuracy of spatial prediction of landslide hazards in kurseong himalayan region",
                    "Darwin series: Domain specific large language models for natural science",
                    "Geogpt: Understanding and processing geospatial tasks through an autonomous gpt",
                    "Machine learning reveals climate forcing from aerosols is dominated by increased cloud cover",
                    "Construction and application of a knowledge graph for iron deposits using text mining analytics and a deep learning algorithm",
                    "Soft prompt tuning for augmenting dense retrieval with large language models",
                    "Legal-bert: The muppets straight out of law school",
                    "Neurospe: A neuro-net spatial relation extractor for natural language text fusing gazetteers and pretrained models",
                    "Similarity of fast and slow earthquakes illuminated by machine learning",
                    "Biogpt: Generative pre-trained transformer for biomedical text generation and mining",
                    "Galactica: A large language model for science",
                    "High resolution pre-stack seismic inversion using few-shot learning",
                    "Geoscience language processing for exploration",
                    "Deep convolutional autoencoders as generic feature extractors in seismological applications",
                    "Scibert: A pretrained language model for scientific text",
                    "Learning a foundation language model for geoscience knowledge understanding and utilization",
                    "Applications of natural language processing to geoscience text data and prospectivity modeling"
                ],
                "related_work": "2Related Work 2.1Machine Learning in Geoscience With the advancement of artificial intelligence, utilizing machine learning, natural language processing, and recent large-scale model techniques to tackle critical problems in geoscience has become a crucial direction. Various subtasks in geoscience involve significant data collected from sensors, making them suitable for end-to-end learning using machine learning approaches. Some studies model multiple aspects of seismic signals using deep learning models to extract information relevant to earthquake prediction. Among them,[16]uses supervised learning with end-to-end training, while[17,18]employs self-supervised learning to obtain models applied to downstream tasks.[19,20]utilize machine learning to explore the latent correlations among different rock properties for rock type prediction. Beyond relatively straightforward classification tasks, there are numerous works applying machine learning to address more complex scenarios in geoscience, such as calculating wellhead flow rate[21], capturing and storing carbon[22], and predicting the condition of SPBM tunnels[23]. Additionally, machine learning is introduced to evaluate the real-world environment:[24]explores the use of Few-Shot Learning (FSL) methods to enhance the accuracy of high-resolution pre-stack seismic inversion, and[25]employs various machine learning techniques and ensemble methods to improve landslide hazard prediction, demonstrating their high practical value. Machine learning is also being used to aid geoscience exploration,[26]attempts to use machine learning to do data-driven modeling of solid earth science,[27]attempts to use machine learning to reveal the link between fast and slow earthquakes,[28]uses machine learning to reveal the impact of aerosols on climate impact.2.2Natural Language Processing in GeoscienceIn addition to the diverse and heterogeneous data collected from various sensors, the field of geoscience also encompasses a significant amount of text data with standardized formats. The application of natural language processing (NLP) in earth science has witnessed remarkable progress.[29,6]embed different sources of textual information into a unified space,[29]employs joint training of language models with text and points of interest (POI) for POI retrieval, while[6]integrates geological attribute information into the textual representation space to enable better knowledge extraction.[30,31]enhance language models with knowledge graph techniques, where[30]constructs a knowledge graph on geological text to discover ore-forming environments, and[31]proposes an automatic entity and relation extraction approach via three-level extraction to build a geological knowledge graph from extracted information in geological reports.[32]combines retrieval techniques with language models creates an integrated solution incorporating contextual retrieval and the GeoBERT model.[33]focuses on various language irregularities encountered in natural language texts, introducing the NeuroSPE model for spatial extraction using neural networks. NLP techniques provide a unified representation space and computational approach for diverse geological data described in various textual formats, narrowing the technological gap between different earth science tasks.2.3Domain-specific Large Language ModelThe recent emergence of large-scale language models marks a significant step towards unified information processing in geoscience. These models are pre-trained on vast amounts of text data and efficiently compress all input data. Currently, in addition to earth science, various domains have seen the development of domain-specific pre-trained models trained on domain-specific corpora.[34,35,36,12,37,38]performs large-scale pre-training on domain-specific texts and has resulted in foundational models equipped with domain knowledge, while[39,40,41]fine-tuning these base models using domain-specific data, achieving models tailored to specific downstream tasks at a lower cost. These works have made significant strides in developing domain-specific LLMs through dedicated data integration and model training efforts. Recently,[42,43,44]explored the use of prompt engineering to unlock the potential of models without additional training, offering the possibility of unifying various geoscience tasks and further reducing the cost of employing large models in domain applications. In the field of geoscience, the exploration of large models is still in its early stages.[15]collected a substantial amount of high-quality data from earth science Wikipedia and research papers, and further fine-tuned the base model, leading to impressive scientific competence and knowledge in earth science. For the first time, our work utilizes a large corpus of earth science papers and textbooks, which were cleaned using a dedicated toolchain for constructing large-scale earth science models, ensuring data quality. Furthermore, our work completes the entire process of \"further pre-training, supervised fine-tuning, augmented learning \"for large foundation models for geoscience, bringing the largest scale and highest quality proprietary language models to the geoscience field. This will unlock tremendous possibilities for future research conducted by earth science researchers.We have outlined the progression of geoscience research with the use of cutting-edge AI techniques, including neural network (NN) , K-nearest neighbor (KNN) , recurrent neural network (RNN) , convolutional neural network (CNN) , backpropagation (BP) , reinforcement learning (RL) , support vector machine (SVM) , long-short term memory (LSTM) , graph convolutional neural network (GCN) , Transformers, BERT, ChatGPT, and large language model (LLM) .[45]The investigation reveals that the time intervals between AI technology advancements and their application in geoscience have significantly shortened, indicating an increasing reliance on advanced AI technology in the field of geoscience. The illustration is presented in Figure2, and detailed information about the progression is shown in AppendixA.",
                "abstract": "Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities, LLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP techniques in geoscience research and practice is wide and convoluted, contributing from knowledge extraction and document classification to question answering and knowledge discovery. In this work, we take the initial step to leverage LLM for science, through a rather straightforward approach. We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset. These efforts result in a model GeoGalactica consisting of 30 billion parameters. To our best knowledge, it is the largest language model for the geoscience domain. More specifically, GeoGalactica is from further pre-training of Galactica. We train GeoGalactica over a geoscience-related text corpus containing 65 billion tokens, preserving as the largest geoscience-specific text corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of questions that demand professional geoscience knowledge to answer. In this technical report, we will illustrate in detail all aspects of GeoGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We open-source our data curation tools and the checkpoints of GeoGalactica during the first 3/4 of pre-training."
            },
            {
                "name": "Getting the most out of your tokenizer for pre-training and domain adaptation",
                "arxiv_id": "2402.01035",
                "subtitles": [
                    "Tokenizers in Machine Translation",
                    "Tokenizers in Code Generation"
                ],
                "reference": [
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Neural machine translation of rare words with subword units",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Llama: Open and efficient foundation language models",
                    "XLM-V: overcoming the vocabulary bottleneck in multilingual masked language models",
                    "Codebpe: Investigating subtokenization options for large language model pretraining on source code",
                    "Evaluating large language models trained on code",
                    "Unified pre-training for program understanding and generation",
                    "How good is your tokenizer? on the monolingual performance of multilingual language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Code llama: Open foundation models for code",
                    "Santacoder: don't reach for the stars",
                    "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x"
                ],
                "related_work": "4Related WorksTokenizers in Machine TranslationTokenization has been a significant area of interest in multilingual tasks, such as Machine Translation, as different tokenization schemes can markedly influence model performance.Sennrich et al.(2016) introduced BPE for tokenization and were the first to use sub-word tokenization a solution for encoding rare or unseen words at test time.Rust et al.(2021) analyze the implications of multilingual tokenization schemes and find that specialized monolingual tokenizers outperform multilingual versions. More recently,Liang et al.(2023) demonstrate that expanding tokenizer vocabulary size and allocating a specific token quota for each language can substantially enhance performance in large-scale multilingual tasks.Liang et al.(2023) also argue that increasing the tokenizer size can be an efficient way to increase the number of trainable parameters, since only a fraction of the embedding matrix is used for a given input.Tokenizers in Code GenerationIn the field of code generation, most LLMs follow standard tokenizer hyper-parameters. Models like SantaCoder(Allal et al.,2023) and InCoder(Fried et al.,2023) train a 50k vocabulary tokenizer on code data. Fine-tuned code models derived from natural language LLMs, such as Codex(Chen et al.,2021) , CodeGeeX(Zheng et al.,2023) , and CodeGen(Nijkamp et al.,2023) , do not update their tokenizers but extend existing ones like GPT-2's111See AppendixCfor an overview of tokenizer re-use in code generation.. Code Llama(Rozi\u00e8re et al.,2023) fine-tunes Llama 2(Touvron et al.,2023b) keeping the original 32k Llama tokenizer(Touvron et al.,2023a) .Chirkova & Troshin(2023) find that custom code-tokenization can compress sequences by up to17%percent1717\\%17 %without sacrificing performance on a PLBART(Ahmad et al.,2021) model.",
                "abstract": "Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size."
            },
            {
                "name": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
                "arxiv_id": "2402.18099",
                "subtitles": [
                    "Memories or Additional Parameters",
                    "Modifying LLMs' Parameters"
                ],
                "reference": [
                    "Mass-Editing Memory in a Transformer",
                    "Locating and Editing Factual Associations in GPT",
                    "Knowledge Neurons in Pretrained Transformers",
                    "Editing Factual Knowledge in Language Models",
                    "Transformer Feed-Forward Layers Are Key-Value Memories",
                    "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors",
                    "Can We Edit Factual Knowledge by In-Context Learning",
                    "Transformer-Patcher: One Mistake Worth One Neuron",
                    "Memory-based model editing at scale",
                    "Investigating Gender Bias in Language Models Using Causal Mediation Analysis",
                    "MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA",
                    "PMET: Precise Model Editing in a Transformer",
                    "Direct and indirect effects",
                    "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
                    "Calibrating Factual Knowledge in Pretrained Language Models",
                    "Fast Model Editing at Scale",
                    "Editing Large Language Models: Problems, Methods, and Opportunities"
                ],
                "related_work": "2.Related WorkWe present current model editing works in two categories followingYao et al.(2023) .Memories or Additional Parameters.The methods of with memories or additional parameters typically involve creating explicit memories to store the required knowledge for editing, or adding additional trainable parameters to LLMs for learning new knowledge(Yu et al.,2023; Dong et al.,2022; Hartvigsen et al.,2022) . SERAC(Mitchell et al.,2022b) utilized explicit memory for storing edits and incorporated a scope classifier to understand the editing scope. Given a sample within the editing scope, it utilized a separate model to make edits, ensuring that the original model remains unaffected. Inspired by the in-context learning ability of LLMs, IKE(Zheng et al.,2023) designed demonstration formatting and organization strategies, including the copy, update, and retain templates, and retrieved relevant knowledge facts from the editing memories as demonstration inputs to guide the editing process. T-Patcher(Huang et al.,2023) retained all original parameters to preserve overall performance while adding trainable neuron patches to the last Feed-Forward Network (FFN) layer of a Transformer for handling sequential model editing. Despite their success, the above methods lack the exploration of the mechanics of knowledge storage in LLMs, which ultimately leads to poor performance in handling complex medical knowledge.Modifying LLMs' Parameters.The methods of this category aim to comprehend how knowledge is stored in LLMs and how it can be effectively altered by changing the parameters(De Cao et al.,2021; Geva et al.,2021; Wu et al.,2023) . KN(Dai et al.,2022) proposed a knowledge attribution method to identify the neurons associated with specific knowledge without fine-tuning, updating facts, and erasing relations by directly modifying the corresponding parameters in FFN. MEND(Mitchell et al.,2022a) introduced auxiliary hyper-networks to transform the gradient during the fine-tuning process, and trained the hyper-networks to ensure edit success and locality when updating LLMs' parameters. ROME(Meng et al.,2022a) applied Causal mediation analysis(Pearl,2022; Vig et al.,2020) to identify decisive neuron activation and modify FFN weights by solving a least squares problem with a linear equality constraint using the Lagrange multiplier. As an extension of ROME(Meng et al.,2022a) , MEMIT(Meng et al.,2022b) introduced a multi-layer algorithm to update multiple cases simultaneously. PMET(Li et al.,2023b) further improved MEMIT(Meng et al.,2022b) by simultaneously optimizing hidden states of self-attention and FFN. Despite impressive progress made by these methods, they often introduce significant modifications to the original parameters. Consequently, unrelated knowledge is affected, resulting in a noticeable impact onLocalityandFluency, as demonstrated in Section Experiments4.Overall, both of them have their advantages and disadvantages. Neuron localization can achieve interpretability between parameters and knowledge, but sometimes optimizing too much can have negative impacts on unrelated knowledge(Meng et al.,2022a; Geva et al.,2021) . On the other hand, adding extra parameters has a smaller impact on unrelated knowledge but lacks exploration of the mechanics of knowledge storage(Huang et al.,2023) . Our proposed method, MedLaSA, aims to combine the strengths of both approaches. It identify the association of knowledge in neurons of different layers, and introduce scalable adapters into the dense layers, enables precise editing without affecting unrelated knowledge.",
                "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge."
            },
            {
                "name": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models",
                "arxiv_id": "2408.12247",
                "subtitles": [
                    "Instruction Fine-tuning",
                    "Instruction Data Generation",
                    "Instruction Data Selection"
                ],
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Distilling the knowledge in a neural network",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "Deep learning on a data diet: Finding important examples early in training",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Lima: Less is more for alignment",
                    "Stanford alpaca: An instruction-following llama model",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "An empirical study of example forgetting during deep neural network learning",
                    "Instruction tuning with gpt",
                    "Assess and summarize: Improve outage understanding with large language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "OWL: A large language model for IT operations",
                    "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                    "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
                    "Language models are few-shot learners",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "IIRelated WorkII-AInstruction Fine-tuningThe potential of LLMs in the specific domain is vast and promising. For example, Microsoft deployed GPT to summarize anomalous events in its services[12]. However, as task complexity and requirements increase, instruction fine-tuning (IFT) is widely adopted to enhance model performance. FLAN[13]achieved significant improvements in generalization by fine-tuning a high-quality instruction dataset. InstructGPT[7]successfully aligned GPT-3[3]with human intent by fine-tuning a dataset rich in real-world instruction forms and task types. OWL[14]collected numerous operation domain instructions and achieved remarkable results in log parsing and anomaly detection. However, these methods require a large amount of manually annotated data, which becomes a bottleneck for widespread application due to the high cost.II-BInstruction Data GenerationResearchers have extensively explored methods to reduce human involvement in generating instruction data. Some methods[8,15,10,9]use advanced commercial models to create instruction datasets. For instance, Alpaca[8]uses a small amount of manually constructed data to extract knowledge from DaVinci-003[16], creating a 52k instruction dataset. It fine-tunes LLaMA to achieve performance close to GPT-3.5. Peng et al.[9]extract knowledge from GPT-4, resulting in higher quality and more diverse responses.Another class of methods[17,18,11]employs a self-guided approach. These methods extract knowledge from the model and then use this newly constructed data to enhance domain or task capabilities. Self-Instruct[17], for instance, proposes using self-generated samples to enhance the instruction-following ability of pre-trained language models. Self-Align[18]mainly adopts topic-guided red-blue adversarial self-guidance and principle-driven self-calibration to construct data and fine-tune models, requiring less than 300 lines of manually constructed data (including 195 seed prompts, 16 principles, and five examples) to achieve high-quality fine-tuned model. The potential of these self-guided methods is certainly worth exploring further.However, these methods still require manually constructed supervision data and are limited by the model's inherent knowledge constraints, preventing them from generating instruction data beyond the model's capabilities.II-CInstruction Data SelectionIn the early stages of IFT research, many works improved model capabilities by building large instruction datasets. However, LIMA[19]proposed that  \"less alignment is more \" showing that fine-tuning the model with only 1,000 high-quality samples can achieve a performance comparable to GPT-4. Appropriate data filtering strategies can improve learning efficiency and help reduce hallucinations caused by overtraining[2].ALPAGASUS[20]uses ChatGPT for scoring but might miss the target model's strengths and lacks clarity. The forgetting score[21]monitors shifts in sample classification during training. GraNd[22]trims data based on the sample's gradient magnitude. Both forgetting score and GraNd are costly, as they need constant model updates, prolonging training time.Instruction Following Difficulty (IFD) [23]stands out for its efficiency, using the representation features of the target model to identify high-quality instruction data. It provides a simpler, cheaper, and interpretable approach by computing the generation complexity of the answer using a single fixed scoring model.",
                "abstract": "Large language models (LLMs) excel at general question-answering (Q&A) but often fall short in specialized domains due to a lack of domain-specific knowledge. Commercial companies face the dual challenges of privacy protection and resource constraints when involving LLMs for fine-tuning. This paper propose a novel framework, Self-Evolution, designed to address these issues by leveraging lightweight open-source LLMs through multiple iterative fine-tuning rounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution employ a strategy that filters and reinforces the knowledge with higher value during the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat using 4,000 documents containing rich domain knowledge from China Mobile, achieving a performance score 174% higher on domain-specific question-answering evaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat. Self-Evolution has been deployed in China Mobile's daily operation and maintenance for 117 days, and it improves the efficiency of locating alarms, fixing problems, and finding related reports, with an average efficiency improvement of over 18.6%. In addition, we release Self-Evolution framework code inthis https URL."
            },
            {
                "name": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models",
                "arxiv_id": "2408.15915",
                "subtitles": [
                    "Efficient Fine-tuning of Parameters",
                    "Mixture-of-Expert Models",
                    "Data Selection for Efficient Tuning"
                ],
                "reference": [
                    "Instruction mining: When data mining meets large language model finetuning",
                    "Influenci\u00e6: A library for tracing the influence back to the data-points",
                    "Self-moe: Towards compositional large language models with self-specialized experts",
                    "Active learning for convolutional neural networks: A core-set approach",
                    "Gradient-matching coresets for rehearsal-based continual learning",
                    "Conditional prompt learning for vision-language models",
                    "Uni-moe: Scaling unified multimodal llms with mixture of experts",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "Small-gan: Speeding up gan training using core-sets",
                    "Retrieval-augmented mixture of lora experts for uploadable machine learning",
                    "Lorahub: Efficient cross-task generalization via dynamic lora composition",
                    "Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Pissa: Principal singular values and singular vectors adaptation of large language models",
                    "Multi-head adapter routing for cross-task generalization",
                    "Adaptive budget allocation for parameter-efficient fine-tuning",
                    "An experimental design framework for label-efficient supervised finetuning of large language models",
                    "Lmd3: Language model data density dependence",
                    "Adapterhub: A framework for adapting transformers",
                    "Airoboros",
                    "Soft merging of experts with adaptive routing",
                    "Adapting and evaluating influence-estimation methods for gradient-boosted decision trees",
                    "Adaptersoup: Weight averaging to improve generalization of pretrained language models",
                    "Mole: Mixture of lora experts",
                    "Token-level adaptation of lora adapters for downstream task generalization",
                    "When less is more: Investigating data pruning for pretraining llms at scale",
                    "Lora: Low-rank adaptation of large language models",
                    "Dora: Weight-decomposed low-rank adaptation",
                    "Branch-train-mix: Mixing expert llms into a mixture-of-experts llm",
                    "Mixture of experts using tensor products",
                    "Prefix-tuning: Optimizing continuous prompts for generation",
                    "Tagcos: Task-agnostic gradient clustered coreset selection for instruction tuning data",
                    "Gpt-4 technical report",
                    "Dataset condensation with distribution matching",
                    "Data selection for language models via importance resampling",
                    "Autopeft: Automatic configuration search for parameter-efficient fine-tuning",
                    "Routing to the expert: Efficient reward-guided ensemble of large language models",
                    "LlamaIndex",
                    "Mods: Model-oriented data selection for instruction tuning",
                    "Towards modular llms by building and reusing a library of loras",
                    "Learning to route among specialized experts for zero-shot generalization",
                    "Adamole: Fine-tuning large language models with adaptive mixture of low-rank adaptation experts",
                    "Dataset quantization",
                    "Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts",
                    "Self-evolved diverse data sampling for efficient instruction tuning",
                    "Mixture-of-loras: An efficient multitask tuning for large language models",
                    "Qlora: Efficient finetuning of quantized llms",
                    "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
                    "Unleashing the power of data tsunami: A comprehensive survey on data assessment and selection for instruction tuning of language models",
                    "Influence functions in deep learning are fragile",
                    "Qwen technical report",
                    "Exploring the benefits of training expert language models over instruction tuning",
                    "The power of scale for parameter-efficient prompt tuning"
                ],
                "related_work": "2Related Works2.1Efficient Fine-tuning of ParametersParameter-efficient fine-tuning methods (PEFTs) are a set of techniques that diverge from traditional full-parameter fine-tuning. In PEFTs, only a certain subset of a model's parameters are adjusted to better suit specific tasks of interest.2.1.1LoRALow-rank adaptation (LoRA) [32]and its variants[112,25,54,60]use low-rank matrices to approximate additive weights during training. These methods are beneficial as they do not necessitate additional computational resources during inference, where the updated weights can be integrated into the model without any complications.2.1.2Prompt TuningPrompt-based methods incorporate randomly initialized soft tokens to the input, usually as a prefix, and train their embeddings while maintaining the LLM's weights constant, as suggested in[49,120,44]. While these methods perform competitively, they do entail a substantial computational load during inference.2.1.3AdaptersAdapter methods involve the training of extra modules (for instance, fully connected layers) on top of the frozen pre-trained model[72,119]. Unfortunately, these adapters are not effortlessly integrated into the original architecture, thereby reducing inference efficiency.2.2Mixture-of-Expert ModelsThe MoE technique[84,38]combines several specialized expert models to efficiently scale up models for improved generalization. The principle behind MoE is that each expert is adept at handling a specific region of the input distributional space, and their combined decision-making outperforms any individual expert. Recent studies[99,56,45,29,50]focus on utilizing the MoE as a PEFT technique. Other methods[33,64,20,67,116]underscore the use of existing LoRA experts for convenient assembly, where the fine-tuning of their parameters is saved. Specifically, both[35]and[10]route to one expert by comparing the input query with the averaged embeddings of the datasets used to train each individual expert. Such reliance on the fine-tuning datasets restricts the practicability of these methods.[28,52,58]all investigate external models such as GPT4[3]and reward model[7]for estimation of the routing policy, which increases their cost of deployment and causes isolation between the routing model and the candidate experts. In spite of the development of merging and routing techniques, few efforts[85,53,69]have been made to combine cross-domain experts to address multi-task problems.Very recently, PHATGOOSE[63]proposes a post-hoc adaptive token-wise gating mechanism to recycle from a collection of specialized experts. It aims at improving the zero-shot generalization of the pre-trained base model by constructing a routing system and performing dynamic token-by-token assignment. Despite the shared MoE principal, PHATGOOSE differs from the proposed method in three key aspects. First, the problem setting of PHATGOOSE is fundamentally different from ours in that they are not targeted at improving specific tasks of interest. There exists no model and data selection procedures in PHATGOOSE for acquiring task-relevant skills. Second, PHATGOOSE assumes that the contributors of the LoRA models provide additional gate modules that are implemented as sigmoid layers in front of each LoRA module. However, it is almost impossible to enforce the same gate training pipeline on the entire open-source community. In practice, one can only find the released LoRA modules from the repositories of Huggingface and Github without gating vectors. Third, our MoE system adapts to different tasks of interest by polishing the routing weights and the constituting experts simultaneously, while PHATGOOSE keeps the LoRA modules and gating vectors fixed with lower flexibility. Arrow[67]is another contemporary method which maintains the LoRA library and proposes a routing mechanism to select the most input-relevant LoRAs. Although Arrow is featured by its training-free routing, it does not consider the potential conflicts when the inputs are not representative enough of the given task. The statistics of the LoRA parameters (e.g., singular vectors) are directly used as the proxy for the routing weights, which is prone be biased towards the original datasets used to train LoRA modules. Furthermore, they do not introduce the K-shot datapoints from downstream tasks for calibrating the expert behavior, and neglect the exploitation of open-source datasets for optimizing the collaboration between experts.2.3Data Selection for Efficient TuningExisting methods on data selection for pre-training and instruction tuning of LLMs can be categorized into: 1) quality-based, 2) diversity-based, and 3) importance-based[75].2.3.1Quality[59]explores perplexity, L2-Norm error, and memorization as quality scores to rank and prune pre-training corpora.[48]presents a novel self-guided approach to autonomously select high-quality samples from open-source datasets using the instruction-following difficulty (IFD) metric.[18]employs the GPT3.5 to score instruction-response pairs and filters out samples with scores below a threshold.[15]introduces instruct mining to automatically select high-quality instruction-following data for LLM fine-tuning.[27]proposes a quality evaluation model to extract high-quality subsets from the original dataset and designs an algorithm to further select a seed instruction dataset with extensive coverage.2.3.2DiversityGeometry-based sampling methods are the most intuitive and widely-used ones for maximizing diversity of the selected samples[83,40,118]. In particular, k-center greedy[81]is favored in diversity sampling on massive pre-training and instruction-tuning corpus[16,11,98,27]. It performs iterative and greedy selection on data that exhibit the most dissimilarity with the already selected set in the embedding space.2.3.3ImportanceTwo kinds of gradient-based methods on importance estimation have been developed: 1) gradient matching[115,8,111], i.e., the gradients of the entire set being approximated by the weighted gradients of the selected set, and 2) gradient influence[13,9,73], i.e., the influence of each training datapoint on the testing set being measured by upweighted gradient multiplication. For importance-oriented sampling,[101]adopts importance resampling to select a subset from a large-scale unlabeled dataset that shares similar distributions with the target set.Although these methods strike a balance between data quality and quantity, they fail to incorporate the data selection pipeline into the task-oriented model development. On the contrary, the proposed method focuses on improving the downstream performance given limited K-shot real-word datapoints under the context of expert fusion. Correspondingly, we simultaneously consider the quality and importance where the resemblance of an open-source instruction to the K-shot set is prioritized during selection. In addition, we treasure the diversity of the selected dataset as its variety helps polish the coordination between experts in token-wise routing. To the best of our knowledge, the proposed method is the pioneer that integrates comprehensive data selection into the advancement of a MoE system for task expertise, where the concepts of quality, diversity, and importance play a critical role throughout the selection of both expert and data candidates.",
                "abstract": "The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Our codes will be available atthis https URL."
            },
            {
                "name": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
                "arxiv_id": "2403.08281",
                "subtitles": [
                    "Large Language Models for Language",
                    "Large Language Models beyond Language",
                    "The Fusion of Large Language Models"
                ],
                "reference": [
                    "Knowledge fusion of large language models",
                    "Pal: Program-aided language models",
                    "Solving quantitative reasoning problems with language models",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Opt: Open pre-trained transformer language models",
                    "Palm: Scaling language modeling with pathways",
                    "Starcoder: may the source be with you",
                    "Santacoder: don't reach for the stars",
                    "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
                    "Unified scaling laws for routed language models",
                    "Toolformer: Language models can teach themselves to use tools",
                    "Length generalization in arithmetic transformers",
                    "Sparse upcycling: Training mixture-of-experts from dense checkpoints",
                    "Glam: Efficient scaling of language models with mixture-of-experts",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
                    "Gshard: Scaling giant models with conditional computation and automatic sharding",
                    "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
                    "Evaluating large language models trained on code",
                    "Mixture-of-experts meets instruction tuning:a winning combination for large language models, 2023a",
                    "Multimodal contrastive learning with limoe: the language-image mixture of experts",
                    "Competition-level code generation with alphacode",
                    "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "Mixture-of-experts with expert choice routing",
                    "Gpt-j-6b: A 6 billion parameter autoregressive language model",
                    "Gpt can solve mathematical problems without a calculator",
                    "Gpt-4 technical report",
                    "Scaling vision-language models with sparse mixture of experts",
                    "Mixtral of experts",
                    "Codegen2: Lessons for training llms on programming and natural languages",
                    "Scaling vision with sparse mixture of experts",
                    "Cross-token modeling with conditional computation",
                    "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Teaching algorithmic reasoning via in-context learning",
                    "Language models are few-shot learners",
                    "Adaptive mixtures of local experts",
                    "Alpaca: A strong, replicable instruction-following model",
                    "Taming sparsely activated transformer with stochastic experts",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March",
                    "Gpt-neox-20b: An open-source autoregressive language model",
                    "Mistral 7b, 2023a",
                    "Beyond distillation: Task-level mixture-of-experts for efficient inference",
                    "St-moe: Designing stable and transferable sparse expert models",
                    "Show your work: Scratchpads for intermediate computation with language models",
                    "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
                    "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
                    "Textbooks are all you need",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related WorkLarge Language Models for Language.With the proliferation of model parameters, enhancements in training data augmentation both in terms of quantity and quality, and continuous refinements in training algorithms, LLMs have exhibited an enhancement in language understanding, generation, and generalization capabilities. These LLMs exhibit remarkable proficiency in accomplishing a wide array of natural language processing tasks, and showcase formidable capabilities in in-context learning and few-shot learning(Brown et al.,2020; Ouyang et al.,2022; OpenAI,2023; Chowdhery et al.,2022; Zhang et al.,2022; Touvron et al.,2023; Taori et al.,2023; Chiang et al.,2023; Xu et al.,2023; Ding et al.,2023; Jiang et al.,2023a) . Despite originating from NLP tasks, as LLMs evolve, the boundaries between NLP tasks are gradually becoming blurred.Large Language Models beyond Language.LLMs excel in processing various symbol systems including code, math symbols, DNA, and protein sequences. Models like StarCoder(Li et al.,2023a) and CodeLlama(Rozi\u00e8re et al.,2023) , trained on vast code repositories and interactions, are adept at code generation, bug fixing, and explanation(Black et al.,2021; Wang & Komatsuzaki,2021; Black et al.,2022; Wang et al.,2021; Chen et al.,2021; Li et al.,2022; Nijkamp et al.,2022,2023; Fried et al.,2022; Gunasekar et al.,2023; Allal et al.,2023) . Similarly, math-focused models, such as Minerva(Lewkowycz et al.,2022) and MathGLM(Yang et al.,2023) , have been developed through specialized training and fine-tuning strategies, including the use of external tools and Chain of Thought techniques(Jelassi et al.,2023; Liu & Low,2023; Nye et al.,2022; Zhou et al.,2022a; Chen et al.,2022; Yang et al.,2023; Gao et al.,2023; Schick et al.,2023) . These models, requiring extensive training, highlight the intensive data demands of LLMs in specialized domains. For example, CodeLlama uses 500 billion tokens for code training, 100 billion tokens for Python training, and more than 20 billion tokens for fine-tuning.The Fusion of Large Language Models.Mixture-of-Experts (MoE) is the neural architecture that distributes tasks among multiple specialized networks (experts) and determines their responsibilities via a gating network(Jacobs et al.,1991) . MoE enhances the capabilities of LLMs and has been extensively utilized(Clark et al.,2022; Lou et al.,2021; Kudugunta et al.,2021; Lepikhin et al.,2020; Mustafa et al.,2022; Zhou et al.,2022b; Riquelme et al.,2021; Shen et al.,2023b; Jiang et al.,2023b; Wan et al.,2024; Jiang et al.,2024) . Many studies have endeavored to comprehend the Mixture-of-Experts (MoE) from the perspective of computational cost, with a specific focus on its sparse nature(Shazeer et al.,2016; Zoph et al.,2022; Zuo et al.,2021; Du et al.,2022; Fedus et al.,2022; Komatsuzaki et al.,2023; Shen et al.,2023a) . The prevailing belief is that the MoE approach can scale up model parameters without incurring an escalation in computational expense. Some work suggests that experts do not necessarily have distinct expertise(Jiang et al.,2024) , while other work verifies the effectiveness of expert specialization(Dai et al.,2024) . We believe both ways could achieve promising performance, unlike those that train MoE models from scratch, this paper seeks to fuse highly specialized models in the fine-tuning phase. Compared to methods like knowledge distillation and knowledge fusion(Wan et al.,2024) , our approach aims to achieve optimal performance by retaining the specialized models and learning to fuse the expertise directly, avoiding potential performance loss brought by inaccurate fashion weight estimation and further distillation training.",
                "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content. This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains."
            },
            {
                "name": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights",
                "arxiv_id": "2401.11641",
                "subtitles": [
                    "Large Language Models",
                    "Named Entity Recognition",
                    "Sentiment Analysis",
                    "Question Answering",
                    "Time Series Forecasting",
                    "Mathematical Reasoning"
                ],
                "reference": [
                    "Policygpt: Automated analysis of privacy policies with large language models",
                    "Differentiate chatgpt-generated and human-written medical texts",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Pharmacygpt: The ai pharmacist",
                    "Agribert: knowledge-infused agricultural language models for matching food and nutrition",
                    "Sentiment analysis on news articles for stocks",
                    "Empirical properties of asset returns: stylized facts and statistical issues",
                    "Exploring new frontiers in agricultural nlp: Investigating the potential of large language models for food applications",
                    "Attention is all you need",
                    "Comparison of kalman filter estimation approaches for state space models with nonlinear measurements",
                    "Language models are unsupervised multitask learners",
                    "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                    "A bert based sentiment analysis and key entity detection approach for online financial texts",
                    "Cohortgpt: An enhanced gpt for participant recruitment in clinical study",
                    "Beam search strategies for neural machine translation",
                    "Named entity recognition and classification on historical documents: A survey",
                    "A survey of large language models",
                    "An attention-based bilstm-crf approach to document-level chemical named entity recognition",
                    "Named entity recognition and relation extraction: State-of-the-art",
                    "The impact of sentiment and attention measures on stock market volatility",
                    "Efficacy of news sentiment for stock market prediction",
                    "Evaluating the potential of leading large language models in reasoning biology questions",
                    "Mask-guided bert for few shot text classification",
                    "Artificial general intelligence for radiation oncology",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task",
                    "Impressiongpt: An iterative optimizing framework for radiology report summarization with chatgpt",
                    "Survey on natural language processing in medical image analysis",
                    "Stock prediction using twitter sentiment analysis",
                    "Radiology-gpt: A large language model for radiology",
                    "Interest rate models-theory and practice: with smile, inflation and credit, vol",
                    "Review of large vision models and visual prompt engineering",
                    "Information availability and return volatility in the bitcoin market: analyzing differences of user opinion and interest",
                    "A selective overview of nonparametric methods in financial econometrics",
                    "Predicting the effects of news sentiments on the stock market",
                    "Named entity recognition and classification in historical documents: A survey",
                    "Mededit: Model editing for medical question answering with external knowledge bases",
                    "Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms",
                    "The effect of news and public mood on stock movements",
                    "Named entity recognition using support vector machine: A language independent approach",
                    "Benchmarking a foundation llm on its ability to re-label structure names in accordance with the aapm tg-263 report",
                    "Sentiment analysis of news headlines for stock price prediction",
                    "Transformation vs tradition: Artificial general intelligence (agi) for arts and humanities",
                    "Sentiment analysis on social media for stock movement prediction",
                    "Multimodality of ai for education: Towards artificial general intelligence",
                    "Artificial general intelligence for medical imaging",
                    "Likelihood-based inference in cointegrated vector autoregressive models",
                    "Deep hedging",
                    "Financial econometrics, mathematics and statistics",
                    "Analysis and prediction in sparse and high dimensional text data: The case of dow jones stock market",
                    "Coarse-to-fine knowledge graph domain adaptation based on distantly-supervised iterative training",
                    "A survey on deep learning for named entity recognition",
                    "Incorporating stock prices and news sentiments for stock market prediction: A case of hong kong",
                    "Enhancing financial sentiment analysis via retrieval augmented large language models",
                    "Gpt-4 technical report",
                    "Stock price prediction using news sentiment analysis",
                    "A new algorithm for data compression",
                    "Forecasting short term interest rates using arma, arma-garch and arma-egarch models",
                    "The science of detecting llm-generated texts",
                    "Predicting stock market behavior using data mining technique and news sentiment analysis",
                    "Vector autoregressive models for multivariate time series",
                    "Time-llm: Time series forecasting by reprogramming large language models",
                    "Radiology-llama2: Best-in-class large language model for radiology",
                    "Ad-autogpt: An autonomous gpt for alzheimer's disease infodemiology",
                    "Auggpt: Leveraging chatgpt for text data augmentation",
                    "Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition",
                    "On the promises and challenges of multimodal foundation models for geographical, environmental, agricultural, and urban planning applications",
                    "Understanding llms: A comprehensive overview from training to inference",
                    "Improving language understanding by generative pre-training",
                    "Evaluating large language models on a highly-specialized topic",
                    "Temporal data meets llm - explainable financial time series forecasting",
                    "Deid-gpt: Zero-shot medical text de-identification by gpt",
                    "Language models are few-shot learners",
                    "Analysis of sentiments using unsupervised learning techniques",
                    "Llama: Open and efficient foundation language models",
                    "Summary of chatgpt-related research and perspective towards the future of large language models",
                    "When brain-inspired ai meets agi",
                    "Twitter mood predicts the stock market"
                ],
                "related_work": "2Related Work 2.1Large Language ModelsLLMs are primarily built upon the Transformer architecture[119], which has been central to their ability to handle complex language tasks. The Transformer model is structured with two key components: the Encoder and the Decoder, each consisting of multiple layers of self-attention and feed-forward neural networks. This architecture facilitates effective management of long-range dependencies within sequences. Attention(q,k,v) =softmax(\\frac{qk}{\\sqrt{d_{k}}}) Self-attention is characterized by its use of queries (Q) , keys (K) , and values (V) , three vectors derived from the input data. Each element in the input sequence is transformed into these three vectors through linear transformation. The self-attention mechanism then computes the attention scores by taking the dot product of the query with all keys. These scores determine how much focus or 'attention' each element in the sequence should have in relation to every other element. The attention scores are normalized using a softmax function, ensuring they sum up to one, thus forming a probability distribution. The final output of the self-attention layer is a weighted sum of the value vectors, where the weights are the softmax-normalized attention scores. This process allows each output element of the self-attention layer to be a combination of the inputs, with the weights specifying the amount of attention given to each input element. The self-attention mechanism's ability to weigh inputs differently allows LLMs to capture complex relationships in the data, such as long-range dependencies, making it exceptionally powerful for tasks that require an understanding of context and sequence.The architecture of LLMs typically falls into one of two categories: Decoder-only and Encoder-Decoder. Decoder-only models, such as those in the GPT series[99,100], generate text in a unidirectional manner[12]. Each token in the input sequence attends only to preceding tokens, making them well-suited for tasks like text generation. The Encoder-Decoder models, like T5[102]and BART[56], feature separate mechanisms for encoding input sequences and decoding them into target sequences. This design allows them to handle a broader range of tasks, including both generation and comprehension.Token generation in LLMs is a vital process, involving vocabulary creation, probability prediction, and techniques like beam search for sequence generation. Vocabulary in LLMs is typically constructed using methods like Byte-Pair Encoding (BPE) [38], which allows the model to break down words into subword units. This method aids in managing the model's vocabulary size, ensuring efficient handling of rare words and morphemes.During the token generation process, LLMs predict the probability of each token given the context provided by the input sequence. This is typically achieved through a softmax layer that converts the output logits into a probability distribution over the vocabulary. The model selects tokens based on these probabilities, either choosing the most likely next token (greedy decoding) or leveraging techniques like beam search. Beam search is a decoding strategy[36]that maintains a fixed number of candidate sequences at each step. It expands each candidate by one token at a time, computes the probability of each expansion, and keeps only the most likely sequences. This method balances between finding the most probable sequence and maintaining a diverse set of candidate sequences, leading to more coherent and contextually appropriate outputs.The capabilities inherent in the Transformer architecture and token generation processes of LLMs have facilitated their application across a wide range of domains. For instance, in Natural Language Generation (NLG) , the Decoder-only models excel at producing contextually relevant text, suitable for creative writing and automated report generation. Encoder-Decoder models, due to their bidirectional processing ability, are highly effective in tasks like machine translation, capable of converting input from one language to another while preserving semantic integrity[140].For example, in conversational AI, LLMs power sophisticated chatbots and virtual assistants[68,55], capable of generating human-like responses in real-time. Their ability to understand and generate language fluently makes them ideal for customer service automation, interactive learning platforms, and personalized communication tools.LLMs also play a crucial role in information extraction and summarization[69,111], distilling lengthy documents into concise, informative summaries. This application is particularly beneficial in fields like journalism and academic research, where quick assimilation of information is essential.Furthermore, the sophisticated understanding of context and language nuances allows LLMs to perform sentiment analysis[68,69], including financial sentiment analysis[136]. This capability is widely used in brand monitoring, market research, and social media analysis, providing insights into public opinion and consumer behavior.Overall, the technical intricacies of LLMs, from their architectural design to their token generation methods, underpin a broad spectrum of applications[65,40,26,77,44]. These models not only enhance existing processes but also open up new possibilities in the way we interact with and process language.2.2Named Entity RecognitionNamed Entity Recognition (NER) is a key technology in the field of NLP, used to identify and classify entities with specific meanings from text, such as names, places, organizations, time expressions, financial terms, etc. NER plays an important role in information extraction, question-answering systems, content analysis, knowledge graph construction, and other fields[93]. There are three main mainstream approaches to solving the NER, namely Rule-Based methods, Machine Learning-Based methods, and Deep Learning-Based methods.[31]Rule-based systems operate based on identifying entities using predefined rules and patterns, such as using a dictionary of place names to recognize locations. It is easily interpretable and does not require training data. While reliant on expert knowledge, these methods have limited flexibility and scalability[57]. Machine Learning-Based Methods: These methods, such as Support Vector Machines (SVM) and Random Forests, learn to recognize entities through training datasets based on manually selected features. They offer more flexibility than rule-based methods but require extensive annotated data[32]. Deep learning techniques for tagging sequences make use of word and character representations that are distributed, by training on sentence or sequence features in an end-to-end manner. These methods mainly use BiLSTM structures or networks based on self-attention. They frequently use a Conditional Random Field (CRF) layer for decoding tags, aiding in the comprehension of label interdependencies. Leveraging these capabilities, deep learning approaches are highly effective in managing intricate patterns and extensive data sets[30,85]. NER is widely used in the financial field, it can be applied for information extraction (extracting key details about companies, stocks, and market events from financial news and reports) , compliance monitoring (automatically identifying and overseeing sensitive entities in financial documents, like money laundering and fraud) , and investment decision support (providing data support for investment decisions by analyzing entities and events in market news and reports) . These applications underscore the vital role of NER in enhancing efficiency, ensuring compliance, and supporting strategic decisions[139].2.3Sentiment AnalysisIn contemporary financial market forecasting, especially regarding Bitcoin trading, the significance of sentiment analysis has been corroborated through numerous academic studies[10,58,133]. This research area primarily bifurcates into two methodological categories: lexicon-based and machine-learning approaches, both pivotal in discerning market trends.Lexicon-Based Methodology:Within this category, approaches are subdivided into dictionary and corpus-based strategies. A notable instance is the model developed by Dev Shah et al.[109], which utilizes the 'pattern' Python library for transforming textual data into numerical vectors. This process involves compiling sentiment scores by quantifying the occurrence of positive and negative words. However, this model faces limitations due to its unweighted sentiment scoring for individual words, potentially leading to inaccuracies in mirroring the actual market sentiment.Machine Learning Techniques:These are split into unsupervised and supervised learning. The unsupervised model by M.S. Usha et al.[118], which leverages the Gibbs sampling algorithm, excels in identifying sentiment and topics simultaneously. Yet, its inefficacy in capturing neutral sentiments poses a constraint. In contrast, the supervised approach by D.K. Kirange et al.[52]focuses on classifying emotions in news content to determine sentiment polarity, employing algorithms such as Naive Bayes, SVM, and KNN, with the latter showing optimal accuracy. Moreover, Sneh Kalra et al.[48]introduced a model that synergizes Naive Bayes sentiment analysis with adjacent date stock variance data from Yahoo Finance, although it is somewhat limited by its reliance on a single data source. Xiadong Li et al.[60]proposed a novel deep learning-based stock prediction system that fuses sentiment analysis with technical stock indicators. Additionally, the field has seen diverse methodologies such as specialized NLP sub-module designs for sentiment analysis[108,95], the application of N-gram and Naive Bayes Algorithms[50], dictionary-based sentiment analysis[49], and mood classification paired with daily sentiment scoring[91,6]. Time series analysis models have also found their application in this area[92].These varied methodologies underscore the complexity and multidimensionality of sentiment analysis in financial forecasting, particularly in the context of news analysis. Each approach offers a unique lens through which market trends can be decoded and anticipated, demonstrating the intricate interplay between market sentiment and financial news analysis.2.4Question AnsweringLarge language models (LLMs) , such as GPT-4, have demonstrated remarkable capabilities in question answering[124], mainly due to their complex architecture and large amounts of training data.LLMs obtains broad knowledge coverage by analyzing large amounts of text data on the Internet. They can answer questions ranging from general knowledge to specialized fields such as finance, history, science, technology, art, and more[114,73,86,63,25,68,41,15,76,111,40,71,14,61,104,80]. LLMs can understand complex queries[127,105,79,121,59,44,73,115,103]. Whether it's long sentences, ambiguous questions, or questions that require the synthesis of different information sources, LLMs can handle it and provide relevant answers[68,43,115]. LLM can maintain contextual coherence in conversations. This means it can understand and answer subsequent questions based on previous conversations, providing more accurate and relevant information[68,74,138]. Top LLMs often have multilingual capabilities and can understand and answer questions in different languages[116], which allows them to serve a wider user base.LLMs have exhibited remarkable capabilities in advanced reasoning. For instance, GPT-4 showcases its ability for common-sense reasoning by leveraging in-context learning. Moreover, the study[124]reveals that when LLMs are provided with well-structured sequential prompts that break down complex, multi-step problems, their performance in tasks involving arithmetic, deductive reasoning, and common-sense understanding improves significantly.2.5Time Series ForecastingFinancial time series forecasting has traditionally hinged on statistical and econometric methods. Models like ARMA-GARCH have been pivotal in discerning patterns and volatility in financial series[101].Over time, these models have been refined to better interpret the intricacies of financial markets. Other methods that have gained prominence include Vector Autoregressive Models (VAM) [145], State-Space Models utilizing Kalman Filters[97], Diffusion Models[34], and Vector Error Correction Model (VECM) [47], forming the bedrock of financial analysis.The emergence of machine learning has introduced a plethora of models for financial forecasting. Decision trees and support vector machines, known for their effectiveness in financial series prediction, have become particularly prominent. Of late, there has been a pivot towards deep learning techniques such as Recurrent Neural Networks (RNNs) , Convolutional Neural Networks (CNNs) , and Transformer models, renowned for their proficiency in unraveling complex, non-linear data relationships.The development of LLMs like GPT-3[12], GPT-4[1], and LLaMA[117], has been a game-changer in the realm of financial time series forecasting. These models excel in parsing and interpreting intricate dependencies in diverse data sets, offering outputs that are comprehensible to humans. There has been considerable advancement in this domain, including the conversion of time series data into textual sequences, the creation of varied prompts for intelligible financial forecasting, and the conceptualization of financial time series as multimodal data, harnessing the combined strengths of LLMs and computer vision. These developments showcase the dynamic and expanding role of LLMs in financial time series forecasting, highlighting a field ripe with innovation and exploration[134,46,17].2.6Mathematical ReasoningMathematical reasoning forms the cornerstone of modern finance, serving as the bedrock upon which complex financial theories, models, and practices are constructed. In the realm of finance, mathematical reasoning extends beyond mere number crunching; it encompasses the application of mathematical principles to analyze and solve financial problems, thereby empowering professionals to make informed decisions, assess risks, and forecast market trends.Central to mathematical reasoning in finance is the integration and application of various mathematical disciplines, such as calculus, statistics, probability, and linear algebra. These mathematical frameworks enable finance professionals to devise and interpret financial models, assess investment strategies, and optimize portfolios. Calculus, for instance, is pivotal in modeling the dynamic behavior of markets and in calculating derivatives, which are key in risk management and the pricing of financial instruments[11]. Moreover, statistics and probability are indispensable in evaluating risks and returns, aiding in asset valuation and the development of predictive models[54].Furthermore, mathematical reasoning in finance is dynamic and continually evolves with the emergence of new theories and the advent of technological advancements. The inception of quantitative finance, which amalgamates mathematical finance, numerical methods, and computer simulations, has transformed the industry. This interdisciplinary approach has led to the creation of intricate models for options pricing, risk management, and algorithmic trading, thereby enhancing the precision and efficiency of financial operations[13].As we traverse an era marked by increasing complexity and interconnectivity in financial markets, the significance of mathematical reasoning becomes increasingly critical. It not only furnishes finance professionals with the necessary tools for understanding and innovation but also instills a rigorous analytical framework, which is vital amidst financial uncertainties. Whether it's in the valuation of complex derivatives, the formulation of robust financial models, or the strategic management of investment portfolios, mathematical reasoning remains an essential component in the repertoire of contemporary finance[24].",
                "abstract": "In recent years, Large Language Models (LLMs) like ChatGPT have seen considerable advancements and have been applied in diverse fields. Built on the Transformer architecture, these models are trained on extensive datasets, enabling them to understand and generate human language effectively. In the financial domain, the deployment of LLMs is gaining momentum. These models are being utilized for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. Leveraging their natural language processing capabilities, LLMs can distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction. In this study, we provide a comprehensive overview of the emerging integration of LLMs into various financial tasks. Additionally, we conducted holistic tests on multiple financial tasks through the combination of natural language instructions. Our findings show that GPT-4 effectively follow prompt instructions across various financial tasks. This survey and evaluation of LLMs in the financial domain aim to deepen the understanding of LLMs' current role in finance for both financial practitioners and LLM researchers, identify new research and application prospects, and highlight how these technologies can be leveraged to solve practical challenges in the finance industry."
            },
            {
                "name": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
                "arxiv_id": "2403.06407",
                "subtitles": [
                    "PEFT Techniques of LLMs",
                    "Medical Vision-Language Models"
                ],
                "reference": [
                    "Dora: Weight-decomposed low-rank adaptation",
                    "Finetuned language models are zero-shot learners",
                    "Miss: A generative pretraining and finetuning approach for med-vqa",
                    "Masked vision and language pre-training with unimodal and multimodal contrastive losses for medical visual question answering",
                    "Language models are few-shot learners",
                    "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                    "Multi-modal masked autoencoders for medical vision-and-language pre-training",
                    "Parameter-efficient transfer learning for nlp",
                    "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
                    "Lora: Low-rank adaptation of large language models",
                    "The power of scale for parameter-efficient prompt tuning"
                ],
                "related_work": "2Related WorkPEFT Techniques of LLMs:Fine-tuning large pretrained language models (PLMs) is resource-intensive, often requiring substantial computational resources and training data. To address this challenge, PEFT techniques have emerged, aiming to enhance PLMs' performance on specific tasks with minimal changes to model parameters. Various methods have been developed in this regard. Adapter Tuning[8], Dora[19], LoRA[9]etc.[17,20]add small components to PLMs or the input embedding[11,1]to realize PEFT. Besides methods that add small components, some PEFT techniques focus on data manipulation to minimize or eliminate changes to the original model weights. OpenAI[1]and Google[24]have independently introduced instruction-tuning methods that modify original data into instruction pairs for fine-tuning models, achieving better generative results compared to multi-task training. While these PEFT methods have been successfully applied to LLMs, their impact on small-scale VLMs remains underexplored, especially in the medical domain.Medical Vision-Language Models:Recent advancements in the pretraining-finetuning paradigm have led to the emergence of medical VLMs[4,3,14]based on VLP models. However, their pretraining and fine-tuning require data scales of more than 100,000 image-text pairs and the number of trainable parameters for global fine-tuning is not much different from that of PEFT in LVLMs. Therefore, under the dual factors of large-scale training data and high training parameters, small-scale VLMs' training costs remain unaffordable for many researchers. Thus, in this work, we systematically review LLMs' tuning methods and discuss their applicability to medical VLMs.",
                "abstract": "While Large Language Models (LLMs) excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments. Due to the model's vast scale, traditional global fine-tuning methods for large models can be computationally expensive and impact generalization. To address this challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT) methods have emerged and achieved remarkable success in both LLMs and Large Vision-Language Models (LVLMs). In the medical domain, fine-tuning a medical Vision-Language Pretrained (VLP) model is essential for adapting it to specific tasks. Can the fine-tuning methods for large models be transferred to the medical field to enhance transfer learning efficiency? In this paper, we delve into the fine-tuning methods of LLMs and conduct extensive experiments to investigate the impact of fine-tuning methods for large models on the existing multimodal model in the medical domain from the training data level and the model structure level. We show the different impacts of fine-tuning methods for large models on medical VLMs and develop the most efficient ways to fine-tune medical VLP models. We hope this research can guide medical domain researchers in optimizing VLMs' training costs, fostering the broader application of VLMs in healthcare fields. The code and dataset have been released atthis https URL."
            },
            {
                "name": "RareBench: Can LLMs Serve as Rare Diseases Specialists?",
                "arxiv_id": "2402.06341",
                "subtitles": [
                    "Medical Benchmarks for LLMs",
                    "LLMs' Medical Capability"
                ],
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Exploring the Boundaries of GPT-4 in Radiology",
                    "Measuring phenotype semantic similarity using human phenotype ontology",
                    "Interpretable clinical genomics with a likelihood ratio paradigm",
                    "Doc2Hpo: a web application for efficient and accurate HPO concept curation",
                    "Capabilities of gpt-4 on medical challenge problems",
                    "RDAD: a machine learning system to support phenotype-based rare disease diagnosis",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Large language models encode clinical knowledge",
                    "Phen2Gene: rapid phenotype-driven gene prioritization for rare diseases",
                    "ClinPhen extracts and prioritizes patient phenotypes directly from medical records to expedite genetic disease diagnosis",
                    "Rare disease discovery: An optimized disease ranking system",
                    "Phen2Disease: a phenotype-driven model for disease and gene prioritization by bidirectional maximum matching semantic similarities",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT",
                    "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
                    "Gpt-4 technical report",
                    "Deep phenotyping on electronic health records facilitates genetic diagnosis by clinical exomes",
                    "PhenoBERT: a combined deep learning method for automated recognition of human phenotype ontology",
                    "Foundation models for generalist medical artificial intelligence",
                    "Benchmarking Large Language Models on CMExam-A Comprehensive Chinese Medical Exam Dataset",
                    "Towards accurate differential diagnosis with large language models",
                    "Using fine-tuned large language models to parse clinical notes in musculoskeletal pain disorders",
                    "MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models",
                    "Large language models in medicine",
                    "Ddxplus: A new dataset for automatic medical diagnosis",
                    "Xrare: a machine learning method jointly modeling phenotypes and genetic evidence for rare disease diagnosis",
                    "Clinical diagnostics in human genetics with semantic similarity searches in ontologies",
                    "PubMedQA: A Dataset for Biomedical Research Question Answering",
                    "Towards Conversational Diagnostic AI",
                    "PhenoTagger: a hybrid method for phenotype concept recognition using human phenotype ontology",
                    "Ethics of large language models in medicine and medical research"
                ],
                "related_work": "2.Related Work Medical Benchmarks for LLMs.Prominent medical question-and-answer data benchmarks, such as MedQA(Jin et al.,2021) , PubMedQA(Jin et al.,2019) , MedMCQA(Pal et al.,2022) , MultiMedQA(Singhal et al.,2023) , and CMExam(Liu et al.,2023b) , primarily derive from medical examinations, typically featuring multiple-choice formats. MedBench(Cai et al.,2023) introduces a large-scale Chinese benchmark for evaluating LLMs in clinical knowledge and diagnostics. Additionally, DDXPlus(Fansi Tchango et al.,2022) provides a detailed medical diagnostic dataset covering symptoms and diagnoses. OurRareBenchextends this landscape by focusing on complex clinical scenarios specific to rare diseases.LLMs' Medical Capability.The evolution of General Medical Artificial Intelligence (GMAI) is reshaping healthcare by automating learning processes and incorporating domain knowledge to reduce the clinical workload(Moor et al.,2023) . GPT-4(Achiam et al.,2023) , a notable example in this field, has demonstrated exceptional skills in medical questions(Nori et al.,2023a) and rivals or surpasses state-of-the-art models in tasks such as radiology text interpretation(Liu et al.,2023a) . Medprompt(Nori et al.,2023b) further enhances this capability through specialized prompting techniques, enabling foundational models like GPT-4 to outperform dedicated healthcare models. Besides GPT-4, models like AMIE(Tu et al.,2024) show superior performance in specific medical tasks, even exceeding the diagnostic abilities of general primary care physicians in some cases. These LLMs not only assist in differential diagnosis but also engage in clinical reasoning, thus improving diagnostic accuracy(McDuff et al.,2023; Kwon et al.,2023) . Moreover, fine-tuned LLMs can efficiently extract valuable data from clinical notes, significantly boosting patient care quality(Vaid et al.,2023) . Despite these advancements, challenges in accuracy, interpretability, and safety persist in the medical application of LLMs, underscoring the need for continuous refinement(Thirunavukarasu et al.,2023; Li et al.,2023) . Notably, the potential of LLMs in rare disease contexts is yet to be fully explored, and our research aims to fill this gap.Diagnosis of Rare Disease.The initial step in clinical diagnosis involves extracting standardized phenotypes from a patient's electronic health record (EHR) . To translate clinical texts into standardized Human Phenotype Ontology (HPO) terms, various natural language processing (NLP) methods(Son et al.,2018; Liu et al.,2019; Deisseroth et al.,2019; Luo et al.,2021; Feng et al.,2022; Yang et al.,2023) have been developed. For rare disease diagnosis, current computational methods comprise many statistical or machine learning-based methods(Zhai et al.,2023; Robinson et al.,2020; Li et al.,2019; Zhao et al.,2020; Son et al.,2018; K\u00f6hler et al.,2009; Jia et al.,2018; Pinol et al.,2017; Peng et al.,2016) .",
                "abstract": "Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as \"ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed\" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field."
            }
        ],
        "survey": {
            "name": "A Survey of Large Language Models in Finance (FinLLMs) ",
            "arxiv_id": "2402.02315",
            "subtitles": [
                {
                    "name": "Evolution Trends: from General to Finance",
                    "key_history": [
                        {
                            "reference_title": "Language models are few-shot learners",
                            "key_word": "GPT"
                        },
                        {
                            "reference_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                            "key_word": "BERT"
                        },
                        {
                            "reference_title": "Llama: Open and efficient foundation language models",
                            "key_word": "LLaMA"
                        },
                        {
                            "reference_title": "Bloom: A 176b-parameter open-access multilingual language model",
                            "key_word": "BLOOM"
                        },
                        {
                            "reference_title": "Finbert: Financial sentiment analysis with pre-trained language models",
                            "key_word": "Finbert"
                        },
                        {
                            "reference_title": "Fingpt: Instruction tuning benchmark for open-source large language models in financial datasets",
                            "key_word": "Fingpt"
                        }
                    ],
                    "references_in_this_section": [
                        "Bert: Pre-training of deep bidirectional transformers for language understanding",
                        "Bloom: A 176b-parameter open-access multilingual language model",
                        "Language models are few-shot learners",
                        "Bloomberggpt: A large language model for finance",
                        "Investlm: A large language model for investment using financial domain instruction tuning",
                        "Improving language understanding by generative pre-training",
                        "Finbert: Financial sentiment analysis with pre-trained language models",
                        "Llama: Open and efficient foundation language models",
                        "Finbert: A pretrained language model for financial communications",
                        "Attention is all you need",
                        "Finbert: A pre-trained financial language representation model for financial text mining",
                        "Language models are unsupervised multitask learners",
                        "When flue meets flang: Benchmarks and large pretrained language model for financial domain",
                        "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
                        "Fingpt: Instruction tuning benchmark for open-source large language models in financial datasets",
                        "Chain-of-thought prompting elicits reasoning in large language models",
                        "Electra: Pre-training text encoders as discriminators rather than generators",
                        "Instruction tuning for large language models: A survey"
                    ]
                },
                {
                    "name": "Techniques: from FinPLMs to FinLLMs",
                    "key_history": [
                        {
                            "reference_title": "Continual pre-training of language models",
                            "key_word": "Continual Pre-training"
                        },
                        {
                            "reference_title": "Finbert: Financial sentiment analysis with pre-trained language models",
                            "key_word": "financial sentiment analysis"
                        },
                        {
                            "reference_title": "Finbert: A pretrained language model for financial communications",
                            "key_word": "Domain-Specific Pre-training from Scratch"
                        },
                        {
                            "reference_title": "When flue meets flang: Benchmarks and large pretrained language model for financial domain",
                            "key_word": "Mixed-Domain Pre-training"
                        }
                    ],
                    "references_in_this_section": [
                        "Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? a study on several typical tasks",
                        "Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts",
                        "Good debt or bad debt: Detecting semantic orientations in economic texts",
                        "Accurate stock movement prediction with self-supervised learning from sparse noisy tweets",
                        "Stock movement prediction from tweets and historical prices",
                        "Finqa: A dataset of numerical reasoning over financial data",
                        "Finer: Financial numeric entity recognition for xbrl tagging",
                        "Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering",
                        "Impact of news on the commodity market: Dataset and results",
                        "Stockemotions: Discover investor emotions for financial sentiment analysis and multivariate time series",
                        "Efficient intent detection with dual sentence encoders",
                        "Fednlp: an interpretable nlp system to decode federal reserve communications",
                        "Hybrid deep sequential modeling for social text-driven stock prediction",
                        "Domain adaption of named entity recognition to support credit risk assessment",
                        "Multiling 2019: Financial narrative summarisation",
                        "Www\u201918 open challenge: financial opinion mining and question answering",
                        "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news",
                        "Trillion dollar words: A new financial dataset, task & market analysis"
                    ]
                },
                {
                    "name": "Evaluation: Benchmark Tasks and Datasets",
                    "key_history": [
                        {
                            "reference_title": "Continual pre-training of language models",
                            "key_word": "FinBERT-19"
                        },
                        {
                            "reference_title": "Finbert: Financial sentiment analysis with pre-trained language models",
                            "key_word": "Domain-Specific Pre-training"
                        },
                        {
                            "reference_title": "Finbert: A pre-trained financial language representation model for financial text mining",
                            "key_word": "FinBERT-21"
                        },
                        {
                            "reference_title": "When flue meets flang: Benchmarks and large pretrained language model for financial domain",
                            "key_word": "Mixed-Domain Pre-training"
                        },
                        {
                            "reference_title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                            "key_word": "Prompt Engineering"
                        },
                        {
                            "reference_title": "Bloomberggpt: A large language model for finance",
                            "key_word": "BloombergGPT"
                        },
                        {
                            "reference_title": "Instruction tuning for large language models: A survey",
                            "key_word": "Instruction Fine-Tuning"
                        },
                        {
                            "reference_title": "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
                            "key_word": "FinMA"
                        },
                        {
                            "reference_title": "Fingpt: Instruction tuning benchmark for open-source large language models in financial datasets",
                            "key_word": "FinGPT"
                        }
                    ],
                    "references_in_this_section": [
                        "Domain adaption of named entity recognition to support credit risk assessment",
                        "Impact of news on the commodity market: Dataset and results",
                        "Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? a study on several typical tasks",
                        "Www'18 open challenge: financial opinion mining and question answering",
                        "Multiling 2019: Financial narrative summarisation",
                        "Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering",
                        "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news",
                        "Efficient intent detection with dual sentence encoders",
                        "Finer: Financial numeric entity recognition for xbrl tagging",
                        "Finqa: A dataset of numerical reasoning over financial data",
                        "Trillion dollar words: A new financial dataset, task & market analysis",
                        "Good debt or bad debt: Detecting semantic orientations in economic texts",
                        "Hybrid deep sequential modeling for social text-driven stock prediction",
                        "Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts",
                        "Stockemotions: Discover investor emotions for financial sentiment analysis and multivariate time series",
                        "Accurate stock movement prediction with self-supervised learning from sparse noisy tweets",
                        "Fednlp: an interpretable nlp system to decode federal reserve communications",
                        "Stock movement prediction from tweets and historical prices"
                    ]
                },
                {
                    "name": "Advanced Financial NLP Tasks and Datasets",
                    "key_history": [
                        {
                            "reference_title": "Finred: A dataset for relation extraction in financial domain",
                            "key_word": "Relation Extraction"
                        },
                        {
                            "reference_title": "Trade the event: Corporate events detection for news-based event-driven trading",
                            "key_word": "Event Detection"
                        },
                        {
                            "reference_title": "The financial document causality detection shared task (fincausal 2020) ",
                            "key_word": "Causality Detection"
                        },
                        {
                            "reference_title": "Finqa: A dataset of numerical reasoning over financial data",
                            "key_word": "Numerical Reasoning"
                        },
                        {
                            "reference_title": "Global table extractor (gte) : A framework for joint table identification and cell structure recognition using visual context",
                            "key_word": "Structure Recognition"
                        },
                        {
                            "reference_title": "Maec: A multimodal aligned earnings conference call dataset for financial risk prediction",
                            "key_word": "Multimodal"
                        },
                        {
                            "reference_title": "Multilingual and cross-lingual intent detection from spoken data",
                            "key_word": "Machine Translation"
                        },
                        {
                            "reference_title": "Monopoly: Financial prediction from monetary policy conference videos using multimodal cues",
                            "key_word": "Market Forecasting"
                        }
                    ],
                    "references_in_this_section": [
                        "Multifin: A dataset for multilingual financial nlp",
                        "Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering",
                        "Maec: A multimodal aligned earnings conference call dataset for financial risk prediction",
                        "Finred: A dataset for relation extraction in financial domain",
                        "The financial document causality detection shared task (fincausal",
                        "Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context",
                        "Finer: Financial numeric entity recognition for xbrl tagging",
                        "Finqa: A dataset of numerical reasoning over financial data",
                        "Multilingual and cross-lingual intent detection from spoken data",
                        "Stockemotions: Discover investor emotions for financial sentiment analysis and multivariate time series",
                        "Trade the event: Corporate events detection for news-based event-driven trading",
                        "Monopoly: Financial prediction from monetary policy conference videos using multimodal cues"
                    ]
                },
                {
                    "name": "Opportunities and Challenges",
                    "key_history": [
                        {
                            "reference_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                            "key_word": "Retrieval Augmented Generation"
                        },
                        {
                            "reference_title": "Is chatgpt a financial expert? evaluating language models on financial natural language processing",
                            "key_word": "Financial NLP Evaluation"
                        },
                        {
                            "reference_title": "Fingpt: Open-source financial large language models",
                            "key_word": "Financial Multimodal Applications"
                        }
                    ],
                    "references_in_this_section": [
                        "Is chatgpt a financial expert? evaluating language models on financial natural language processing",
                        "Fingpt: Open-source financial large language models",
                        "Retrieval-augmented generation for knowledge-intensive nlp tasks"
                    ]
                }
            ],
            "all_references": [
                "Investlm: A large language model for investment using financial domain instruction tuning",
                "The financial document causality detection shared task (fincausal",
                "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news",
                "Multifin: A dataset for multilingual financial nlp",
                "Multiling 2019: Financial narrative summarisation",
                "Improving language understanding by generative pre-training",
                "Finbert: A pretrained language model for financial communications",
                "Fingpt: Instruction tuning benchmark for open-source large language models in financial datasets",
                "Hybrid deep sequential modeling for social text-driven stock prediction",
                "Trillion dollar words: A new financial dataset, task & market analysis",
                "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                "Fednlp: an interpretable nlp system to decode federal reserve communications",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
                "Llama: Open and efficient foundation language models",
                "Global table extractor (gte) : A framework for joint table identification and cell structure recognition using visual context",
                "Monopoly: Financial prediction from monetary policy conference videos using multimodal cues",
                "Finqa: A dataset of numerical reasoning over financial data",
                "Is chatgpt a financial expert? evaluating language models on financial natural language processing",
                "Www'18 open challenge: financial opinion mining and question answering",
                "Continual pre-training of language models",
                "Instruction tuning for large language models: A survey",
                "Language models are few-shot learners",
                "Finbert: Financial sentiment analysis with pre-trained language models",
                "Stock movement prediction from tweets and historical prices",
                "Efficient intent detection with dual sentence encoders",
                "On the opportunities and risks of foundation models",
                "Finbert: A pre-trained financial language representation model for financial text mining",
                "A survey of large language models",
                "Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering",
                "Attention is all you need",
                "Maec: A multimodal aligned earnings conference call dataset for financial risk prediction",
                "Lora: Low-rank adaptation of large language models",
                "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                "Finer: Financial numeric entity recognition for xbrl tagging",
                "Multilingual and cross-lingual intent detection from spoken data",
                "Language models are unsupervised multitask learners",
                "When flue meets flang: Benchmarks and large pretrained language model for financial domain",
                "Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts",
                "Good debt or bad debt: Detecting semantic orientations in economic texts",
                "Stockemotions: Discover investor emotions for financial sentiment analysis and multivariate time series",
                "Trade the event: Corporate events detection for news-based event-driven trading",
                "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
                "Electra: Pre-training text encoders as discriminators rather than generators",
                "Large language models in finance: A survey",
                "Bloom: A 176b-parameter open-access multilingual language model",
                "Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? a study on several typical tasks",
                "Finred: A dataset for relation extraction in financial domain",
                "Domain adaption of named entity recognition to support credit risk assessment",
                "Impact of news on the commodity market: Dataset and results",
                "Fingpt: Open-source financial large language models",
                "Accurate stock movement prediction with self-supervised learning from sparse noisy tweets",
                "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "Bloomberggpt: A large language model for finance"
            ]
        },
        "topic_history": [
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2023a",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "GeoGalactica: A Scientific Large Language Model in Geoscience",
                "arxiv_id": "2401.00434",
                "reference": [
                    "Application of machine learning in carbon capture and storage: An in-depth insight from the perspective of geoscience",
                    "Machine learning for data-driven discovery in solid earth geoscience",
                    "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                    "Unsupervised pre-stack seismic facies analysis constrained by spatial continuity",
                    "Impressiongpt: an iterative optimizing framework for radiology report summarization with chatgpt",
                    "Prediction of geology condition for slurry pressure balanced shield tunnel with super-large diameter by machine learning algorithms",
                    "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
                    "Application of machine learning for lithofacies prediction and cluster analysis approach to identify rock type",
                    "A new correlation for calculating wellhead oil flow rate using artificial neural network",
                    "Domain-specific language model pretraining for biomedical natural language processing",
                    "Geo-bert pre-training model for query rewriting in poi search",
                    "Understanding geological reports based on knowledge graphs using a deep learning approach",
                    "Toward earthquake early warning: A convolutional neural network for repaid earthquake magnitude estimation",
                    "Machine learning elucidates the anatomy of buried carbonate reef from seismic reflection data",
                    "Bloomberggpt: A large language model for finance",
                    "History of artificial neural networks",
                    "Integrating the artificial intelligence and hybrid machine learning algorithms for improving the accuracy of spatial prediction of landslide hazards in kurseong himalayan region",
                    "Darwin series: Domain specific large language models for natural science",
                    "Geogpt: Understanding and processing geospatial tasks through an autonomous gpt",
                    "Machine learning reveals climate forcing from aerosols is dominated by increased cloud cover",
                    "Construction and application of a knowledge graph for iron deposits using text mining analytics and a deep learning algorithm",
                    "Soft prompt tuning for augmenting dense retrieval with large language models",
                    "Legal-bert: The muppets straight out of law school",
                    "Neurospe: A neuro-net spatial relation extractor for natural language text fusing gazetteers and pretrained models",
                    "Similarity of fast and slow earthquakes illuminated by machine learning",
                    "Biogpt: Generative pre-trained transformer for biomedical text generation and mining",
                    "Galactica: A large language model for science",
                    "High resolution pre-stack seismic inversion using few-shot learning",
                    "Geoscience language processing for exploration",
                    "Deep convolutional autoencoders as generic feature extractors in seismological applications",
                    "Scibert: A pretrained language model for scientific text",
                    "Learning a foundation language model for geoscience knowledge understanding and utilization",
                    "Applications of natural language processing to geoscience text data and prospectivity modeling"
                ]
            },
            {
                "name": "Getting the most out of your tokenizer for pre-training and domain adaptation",
                "arxiv_id": "2402.01035",
                "reference": [
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Neural machine translation of rare words with subword units",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Llama: Open and efficient foundation language models",
                    "XLM-V: overcoming the vocabulary bottleneck in multilingual masked language models",
                    "Codebpe: Investigating subtokenization options for large language model pretraining on source code",
                    "Evaluating large language models trained on code",
                    "Unified pre-training for program understanding and generation",
                    "How good is your tokenizer? on the monolingual performance of multilingual language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Code llama: Open foundation models for code",
                    "Santacoder: don't reach for the stars",
                    "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x"
                ]
            },
            {
                "name": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
                "arxiv_id": "2402.18099",
                "reference": [
                    "Mass-Editing Memory in a Transformer",
                    "Locating and Editing Factual Associations in GPT",
                    "Knowledge Neurons in Pretrained Transformers",
                    "Editing Factual Knowledge in Language Models",
                    "Transformer Feed-Forward Layers Are Key-Value Memories",
                    "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors",
                    "Can We Edit Factual Knowledge by In-Context Learning",
                    "Transformer-Patcher: One Mistake Worth One Neuron",
                    "Memory-based model editing at scale",
                    "Investigating Gender Bias in Language Models Using Causal Mediation Analysis",
                    "MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA",
                    "PMET: Precise Model Editing in a Transformer",
                    "Direct and indirect effects",
                    "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
                    "Calibrating Factual Knowledge in Pretrained Language Models",
                    "Fast Model Editing at Scale",
                    "Editing Large Language Models: Problems, Methods, and Opportunities"
                ]
            },
            {
                "name": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models",
                "arxiv_id": "2408.12247",
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Distilling the knowledge in a neural network",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "Deep learning on a data diet: Finding important examples early in training",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Lima: Less is more for alignment",
                    "Stanford alpaca: An instruction-following llama model",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "An empirical study of example forgetting during deep neural network learning",
                    "Instruction tuning with gpt",
                    "Assess and summarize: Improve outage understanding with large language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "OWL: A large language model for IT operations",
                    "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                    "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
                    "Language models are few-shot learners",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models",
                "arxiv_id": "2408.15915",
                "reference": [
                    "Instruction mining: When data mining meets large language model finetuning",
                    "Influenci\u00e6: A library for tracing the influence back to the data-points",
                    "Self-moe: Towards compositional large language models with self-specialized experts",
                    "Active learning for convolutional neural networks: A core-set approach",
                    "Gradient-matching coresets for rehearsal-based continual learning",
                    "Conditional prompt learning for vision-language models",
                    "Uni-moe: Scaling unified multimodal llms with mixture of experts",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "Small-gan: Speeding up gan training using core-sets",
                    "Retrieval-augmented mixture of lora experts for uploadable machine learning",
                    "Lorahub: Efficient cross-task generalization via dynamic lora composition",
                    "Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Pissa: Principal singular values and singular vectors adaptation of large language models",
                    "Multi-head adapter routing for cross-task generalization",
                    "Adaptive budget allocation for parameter-efficient fine-tuning",
                    "An experimental design framework for label-efficient supervised finetuning of large language models",
                    "Lmd3: Language model data density dependence",
                    "Adapterhub: A framework for adapting transformers",
                    "Airoboros",
                    "Soft merging of experts with adaptive routing",
                    "Adapting and evaluating influence-estimation methods for gradient-boosted decision trees",
                    "Adaptersoup: Weight averaging to improve generalization of pretrained language models",
                    "Mole: Mixture of lora experts",
                    "Token-level adaptation of lora adapters for downstream task generalization",
                    "When less is more: Investigating data pruning for pretraining llms at scale",
                    "Lora: Low-rank adaptation of large language models",
                    "Dora: Weight-decomposed low-rank adaptation",
                    "Branch-train-mix: Mixing expert llms into a mixture-of-experts llm",
                    "Mixture of experts using tensor products",
                    "Prefix-tuning: Optimizing continuous prompts for generation",
                    "Tagcos: Task-agnostic gradient clustered coreset selection for instruction tuning data",
                    "Gpt-4 technical report",
                    "Dataset condensation with distribution matching",
                    "Data selection for language models via importance resampling",
                    "Autopeft: Automatic configuration search for parameter-efficient fine-tuning",
                    "Routing to the expert: Efficient reward-guided ensemble of large language models",
                    "LlamaIndex",
                    "Mods: Model-oriented data selection for instruction tuning",
                    "Towards modular llms by building and reusing a library of loras",
                    "Learning to route among specialized experts for zero-shot generalization",
                    "Adamole: Fine-tuning large language models with adaptive mixture of low-rank adaptation experts",
                    "Dataset quantization",
                    "Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts",
                    "Self-evolved diverse data sampling for efficient instruction tuning",
                    "Mixture-of-loras: An efficient multitask tuning for large language models",
                    "Qlora: Efficient finetuning of quantized llms",
                    "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
                    "Unleashing the power of data tsunami: A comprehensive survey on data assessment and selection for instruction tuning of language models",
                    "Influence functions in deep learning are fragile",
                    "Qwen technical report",
                    "Exploring the benefits of training expert language models over instruction tuning",
                    "The power of scale for parameter-efficient prompt tuning"
                ]
            },
            {
                "name": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
                "arxiv_id": "2403.08281",
                "reference": [
                    "Knowledge fusion of large language models",
                    "Pal: Program-aided language models",
                    "Solving quantitative reasoning problems with language models",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Opt: Open pre-trained transformer language models",
                    "Palm: Scaling language modeling with pathways",
                    "Starcoder: may the source be with you",
                    "Santacoder: don't reach for the stars",
                    "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
                    "Unified scaling laws for routed language models",
                    "Toolformer: Language models can teach themselves to use tools",
                    "Length generalization in arithmetic transformers",
                    "Sparse upcycling: Training mixture-of-experts from dense checkpoints",
                    "Glam: Efficient scaling of language models with mixture-of-experts",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
                    "Gshard: Scaling giant models with conditional computation and automatic sharding",
                    "Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks",
                    "Evaluating large language models trained on code",
                    "Mixture-of-experts meets instruction tuning:a winning combination for large language models, 2023a",
                    "Multimodal contrastive learning with limoe: the language-image mixture of experts",
                    "Competition-level code generation with alphacode",
                    "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "Mixture-of-experts with expert choice routing",
                    "Gpt-j-6b: A 6 billion parameter autoregressive language model",
                    "Gpt can solve mathematical problems without a calculator",
                    "Gpt-4 technical report",
                    "Scaling vision-language models with sparse mixture of experts",
                    "Mixtral of experts",
                    "Codegen2: Lessons for training llms on programming and natural languages",
                    "Scaling vision with sparse mixture of experts",
                    "Cross-token modeling with conditional computation",
                    "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Teaching algorithmic reasoning via in-context learning",
                    "Language models are few-shot learners",
                    "Adaptive mixtures of local experts",
                    "Alpaca: A strong, replicable instruction-following model",
                    "Taming sparsely activated transformer with stochastic experts",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March",
                    "Gpt-neox-20b: An open-source autoregressive language model",
                    "Mistral 7b, 2023a",
                    "Beyond distillation: Task-level mixture-of-experts for efficient inference",
                    "St-moe: Designing stable and transferable sparse expert models",
                    "Show your work: Scratchpads for intermediate computation with language models",
                    "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
                    "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
                    "Textbooks are all you need",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights",
                "arxiv_id": "2401.11641",
                "reference": [
                    "Policygpt: Automated analysis of privacy policies with large language models",
                    "Differentiate chatgpt-generated and human-written medical texts",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Pharmacygpt: The ai pharmacist",
                    "Agribert: knowledge-infused agricultural language models for matching food and nutrition",
                    "Sentiment analysis on news articles for stocks",
                    "Empirical properties of asset returns: stylized facts and statistical issues",
                    "Exploring new frontiers in agricultural nlp: Investigating the potential of large language models for food applications",
                    "Attention is all you need",
                    "Comparison of kalman filter estimation approaches for state space models with nonlinear measurements",
                    "Language models are unsupervised multitask learners",
                    "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                    "A bert based sentiment analysis and key entity detection approach for online financial texts",
                    "Cohortgpt: An enhanced gpt for participant recruitment in clinical study",
                    "Beam search strategies for neural machine translation",
                    "Named entity recognition and classification on historical documents: A survey",
                    "A survey of large language models",
                    "An attention-based bilstm-crf approach to document-level chemical named entity recognition",
                    "Named entity recognition and relation extraction: State-of-the-art",
                    "The impact of sentiment and attention measures on stock market volatility",
                    "Efficacy of news sentiment for stock market prediction",
                    "Evaluating the potential of leading large language models in reasoning biology questions",
                    "Mask-guided bert for few shot text classification",
                    "Artificial general intelligence for radiation oncology",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task",
                    "Impressiongpt: An iterative optimizing framework for radiology report summarization with chatgpt",
                    "Survey on natural language processing in medical image analysis",
                    "Stock prediction using twitter sentiment analysis",
                    "Radiology-gpt: A large language model for radiology",
                    "Interest rate models-theory and practice: with smile, inflation and credit, vol",
                    "Review of large vision models and visual prompt engineering",
                    "Information availability and return volatility in the bitcoin market: analyzing differences of user opinion and interest",
                    "A selective overview of nonparametric methods in financial econometrics",
                    "Predicting the effects of news sentiments on the stock market",
                    "Named entity recognition and classification in historical documents: A survey",
                    "Mededit: Model editing for medical question answering with external knowledge bases",
                    "Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms",
                    "The effect of news and public mood on stock movements",
                    "Named entity recognition using support vector machine: A language independent approach",
                    "Benchmarking a foundation llm on its ability to re-label structure names in accordance with the aapm tg-263 report",
                    "Sentiment analysis of news headlines for stock price prediction",
                    "Transformation vs tradition: Artificial general intelligence (agi) for arts and humanities",
                    "Sentiment analysis on social media for stock movement prediction",
                    "Multimodality of ai for education: Towards artificial general intelligence",
                    "Artificial general intelligence for medical imaging",
                    "Likelihood-based inference in cointegrated vector autoregressive models",
                    "Deep hedging",
                    "Financial econometrics, mathematics and statistics",
                    "Analysis and prediction in sparse and high dimensional text data: The case of dow jones stock market",
                    "Coarse-to-fine knowledge graph domain adaptation based on distantly-supervised iterative training",
                    "A survey on deep learning for named entity recognition",
                    "Incorporating stock prices and news sentiments for stock market prediction: A case of hong kong",
                    "Enhancing financial sentiment analysis via retrieval augmented large language models",
                    "Gpt-4 technical report",
                    "Stock price prediction using news sentiment analysis",
                    "A new algorithm for data compression",
                    "Forecasting short term interest rates using arma, arma-garch and arma-egarch models",
                    "The science of detecting llm-generated texts",
                    "Predicting stock market behavior using data mining technique and news sentiment analysis",
                    "Vector autoregressive models for multivariate time series",
                    "Time-llm: Time series forecasting by reprogramming large language models",
                    "Radiology-llama2: Best-in-class large language model for radiology",
                    "Ad-autogpt: An autonomous gpt for alzheimer's disease infodemiology",
                    "Auggpt: Leveraging chatgpt for text data augmentation",
                    "Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition",
                    "On the promises and challenges of multimodal foundation models for geographical, environmental, agricultural, and urban planning applications",
                    "Understanding llms: A comprehensive overview from training to inference",
                    "Improving language understanding by generative pre-training",
                    "Evaluating large language models on a highly-specialized topic",
                    "Temporal data meets llm - explainable financial time series forecasting",
                    "Deid-gpt: Zero-shot medical text de-identification by gpt",
                    "Language models are few-shot learners",
                    "Analysis of sentiments using unsupervised learning techniques",
                    "Llama: Open and efficient foundation language models",
                    "Summary of chatgpt-related research and perspective towards the future of large language models",
                    "When brain-inspired ai meets agi",
                    "Twitter mood predicts the stock market"
                ]
            },
            {
                "name": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
                "arxiv_id": "2403.06407",
                "reference": [
                    "Dora: Weight-decomposed low-rank adaptation",
                    "Finetuned language models are zero-shot learners",
                    "Miss: A generative pretraining and finetuning approach for med-vqa",
                    "Masked vision and language pre-training with unimodal and multimodal contrastive losses for medical visual question answering",
                    "Language models are few-shot learners",
                    "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
                    "Multi-modal masked autoencoders for medical vision-and-language pre-training",
                    "Parameter-efficient transfer learning for nlp",
                    "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
                    "Lora: Low-rank adaptation of large language models",
                    "The power of scale for parameter-efficient prompt tuning"
                ]
            },
            {
                "name": "RareBench: Can LLMs Serve as Rare Diseases Specialists?",
                "arxiv_id": "2402.06341",
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Exploring the Boundaries of GPT-4 in Radiology",
                    "Measuring phenotype semantic similarity using human phenotype ontology",
                    "Interpretable clinical genomics with a likelihood ratio paradigm",
                    "Doc2Hpo: a web application for efficient and accurate HPO concept curation",
                    "Capabilities of gpt-4 on medical challenge problems",
                    "RDAD: a machine learning system to support phenotype-based rare disease diagnosis",
                    "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
                    "Large language models encode clinical knowledge",
                    "Phen2Gene: rapid phenotype-driven gene prioritization for rare diseases",
                    "ClinPhen extracts and prioritizes patient phenotypes directly from medical records to expedite genetic disease diagnosis",
                    "Rare disease discovery: An optimized disease ranking system",
                    "Phen2Disease: a phenotype-driven model for disease and gene prioritization by bidirectional maximum matching semantic similarities",
                    "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
                    "Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT",
                    "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
                    "Gpt-4 technical report",
                    "Deep phenotyping on electronic health records facilitates genetic diagnosis by clinical exomes",
                    "PhenoBERT: a combined deep learning method for automated recognition of human phenotype ontology",
                    "Foundation models for generalist medical artificial intelligence",
                    "Benchmarking Large Language Models on CMExam-A Comprehensive Chinese Medical Exam Dataset",
                    "Towards accurate differential diagnosis with large language models",
                    "Using fine-tuned large language models to parse clinical notes in musculoskeletal pain disorders",
                    "MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models",
                    "Large language models in medicine",
                    "Ddxplus: A new dataset for automatic medical diagnosis",
                    "Xrare: a machine learning method jointly modeling phenotypes and genetic evidence for rare disease diagnosis",
                    "Clinical diagnostics in human genetics with semantic similarity searches in ontologies",
                    "PubMedQA: A Dataset for Biomedical Research Question Answering",
                    "Towards Conversational Diagnostic AI",
                    "PhenoTagger: a hybrid method for phenotype concept recognition using human phenotype ontology",
                    "Ethics of large language models in medicine and medical research"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Comparing Code Explanations Created by Students and Large Language Models",
            "arxiv_id": "2304.03938",
            "isAPA": true,
            "abstract": "Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course ( n \u2248 1000 ) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.",
            "reference": [
                "Raymond Lister, Colin Fidge, and Donna Teague. 2009. Further Evidence of a Relationship between Explaining, Tracing and Writing Skills in Introductory Programming. SIGCSE Bull",
                "Ronald L Wasserstein and Nicole A Lazar. 2016. The ASA statement on p-values: context, process, and purpose. The American Statistician",
                "Nea Pirttinen, Vilma Kangas, Irene Nikkarinen, Henrik Nygren, Juho Leinonen, and Arto Hellas. 2018. Crowdsourcing programming assignments with CrowdSorcerer",
                "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv",
                "Regina Hebig, Truong Ho-Quang, Rodi Jolak, Jan Schr\u00f6der, Humberto Linero, Magnus \u00c5gren, and Salome Honest Maro. 2020. How do Students Experience and Judge Software Comprehension Techniques",
                "Steve Oney, Christopher Brooks, and Paul Resnick. 2018. Creating Guided Code Explanations with Chat.Codes. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 131 (nov 2018) , 20 pages. https://doi.org",
                "Arto Vihavainen, Craig S Miller, and Amber Settle. 2015. Benefits of self-explanation in introductory programming",
                "Nea Pirttinen and Juho Leinonen. 2022. Can Students Review Their Peers? Comparison of Peer and Instructor Reviews",
                "Bas Cornelissen, Andy Zaidman, and Arie van Deursen. 2011. A Controlled Experiment for Program Comprehension through Trace Visualization. IEEE Transactions on Software Engineering",
                "Rui Zhi, Thomas W. Price, Samiha Marwan, Alexandra Milliken, Tiffany Barnes, and Min Chi. 2019. Exploring the Impact of Worked Examples in a Novice Programming Environment",
                "Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, and Ziheng Huang. 2022. Generating Diverse Code Explanations Using the GPT-3 Large Language Model",
                "Samiha Marwan, Nicholas Lytle, Joseph Jay Williams, and Thomas Price. 2019. The Impact of Adding Textual Explanations to Next-Step Hints in a Novice Programming Environment",
                "Stephen MacNeil, Andrew Tran, Arto Hellas, Joanne Kim, Sami Sarsa, Paul Denny, Seth Bernstein, and Juho Leinonen. 2023. Experiences from using code explanations generated by large language models in a web software development e-book",
                "Kenneth O McGraw and Seok P Wong. 1992. A common language effect size statistic. Psychological bulletin",
                "Judy Sheard, Angela Carbone, Raymond Lister, Beth Simon, Errol Thompson, and Jacqueline L. Whalley. 2008. Going SOLO to Assess Novice Programmers",
                "Paul Denny, Viraj Kumar, and Nasser Giacaman. 2023. Conversing with Copilot: Exploring prompt engineering for solving CS1 problems using natural language",
                "Zahid Ullah, Adidah Lajis, Mona Jamjoom, Abdulrahman Altalhi, Abdullah Al-Ghamdi, and Farrukh Saleem. 2018. The effect of automatic assessment on novice programming: Strengths and limitations of existing systems. Computer Applications in Engineering Education",
                "Solmaz Abdi, Hassan Khosravi, Shazia Sadiq, and Gianluca Demartini. 2021. Evaluating the Quality of Learning Resources: A Learnersourcing Approach. IEEE Transactions on Learning Technologies",
                "Henry B Mann and Donald R Whitney. 1947. On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics",
                "Kate Sanders, Judy Sheard, Brett A Becker, Anna Eckerdal, and Sally Hamouda. 2019. Inferential statistics in computing education research: A methodological review",
                "Philip J Guo. 2013. Online python tutor: embeddable web-based program visualization for cs education",
                "Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor O'Rourke. 2022. Bringing  \"High-Level \" Down to Earth: Gaining Clarity in Conversational Programmer Learning Goals",
                "Andrew Ettles, Andrew Luxton-Reilly, and Paul Denny. 2018. Common logic errors made by novice programmers",
                "Julie S Hui, Darren Gergle, and Elizabeth M Gerber. 2018. Introassist: A tool to support writing introductory help requests",
                "Juho Leinonen, Nea Pirttinen, and Arto Hellas. 2020. Crowdsourcing Content Creation for SQL Practice",
                "Jean M. Griffin. 2016. Learning by Taking Apart: Deconstructing Code by Reading, Tracing, and Debugging",
                "Dave S Kerby. 2014. The simple difference formula: An approach to teaching nonparametric correlation. Comprehensive Psychology 3 (2014) , 11-IT",
                "Teemu Lehtinen, Aleksi Lukkarinen, and Lassi Haaranen. 2021. Students Struggle to Explain Their Own Program Code",
                "Ron Sun, Edward Merrill, and Todd Peterson. 2000. Knowledge Acquisition Via Bottom-up Learning. Knowledge-Based Systems",
                "Brian Hanks, Sue Fitzgerald, Ren\u00e9e McCauley, Laurie Murphy, and Carol Zander. 2011. Pair programming in education: a literature review. Computer Science Education 21, 2 (2011) , 135-173. https://doi.org",
                "Margaret M. Reek. 1995. A Top-down Approach to Teaching Programming",
                "James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather. 2022. The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming",
                "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems",
                "Paul Denny, Andrew Luxton-Reilly, and Ewan Tempero. 2012. All Syntax Errors Are Not Equal",
                "Henrik Nygren, Juho Leinonen, Nea Pirttinen, Antti Leinonen, and Arto Hellas. 2019. Experimenting with model solutions as a support mechanism",
                "Stephen MacNeil, Zijian Ding, Kexin Quan, Thomas j Parashos, Yajie Sun, and Steven P Dow. 2021. Framing Creative Work: Helping Novices Frame Better Problems through Interactive Scaffolding",
                "Brett A. Becker, Paul Denny, James Finnie-Ansley, Andrew Luxton-Reilly, James Prather, and Eddie Antonio Santos. 2023. Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation",
                "Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. 2022. Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models",
                "Leigh Ann Sudol-DeLyser, Mark Stehlik, and Sharon Carver. 2012. Code Comprehension Problems as Learning Events",
                "A. Von Mayrhauser and A.M. Vans. 1995. Program comprehension during software maintenance and evolution. Computer",
                "Wengran Wang, Yudong Rao, Rui Zhi, Samiha Marwan, Ge Gao, and Thomas W. Price. 2020. Step Tutor: Supporting Students through Step-by-Step Example-Based Feedback",
                "Paul Denny, Andrew Luxton-Reilly, and Beth Simon. 2009. Quality of Student Contributed Questions Using PeerWise",
                "Siti-Soraya Abdul-Rahman and Benedict du Boulay. 2014. Learning programming via worked-examples: Relation of learning styles to cognitive load. Computers in Human Behavior 30 (2014) , 286-298. https://doi.org/10.1016/j.chb",
                "Paul Denny, Sami Sarsa, Arto Hellas, and Juho Leinonen. 2022. Robosourcing Educational Resources-Leveraging Large Language Models for Learnersourcing. arXiv preprint arXiv",
                "Honglin Wu, Fu Zhang, Jingwei Cheng, and Ke Wang. 2019/11. Determine Teaching Content using a Bottom-up Approach",
                "Jacqueline L. Whalley, Raymond Lister, Errol Thompson, Tony Clear, Phil Robbins, P. K. Ajith Kumar, and Christine Prasad. 2006. An Australasian Study of Reading and Comprehension Skills in Novice Programmers, Using the Bloom and SOLO Taxonomies",
                "Laurie Murphy, Sue Fitzgerald, Raymond Lister, and Ren\u00e9e McCauley. 2012. Ability to 'explain in Plain English' Linked to Proficiency in Computer-Based Programming",
                "Simon and Susan Snowdon. 2011. Explaining Program Code: Giving Students the Answer Helps - but Only Just"
            ],
            "related work": "2.Related Work 2.1.Code Comprehension Code comprehension skills are important for helping programming students understand the logic and functionality behind code snippets (Sudol-DeLyser et al., 2012). Programmers can employ various code comprehension strategies that give them flexibility in the ways they comprehend programming concepts  (Von Mayrhauser and Vans, 1995). Some strategies include trace execution (Cornelissen et al., 2011), explanations (Oney et al., 2018), and notional machines (Guo, 2013). These strategies take time and vary in effectiveness between students  (Hebig et al., 2020). Regardless, students may face roadblocks, including logical errors (Ettles et al., 2018) and syntactical errors (Denny et al., 2012) when trying to understand code. Top-down and bottom-up learning are two approaches to learning that focus on the big picture and the details, respectively (Wu et al., 1911). Top-down learning starts with the high-level concept and works its way down to the specifics, while bottom-up learning begins with the details and gradually works up to the high-level  (Sun et al., 2000). Both approaches can be useful when teaching complex topics, as they provide a way for learners to understand the whole concept by understanding its parts. In computer science and programming, these two approaches can be used to help learners understand the fundamentals of coding and programming (Reek, 1995). 2.2.Pedagogical Benefits of Code Explanations Explanations are vital teaching resources for students. Explanations help students develop their understanding of how a code snippet executes (Marwan et al., 2019), which can help students improve their reasoning about writing their own code (Murphy et al., 2012). They also reduce stress by breaking down complex concepts (Griffin, 2016). Early approaches for code explanation, such as the BRACElet project, provided students with 'explain-in-plain-English' type questions to encourage students to explain the purpose of their code at a higher level of abstraction (Whalley et al., 2006). This process of explaining one's own code provided both short and long-term learning benefits for students (Vihavainen et al., 2015; Murphy et al., 2012). In large classrooms, the process of explaining code can also be a collaborative activity where peers explain code to each other. This process can be more informal, such as in the case of pair programming when students explain their code and their thought process to a partner as they write their code (Hanks et al., 2011). Even though explaining code is an important skill and previous work has explored code explanation tasks, students are rarely exposed to example code explanations, especially ones created by their peers. Having easily available example code explanations could help expose students to code explanations, which could support learning to explain their own code. Having the instructor create such explanations is a time-consuming task. In big classrooms, it would be hard to find the time to provide personalized explanations for students (Ullah et al., 2018). Thus, studying if such explanations could be created at scale with the help of LLMs is a relevant research topic. 2.3.Large Language Models in CS Education The recent emergence of AI-based code generation models has sparked considerable interest within the field of computing education research (Becker et al., 2023). Initial studies in this area have primarily focused on evaluating the performance of these models when solving programming problems commonly encountered in introductory courses. A seminal study in this field, entitled \"The Robots are Coming\" (Finnie-Ansley et al., 2022), utilized the Codex model and a private repository of programming problems drawn from high-stakes summative assessments. The results of the study indicated that the solutions generated by Codex scored approximately 80% on the assessments, surpassing the performance of three-quarters of students when compared to historical course data. Similar work involving a public dataset of programming problems found that Codex produced correct solutions on its first attempt approximately half of the time, increasing to 80% when repeated attempts and minor adjustments to the input prompt were allowed  (Denny et al., 2023). In addition to evaluating performance, a complementary body of research has investigated the potential of AI-based code-generation models to generate learning resources. For example, Sarsa et al. explored various prompts and approaches for using the Codex model to generate code explanations and programming exercises, finding that it frequently produced novel and high-quality resources (Sarsa et al., 2022). However, their evaluation was conducted solely by experts and did not involve the use of resources by students in a practical setting. MacNeil et al. used the GPT-3 model to generate explanations of short code fragments which then were presented to students in an online e-book alongside the corresponding code (MacNeil et al., 2023). Although their evaluation was conducted on a small scale with approximately 50 participants, students found the explanations to be useful when they chose to engage with them. However, as the authors noted, this engagement was lower than anticipated, and the students were not involved in the creation of either the code examples or the accompanying explanations. The current study makes a unique contribution by directly comparing code explanations generated by students with those generated by AI models. While prior research has demonstrated that LLMs can produce explanations of code that are deemed high-quality by both experts and novices, this is the first study to investigate how students evaluate code explanations generated by their peers in comparison to those generated by AI models.",
            "date": "2023"
        },
        "topic": "Challenges of LLMs in Education",
        "year_start": "2023",
        "year_end": "2024",
        "target_list": [
            {
                "name": "A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education",
                "arxiv_id": "2402.17456",
                "subtitles": [
                    "Teaching Adolescents about Cyberbullying and Bystander Intervention",
                    "Teachers Creating Chatbots for Teaching",
                    "Creating Controllable LLM Chatbots"
                ],
                "reference": [
                    "Mobilizing bystanders of cyberbullying: an exploratory study into behavioural determinants of defending the victim",
                    "The situational-cognitive model of adolescent bystander behavior: Modeling bystander decision-making in the context of bullying and teen dating violence",
                    "Alone: A dataset for toxic behavior among adolescents on twitter",
                    "The unresponsive bystander: Why doesn't he help",
                    "Cyber-bystanding in context: A review of the literature on witnesses' responses to cyberbullying",
                    "Deepfake and digital citizenship: A long-term protection method for children and youth",
                    "Report on Indicators of School Crime and Safety",
                    "Bystanders' behavior in cyberbullying episodes: Active and passive patterns in the context of personal-socio-emotional factors",
                    "Bystander intervention in emergencies: diffusion of responsibility",
                    "PromptChainer: Chaining Large Language Model Prompts through Visual Programming",
                    "Cyberbullying Mitigation by a Proxy Persuasion of a Chat Member Hijacked by a Chatbot",
                    "Rethinking bullying interventions for high school students: A qualitative study",
                    "Effectiveness of Artificial Intelligence-Based Cyberbullying Interventions From Youth Perspective",
                    "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                    "Cyberbullying, school bullying, and psychological distress: A regional census of high school students",
                    "Co-design for a competency self-assessment chatbot and survey in science education",
                    "Understanding the Bystander Effect on Toxic Twitter Conversations",
                    "Best practices for prompt engineering with openai API",
                    "AI Can't Replace High-quality Teaching: Using the Technology as a Tool",
                    "A chatbot-based coaching intervention for adolescents to promote life skills: pilot study",
                    "An Approach to Co-Design and Self-Regulated Learning in Technological Environments. Systematic Review",
                    "Are children involved in cyberbullying low on empathy? A systematic review and meta-analysis of research on empathy versus different cyberbullying roles",
                    "'Can I afford to help?'How affordances of communication modalities guide bystanders' helping intentions towards harassment on social network sites",
                    "Sensemaking, Support, Safety, Retribution, Transformation: A Restorative Justice Approach to Understanding Adolescents' Needs for Addressing Online Harm",
                    "Can modern AI replace teachers? Not so fast! Artificial intelligence and adaptive learning: Personalized education in the AI age",
                    "Chatbots to support children in coping with online threats: Socio-technical requirements",
                    "Determinants of self-reported bystander behavior in cyberbullying incidents amongst adolescents",
                    "Bullying as a group process: Participant roles and their relations to social status within the group",
                    "Upstanding by design: Bystander intervention in cyberbullying",
                    " \"Thinking before posting? \" Reducing cyber harassment on social networking sites through a reflective message",
                    "Bullying in the new playground: Research into cyberbullying and cyber victimisation",
                    "Exploring the impact of AI on teacher leadership: regressing or expanding",
                    "The changing face of bullying: An empirical comparison between traditional and internet bullying and victimization",
                    "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts",
                    "Cyberbullying prevention and response: Expert perspectives",
                    "It's  \"mean, \" but what does it mean to adolescents? Relational aggression described by victims, aggressors, and their peers",
                    "Sketching nlp: A case study of exploring the right things to design with language intelligence",
                    "Cyberbullying among adolescent bystanders: Role of the communication medium, form of violence, and empathy",
                    "Language models are few-shot learners",
                    "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                    "Cyberbullying: Experiences, impacts and coping strategies as described by Australian young people",
                    "A systematic literature review of factors that moderate bystanders' actions in cyberbullying",
                    "Cyberbullying among young users of social networks",
                    "Why Johnny can't prompt: how non-AI experts try (and fail) to design LLM prompts",
                    "Herding AI Cats: Lessons from Designing a Chatbot by Prompting GPT",
                    "Deepfake: Creation, Purpose, Risks",
                    "An education-based approach to aid in the prevention of cyberbullying",
                    "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                    "Cyber bullying in ADHD and Asperger Syndrome populations",
                    "Peer victimisation and depressive symptoms: Can specific coping strategies buffer the negative impact of cybervictimisation"
                ],
                "related_work": "2.Related WorkThis section discusses the importance and difficulties of teaching about cyberbullying, as well as the current state of teacher-designed chatbots for this purpose.2.1.Teaching Adolescents about Cyberbullying and Bystander InterventionCyberbullying is a form of online aggression intentionally and repeatedly carried out against victims who are unable to defend themselves(Van Royen et al.,2017) . In contrast to offline bullying, cyberbullying can exhibit more complex social dynamics(Law et al.,2012) and incorporate, as part of their attacks, a rich array of media, such as texts, photos and videos(Li,2007) , and include manipulated imagery and deepfakes(Turan,2021; Busacca and Monaca,2023) . Because the power imbalance is at its heart, cyberbullying is known to further existing social inequalities and deplete the mental health of children and adolescents, especially those from minority groups(Kiriukhina,2019; Kowalski and Fedina,2011; Machmutow et al.,2012; Price and Dalgleish,2010; Schneider et al.,2012) . Addressing the needs of the adolescent victims goes beyond content moderation on social media platforms and requires a consideration of emotional impacts, victimization, and the involvement of social circles(Xiao et al.,2022) .Bystander intervention is widely recognized as a crucial antidote to cyberbullying and its disastrous effects on youth (see review(Dom\u00ednguez-Hern\u00e1ndez et al.,2018) ) . Many U.S. students experience bullying online(Irwin et al.,2021) , but only a small minority tell an adult or a school teacher(Patchin and Hinduja,2012) . In this context, whether bystanders choose to reinforce a bully, stay silent on the sidelines, or support the victim becomes especially important. Bystander actions can be public or private, subtle or direct, ranging from flagging the problematic comment to publicly defending the victim or confronting the bully(Salmivalli et al.,1996; DiFranzo et al.,2018) .To understand the problem of bystander inaction, researchers have conducted surveys(Patchin and Hinduja,2012) and qualitative studies such as interviews, focus groups, and controlled experiments(DeSmet et al.,2014,2012) . Most studies have drawn on Darley and Latane'sFive Stages of Bystander Interventionframework(Darley and Latan\u00e9,1968; Latan\u00e9 and Darley,1970) . According to this framework bystanders must first 1) notice the event, 2) appraise it as an emergency, 3) accept responsibility, 4) have the knowledge and skills on how to intervene, and 5) act. A related theoretical approach - the situational-cognitive model of bystander behavior(Casey et al.,2017) - extends the bystander intervention model by accounting for additional cognitive influences (e.g., attitudes toward intervening and perceived norms for intervening) , group affiliation factors, and target/perpetrator factors. These additional factors capture the influence of the social environment, which poses many perceived barriers to intervening in the eyes of adolescent bystanders.Indeed, previous research has shown that adolescent bystanders face challenges at almost every step leading to the bystander intervention action(Allison and Bussey,2016; Zych et al.,2019) . For example, they do not always appraise bullying as an emergency because the consequences of the incident for the victim, the offender, and other witnesses are often not instantly visible(Barli\u0144ska et al.,2013; Bastiaensens et al.,2015) . Adolescent bystanders receive little encouragement from their social environment to be upstanders(Dom\u00ednguez-Hern\u00e1ndez et al.,2018; Olenik-Shemesh et al.,2017) . Moreover, strong evidence indicates that their actions are highly dependent on contextual factors, such as social cues from peers and adult figures, that they are expected to act prosocially(DeSmet et al.,2014,2012) . In contrast to offline bullying, specific aspects of online interactions, such as its asynchronous nature and large community sizes, might further inhibit upstanding behavior(Allison and Bussey,2016) . Finally, youth often lack the skills to execute bystander intervention strategies in practice(DeSmet et al.,2012) .Considering the need for intervention and the difficulty the youth face in performing it, it is crucial that adolescents learn strategies for upstanding.Midgett et al.(2018) , e.g., created STAC, an educational program that teaches middle schoolers to develop knowledge of specific strategies to act as peer advocates. For example:  \"Accompany others \": Reaching out to and supporting students who were the target of bullying;  \"Coaching compassion \": Gently confronting the bully to foster empathy toward the victim and communicating that the bullying behavior is unacceptable.These speech acts exemplify how conversations can simultaneously provide knowledge and social guidance, thereby effectively improving bystander skills and behaviors. Further, by guiding the youth bystander through these steps, teachers could help the youth bystander practice multiple upstanding skills as the conversation unfolds. What strategy to use, however, depends on the student, and training activities are instrumental in helping students learn and practice appropriate strategies(Midgett et al.,2018) .2.2.Teachers Creating Chatbots for TeachingTo scale up successful conversational guidance like STAC, chatbots could become impactful educational tools. Conversational AI technology has the potential to provide personalized and empathetic guidance to adolescents, helping them become more effective prosocial bystanders. Just as one bystander's response to cyberbullying could empower others and help curb online aggression(Aleksandric et al.,2022; Bastiaensens et al.,2015; Allison and Bussey,2016) , a thoughtfully designed conversational AI system likewise has the potential to mobilize young people to intervene safely and effectively.Researchers have started creating proof-of-concept chatbots that teach youth bystander intervention strategies(Ueda et al.,2021; Gabrielli et al.,2020; Piccolo et al.,2021; Cohen et al.,2018; Milosevic et al.,2023) . These works, largely based on Wizard-of-Oz, have repeatedly shown that chatbots have the potential to guide youth bystanders to action, although none of the proposed chatbots have been implemented or evaluated with real users after a period of use. Despite its promises, bringing such conversation AI agents to the classroom still faces both conceptual and technical barriers.To achieve an impact in schools, chatbots need to fit into the larger curriculum and become part of the educational process. Researchers have been advocating for the inclusion of teachers in the design process of learning tools(Villatoro Moral and de Benito,2021) . A chatbot alone cannot replace a teacher, rather, it can enhance their teaching practice and should be seen as a new tool that supports teachers(Ghamrawi et al.,2023; Kolchenko,2018; Kupperstein,2023) . Furthermore, involving teachers in the design process has the potential to elevate their adoption of new technologies(Durall and Kapros,2020) . Thus, it is crucial that the viewpoint of the teacher is considered in the design and adoption process and that teachers are given control over the chatbots. The individual teacher needs to be able to adapt the chatbot so that it fits into their curriculum and becomes a useful aid to them.Building a chatbot to help youth upstand to cyberbullying is also challenging from an AI perspective. Adolescent cyberbullying is often characterized by relational aggression (e.g.,  \"You are not one of us! \") rather than explicit language(Pronk and Zimmer-Gembeck,2010; Wijesiriwardene et al.,2020) , making it harder to build AI to detect, much less respond to it appropriately. Moreover, the AI needs to be empathetic, engaging, and responsive to the teen's behaviors. It also needs to monitor and regulate the escalation of emotions, considering the sensitive nature of a conversation about cyberbullying. Furthermore, lack of data, limited ML performance, and canned responses have been a longstanding issue for chatbot interfaces(Yang et al.,2019; Kolchenko,2018) , and this is likely also limiting the advancement in chatbots for youth bystander intervention.2.3.Creating Controllable LLM ChatbotsTeacher-built chatbots based on large language models could address both of the aforementioned issues, providing better chatbots from a technical perspective while ensuring that the chatbot fits into the classroom.LLMs have revolutionized the field of Natural Language Processing (NLP) and could help overcome the aforementioned technical chatbot challenges. LLMs can better generalize to new domains requiring only a small set of instructions and examples of desired interactions, so-called prompts(Brown et al.,2020) . Prompting LLMs thus offers an exciting new approach to chatbot development, shifting the focus from a data question to a design question.While prompted LLMs advance the field of chatbot design, they also bring new challenges. A core issue is controlling the chatbot's behavior, where prompting seems even less reliable than the previous ML-based design approaches(Liu et al.,2021) . While guidelines for designing effective prompting exist(Shieh,2023; Bach et al.,2022) , understanding how prompts impact the output of LLMs remains an open research area in NLP(Sanh et al.,2021; Liu et al.,2021) . Particularly, non-AI-experts struggle when designing chatbots, suffering from both the fickleness of the prompting mechanisms(Zamfirescu-Pereira et al.,2023a) and misunderstanding the prompting capabilities, such as overgeneralizing from a single example(Zamfirescu-Pereira et al.,2023b) .LLM-Chains can make LLM-based chatbots more controllable but they need further evaluation. By chaining independently prompted LLM components together, the users feel more in control of the system(Wu et al.,2022b) . With PromptChainer(Wu et al.,2022a) non-AI-experts can visually design LLM-Chains, connecting LLM components in a structured flow and specifying the functionality of each component with examples. Participants in the PromptChainer study successfully built such chains, including those for a chatbot. This promising evidence suggests the utility of this approach for giving teachers control over LLM-based chatbots. However, the previous study only considered a simple music chatbot that processed one step of user interaction. What is currently missing is an evaluation of complex conversations, as one would expect from a dialogue about cyberbullying.The advancements in LLMs might make educational chatbots that help youth learn and practice upstanding skills a reality from a technical perspective, and LLM-Chains could potentially give teachers control over the chatbots so that they could use them in a way that fits their individual teaching and curriculum needs. This raises the question of how they want to utilize and control the chatbots for teaching about cyberbullying, how far LLM-Chains can already fulfill these requirements and what additional levers teachers need to make chatbots effective tools in their classroom. Answering these questions is our aim in this work.",
                "abstract": "Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities LLM-Chains offer for empowering teachers and the research opportunities this work opens up."
            },
            {
                "name": "How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee Responses",
                "arxiv_id": "2405.00970",
                "subtitles": [
                    "Significance of Feedback on Learning",
                    "Feedback Generation",
                    "Using Large Language Models for Feedback Generation"
                ],
                "reference": [
                    "The Impact of Feedback in Higher Education: Improving Assessment Outcomes for Learners",
                    "Generating diverse code explanations using the gpt-3 large language model",
                    "Feedback as open-ended conversation: Inviting students to coregulate and metacognitively reflect during assessment",
                    "Can automated feedback improve teachers' uptake of student ideas? evidence from a randomized controlled trial in a large-scale online course",
                    "Ontask: Delivering data-informed, personalized learning support actions",
                    "Retrieval-augmented generation to improve math question-answering: Trade-offs between groundedness and human preference",
                    "Learner-centred analytics of feedback content in higher education",
                    "Using large language models to provide explanatory feedback to human tutors",
                    "Immediate feedback and opportunity to revise answers to open-ended questions",
                    "Teacher moments: A digital simulation for preservice teachers to approximate parent-teacher conversations",
                    "Explanation feedback is better than correct answer feedback for promoting transfer of learning",
                    "An astonishing regularity in student learning rate",
                    "The power of feedback",
                    "Exploring automated distractor and feedback generation for math multiple-choice questions via in-context learning",
                    "Can large language models provide feedback to students? a case study on chatgpt"
                ],
                "related_work": "2Related Work2.1Significance of Feedback on LearningFeedback plays a crucial role in improving the students' learning outcomes and performance[10,9,14]. In the field of feedback research, theoretical models have been developed to explain the impact of feedback on learning and to identify the core principles that underpin effective feedback design.Hattie and Timperley[10]defined feedback as the information about the correctness of a learner's actions or decisions, along with explanations about why those actions or decisions are right or wrong, underlines the significance of feedback. As emphasized in their work[10], the influence of feedback on learning varies based on the type and timing of its delivery.Effective feedback should assist learners in understanding the rationale behind the feedback, which is crucial for deeper learning[9]. Moreover, including the correct answer within the feedback substantially enhances its efficacy by offering learners the information needed to correct their errors[11]. This is especially relevant when learners answer open-ended questions, as simply knowing that their response is incorrect may not suffice to improve their understanding[11]. By presenting the correct answer (or correct responses to open-ended question) in the feedback, learners can compare their responses with the correct responses, identify areas for improvement, and gain guidance on how to approach similar questions in the future[12,13]. To help learners identify their misconception in the open-ended question, we posit that it is necessary to include the correct responses in the feedback. However, providing timely explanatory feedback faces challenges since crafting effective explanatory feedback is often time-consuming and labor-intensive nature[14,8,6]. To address this issue, it is necessary to develop automated feedback generation system.2.2Feedback GenerationThe development of automated feedback has received significant attention from educational researchers[8,5,6,15,16]. For example, Ontask[15]is a rule-based feedback provision system designed to assist instructors in delivering personalized feedback based on specific conditions of learners (e.g., the duration spent on the learning system) . Additionally,Demszky et al.[16]developed a feedback system that automatically delivers explanatory feedback to instructors via email within two to four days after their tutoring sessions. Their study results[16]indicate that timely explanatory feedback enhanced learners' satisfaction.Lin et al.[5]used sequence labeling techniques to provide automated explanatory feedback, which demonstrated the potential of the large language models on identifying the effective components of feedback. Despite demonstrating the effectiveness of automated feedback systems, the provision of feedback with correct responses to open-ended question is still under-explored, which are needed to advance feedback systems2.3Using Large Language Models for Feedback GenerationInspired by recent research on using large language models for feedback generation[17,5,6,18,19,20], we posit that GPT-based large language models hold potential for advancing the development of automated feedback. For example,Dai et al.[8]investigated the capability of GPT-3.5 model (ChatGPT) to generate feedback for students' writing assignment and they[8]found that GPT-3.5 could produce feedback that was more readable than that of human instructors. Subsequently,Dai et al.[20]found that GPT-4 outperformed both GPT-3.5 and human instructors in providing effective feedback based on the feedback attributes proposed by[10]. Then,Hirunyasiri et al.[6]leveraged the GPT-4 model to provide timely feedback for human tutors' training. Their results[6]indicated that GPT-4 outperformed human educational experts in identifying a specific tutoring practice, giving effective praise. While these studies have demonstrated the feasibility of GPT-based models in feedback generation, none have ventured into generating explanatory feedback with correct responses to open-ended questions. Given that GPT-4 has shown remarkable performance on various educational tasks (e.g., generating high-quality answer responses for middle school math questions[17]and providing feedback for multiple-choice questions at the middle-school math level[18]) , our study also leveraged the GPT-4 model to further explore its capabilities in automatically generating explanatory feedback.",
                "abstract": "One-on-one tutoring is widely acknowledged as an effective instructional method, conditioned on qualified tutors. However, the high demand for qualified tutors remains a challenge, often necessitating the training of novice tutors (i.e., trainees) to ensure effective tutoring. Research suggests that providing timely explanatory feedback can facilitate the training process for trainees. However, it presents challenges due to the time-consuming nature of assessing trainee performance by human experts. Inspired by the recent advancements of large language models (LLMs), our study employed the GPT-4 model to build an explanatory feedback system. This system identifies trainees' responses in binary form (i.e., correct/incorrect) and automatically provides template-based feedback with responses appropriately rephrased by the GPT-4 model. We conducted our study on 410 responses from trainees across three training lessons: Giving Effective Praise, Reacting to Errors, and Determining What Students Know. Our findings indicate that: 1) using a few-shot approach, the GPT-4 model effectively identifies correct/incorrect trainees' responses from three training lessons with an average F1 score of 0.84 and an AUC score of 0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases incorrect trainees' responses into desired responses, achieving performance comparable to that of human experts."
            },
            {
                "name": "LLMs for Coding and Robotics Education",
                "arxiv_id": "2402.06116",
                "subtitles": [
                    "Large Language Models",
                    "Robot Coding for Education",
                    "LLMs in Robot Code Generation"
                ],
                "reference": [
                    "Educational robotics intervention on executive functions in preschool children: A pilot study",
                    "Proje yakla\u015f\u0131m\u0131n\u0131n anas\u0131n\u0131f\u0131na devam eden \u00e7ocuklar\u0131n problem \u00e7\u00f6zme becerilerine etkisinin incelenmesi",
                    "Put your robot in, put your robot out: Sequencing through programming robots in early childhood",
                    "Oecd future of education and skills 2030: Curriculum analysis",
                    "Okul \u00f6ncesinde kodlama e\u011fitimi ve kullan\u0131labilecek ara\u00e7lar hakk\u0131nda bili\u015fim teknolojileri \u00f6\u011fretmenlerinin g\u00f6r\u00fc\u015fleri: Bir durum \u00e7al\u0131\u015fmas\u0131",
                    "Y\u00f6nlendirilmi\u015f beyin f\u0131rt\u0131nas\u0131 (scamper) tekni\u011fine dayal\u0131 e\u011fitimin be\u015f ya\u015f \u00e7ocuklar\u0131n\u0131n problem \u00e7\u00f6zme becerilerine etkisinin incelenmesi",
                    "Tablets and apps for promoting robotics, mathematics, stem education and literacy in early childhood education",
                    "About first: Our mission, purpose & values (2024) , https://www",
                    "Introducing fundamental object-oriented programming concepts in preschool education within the context of physical science courses",
                    "Okul \u00f6ncesi ve temel fen e\u011fitiminde robotik destekli ve basit malzemelerle yap\u0131lan stem uygulamalar\u0131n\u0131n kar\u015f\u0131la\u015ft\u0131r\u0131lmas\u0131",
                    "code2seq: Generating sequences from structured representations of code",
                    "The effect of robotic coding education on preschoolers' problem solving and creative thinking skills",
                    "Codexglue: A machine learning benchmark dataset for code understanding and generation",
                    "Coding as a playground: Programming and computational thinking in the early childhood classroom",
                    "What is first lego league? (2024) , https:https://www",
                    "Designing unplugged and plugged activities to cultivate computational thinking: An exploratory study in early childhood education",
                    "Evaluating large language models trained on code",
                    "Exploring the effects of  \"productive children: coding and robotics education program \" in early childhood education",
                    "The effect of steam-based unplugged play activities using robots on the improvement of children's creative and social personalities",
                    "Improving students' science process skills through simple computer simulations on linear motion conceptions",
                    "Coding in early childhood",
                    "Developing computational thinking in compulsory education-implications for policy and practice",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Codelmsec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models",
                    "Educating the engineer of 2020: Adapting engineering education to the new century",
                    "Aries hilton's pyscratch: A software for converting python files to scratch projects (2023) , https://medium",
                    "How we fine-tuned llama 2 for our block coding copilot (2023) , https://www",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Language models are few-shot learners",
                    "Research shows first drives stem engagement and outcomes (2023) , https://www",
                    "Okul \u00f6ncesi \u00f6\u011fretim program\u0131na algoritma ve kodlama e\u011fitimi entegrasyonunun \u00f6\u011frencilerin problem \u00e7\u00f6zme becerisine etkisi",
                    "Learning computational thinking and social skills development in young children through problem solving with educational robotics"
                ],
                "related_work": "2Related Work2.1Large Language ModelsThe introduction of GPT-3 by Brown et al.[6]provided an innovative basis for subsequent LLM applications in coding, demonstrating the model's ability to generate human-like text, including code. This work laid the groundwork for specialized models like Codex[10], further tailored for programming languages and code generation tasks. Similarly, Devlin et al.[13]introduced BERT, which, although not specifically designed for code, has inspired successive adaptations like CodeBERT by Feng et al.[16], merging natural language understanding with code semantics.Following the path blazed by Codex, Alon et al.[3]proposed the Code2Seq framework, which represents a novel approach to generating code summaries and assisting in code documentation by converting code into a sequence of tokens. This model emphasizes the importance of understanding the structural and syntactical implications of programming languages in LLM applications. Additionally, Lu et al.[33]introduced CodeGPT, a variation of GPT specifically fine-tuned for code generation tasks, showcasing improvements in generating syntactically correct and logically coherent code snippets.LLMs, such as OpenAI's Codex[10], which powers GitHub Copilot, have significantly enhanced developer productivity by providing code suggestions and completions. Codex, built on the GPT-3 model, demonstrates an advanced ability to generate programming code based on natural language prompts, thereby accelerating the software development process and reducing manual coding effort. Research on utilizing LLMs for debugging and error correction has shown promising results. For instance, a model like CodeBERT[16], designed to understand and generate code from natural language, can be applied to identify syntax errors and logical bugs within software projects, providing a foundation for automated debugging tools. The security of code generated by LLMs and the mitigation of biases within these models are critical concerns. Research efforts such as those by Hajipour et al.[19]have focused on developing methodologies to ensure the generation of secure code and to identify potential biases in model outputs, aiming to establish best practices for the safe and ethical use of LLMs in software development.2.2Robot Coding for EducationIn recent years, numerous organizations have dedicated efforts to identify essential life skills for students in educational environment, which inspires plenty of researches in this area. As a result, these skills are summarized as 21st century skills, and a basic frame for these skills has been attempted to be designed[8]. Organisation for Economic Co-operation and Development (OECD) attempt to divide the skills into three categories: Creating new value (being able to develop new perspectives) , reconciling tensions and dilemmas (being able to think in a systematic way) , as well as taking responsibility (self-regulation, self-efficacy, self-control, and problem-solving) [39].Among required skills, Learning and Innovation Skills (creativity and innovation, critical thinking and problem-solving, communication, collaboration) are regraded as one of the most important realms for development of teenagers. They promise the success of students in such a information explosion century by reaching and filtering valuable information efficiently, innovating on current platform and cooperating within a team. Problem-solving involves children's understanding on concepts, discovering the concepts and values independently and developing them[38]. To empower young people for problem-solving, previous methods include structured or unstructured gaming activities, brainstorming, project-based education and etc[8,44,35]. Recently, coding has become a systematic method in terms of solving problems, which prompt teenagers to improve critical thinking, computational thinking and innovative thinking[5,27,34]. More robotics and coding instruction, commonly preferred in primary and secondary education, has been used as a teaching technique in preschool education[8].[12]observed that after a brief period of five weeks engaged in unplugged coding activities, a group of 5-year-old children demonstrated marked progress in the development of creative and social personality traits. Other researches show that teaching basic practices for robot and coding supports sequencing skills in children[7,28,37]. Teenagers who take part in robotics and coding activities outperform in skills vary from visual-spatial, working memory, inhibitory control to problem-solving[2,29,14].However, coding is an abstract and complicated method because coding itself can be defined as a problem-solving procedure. Robotics combines software code design as well as robotic structure, which requires even higher comprehensive capability. The difficulty of coding and robotics becomes the barrier hindering young children from learning. For this reason, numerous educational platforms have been developed to facilitate learning in coding and robotics. Such platforms include WeDo, Lego, Bee-bot, Clementino, and Robokids. Generally, these platforms are equipped with coding kits and designed with a visual interface, making them more user-friendly and easier for young children to learn. Children consider them easy and funny to learn, thus promoting their passion.Other challenges lie on how to provide concrete, meaningful and problem-based learning activities to teach coding to young children[15].[4]demonstrates the importance of proper coding and robotics activities. It also emphasizes the participation of children in terms of playing with robots, exploring unknown regions , socializing with others and creating innovative solutions[30]. Besides, coding and robotics education is a new concept for early childhood teachers and has not been comprehensively integrated into most early childhood curricula[9]. All of these existing problems motivate this paper to investigate the possibility of applying LLMs in robotics coding to provide assistance to both children and teachers.2.3LLMs in Robot Code Generation2.3.1FIRST Tech ChallengeThe FIRST Tech Challenge (FTC) is a dynamic, global robotics program that ignites passion in young minds for science, technology, engineering, and mathematics (STEM) . The FTC robot code is typically written in Java. Established in 2005, FTC provides a platform for students in grades 4-18 to engage in hands-on robotics challenges, fostering invaluable skills in problem-solving, teamwork, and innovation[22]. By designing, building, and programming robots, participants are immersed in real-world engineering experiences, competing in alliances against teams at local, regional, and international levels[22].The program is a flagship initiative of For Inspiration and Recognition of Science and Technology (FIRST) , a non-profit organization founded by inventor Dean Kamen in 1989. FIRST's mission is to inspire young people to become leaders in science and technology. FTC plays a crucial role in providing a platform which is accessible and inclusive, regardless of a participant's background or experience level[22].The impact of FTC extends beyond the technical skills. Research indicates that participants are significantly more likely to attend college and major in a STEM field, demonstrating the program's effectiveness in shaping the next generation of STEM professionals[21]. Moreover, FTC's emphasis on teamwork, communication, and leadership prepares students for future challenges, aligning with educational goals around the globe[24].2.3.2FIRST LEGO LeagueFIRST LEGO League (FLL) is an internationally recognized, innovative program that ignites enthusiasm for discovery, science, and technology in young minds. Designed to inspire and challenge students aged 4 to 16, the league combines the hands-on fun of LEGO building with real-world engineering and problem-solving challenges[23]. Originating from a partnership between FIRST and the LEGO Group, FLL provides a platform for children to learn critical skills while engaging in playful and meaningful competition.The structure of FLL is centered around theme-based Challenges that teams must navigate through. In these Challenges, participants build and program autonomous robots using LEGO MINDSTORMS technology to score points on a thematic playing surface, creating innovative solutions to a problem as part of their research project. The process fosters valuable life skills and competencies such as problem-solving, teamwork, and creative thinking.FLL block diagrams are a fundamental component of the FLL robotics experience, offering a visual and intuitive way to program LEGO MINDSTORMS robots. These diagrams are designed to be user-friendly, ensuring that even individuals without prior programming experience can engage with and understand the basics of robot programming. The essence of FLL block diagrams lies in their drag-and-drop interface, where users can select from a variety of command blocks - each representing a specific action or decision in the robot's operation. These blocks can be pieced together to form a sequence of instructions, creating a 'flow' of actions that the robot will execute.2.3.3Cooperating LLMs in Robot CodingUnlike traditional text-based coding languages like Python or Java, visual block coding platforms like Tynker, Scratch, MakeCode, and Snap operate at a higher level of abstraction. They often transcend standard programming constructs like loops and conditionals. For example, a single block like  \"move n pixels \" could equate to 20 lines of C code. Beginners would need to navigate the intricacies of graphics libraries, variable tracking, and angular calculations to achieve the same result[11]. Therefore, companies are leveraging LLMs to facilitate the interaction between the robot and the young learners, allowing for more intuitive and meaningful interactions. For example, Aries Hilton's PyScratch is a software that can convert Python files to Scratch projects, which are interactive stories, games, and animations that can be shared online. PyScratch uses LLMs to map the Python commands and arguments to the corresponding Scratch blocks and inputs, and also converts the Pygame images to SVG files. PyScratch can help Python learners to create Scratch projects without having to learn a new language, and also help Scratch users to learn Python by seeing how their projects can be translated into code[20].[11]trained a model specifically designed to autocomplete the next block of code in a Tynker visual block coding project based on the BERT in 2018 using their expansive repository of millions of user-published projects. With the emergence of ChatGPT, they guide ChatGPT into generating block-like code that could be converted into Tynker's native coding language to help kids effortlessly translate their ideas into code, understand any given code's functionality, and provide real-time debugging assistance. Further more, they fine tune a Llama2 chat model to meet Tynker's specialized visual coding requirements.",
                "abstract": "Large language models and multimodal large language models have revolutionized artificial intelligence recently. An increasing number of regions are now embracing these advanced technologies. Within this context, robot coding education is garnering increasing attention. To teach young children how to code and compete in robot challenges, large language models are being utilized for robot code explanation, generation, and modification. In this paper, we highlight an important trend in robot coding education. We test several mainstream large language models on both traditional coding tasks and the more challenging task of robot code generation, which includes block diagrams. Our results show that GPT-4V outperforms other models in all of our tests but struggles with generating block diagram images."
            },
            {
                "name": "\"The teachers are confused as well\": A Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education",
                "arxiv_id": "2401.12453",
                "subtitles": [
                    "LLMs in Higher Education",
                    "LLMs in Computer Science Education",
                    "Ethics and Governance of LLMs in Computer Science Education"
                ],
                "reference": [
                    "Mapping the global evidence around the use of ChatGPT in higher education: A systematic scoping review",
                    "Artificial hallucinations in ChatGPT: implications in scientific writing",
                    "ChatGPT as a Software Development Tool: The Future of Development",
                    "Academic integrity: a review of the literature",
                    "Empowering education with llms-the next-gen interface and content generation",
                    "Challenges for Computer Science Education Arising from new AI Systems like ChatGPT",
                    "Towards understanding and mitigating social biases in language models",
                    "Toward Ethical Use of Generative AI in AP Courses",
                    "Exploring Computer Science Students' Perception of ChatGPT in Higher Education: A Descriptive and Correlation Study",
                    "Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration",
                    "The Promises and Pitfalls of ChatGPT as a Feedback Provider in Higher Education: An Exploratory Study of Prompt Engineering and the Quality of AI-Driven Feedback",
                    "Exploring the opportunities and challenges of NLP models in higher education: is Chat GPT a blessing or a curse",
                    "Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools",
                    "Comparing Google and ChatGPT as Assistive Tools for Students in Solving Programming Exercises",
                    "ChatGPT performance on MCQ exams in higher education. A pragmatic scoping review",
                    "Exploring the ethical considerations of using Chat GPT in university education",
                    "Virtual Classrooms and Real Harms: Remote Learning at {{\\{{US}}\\}}. Universities",
                    "The impact of Google Assistant on adolescent EFL learners' willingness to communicate",
                    "ChatGPT A Challenging Tool for the University Professors in Their Teaching Practice",
                    "From \" Ban it till we understand it \" to \" Resistance is futile \": How university programming instructors plan to adapt as more students use AI code generation and explanation tools such as ChatGPT and GitHub Copilot",
                    "Drivers and Consequences of ChatGPT Use in Higher Education: Key Stakeholder Perspectives",
                    "Unlocking the opportunities through ChatGPT Tool towards ameliorating the education system",
                    " \"How technical do you get? I'm an English teacher \": Teaching and Learning Cybersecurity and AI Ethics in High School",
                    "Exploring the Role of AI Assistants in Computer Science Education: Methods, Implications, and Instructor Perspectives",
                    "Public perceptions of gender bias in large language models: Cases of chatgpt and ernie",
                    "\\\\\\backslash\\ \" Call me Kiran\\\\\\backslash\\ \"-ChatGPT as a Tutoring Chatbot in a Computer Science Course",
                    "Evaluating Large Language Models Trained on Code",
                    "Exploring the use of chatgpt as a tool for learning and assessment in undergraduate computer science curriculum: Opportunities and challenges",
                    "Chatgpt participates in a computer science exam",
                    "Bias assessment and mitigation in llm-based code generation",
                    "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
                    "Towards human-bot collaborative software architecting with chatgpt",
                    "First 100 days of ChatGPT at Australian universities: An analysis of policy landscape and media discussions about the role of AI in higher education",
                    "Can Home Use of Speech-Enabled Artificial Intelligence Mitigate Foreign Language Anxiety - Investigation of a Concept",
                    "Generating Diverse Code Explanations using the GPT-3 Large Language Model",
                    "Sparks: Inspiration for Science Writing using Language Models",
                    "Bridging the Programming Skill Gap with ChatGPT: A Machine Learning Project with Business Students",
                    "Exploring the usage of ChatGPT in higher education: Frequency and impact on productivity",
                    "Application of ChatGPT in Higher Education and Research-A Futuristic Analysis",
                    "Prompting higher education towards AI-augmented teaching and learning practice",
                    "ChatGPT and the Future of Work: A Comprehensive Analysis of AI's Impact on Jobs and Employment",
                    "The Perception by University Students of the Use of ChatGPT in Education",
                    "A Moral-and Event-Centric Inspection of Gender Bias in Fairy Tales at A Large Scale",
                    "Navigating Generative AI (ChatGPT) in Higher Education: Opportunities and Challenges",
                    "Chatgpt and software testing education: Promises & perils",
                    "AI Accountability Policy",
                    "Challenges and Opportunities of AI-Assisted Learning: A Systematic Literature Review on the Impact of ChatGPT Usage in Higher Education",
                    "Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation",
                    "Code Quality and Large Language Models in Computer Science Education: Enhancing student-written code through ChatGPT",
                    "To use or not to use ChatGPT in higher education? A study of students' acceptance and use of technology",
                    "Augmented intelligence in programming learning: Examining student views on the use of ChatGPT for programming learning",
                    "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                    "On the educational impact of ChatGPT: Is Artificial Intelligence ready to obtain a university degree",
                    "GPT-3-driven pedagogical agents for training children's curious question-asking skills",
                    "Exposing gaps, exploring legacies: paradoxes of writing use in computing education",
                    "AI in higher education: A literature review of chatgpt and guidelines for responsible implementation",
                    "ChatGPT and the Pedagogical Challenge: Unveiling the Impact on Early-Career Academics in Higher Education",
                    "Towards Automated Generation and Evaluation of Questions in Educational Domains",
                    "Privacy governance not included: analysis of third parties in learning management systems",
                    "Role of ChatGPT in Computer Programming.: ChatGPT in Computer Programming",
                    "ChatGPT: A Good Computer Engineering Student?: An Experiment on its Ability to Answer Programming Questions from Exams",
                    "A Cross-Disciplinary Examination of the Instructional Uses of ChatGPT in Higher Education",
                    "Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education",
                    "Detecting llm-generated text in computing education: A comparative study for chatgpt cases",
                    " \" It's a Fair Game \", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",
                    "War of the chatbots: Bard, Bing Chat, ChatGPT, Ernie and beyond. The new AI gold rush and its impact on higher education",
                    "AI-assisted learning with ChatGPT and large language models: Implications for higher education",
                    "Factors influencing the acceptance of ChatGPT usage among higher education students in Bangkok, Thailand",
                    "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
                    "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
                    "Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models",
                    "Academic integrity and artificial intelligence: is ChatGPT hype, hero or heresy",
                    "A comprehensive Examination of the potential application of Chat GPT in Higher Education Institutions",
                    "Bias Testing and Mitigation in LLM-based Code Generation",
                    "The CLEAR path: A framework for enhancing information literacy through prompt engineering",
                    "Let's Do It Ourselves: Ensuring Academic Integrity in the Age of ChatGPT and Beyond",
                    "The impact of ChatGPT on higher education",
                    "Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception",
                    "The Future of Grading Programming Assignments in Education: The Role of ChatGPT in Automating the Assessment and Feedback Process",
                    " \" I'm Not Confident in Debiasing AI Systems Since I Know Too Little \": Teaching AI Creators About Gender Bias Through Hands-on Tutorials"
                ],
                "related_work": "2.Related WorkWe situate this work in two fields: (1) LLMs in higher education and (2) LLMs in CS education, including ethics and governance discussions.2.1.LLMs in Higher EducationLLMs in education is an emerging HCI topic area(Zhou et al.,2023b) . The literature is relatively immature; LLMs only made the leap to widely available and powerful chatbot products around November 2022, when ChatGPT was released(Fui-Hoon Nah et al.,2023) . Notably, nearly all of the research that surfaced with the keywords {LLM, large language model, generative AI} concerned ChatGPT, suggesting its wide adoption and impact in the previous year. This area of research is closely related to broader fields of AI scholarship, spanning privacy/security education; FATE (fairness, accountability, transparency, and ethics in AI) ; and so on(Malinka et al.,2023; Mischak et al.,2023; Orenstrakh et al.,2023; Singh et al.,2023; Felkner et al.,2023; Huang et al.,2023; Alkaissi and McFarlane,2023; Lau and Guo,2023; Gupta et al.,2023; Zhang et al.,2023; George et al.,2023; Zhou et al.,2022,2023a; Kilhoffer et al.,2023a) .Situated among broader studies of LLMs in education(Abdelghani et al.,2023; Moore et al.,2023; Javaid et al.,2023) , researchers have considered LLMs in the higher education context(Laato et al.,2023; Ansari et al.,2023; Aithal and Aithal,2023; Perera and Lankathilaka,2023; Wang,2023; Vargas-Murillo et al.,2023; Fuchs,2023; Tajik and Tajik,2023; Dempere et al.,2023) . Researchers have used surveys and interviews to understand how students(Ngo,2023; Shaengchart et al.,2023; Strzelecki,2023) and instructors(Kiryakova and Angelova,2023; Baskara et al.,2023) use LLMs in various use cases. Other topics include: benchmarking LLM capabilities vs. each other and humans(Rudolph et al.,2023b; Newton and Xiromeriti,2023) ; optimization strategies (e.g., prompt engineering(Lo,2023) ) ; ethical challenges(Huallpa et al.,2023) ; and LLM governance/policy(Fowler et al.,2023) .Learning with LLMs.Given the many use cases of LLMs in educational settings, both discovered and not, many studies investigated or proposed novel ways students use LLMs and the potential benefits. A systematic literature review revealed notable benefits of utilizing ChatGPT in higher education, including research support, and automated grading(Dempere et al.,2023) . For example, LLMs can serve as alanguage tool. LLMs were found to help international students cope with language anxieties by serving as  \"conversational companions \"(Bao,[n.d.]) . LLMs were found to promote English as a Foreign Language (EFL) learners' willingness to communicate(Tai and Chen,2023) . LLMs also seem to enhance curiosity-driven learning and serve to promote curiosity expression(Bao,[n.d.]; Tai and Chen,2023) . Students noted benefits of LLMs, such as time efficiency, convenient information access, personalized tutoring and feedback, and writing assistance(Ngo,2023) . On the other hand, concerns about LLMs in higher education included plagiarism, digital literacy gaps, and AI-induced anxiety(Dempere et al.,2023) . Students expressed concerns about LLM inaccuracy, such as hallucinated citations and incorrect idiom usage(Ngo,2023) .Teaching with LLMs.LLMs can assist teachers with many routine and creative tasks(Firaina and Sulisworo,2023) . University instructors perceived LLMs as a means to support time-consuming teaching activities, stimulate critical thinking and creativity, and engage learners(Kiryakova and Angelova,2023) . Early-career lecturers found LLMs beneficial to enhance their ability to construct complicated academic arguments, improve efficiency in research and teaching, and enhance critical thinking capabilities(Baskara et al.,2023; Firaina and Sulisworo,2023) . LLMs were found useful to help formulate learning goals if provided high-quality prompts(Jacobsen and Weber,2023) . Researchers have explored fine-tuning language models for crafting quality data science learning materials(Bhat et al.,2022) . Onal and Kulavuz-Onal found ChatGPT effective in writing assessment questions and tasks, provided that instructors carefully evaluate the reliability and accuracy of the LLM's output text(Onal and Kulavuz-Onal,2023) . Simultaneously, challenges emerged, such as initial technical hurdles, occasional incorrect outputs, and concerns about over-dependency(Baskara et al.,2023; Firaina and Sulisworo,2023) . Professors were concerned that (1) unethical LLM use threatened the validity and fairness of assessment practices, and (2) learners would blindly trust LLMs without validating their text outputs, harming the acquisition of knowledge and skills.Overall, LLMs are a double-edged sword. Education stakeholders including students, faculty, and education experts/leaders agreed that LLMs can be readily used or misused(Hasanein and Sobaih,2023) . Ethical harms, like plagiarism, and practical benefits, like supportive learning environments, are both foreseeable outcomes of further LLM use(Eager and Brunton,2023) .2.2.LLMs in Computer Science EducationCS education has particularities that make it valuable to explore. For example, CS education often covers AI ethics as a topic(Kilhoffer et al.,2023b) ; classes often emphasize both coding and writing as part of learning and assessment(Dansdill et al.,2008) . Lastly, there is a high likelihood that CS students will become tomorrow's AI architects and developers.LLMs are valuable but imperfect tools for coding tasks.LLMs can generate usable code upon receiving natural language prompts regarding code requirements(Liu et al.,2023) , or produce natural language explanations from code prompts(MacNeil et al.,2022,2022) - a valuable task if the intention is to learn about coding. Additional uses include resolving bugs(Biswas,2023) , improving code quality (e.g., fixing formatting issues, refactoring) (Backstr\u00f6m and Kihlert,2023) , and even helping with complex software architecting tasks(Ahmad et al.,2023) . CS instructors could benefit from many applications, like using LLMs for grading programming tasks(Jukiewicz,[n.d.]) .However, research suggests limitations for LLM-produced code, requiring verification from human coders(Liu et al.,2023) . At least two studies found LLMs performed well in code generation but not software testing(Loubier,2023; Jalil et al.,2023) . Achieving good results for long/complex programming tasks required careful and repeated prompting. Otherwise, code inconsistencies and truncations caused problems(Qureshi,2023) . In light of these limitations, researchers have considered how to improve LLM results for coding tasks. Strategies in the literature have included iteratively prompting and validating responses(Rajala et al.,2023) , prompt engineering(Gero et al.,2022; Qadir,2022) , and meta-prompting(Reynolds and McDonell,2021) . Kumar et al. tested different instructor guidance strategies for using LLMs on coding tasks. The authors emphasized the nuance of interactions between undergrad CS students, instructors, and LLMs, suggesting that helpful and unhelpful interactions are both possible(Kumar et al.,2023) . Thus, instructors must provide considerable guidance for beginner coders to succeed in using LLMs as coding tools.Coding without groking.Researchers have considered how LLMs serve as a functional and/or educational coding tool. Numerous studies suggest a tradeoff; LLMs may help students code better and more efficiently(Biswas,2023; Backstr\u00f6m and Kihlert,2023; Ahmad et al.,2023) , but also inhibit their understanding of programming or AI concepts(Reiche and Leidner,2023; Arias Sosa and Godow,2023; Wang et al.,2023) . The issue seems related to the fact that LLMs allow  \"doing coding \" without the cognitive effort required for understanding. One study suggests that current LLM debates echo earlier research on automated hints for CS education - ultimately, educators must consider the balance between allowing LLMs for immediate coding help and considerations for long-term learning(Kumar et al.,2023) .LLMs for noobs.Studies indicate that coding experience significantly impacts trust in and reliance on LLMs for coding tasks; in particular, LLMs facilitate coding but may harm beginners' efforts to learn coding. Wang et al. interviewed CS instructors, who believed that 1st-year students often trust and use LLM-generated code with little consideration. Instructors therefore felt concerned that  \"students with underdeveloped mental models may experience shallow learning when using AI assistants \"(Wang et al.,2023) . One industry study considered how software developers used LLMs for coding, finding that junior developers were more likely to trust and rely on LLM-generated code(H\u00f6rnemalm,2023) . Laato et al. argue that over-reliance on ChatGPT for programming can hinder students' learning of basic routine programming skills(Laato et al.,2023) . Further, Arias et al. separated first-year CS students into two groups - one using ChatGPT and one using Google - in an experiment comparing their performance on programming tasks(Arias Sosa and Godow,2023) . To that end, the ChatGPT group performed programming tasks better and faster. However, the group using Google performed better in a test of their comprehension. The authors observed that the ChatGPT group tended to copy and paste code rather than think, which hindered their learning. This evidence suggests LLMs are effective tools if the goal is to succeed in coding tasks, but overcoming challenges too easily may interfere with students' learning opportunities. This distinction is important if the goal is for students to understand and complete coding tasks on their own, as is more likely the case for early CS students. Still, overly restrictive guardrails are inadvisable as beginner coders also value LLMs for Q&A, improving thinking skills, and building their confidence - not merely generating code that  \"works \"(Yilmaz and Yilmaz,2023) .2.3.Ethics and Governance of LLMs in Computer Science EducationMany ethical topics arise with LLMs in CS education. Here we outline several important issues already identified.Academic integrityissues have a long history in higher education and academia(Macfarlane et al.,2014) - LLMs likely complicate the landscape. Evidence thus far suggests LLMs have a complex, disruptive impact on assessment and other elements of academic integrity. This will likely require adapting curricula and/or governance strategies. To start, LLMs can be easily used to cheat in CS courses(Malinka et al.,2023) . Traditional plagiarism checkers will not typically work because LLMs generate unique texts. Moreover, detecting LLM-generated content remains difficult, and existing solutions are inaccurate(Mischak et al.,2023) . One study evaluated state-of-the-art LLM-content detectors, finding they generally perform poorly but are especially inaccurate with programming code and non-English text(Orenstrakh et al.,2023) . Azoulay et al. proposed combining educational initiatives, plagiarism detection mechanisms, and watermarking techniques to regulate AI-produced code(Azoulay et al.,2023) .Hallucination/inaccuracy.Currie et al. found that ChatGPT was prone to hallucinating information, posing a risk to professionalism, ethics, and integrity(Currie,2023) . Multiple researchers expressed concern about equity within higher education; for example, if the free and paid versions of ChatGPT perform differently, students with fewer resources may fall behind in learning and assessment outcomes(Lau and Guo,2023; Bordt and von Luxburg,2023) .Biasis prevalent in LLMs(Zhou and Sanfilippo,2023) , originating in training data and appearing in generated text(Liang et al.,2021; Bender et al.,2021) . LLM code also contains bias; code may contain offensive stereotypes in variable names or strings(Becker et al.,2023; Bird et al.,2022; Chen et al.,2021) , or privilege certain people over others (e.g.,  \"write code that decides if someone gets social security benefits \"(Huang et al.,2024) ) . Countermeasures may include carefully prompting ChatGPT or explicitly teaching students about bias.Privacyis an important consideration as LLM users constantly encounter trade-offs between privacy, utility, and convenience(Zhang et al.,2023) . In a student survey (n=400) in Thailand, Shaengchart et al. found that privacy and security were not significantly related to students' acceptance of ChatGPT in educational contexts(Shaengchart et al.,2023) . This finding could suggest that students lack awareness of LLM vulnerabilities. However, it is unclear how the survey asked about privacy and security and how the responses were operationalized. Recent research on applications of emerging technology in higher education, including video conferencing and AI-based academic integrity tools, are inadequately governed with respect to privacy under the auspices of the Family Educational Rights and Privacy Act (FERPA) (Cohney et al.,2021; Sanfilippo et al.,2023) . Many states have sought to fill in the regulatory gaps left by FERPA, with 128 different laws in total(Cohney et al.,2021) . However, this leaves significant uncertainty and inconsistency in privacy protections concerning LLMs in educational settings.",
                "abstract": "Large Language Models (LLMs) are advancing quickly and impacting people's lives for better or worse. In higher education, concerns have emerged such as students' misuse of LLMs and degraded education outcomes. To unpack the ethical concerns of LLMs for higher education, we conducted a case study consisting of stakeholder interviews (n=20) in higher education computer science. We found that students use several distinct mental models to interact with LLMs - LLMs serve as a tool for (a) writing, (b) coding, and (c) information retrieval, which differ somewhat in ethical considerations. Students and teachers brought up ethical issues that directly impact them, such as inaccurate LLM responses, hallucinations, biases, privacy leakage, and academic integrity issues. Participants emphasized the necessity of guidance and rules for the use of LLMs in higher education, including teaching digital literacy, rethinking education, and having cautious and contextual policies. We reflect on the ethical challenges and propose solutions."
            },
            {
                "name": "Towards Integrating Emerging AI Applications in SE Education",
                "arxiv_id": "2405.18062",
                "subtitles": [
                    "Identifying AI Trends in Research and Education",
                    "Related Work"
                ],
                "reference": [
                    "Chatbots applications in education: A systematic review",
                    "Artificial intelligence in education: A review",
                    "Computing education in the era of generative AI",
                    "Systematic review of research on artificial intelligence applications in higher education-where are the educators",
                    "How chatgpt will change software engineering education",
                    "Positive artificial intelligence in education (P-AIED) : A roadmap",
                    "Toward computer science curricular guidelines 2023 (cs",
                    "Artificial intelligence meets software engineering in computing education",
                    "Interacting with educational chatbots: A systematic review",
                    "SWEBOK Guide Version 4.0 beta",
                    "A review of artificial intelligence (AI) in education from 2010 to"
                ],
                "related_work": "2   Study Design & Related WorkIn this section, we first provide a brief motivation for the need to systematically introduce AI in SE education, and further discuss existing related work in this area.2.1Identifying AI Trends in Research and EducationTo gain better insights into current trends - particularly within the past year - we started a structured literature analysis for both research and educational venues in software engineering. We specifically chose to include both research and education venues, hoping this will provide a more comprehensive understanding of current research trends and the extent to which they have been integrated into SE education. Furthermore, we aim to document challenges educators need to be aware of and, as a result, potentially adapt didactic concepts and/or teaching materials. In this short paper, we report on our work in progress and the first results of this analysis. For this purpose, we discuss initial findings (cf. Section3) and potential applications and opportunities (cf. Section4) .To enable a more structured analysis, we further aim to categorize our findings alongside the different SE activities and areas. For SE, numerous taxonomies and classification schemes have been proposed, most notably the Software Engineering Body of Knowledge (SEBoK) [17], with its 18 knowledge areas. As our primary focus, however, is on SE education, we have chosen to group our findings according to the ACM Computer Science Curriculum Guidelines version \"Gamma \"(CS2023) , comprising 17 different knowledge areas, which also incorporate - to a very large extent - the areas proposed in the SEBoK. The ACM Computer Science Curriculum Guidelines are designed to assist educators in the area of computer science, describing the general structure, as well as listing important topics to cover, core and elective CS courses[32]. Most notably, the most recent version, CS2023, currently discussed by the ACM task force, will take into account competency-based education, incorporating a competency model, comprising required skills, and knowledge to be acquired by students. Introducing Learning Analytics and competency-based education in classes typically requires a significant effort and is also something we see huge potential for leveraging AI and alleviating the burden of, for example, manually assessing if all teaching materials correspond to specified competencies (cf. Section4) .While CS2023 addresses a broad and comprehensive education, covering 17 different knowledge areas, ranging from computer science foundations, to AI, and parallel computing, we focus on two main areas: Software Development Fundamentals (SDF) that cover basic programming education, and second, Software Engineering (SE) . A brief overview of the sub-categories for these two areas is shown in Table1. The goal of our work is to identify existing work and approaches for each sub-category of the two areas and collect different means of how AI can be incorporated, for example in teaching aspects of software architecture, such as design patterns, or architectural styles. For this purpose, as a starting point, to gain an initial understanding of current research and trends, we collected an initial set of research papers from 2023, of two prime software engineering conferences: the International Conf. of Software Engineering, and the International Conf. on the Foundations of Software Engineering, as well as four educational conferences/tracks: Software Engineering Education and Training Track@ICSE, the International Conf. on Software Engineering Education and Training, the Hawaii International Conf. on System Sciences, and the Conf. on Innovation and Technology in Computer Science Education.The goal was to identify trends and hot topics and uncover areas that might be covered by research but not yet as part of the CS and SE education. In total, we started with a set of 51 publications (33 research-related and 18 education-related) , which was extended to 71 after performing an extra round of snowballing. We focused on publications that presented AI applications or educational concepts in the area of SE and programming, but excluded any kind of study, analysis, or review that did not present a specific approach. After carefully reading the titles and abstracts, we excluded 17 papers that were deemed out of scope (e.g. tertiary studies) , with a final set of 54 papers remaining.After selecting our set of papers, we divided them randomly, with four researchers evaluating the papers independently. For the research papers, we focused on the purpose (e.g., test case generation) , the technology used, and the specific application area. (Please note, that for the purpose of this paper, and for the sake of brevity, we mainly focus on the intent, and the relation to the CS2023 categories, but future work will broaden the scope of this analysis) . For education papers, we also focused on the CS2023 categories, and tried to relate them to the research areas - and ultimately identify gaps and areas that are not yet considered in education. Based on this analysis, we then selected the respective ACM curriculum categories (i.e., Knowledge Units) where the proposed approach, tool, or research could be relevant or applied. After this step, we consolidated all assessments, discussed and resolved conflicts until mutual agreement was achieved, and tried to identify clusters of tools and approaches, as well as gaps. The final goal of the study is to derive a set of concrete actionable guidelines for different courses part of the CS curriculum, with a set of tools and approaches that could be used, and aspects that need to be considered. In the following sections, we discuss the initial assignment of approaches to the CS2023 Knowledge Units and potential application scenarios.2.2Related WorkSeveral systematic reviews have investigated and classified aspects of AI applications in education. Bittencourtet al.[4]conducted an analysis exploring the intersection of positive psychology and AI in education (P-AIED) , identifying P-AIED as a new global movement focusing on positive emotion and engagement related to AI-supported teaching and learning. Kuhailet al.[21]and Okonkwo and Ade-Ibijola[29]performed studies related to the application of chatbots in education. Kuhailet al.analyzed work on educational chatbots and found that these are primarily used in computer science, language learning, general education, and to a lesser extent in fields like engineering and mathematics. Most chatbots were web-based and more than half functioned as teaching agents, while over a third acted as peer agents. Okonkwo and Ade-Ibijola analyze 53 primary studies and identify an increasing trend in the application of chatbots in education.Zhaiet al.[41]analyzed 100 papers focusing on the application of AI in the education sector from 2010 to 2020. They classified primary studies into a development layer (classification, matching, recommendation, and deep learning) , an application layer (feedback, reasoning, and adaptive learning) , and an integration layer (affection computing, role-playing, immersive learning, and gamification) . Chenet al.[6]performed a study investigating the impact of AI on education, focusing on its application and effects in administrative, instructional, and learning contexts. Similarly, Zawacki-Richteret al.[40]conducted a systematic review of research on the application of AI in higher education. The majority of research comes from the fields of Computer Science and STEM with AI being mainly used in areas of student profiling and predictive analytics, as well as intelligent tutoring systems.Several studies have focused on AI in computer science education. Dennyet al.[9]explore the challenges and opportunities presented by recent advances in AI, particularly code generation models, and their potential impact on computing education. Kalles[19]reports on the experience of using AI systems as the basis of educating IT students. The application areas were mainly decision tree lifecycle management and board game learning mechanisms. Daun and Brings[8]discuss the role of generative AI technologies, such as ChatGPT, in SE education. They discuss potential risks associated with students' use of generative AI but also identify several opportunities that these technologies can offer in enhancing educational practices such as individualization of education and personalized feedback.Current work in this regard primarily addresses only parts of SE education, for example, programming. With our work, we aim to cover a broader spectrum of SE education, leveraging the ACM Computer Science Curriculum Guidelines.",
                "abstract": "Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas."
            },
            {
                "name": "Desirable Characteristics for AI Teaching Assistants in Programming Education",
                "arxiv_id": "2405.14178",
                "subtitles": [
                    "LLMs in Computing Education",
                    "Student and TA interactions in CS"
                ],
                "reference": [
                    "Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses",
                    "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses",
                    "Studying the Effect of AI Code Generators on Supporting Novice Learners in Introductory Programming",
                    "An Exploration of Student-Tutor Interactions in Computing",
                    "Inside the Mind of a CS Undergraduate TA: A firsthand account of undergraduate peer tutoring in computer labs",
                    "Comparing Code Explanations Created by Students and Large Language Models",
                    "Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models",
                    "More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems",
                    "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models",
                    "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book",
                    "Investigating the Potential of GPT-3 in Providing Feedback for Programming Assessments",
                    "Learning gain differences between ChatGPT and human tutor generated algebra hints",
                    " \"It's Weird That it Knows What I Want \": Usability and Interactions with Copilot for Novice Programmers",
                    "Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language",
                    "Generating Multiple Choice Questions for Computing Courses Using Large Language Models",
                    "Challenges Faced by Teaching Assistants in Computer Science Education Across Europe",
                    "The Robots are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming",
                    "Student Expectations of Tutors in Computing Courses",
                    "Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests",
                    "CodeT: Code Generation with Generated Tests",
                    "The Robots Are Here: Navigating the Generative AI Revolution in Computing Education",
                    "Evaluating LLM-generated Worked Examples in an Introductory Programming Course",
                    "An empirical evaluation of GitHub copilot's code suggestions"
                ],
                "related_work": "2.Related work2.1.LLMs in Computing EducationLLMs have been extensively studied in the context of computing education(Prather et al.,2023a) . There is a growing body of work focusing on the capabilities of LLMs for solving programming exercises(Finnie-Ansley et al.,2022; Denny et al.,2023; Hou et al.,2024a; Savelka et al.,2023b) , as well as answering(Savelka et al.,2023a) and generating programming-related MCQs(Tran et al.,2023) . Other studied tasks include bug detection(Macneil et al.,2024) , test suite authoring(Chen et al.,2022) , automatic generation of programming exercises(Sarsa et al.,2022) , code explanations(Leinonen et al.,2023; MacNeil et al.,2023) , and worked examples(Jury et al.,2024) , and the ways that students interact with code-generating LLMs when programming(Kazemitabaar et al.,2023; Prather et al.,2023b) . When employed as digital TAs, it is not clear if LLMs are as effective as human tutors(Savelka et al.,2023a; Nguyen and Nadi,2022) . For example, a study in the context of mathematics education by Pardos and Bhandari found that significant learning gains were only observed for students who were provided human-created hints, and not for those who were given LLM-generated hints(Pardos and Bhandari,2023) . There remains much ongoing discussion and work exploring the quality and completeness of feedback generated by LLMs in computing education contexts(Hellas et al.,2023; Balse et al.,2023) .2.2.Student and TA interactions in CSPrior work has shown that student and TA interactions are challenging for both students and tutors.Krause-Levy et al.examined interactions between students and undergraduate TAs during help-seeking sessions focused on debugging assignments in introductory computing courses(Krause-Levy et al.,2022) . They found that the TAs often gave students the solution without teaching them the underlying concepts of the process of debugging. This behavior results in students leaving tutoring sessions with resolved issues but potentially without a deeper understanding of how to independently tackle similar problems in the future.Human TAs face challenges providing effective help for students because of a need to triage and prioritize multiple student requests, engage students effectively, and balance teaching with time constraints(Markel and Guo,2021; Riese et al.,2021) . In addition, TAs must deal with their own understanding of the material, unprepared students, and diagnosing student knowledge to provide appropriate assistance(Riese et al.,2021) .Most directly related to this work is that ofLim et al.(2023) who explored student expectations of human TAs. The main findings from interviews with students in a CS2 course indicate that student expectations and desires can be misaligned with their learning goals, particularly under the pressure of imminent assignment deadlines. While students acknowledge the importance of understanding and learning from tutors, many prefer direct answers or solutions as deadlines approach to prioritize completing assignments over deeper learning. Additionally, students do not always value the importance of identifying their own bugs and may expect tutors to do it for them, potentially creating a reliance on tutors and compromising long-term learning.",
                "abstract": "Providing timely and personalized feedback to large numbers of students is a long-standing challenge in programming courses. Relying on human teaching assistants (TAs) has been extensively studied, revealing a number of potential shortcomings. These include inequitable access for students with low confidence when needing support, as well as situations where TAs provide direct solutions without helping students to develop their own problem-solving skills. With the advent of powerful large language models (LLMs), digital teaching assistants configured for programming contexts have emerged as an appealing and scalable way to provide instant, equitable, round-the-clock support. Although digital TAs can provide a variety of help for programming tasks, from high-level problem solving advice to direct solution generation, the effectiveness of such tools depends on their ability to promote meaningful learning experiences. If students find the guardrails implemented in digital TAs too constraining, or if other expectations are not met, they may seek assistance in ways that do not help them learn. Thus, it is essential to identify the features that students believe make digital teaching assistants valuable. We deployed an LLM-powered digital assistant in an introductory programming course and collected student feedback ($n=813$) on the characteristics of the tool they perceived to be most important. Our results highlight that students value such tools for their ability to provide instant, engaging support, particularly during peak times such as before assessment deadlines. They also expressed a strong preference for features that enable them to retain autonomy in their learning journey, such as scaffolding that helps to guide them through problem-solving steps rather than simply being shown direct solutions."
            },
            {
                "name": "Educational Personalized Learning Path Planning with Large Language Models",
                "arxiv_id": "2407.11773",
                "subtitles": [
                    "Large Language Models",
                    "Learning Path Planning"
                ],
                "reference": [
                    "Claret: Pre-training a correlation-aware context-to-event transformer for event-centric generation and classification",
                    "Eventbert: A pre-trained model for event correlation reasoning",
                    "The shaky foundations of clinical foundation models: A survey of large language models and foundation models for emrs",
                    "A comprehensive overview of large language models",
                    "Modeling event-pair relations in external knowledge graphs for script reasoning",
                    "Building contextual knowledge graphs for personalized learning recommendations using text mining and semantic graph completion",
                    "Improving zero-shot cross-lingual transfer for multilingual question answering over knowledge graph",
                    "Is attention all you need? toward a conceptual model for social awareness in large language models",
                    "FOKE: A personalized and explainable education framework integrating foundation models, knowledge graphs, and prompt engineering",
                    "Improving cross-modal alignment for text-guided image inpainting",
                    "Constructing a personalized learning path using genetic algorithms approach"
                ],
                "related_work": "2Related WorkIn this section, we review the related work in the domains of Large Language Models and Learning Path Planning, highlighting the advancements and contributions that have shaped the current landscape of personalized learning systems.2.1Large Language ModelsLarge Language Models (LLMs) have revolutionized the field of natural language processing with their ability to understand and generate human-like text. Recent surveys and comprehensive reviews have highlighted the capabilities and impact of LLMs like GPT-4 and LLama-2-70B in various domains, including educationZhou et al. (2022a,2021a) ; Naveed et al. (2023) ; Wornow et al. (2023) . These models leverage extensive training data and sophisticated architectures, such as transformers with attention mechanisms, to perform a wide range of language tasks with high accuracy and efficiencyVoria et al. (2024) ; Zhou et al. (2022b) ; Zhou and Long (2023) .Prompt engineering has emerged as a powerful technique to harness the potential of LLMs by crafting specific input prompts that guide the model's behavior without altering its underlying parameters. This approach has been shown to enable zero-shot and few-shot learning, reducing the need for task-specific training data and preserving the model's general knowledgeHu and Wang (2024) . In the context of education, prompt engineering can generate adaptive learning content and personalized feedback by incorporating learner-specific information into the prompts, making LLMs a valuable tool for personalized learning path planningHu and Wang (2024) ; Zhou et al. (2021b) .2.2Learning Path PlanningLearning Path Planning (LPP) focuses on creating tailored educational experiences that adapt to the unique needs, preferences, and prior knowledge of individual learners. Traditional approaches to LPP, such as those using genetic algorithms, have been employed to optimize learning paths by considering factors like topic difficulty, duration, and learner performanceElshani and Nu\u00e7i (2021) . These methods aim to enhance learning efficiency and outcomes by providing a sequence of educational content that is best suited to the learner's current state and goals.Recent advancements in LPP have leveraged knowledge graphs, machine learning, and artificial intelligence to create more dynamic and responsive learning environments. For instance, contextual knowledge graphs built using text mining and semantic graph completion techniques can provide personalized learning recommendations by analyzing the relationships between different learning conceptsAbu-Rasheed et al. (2024) . Moreover, integrating AI with educational frameworks, such as the FOKE framework, has demonstrated the potential to create interactive and explainable learning experiences that adapt in real-time to learner feedback and performanceHu and Wang (2024) .Overall, the integration of LLMs with advanced LPP techniques presents a promising avenue for developing sophisticated personalized learning systems that can significantly improve learner engagement and achievement.",
                "abstract": "Educational Personalized Learning Path Planning (PLPP) aims to tailor learning experiences to individual learners' needs, enhancing learning efficiency and engagement. Despite its potential, traditional PLPP systems often lack adaptability, interactivity, and transparency. This paper proposes a novel approach integrating Large Language Models (LLMs) with prompt engineering to address these challenges. By designing prompts that incorporate learner-specific information, our method guides LLMs like LLama-2-70B and GPT-4 to generate personalized, coherent, and pedagogically sound learning paths. We conducted experiments comparing our method with a baseline approach across various metrics, including accuracy, user satisfaction, and the quality of learning paths. The results show significant improvements in all areas, particularly with GPT-4, demonstrating the effectiveness of prompt engineering in enhancing PLPP. Additional long-term impact analysis further validates our method's potential to improve learner performance and retention. This research highlights the promise of LLMs and prompt engineering in advancing personalized education."
            },
            {
                "name": "Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments",
                "arxiv_id": "2404.01754",
                "subtitles": [
                    "Large Language Models",
                    "Automated Program Repair"
                ],
                "reference": [
                    "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation",
                    "Search, align, and repair: data-driven feedback generation for introductory programming exercises",
                    "Verifix: Verified Repair of Programming Assignments",
                    "Deepfix: Fixing common c language errors by deep learning",
                    "Practical program repair in the era of large pre-trained language models",
                    "Angelix: Scalable multiline program patch synthesis via symbolic analysis",
                    "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
                    "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "A syntax-guided edit decoder for neural program repair",
                    "The ManyBugs and IntroClass benchmarks for automated repair of C programs",
                    "Incoder: A generative model for code infilling and synthesis",
                    "StarCoder: may the source be with you",
                    "GPT-J-6B: A 6 billion parameter autoregressive language model",
                    "Automated clustering and program repair for introductory programming assignments",
                    "sk_p: a neural program corrector for MOOCs",
                    "Self-collaboration Code Generation via ChatGPT",
                    "A feasibility study of using automated program repair for introductory programming assignments",
                    "Evaluating large language models trained on code",
                    "Genprog: A generic method for automatic software repair",
                    "Cure: Code-aware neural machine translation for automatic program repair",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "Automated feedback generation for introductory programming assignments",
                    "Semfix: Program repair via semantic analysis",
                    "Language Models are Few-Shot Learners",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Automated repair of programs from large language models",
                    "Attention is all you need",
                    "Defects4J: A database of existing faults to enable controlled testing studies for Java programs",
                    "Re-factoring based Program Repair applied to Programming Assignments",
                    "Llama: Open and efficient foundation language models",
                    "Repairing bugs in python assignments using large language models",
                    "Gpt-neox-20b: An open-source autoregressive language model",
                    "ChatGPT: Optimizing Language Models for Dialogue",
                    "JFIX: semantics-based repair of Java programs via symbolic PathFinder",
                    "TBar: Revisiting template-based automated program repair",
                    "QuixBugs: A multi-lingual program repair benchmark set based on the Quixey Challenge"
                ],
                "related_work": "2.Related Works&Background2.1.Large Language ModelsLarge language models (LLMs) are a category of large-scale models that have been pre-trained on a massive textual corpus with self-supervised pre-training objectives(Devlin et al.,2018; Nijkamp et al.,2022) . Most LLMs are built on the Transformer architecture(Vaswani et al.,2017) , and can be categorized into three groups: encoder-only models(Devlin et al.,2018) , decoder-only models(Brown et al.,2020) , and encoder-decoder models(Raffel et al.,2020) . Later, researchers further incorporated reinforcement learning to align LLMs with human intent. One notable LLM, OpenAI's ChatGPT(OpenAI,2022) , applies reinforcement learning from human feedback (RLHF) to optimize the model. This approach has garnered significant attention due to its remarkable ability to tackle a wide range of tasks. In the meanwhile, various open-source LLMs have emerged in the AI community. Notable contributions include GPT-NeoX(Black et al.,2022) , GPT-J(Wang and Komatsuzaki,2021) , InCoder(Fried et al.,2022) , CodeGen(Nijkamp et al.,2022) , LLaMA(Touvron et al.,2023a) , StarCoder(Li et al.,2023) , Code Llama(Roziere et al.,2023) ,etc. These models also have showcased remarkable progress in various tasks. It was worth noting that LLMs are highly effective in few-shot learning scenarios, which can successfully perform tasks for which they were not explicitly trained by providing only a few (or no) examples during the inference. This approach is commonly known as prompt-based learning(Liu et al.,2023) . A prompt generally refers to a predefined textual template that is given as an input instruction to the LLM, and it typically consists of a query and may include a few examples (also known as shots) of the task. LLM can generate results of corresponding tasks based on the prompt. There have been recent studies exploring ChatGPT and other open-source LLMs' potential in software engineering tasks, such as program repair(Fan et al.,2023; Xia and Zhang,2023) , code generation(Dong et al.,2023) , unit test generation(Yuan et al.,2023) ,etc.2.2.Automated Program Repair2.2.1.General Purpose Program RepairAutomated Program Repair (APR) techniques are capable of identifying and fixing software bugs or defects without human intervention, aiming to reduce the effort required to improve software quality. A variety of APR approaches have been proposed across the years, including search-based methods, semantic-based approaches, and Deep Learning-based techniques. The search-based approaches first produce patches with predefined code transformation operators, and then search for correct patches among the pre-defined space(Le Goues et al.,2011; Liu et al.,2019) . The search-based repair approaches demonstrate the ability to handle large programs. However, they may struggle when confronted with expansive search spaces. The semantic-based tools usually correct code errors via symbolic execution(Nguyen et al.,2013; Mechtaev et al.,2016; Le et al.,2017) . Specifically, they need to build a constraint that a program must meet to pass a given test suite, and then solve the constraint to generate the patches. Deep Learning-based APR approaches predominantly rely on Neural Machine Translation (NMT) techniques(Gupta et al.,2017; Jiang et al.,2021; Zhu et al.,2021) , which consider the program repair as a translation task, where the goal is to transform defective code into correct code. However, the effectiveness of NMT-based APR tools heavily relies on the quality and abundance of training data. More recently, researchers have begun to directly harness the capabilities of LLMs for APR(Xia and Zhang,2023; Xia et al.,2022; Fan et al.,2023) , which have surpassed the existing state-of-the-art methods across various program repair tasks. For instance, ChatRepair(Xia and Zhang,2023) , which is an APR tool powered by ChatGPT, fixes 114 bugs on Defects4J 1.2, with 15 more than the previous state-of-the-art APR tools. These tools work by first creating an input prompt with the original buggy code as well as the task description. Then they query the LLM to either fill in the correct code at the bug location or generate a new code snippet as the patch.2.2.2.Programming Assignment Program RepairResearchers have extended the application of APR to student programs in education, providing feedback to help students correct their programs(Singh et al.,2013; Gulwani et al.,2018; Wang et al.,2018) . AutoGrader(Singh et al.,2013) is an early feedback generation system that takes an erroneous student program, a reference solution, and a set of potential corrections provided by the instructor as inputs. It employs program synthesis techniques to find minimal corrections. CLARA(Gulwani et al.,2018) and sk_p(Pu et al.,2016) employed clustering and machine learning techniques to generate repairs for the assignment data. However, these repairs frequently exhibit imprecision and lack minimality, thereby diminishing the overall quality of the feedback. Verifix(Ahmed et al.,2021) aims to generate verifiable correct program repairs by aligning their assignments with a reference solution in terms of control flow. They utilize failed verification attempts to obtain potential repairs through the MaxSMT solver. More recently, researchers have begun to employ LLMs to fix bugs in student assignments.Zhang et al.(2022) built an APR system based on Codex(Chen et al.,2021) for introductory Python programming assignments. The empirical results on 286 Python programs demonstrated that their system outperforms other tools.Xia and Zhang (2023) proposed a conversation-driven APR approach based on ChatGPT(OpenAI,2022) , which utilized instant feedback to perform patch generation in a conversational style. They evaluated both their approach on the Defects4J(Just et al.,2014) and QuixBugs(Lin et al.,2017) dataset, and the results demonstrated their approach obtains the superior result on both datasets. However, there is still limited understanding regarding LLM's effectiveness in fixing bugs in advanced student programming assignments.2.2.3.Benchmarks for Programming Assignment Program RepairExisting programming assignment-oriented approaches to APR focus on fixing bugs in C and Python. The widely used datasets in C are IntroClass(Le Goues et al.,2015) and ITSP(Yi et al.,2017) . IntroClass is collected from an introductory C programming class with an enrollment of about 200 students. It consists of 998 submitted programs with 572 unique defects out of 1143 defects in total (sometimes the students submitted identical code, thus there may exist identical defects) . The average line of code is relatively short with around 20 lines of code. ITSP consists of 661 buggy-repaired program pairs submitted by students from an introductory C programming course for 74 assignments. For Python, the dataset used inHu et al.(2019b) is collected from an introductory Python programming course credited by 361 students, and contains 2442 correct and 1783 buggy program attempts with an average lines of code of 14. Another dataset used inGulwani et al.(2018) consisting of student attempts from MITx MOOC is not publicly available.",
                "abstract": "Automated generation of feedback on programming assignments holds significant benefits for programming education, especially when it comes to advanced assignments. Automated Program Repair techniques, especially Large Language Model based approaches, have gained notable recognition for their potential to fix introductory assignments. However, the programs used for evaluation are relatively simple. It remains unclear how existing approaches perform in repairing programs from higher-level programming courses. To address these limitations, we curate a new advanced student assignment dataset named Defects4DS from a higher-level programming course. Subsequently, we identify the challenges related to fixing bugs in advanced assignments. Based on the analysis, we develop a framework called PaR that is powered by the LLM. PaR works in three phases: Peer Solution Selection, Multi-Source Prompt Generation, and Program Repair. Peer Solution Selection identifies the closely related peer programs based on lexical, semantic, and syntactic criteria. Then Multi-Source Prompt Generation adeptly combines multiple sources of information to create a comprehensive and informative prompt for the last Program Repair stage. The evaluation on Defects4DS and another well-investigated ITSP dataset reveals that PaR achieves a new state-of-the-art performance, demonstrating impressive improvements of 19.94% and 15.2% in repair rate compared to prior state-of-the-art LLM- and symbolic-based approaches, respectively"
            },
            {
                "name": "Evaluating the Effectiveness of LLMs in Introductory Computer Science Education: A Semester-Long Field Study",
                "arxiv_id": "2404.13414",
                "subtitles": [
                    "Intelligent Tutoring Systems",
                    "Large Language Models in CS Education"
                ],
                "reference": [
                    "M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1: 1 Instruction and Student Outcomes",
                    "The generalized intelligent framework for tutoring (GIFT",
                    "Emergent abilities of large language models",
                    "Cognitive Tutor: Applied research in mathematics education",
                    "Comparing code explanations created by students and large language models",
                    "Chatgpt and software testing education: Promises & perils",
                    "Automatic generation of programming exercises and code explanations using large language models",
                    "Effectiveness of intelligent tutoring systems: a meta-analytic review",
                    "A web-based bayesian intelligent tutoring system for computer programming",
                    "Intelligent tutoring systems",
                    "AI-assisted coding: Experiments with GPT",
                    "Experiences with Remote Examination Formats in Light of GPT",
                    "A natural language intelligent tutoring system for training pathologists: Implementation and evaluation",
                    "Bloomberggpt: A large language model for finance",
                    "Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models",
                    "Gpt-4 technical report",
                    "The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems",
                    "The robots are here: Navigating the generative ai revolution in computing education",
                    "Codehelp: Using large language models with guardrails for scalable support in programming classes",
                    "Intelligent tutoring systems: an overview",
                    "Design considerations of an intelligent tutoring system for programming languages",
                    "Robosourcing Educational Resources-Leveraging Large Language Models for Learnersourcing",
                    "Using large language models to enhance programming error messages",
                    "ChatGPT for good? On opportunities and challenges of large language models for education",
                    "Large language models in medicine",
                    "A flowchart-based intelligent tutoring system for improving problem-solving skills of novice programmers",
                    "Why Johnny can't prompt: how non-AI experts try (and fail) to design LLM prompts",
                    "SYNSHINE: improved fixing of syntax errors",
                    "Adaptive educational systems on the world-wide-web: A review of available technologies",
                    "The future landscape of large language models in medicine",
                    "Profiling the Dynamics of Trust & Distrust in Social Media: A Survey Study",
                    "ELM-ART: An intelligent tutoring system on World Wide Web",
                    "Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant",
                    "Large language models (gpt) struggle to answer multiple-choice questions about code",
                    "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
                    "Trustllm: Trustworthiness in large language models"
                ],
                "related_work": "2.Related Work2.1.Intelligent Tutoring SystemsUsing computerized tools for assisting educational purposes is not a new idea. As early as the 1950s, the first concept of using computers to assist learning has already emerged(Nwana,1990) . From where the factor of intelligence had been considered and it had started evolving intoIntelligent Tutoring Systems (ITS) (Sleeman and Brown,1982) . ITS leverages artificial intelligence to provide personalized learning experiences in computer science education, adapting instruction and feedback to individual student needs(Anderson et al.,1985; Elsom-Cook,1984) . These systems have enhanced student engagement, comprehension, and problem-solving skills by offering tailored support and immediate feedback, similar to one-on-one tutoring(VanLehn,2011; Demszky and Liu,2023) . Research has demonstrated that ITS can significantly improve understanding of complex concepts in programming courses compared to traditional teaching methods, leading to higher student satisfaction due to the personalized learning environment(Corbett et al.,1997; Ritter et al.,2007) . The Internet also empowered ITS to offer more interactivity and adaptivity(Brusilovsky et al.,1996,1998; Butz et al.,2006) , leveraging the path of later boost with natural language processing techniques(El Saadawi et al.,2008; Hooshyar et al.,2015) .However, prior work has shown that as the granularity of tutoring decreases, its effectiveness increases(VanLehn,2011) . Significant limitations for ITS include the complexity and cost of building them, the incapability to answer questions and tasks out of their programmed domains, and the difficulty to develop with the purpose of productively used by individuals without expertise(Graesser et al.,2018) . Even though the Generalized Intelligent Framework for Tutoring (GIFT) framework(Sottilare et al.,2012) was proposed and evolved for developing ITS for use at scale, those limitations mostly remain unresolved.2.2.Large Language Models in CS EducationThe release of ChatGPT and other Generative AI applications brought LLMs into the public view and attracted enormous attention(Achiam et al.,2023; Sun et al.,2024) . LLMs offer researchers and users the flexibility to employ a single tool across various tasks(Wei et al.,2022) , such as medical research(Thirunavukarasu et al.,2023; Clusmann et al.,2023) , finance(Wu et al.,2023a) , and education(Kasneci et al.,2023) . Adopting LLM-powered tools in educational settings is facilitated by their broad accessibility and cost-free nature(Zamfirescu-Pereira et al.,2023) . Recent studies have looked into the potential of AI assistants to enhance student learning by helping with students' problem-solving(Ahmed et al.,2022; Leinonen et al.,2023b; Phung et al.,2023a) and generating computer science content(Sarsa et al.,2022; Denny et al.,2022) . Current research on the use of LLMs in education has primarily looked into their performance and capabilities(Prather et al.,2023) compared to humans, such as generating code for programming tasks(Leinonen et al.,2023a; Poldrack et al.,2023) , answering general inquriries(Savelka et al.,2023; Phung et al.,2023b) , addressing textbook questions(Jalil et al.,2023) and exam questions(Dobslaw and Bergh,2023) .Despite the growing interest in examining the capabilities of LLMs in education, very few empirical studies have examined the emerging concerns regarding their impact. Therefore, there is an urgent need for research into the long-term effects of LLMs in CS education and the development of strategies to counteract potential negative consequences. One exceptional work was conducted by Liffiton et al.(Liffiton et al.,[n.d.]) , who developed a tool called CodeHelp for assisting students with their debug needs in an undergraduate course over 12 weeks. Their follow-up study(Sheese et al.,2023) categorized history messages in their tool, and found a positive relationship between tool usage and course performance. However, their study specifically focused on debugging issues and did not compare the outcomes with those achieved through traditional TA methods.Furthermore, prior research has demonstrated that individual differences, such as gender, race, and prior experiences with technologies, significantly influence the effectiveness of intelligent tutoring systems(Kulik and Fletcher,2016) . However, work that examines how individual differences affect interactions with and perceptions of LLM-powered tools in educational settings is sparse, even though understanding the role of demographic and individual variability is crucial(Zhang et al.,2024) . This is particularly important for developing inclusive and effective educational tools that suit the diverse needs of students.Our work seeks to address these research gaps by conducting a field study that evaluates the use of LLM-powered tools for an extended period of time. Particularly, our study not only aims to evaluate the practicality of LLMs in programming learning educational contexts, but also intends to contribute to a more nuanced understanding of their long-term implications for learning and teaching methodologies.",
                "abstract": "The integration of AI assistants, especially through the development of Large Language Models (LLMs), into computer science education has sparked significant debate. An emerging body of work has looked into using LLMs in education, but few have examined the impacts of LLMs on students in entry-level programming courses, particularly in real-world contexts and over extended periods. To address this research gap, we conducted a semester-long, between-subjects study with 50 students using CodeTutor, an LLM-powered assistant developed by our research team. Our study results show that students who used CodeTutor (the experimental group) achieved statistically significant improvements in their final scores compared to peers who did not use the tool (the control group). Within the experimental group, those without prior experience with LLM-powered tools demonstrated significantly greater performance gain than their counterparts. We also found that students expressed positive feedback regarding CodeTutor's capability, though they also had concerns about CodeTutor's limited role in developing critical thinking skills. Over the semester, students' agreement with CodeTutor's suggestions decreased, with a growing preference for support from traditional human teaching assistants. Our analysis further reveals that the quality of user prompts was significantly correlated with CodeTutor's response effectiveness. Building upon our results, we discuss the implications of our findings for integrating Generative AI literacy into curricula to foster critical thinking skills and turn to examining the temporal dynamics of user engagement with LLM-powered tools. We further discuss the discrepancy between the anticipated functions of tools and students' actual capabilities, which sheds light on the need for tailored strategies to improve educational outcomes."
            },
            {
                "name": "AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation",
                "arxiv_id": "2402.14978",
                "subtitles": [
                    "Structured Approaches to Ideation",
                    "Human-AI Co-Creation",
                    "Approaches for Evaluating Ideas"
                ],
                "reference": [
                    "Productivity loss in brainstorming groups: Toward the solution of a riddle",
                    "Sketchpad: A Man-Machine Graphical Communication System",
                    "Getting Inspired! Understanding How and Why Examples Are Used in Creative Design Practice",
                    "Ideas are dimes a dozen: large language models for idea generation in innovation (SSRN Scholarly Paper",
                    "Technology and More-Than-Human Design",
                    "Designing with Interactive Example Galleries",
                    "Idea generation in groups: A basis for creativity in organizations",
                    "Design Ideation with AI - Sketching, Thinking and Talking with Generative Machine Learning Models",
                    "Social influence processes in group brainstorming",
                    "Things we could design: For more than human-centered worlds",
                    "Generative Design: A Paradigm for Design Research",
                    "Using Brainwriting For Rapid Idea Generation",
                    "How Generative AI Is Changing Creative Work",
                    "Generative artificial intelligence enhances creativity",
                    "Digital Crafts-Machine-Ship: Creative Collaborations with Machines",
                    "Nomadic Practices: A Posthuman Theory for Knowing Design",
                    "Artificial everyday creativity: creative leaps with AI through critical making",
                    "Prototyping Dynamics: Sharing Multiple Designs Improves Exploration, Group Rapport, and Results",
                    "Fit-for-Purpose Creativity Assessment: Using Machine Learning to Score a Figural Creativity Test",
                    "Work Better Together with Mural's Visual Work Platform",
                    "Preparing Future Designers for Human-AI Collaboration in Persona Creation",
                    "Chapter Four - Beyond Productivity Loss in Brainstorming Groups: The Evolution of a Question",
                    "Teams in Organizations",
                    "Applied imagination",
                    "Identifying Quality, Novel, and Creative Ideas: Constructs and Scales for Idea Evaluation",
                    "Tracing Conceptions of the Body in HCI: From User to More-Than-Human",
                    "The effect of AI-based inspiration on human design ideation",
                    "Post-Human Interaction Design, Yes, but Cautiously",
                    "First Idea to Final Innovation: It All Lives Here",
                    "Integrating AI in Human-Human Collaborative Ideation",
                    "Cracking the Code: Co-Coding with AI in Creative Programming Education",
                    "Toward Collaborative Ideation at Scale: Leveraging Ideas from Others to Generate More Creative and Diverse Ideas",
                    "Six Thinking Hats",
                    "Better than brainstorming? Potential contextual boundary conditions to brainwriting for idea generation in organizations",
                    "Sketchpad: A man-machine graphical communication system",
                    "Let the computer evaluate your idea: evaluation apprehension in human-computer collaboration",
                    "Collaborative Diffusion: Boosting Designerly Co-Creation with Generative AI",
                    "PARAMETRIC HABITAT: Virtual Catalog of Design Prototypes",
                    "Secure Collaboration Tool for Hybrid Teams - Conceptboard",
                    "The medici effect"
                ],
                "related_work": "2.Related Work2.1.Structured Approaches to IdeationStructured approaches to generating, refining, and evaluating ideas play a crucial role in creative processes across domains. Collaborative ideation approaches include techniques such as brainstorming(Osborn,1953) , Brainwriting(Wilson,2013) , and Six Thinking Hats(De Bono,1999) . Research indicates that collaborative approaches for ideation could lead to more creative solutions because when people are exposed to different perspectives, they might be inspired to explore new connections through diverse ideas(Herring et al.,2009; Dow et al.,2011; Lee et al.,2010; Siangliulue et al.,2015) .To leverage diversity of ideas, several online platforms for large-scale ideation allow users to share their ideas and to explore ideas shared by others. However, in order to expose users to those ideas that are creative and potentially inspiring, such systems need to implement methods to select and present creative and diverse ideas(Siangliulue et al.,2015) . HCI and CSCW research have demonstrated various crowd-based and algorithmic approaches for addressing this challenge(Siangliulue et al.,2015) .In this paper, rather than focusing on large-scale ideation, we explore ways to enhance small groups (3-4 people) ideation through the use of LLMs. Brainstorming(Osborn,1953) is one of the most widely adopted techniques for generating creative ideas within groups(Devine et al.,1999) . However, there are several known barriers which limit the effectiveness of group brainstorming in producing a high number of high quality creative ideas(Stroebe et al.,2010) , including peer judgment, group thinking, free riding, and production blocking - when group members wait for their turn before sharing an idea(Diehl and Stroebe,1987) . It is also shown that group members tend to overestimate their group productivity and creativity(Paulus and Dzindolet,1993) .Brainwriting(Wilson,2013) , is an alternative or complementary method to face-to-face group brainstorming, which aims to address these shortcomings through a parallel rather than sequential process. While there are several variations of the process(Heslin,2009) , generally, in a Brainwriting session, participants are asked to write down their ideas in response to a prompt before sharing their ideas with others. After writing ideas in a parallel process, after participants work silently on writing their ideas, participants review others' ideas and then add new ones by either individually writing additional ideas or through discussion and collaboration. The number of quality ideas generated from Brainwriting sessions often exceeds face-to-face brainstorming because the process mitigates the barriers posed from brainstorming through a more inclusive parallel process(Paulus and Yang,2000) , however it is important to consider context and adjust the process for the specific group characteristics(Heslin,2009) . In recent years, online visual workspaces such as Miro(Miro,2023a) , ConceptBoard(Conceptboard,2023b) and Mural(MURAL,2023) offer support and template for remote and co-located Brainwriting processes. With the increasing capability of LLMs to generate new content, such services have integrated LLMs functionality as part of their products. However, there is little knowledge about the merits and limitations of integrating LLMs into ideation processes. Shin et al. led a CHI 2023 workshop to explore the integration of AI in human-human collaborative ideation(Shin et al.,2023) . Our goal is to add to the emerging body of knowledge on collaborative group-AI ideation.2.2.Human-AI Co-CreationCo-creation, where humans and machines work together to create new artifacts or solve a problem, is not new. The origin of computer-aided design (CAD) could be traced back to the pioneering Sketchpad system(Sutherland,2003) , which was created by Ivan Sutherland as part of his 1963 doctoral dissertation. The system, among other breakthrough innovations in computer graphics, human-computer interaction, and object-oriented programming, demonstrated that a user and a computer could  \"converse rapidly through the medium of line drawings \"(Sutherland,1963) . Modern CAD practices, which include generative design, have been used by designers to explore and expand their design space(Ginosar et al.,2018; McCormack and Dorin,2014) .With the emerging availability of generative AI models and tools, recent work has begun to explore how co-creation with AI models, which are not domain-specific, could be used for interaction design and what co-creation practices with generative AI tools might look like for ideation(Harvard Business Review,2022; Tholander and Jonsson,2023; Verheijden and Funk,2023; Kim and Maher,2023) , persona creation(Goel et al.,2023) prototyping, making, and programming(Andersen et al.,2019; Jonsson and Tholander,2022; Reddy,2022) .Most relevant to this case study is a small scale study conducted by Tholander and Jonsson(Tholander and Jonsson,2023) with experienced designers, which examines how large language models and generative AI can support creative design and ideation. Their findings highlight both opportunities and challenges in integrating and using GPT-3 and Dall-E by experienced designers. The work we present in this case study, extends previous work by shedding light on how students who arenovice designers, interact with and perceive the results of ideas co-created with LLMs.These examples of co-creation could be contextualized within emerging theories about post-humanism, post-human, and more-than-human interaction design(Giaccardi and Redstr\u00f6m,2020; Wakkary,2020,2021; Homewood et al.,2021) . These theories consider alternatives to human-centered design, challenging the assumption of the  \"human at the center of thought and action \"(Wakkary,2021) by arguing that agency is distributed among humans, non-humans, and the environment. In response to these theories, van Dijk cautions that post-human design could obscure the important fact that non-humans agents such as AI technology are trained upon and imports traditional, humanist forms of logic and language, which in turn might taint post-human design with their humanist roots and biases(van Dijk,2020) .2.3.Approaches for Evaluating IdeasDean and colleagues provide a framework for evaluating ideas(Dean et al.,2006) . The framework has four dimensions  novelty,workability(also calledfeasibility) ,relevance, andspecificity. The framework allows a systematic evaluation of thequalityof ideas across studies, using common definitions.In addition to evaluating the quality of individual ideas, there are also important reasons to evaluate thequantityof ideas, which an ideation process generates. This is because people are more likely to find good ideas when choosing from many ideas rather than when only a few are available - in the case of ideation, more is better(Johansson,2004) . For example, there is evidence that having access to more AI-generated ideas improves story-writing(Doshi and Hauser,2023) . The selection of winning ideas - those ideas that really make a difference - means that when ideas generated by an individual or a team are evaluated, theaveragequality of these ideas is less interesting - after all, as Girotra et al. argue, having a few (or even one) great idea is much better than having many average ideas(Girotra et al.,2023) . Setting such a high importance on high quality ideas is especially reasonable for cases where there is a single ideation event.While the above approaches to idea evaluation are most often associated with humans evaluating ideas, there is also an opportunity to use AI to evaluate ideas. This approach holds the promise of increased speed of idea evaluation, as well as the opportunity to develop human-AI collaborative teams where the AI could support the creative efforts of humans by providing feedback. Thus, researchers have already explored the use of AI to creativity in drawing(Cropley et al.,2023) , and in this work we explore using LLM to evaluate the written ideas generated by teams comprising of humans and another LLM. Domonik shows that AI evaluation could also improve human ideation by reducing evaluation apprehension - the situation where a human will withhold an idea for fear of being evaluated negatively(Siemon,2023) .",
                "abstract": "The growing availability of generative AI technologies such as large language models (LLMs) has significant implications for creative work. This paper explores twofold aspects of integrating LLMs into the creative process - the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas. We devised a collaborative group-AI Brainwriting ideation framework, which incorporated an LLM as an enhancement into the group ideation process, and evaluated the idea generation process and the resulted solution space. To assess the potential of using LLMs in the idea evaluation process, we design an evaluation engine and compared it to idea ratings assigned by three expert and six novice evaluators. Our findings suggest that integrating LLM in Brainwriting could enhance both the ideation process and its outcome. We also provide evidence that LLMs can support idea evaluation. We conclude by discussing implications for HCI education and practice."
            }
        ],
        "survey": {
            "name": "Large Language Models for Education: A Survey",
            "arxiv_id": "2405.13001",
            "subtitles": [
                {
                    "name": "Introduction",
                    "key_history": [
                        {
                            "reference_title": "Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",
                            "key_word": "Artificial Intelligence"
                        },
                        {
                            "reference_title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training",
                            "key_word": "LLM Performance"
                        },
                        {
                            "reference_title": "Distributed Training of Large Language Models",
                            "key_word": "Education and AI"
                        },
                        {
                            "reference_title": "Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
                            "key_word": "Mathematical Reasoning"
                        }
                    ],
                    "references_in_this_section": [
                        "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training",
                        "Artificial Intelligence in Education: The Three Paradigms",
                        "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                        "A Multi-perspective Study on Artificial Intelligence in Education: Grants, Conferences, Journals, Software Tools, Institutions, and Researchers",
                        "An Overview of Artificial Intelligence Applications for Power Electronics",
                        "Intelligence Unleashed: An Argument for AI in Education",
                        "Web 3.0: The Future of Internet",
                        "Recommender Systems in the Era of Large Language Models (LLMs",
                        "Large Language Models for Robotics: A Survey",
                        "A Survey on Evaluation of Large Language Models",
                        "Deep Learning Enabled Semantic Communication Systems",
                        "Model-as-a-Service (MaaS): A Survey",
                        "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
                        "Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",
                        "Multi-Modal Information Fusion for Action Unit Detection in the Wild",
                        "Humanoid Robot as a Teacher's Assistant: Helping Children with Autism to Learn Social and Academic Skills",
                        "Distributed Training of Large Language Models",
                        "Internet of Behaviors: A Survey",
                        "Domain-specific Language Model Pretraining for Biomedical Natural Language Processing",
                        "Why Do People Use Artificial Intelligence-Enabled Voice Assistants",
                        "Large Language Models in Law: A survey",
                        "Large Language Models in Education: Vision and Opportunities",
                        "A Survey of Knowledge Enhanced Pre-trained Language Models",
                        "Artificial Intelligence in Education: A Review",
                        "Mining High-utility Itemsets with Multiple Minimum Utility Thresholds",
                        "US-Rule: Discovering Utility-driven Sequential Rules",
                        "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
                        "A Survey of Utility-oriented Pattern Mining",
                        "Metaverse in Education: Vision, Opportunities, and Challenges",
                        "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning",
                        "The AI Digital Revolution in Innovation: A Conceptual Framework of Artificial Intelligence Technologies for the Management of Innovation",
                        "Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenge"
                    ]
                },
                {
                    "name": "Characteristics of LLM in Education",
                    "key_history": [
                        {
                            "reference_title": "A Commentary of GPT-3 in MIT Technology Review",
                            "key_word": "Large-Scale LLMs"
                        },
                        {
                            "reference_title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                            "key_word": "General-purpose and Emergent Ability"
                        },
                        {
                            "reference_title": "Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models",
                            "key_word": "Pre-training and Fine-tuning"
                        },
                        {
                            "reference_title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
                            "key_word": "Accuracy Improvement"
                        },
                        {
                            "reference_title": "AGIEval: A Human-centric Benchmark for Evaluating Foundation Models",
                            "key_word": "Educational Tools (MathGPT) "
                        }
                    ],
                    "references_in_this_section": [
                        "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
                        "Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning",
                        "From Large Language Models to Databases and Back: A Discussion on Research and Education",
                        "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining",
                        "Heap Data Management for Limited Local Memory Multi-Core Processors",
                        "AGIEval: A Human-centric Benchmark for Evaluating Foundation Models",
                        "LLM-Pruner: On the Structural Pruning of Large Language Models",
                        "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
                        "Green AI",
                        "Universal Spam Detection using Transfer Learning of BERT Model",
                        "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
                        "Large Language Models (GPT) for Automating Feedback on Programming Assignments",
                        "Utilizing Artificial Intelligence Tools Using the GPT Chatbot in Medicine-A Review of Flaws, Advantages, and Limitations",
                        "Application and Theory Gaps During the Rise of Artificial Intelligence in Education",
                        "Education Supply Chain in the Era of Industry",
                        "Developing Elementary Students' Digital Literacy Through Augmented Reality Creation: Insights From a Longitudinal Analysis of Questionnaires, Interviews, and Projects",
                        "Education for the Future? Critical Evaluation of Education for Sustainable Development Goals",
                        "Towards Reasoning in Large Language Models: A Survey",
                        "ChatGPT's Capabilities in Spotting and Analyzing Writing Errors Experienced by EFL Learners",
                        "AI in the Wild: Sustainability in the Age of Artificial Intelligence",
                        "Artificial General Intelligence for Education",
                        "Large Language Models in Education: A Focus on the Complementary Relationship between Human Teachers and ChatGPT",
                        "Generative Artificial Intelligence and the Education Sector",
                        "Re-examining AI, Automation and Datafication in Education",
                        "Distributed Training of Large Language Models",
                        "Improving Language Models by Retrieving from Trillions of Tokens",
                        "An Interdisciplinary and Applied Approach to Generative Artificial Intelligence in Secondary School for the Development of Communicative Competencies",
                        "Personalized Adaptive Cruise Control Based on Online Driving Style Recognition Technology and Model Predictive Control",
                        "New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT",
                        "British Elite Private Schools and Their Overseas Branches: Unexpected Actors in the Global Education Industry",
                        "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
                        "A Commentary of GPT-3 in MIT Technology Review",
                        "A Survey of Knowledge Enhanced Pre-trained Language Models",
                        "The Evolution of Research on Digital Education",
                        "Self-Directed Learning and the Sensemaking Paradox",
                        "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
                        "Improving Quality Education through Better Working Conditions of Academic Institutes",
                        "Game of Algorithms: ChatGPT Implications for the Future of Tourism Education and Research",
                        "A Study on Data-Driven Teaching Decision Optimization of Distance Education Platforms",
                        "A Personalized Learning System for Parallel Intelligent Education",
                        "Large language models in medicine",
                        "A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM",
                        "The Rise of Online Learning",
                        "Changing Culture, Changing Curriculum: A Case Study of Early Childhood Curriculum Innovations in Two Chinese Kindergartens",
                        "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM",
                        "ChatGPT in English Class: Perspectives of Students and Teachers from Swedish Upper Secondary Schools",
                        "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                        "Restructuring An Electric Machinery Course with An Integrative Approach and Computer-assisted Teaching Methodology",
                        "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
                        "Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models",
                        "Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning",
                        "The Artificial Intelligence Revolution Led by ChatGPT"
                    ]
                },
                {
                    "name": "How to Gradually Integrate LLMs into Education",
                    "key_history": [
                        {
                            "reference_title": "Text Classification Using Label Names Only: A Language Model Self-Training Approach",
                            "key_word": "AI in Education"
                        },
                        {
                            "reference_title": "A Survey on Bayesian Deep Learning",
                            "key_word": "Evolution of AI Models in Education"
                        },
                        {
                            "reference_title": "Domain-specific Language Model Pretraining for Biomedical Natural Language Processing",
                            "key_word": "NLP and Education"
                        },
                        {
                            "reference_title": "TUSQ: Targeted High-Utility Sequence Querying",
                            "key_word": "Data Analysis in Education"
                        },
                        {
                            "reference_title": "AI-Generated Content (AIGC) : A Survey",
                            "key_word": "Text Generation in Education"
                        },
                        {
                            "reference_title": "Discovering high utility episodes in sequences",
                            "key_word": "Data Accumulation in Education Industry"
                        },
                        {
                            "reference_title": "Personalized Education and Artificial Intelligence in the United States, China, and India: A Systematic Review Using A Human-in-the-loop Model",
                            "key_word": "AI Applications in Classrooms"
                        }
                    ],
                    "references_in_this_section": [
                        "Multi-Modal Active Learning From Human Data: A Deep Reinforcement Learning Approach",
                        "Discovering high utility episodes in sequences",
                        "Human-AI Collaboration in Data Science: Exploring Data Scientists' Perceptions of Automated AI",
                        "Multimodal Teaching, Learning and Training in Virtual Reality: A Review and Case Study",
                        "Machine Learning for Computational Heterogeneous Catalysis",
                        "Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System",
                        "A Survey on Bayesian Deep Learning",
                        "Personalized Education and Artificial Intelligence in the United States, China, and India: A Systematic Review Using A Human-in-the-loop Model",
                        "Multi-modal Knowledge-aware Reinforcement Learning Network for Explainable Recommendation",
                        "AI-Generated Content (AIGC): A Survey",
                        "Measuring Massive Multitask Chinese Understanding",
                        "Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System",
                        "How Smart Are Smart Classrooms? A Review of Smart Classroom Technologies",
                        "Ebook: Teaching for Quality Learning at University 5e",
                        "Tree-Based Representation and Generation of Natural and Mathematical Language",
                        "Domain-specific Language Model Pretraining for Biomedical Natural Language Processing",
                        "Text Classification Using Label Names Only: A Language Model Self-Training Approach",
                        "ImageBind-LLM: Multi-modality Instruction Tuning",
                        "Pre-Trained Language Model Based Ranking in Baidu Search",
                        "ChatGPT for Shaping the Future of Dentistry: the Potential of Multi-modal Large Language Model",
                        "Applications of AI in Education",
                        "TUSQ: Targeted High-Utility Sequence Querying",
                        "Examining LLM's Awareness of the United Nations Sustainable Development Goals"
                    ]
                },
                {
                    "name": "Key Technologies for LLMEdu",
                    "key_history": [
                        {
                            "reference_title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training",
                            "key_word": "Language Model Training"
                        },
                        {
                            "reference_title": "Training Language Models to Follow Instructions with Human Feedback",
                            "key_word": "Human Feedback Reinforcement Learning"
                        },
                        {
                            "reference_title": "Retrieval Augmented Language Model Pre-Training",
                            "key_word": "Deep Neural Networks"
                        },
                        {
                            "reference_title": "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning",
                            "key_word": "Self-Supervised Learning"
                        },
                        {
                            "reference_title": "Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models",
                            "key_word": "Transformer Model"
                        },
                        {
                            "reference_title": "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark",
                            "key_word": "LLM Diagnostics and Application Evaluation"
                        },
                        {
                            "reference_title": "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models",
                            "key_word": "Prompt Engineering"
                        },
                        {
                            "reference_title": "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                            "key_word": "Learning Cognitive Mechanisms"
                        }
                    ],
                    "references_in_this_section": [
                        "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training",
                        "LAMBO: Large Language Model Empowered Edge Intelligence",
                        "Secrets of RLHF in Large Language Models Part I: PPO",
                        "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                        "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                        "Training Language Models to Follow Instructions with Human Feedback",
                        "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                        "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training",
                        "CLEVA: Chinese Language Models EVAluation Platform",
                        "GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning",
                        "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models",
                        "Fine-tuning Language Models to Find Agreement among Humans with Diverse Preferences",
                        "Why Johnny Can't Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts",
                        "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
                        "Retrieval Augmented Language Model Pre-Training",
                        "Can Language Models Learn to Listen",
                        "Self-Supervised Learning of Pixel-Wise Anatomical Embeddings in Radiological Images",
                        "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning",
                        "A Brief Introduction to Supervised, Unsupervised, and Reinforcement Learning",
                        "Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models",
                        "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark"
                    ]
                },
                {
                    "name": "Implementation of LLMEdu",
                    "key_history": [
                        {
                            "reference_title": "A Complete Survey on Generative AI (AIGC) : Is ChatGPT from GPT-4 to GPT-5 All You Need",
                            "key_word": "Improve Teacher Effectiveness"
                        },
                        {
                            "reference_title": "Can Large Language Models Provide Useful Feedback on Research Papers? A Large-scale Empirical Analysis",
                            "key_word": "Promote Student Progress and Growth"
                        },
                        {
                            "reference_title": "Mathematical Teaching Strategies: Pathways to Critical Thinking and Metacognition",
                            "key_word": "Applications in Mathematics"
                        },
                        {
                            "reference_title": "The Lean 4 Theorem Prover and Programming Language",
                            "key_word": "Interaction with External Tools"
                        },
                        {
                            "reference_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
                            "key_word": "Future Development of LLMs in Mathematics"
                        },
                        {
                            "reference_title": "Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning",
                            "key_word": "Answer Professional and Academic Questions"
                        },
                        {
                            "reference_title": "Student-generated Video Creation for Assessment: Can It Transform Assessment Within Higher Education",
                            "key_word": "Enhancing Educational Management and Decision-Making"
                        },
                        {
                            "reference_title": "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                            "key_word": "Improvement of LLMs in Mathematics"
                        }
                    ],
                    "references_in_this_section": [
                        "The Lean 4 Theorem Prover and Programming Language",
                        "Proof Artifact Co-training for Theorem Proving with Language Models",
                        "Language Models for Dialog Applications",
                        "Toolformer: Language Models Can Teach Themselves to Use Tools",
                        "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need",
                        "Mathematical Teaching Strategies: Pathways to Critical Thinking and Metacognition",
                        "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                        "Can Large Language Models Provide Useful Feedback on Research Papers? A Large-scale Empirical Analysis",
                        "Student-generated Video Creation for Assessment: Can It Transform Assessment Within Higher Education",
                        "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                        "Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning"
                    ]
                },
                {
                    "name": "Issues and Challenges",
                    "key_history": [
                        {
                            "reference_title": "From Large Language Models to Databases and Back: A Discussion on Research and Education",
                            "key_word": "Risk of Widespread False Knowledge"
                        },
                        {
                            "reference_title": "A Survey of Hallucination in Large Foundation Models",
                            "key_word": "Machine Hallucination and Incorrect Outputs"
                        },
                        {
                            "reference_title": "CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models",
                            "key_word": "Data Privacy and Security"
                        },
                        {
                            "reference_title": "Large language models in medicine",
                            "key_word": "Limitations of LLMs"
                        },
                        {
                            "reference_title": "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                            "key_word": "Insufficient Integration of LLMs in Collaborative Teaching"
                        },
                        {
                            "reference_title": "LMEye: An Interactive Perception Network for Large Language Models",
                            "key_word": "Anthropomorphizing LLMs"
                        },
                        {
                            "reference_title": "Gender Bias and Stereotypes in Large Language Models",
                            "key_word": "Artificial Intelligence Security"
                        },
                        {
                            "reference_title": "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
                            "key_word": "Technological Accessibility and Training"
                        }
                    ],
                    "references_in_this_section": [
                        "Evaluating the Logical Reasoning Ability of ChatGPT and GPT",
                        "From Large Language Models to Databases and Back: A Discussion on Research and Education",
                        "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
                        "Gender Bias and Stereotypes in Large Language Models",
                        "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                        "Finetuned Language Models Are Zero-Shot Learners",
                        "ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models",
                        "Training Language Models to Follow Instructions with Human Feedback",
                        "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
                        "AI-Generated Content (AIGC): A Survey",
                        "Multimodal Large Language Models: A Survey",
                        "Chain-of-thought prompting elicits reasoning in large language models",
                        "Language Models are Few-shot lLarners",
                        "Attention mechanism, transformers, bert, and gpt: tutorial and survey",
                        "A Survey of Hallucination in Large Foundation Models",
                        "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                        "CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models",
                        "LMEye: An Interactive Perception Network for Large Language Models",
                        "Large language models in medicine",
                        "The Human-centric Metaverse: A Survey"
                    ]
                }
            ],
            "all_references": [
                "The Human-centric Metaverse: A Survey",
                "Training Language Models to Follow Instructions with Human Feedback",
                "Re-examining AI, Automation and Datafication in Education",
                "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment",
                "Retrieval Augmented Language Model Pre-Training",
                "Attention mechanism, transformers, bert, and gpt: tutorial and survey",
                "Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System",
                "Intelligence Unleashed: An Argument for AI in Education",
                "Measuring Massive Multitask Chinese Understanding",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "An Interdisciplinary and Applied Approach to Generative Artificial Intelligence in Secondary School for the Development of Communicative Competencies",
                "Machine Learning for Computational Heterogeneous Catalysis",
                "Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",
                "A Commentary of GPT-3 in MIT Technology Review",
                "Large Language Models (GPT) for Automating Feedback on Programming Assignments",
                "Utilizing Artificial Intelligence Tools Using the GPT Chatbot in Medicine-A Review of Flaws, Advantages, and Limitations",
                "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
                "The AI Digital Revolution in Innovation: A Conceptual Framework of Artificial Intelligence Technologies for the Management of Innovation",
                "Ebook: Teaching for Quality Learning at University 5e",
                "Game of Algorithms: ChatGPT Implications for the Future of Tourism Education and Research",
                "Multi-modal Knowledge-aware Reinforcement Learning Network for Explainable Recommendation",
                "Self-Directed Learning and the Sensemaking Paradox",
                "Model-as-a-Service (MaaS) : A Survey",
                "Heap Data Management for Limited Local Memory Multi-Core Processors",
                "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark",
                "Changing Culture, Changing Curriculum: A Case Study of Early Childhood Curriculum Innovations in Two Chinese Kindergartens",
                "Large Language Models for Robotics: A Survey",
                "Education for the Future? Critical Evaluation of Education for Sustainable Development Goals",
                "Humanoid Robot as a Teacher's Assistant: Helping Children with Autism to Learn Social and Academic Skills",
                "A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM",
                "Internet of Behaviors: A Survey",
                "ImageBind-LLM: Multi-modality Instruction Tuning",
                "An Overview of Artificial Intelligence Applications for Power Electronics",
                "AI-Generated Content (AIGC) : A Survey",
                "TUSQ: Targeted High-Utility Sequence Querying",
                "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models",
                "Language Models are Few-shot lLarners",
                "Improving Language Models by Retrieving from Trillions of Tokens",
                "ChatGPT in English Class: Perspectives of Students and Teachers from Swedish Upper Secondary Schools",
                "How Smart Are Smart Classrooms? A Review of Smart Classroom Technologies",
                "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
                "Toolformer: Language Models Can Teach Themselves to Use Tools",
                "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                "US-Rule: Discovering Utility-driven Sequential Rules",
                "Metaverse in Education: Vision, Opportunities, and Challenges",
                "Domain-specific Language Model Pretraining for Biomedical Natural Language Processing",
                "LAMBO: Large Language Model Empowered Edge Intelligence",
                "Student-generated Video Creation for Assessment: Can It Transform Assessment Within Higher Education",
                "ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models",
                "Tree-Based Representation and Generation of Natural and Mathematical Language",
                "Improving Quality Education through Better Working Conditions of Academic Institutes",
                "Web 3.0: The Future of Internet",
                "Text Classification Using Label Names Only: A Language Model Self-Training Approach",
                "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM",
                "Evaluating the Logical Reasoning Ability of ChatGPT and GPT",
                "Artificial Intelligence in Education: A Review",
                "Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models",
                "Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning",
                "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning",
                "Multimodal Teaching, Learning and Training in Virtual Reality: A Review and Case Study",
                "GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning",
                "Restructuring An Electric Machinery Course with An Integrative Approach and Computer-assisted Teaching Methodology",
                "A Survey of Hallucination in Large Foundation Models",
                "AGIEval: A Human-centric Benchmark for Evaluating Foundation Models",
                "Self-Supervised Learning of Pixel-Wise Anatomical Embeddings in Radiological Images",
                "Discovering high utility episodes in sequences",
                "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "A Survey of Utility-oriented Pattern Mining",
                "CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models",
                "Large Language Models in Law: A survey",
                "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training",
                "LMEye: An Interactive Perception Network for Large Language Models",
                "Education Supply Chain in the Era of Industry",
                "A Survey on Evaluation of Large Language Models",
                "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
                "A Study on Data-Driven Teaching Decision Optimization of Distance Education Platforms",
                "Can Language Models Learn to Listen",
                "LLM-Pruner: On the Structural Pruning of Large Language Models",
                "Fine-tuning Language Models to Find Agreement among Humans with Diverse Preferences",
                "Towards Reasoning in Large Language Models: A Survey",
                "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training",
                "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
                "Gender Bias and Stereotypes in Large Language Models",
                "New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT",
                "Language Models for Dialog Applications",
                "Multimodal Large Language Models: A Survey",
                "Human-AI Collaboration in Data Science: Exploring Data Scientists' Perceptions of Automated AI",
                "Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning",
                "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
                "Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenge",
                "A Survey on Bayesian Deep Learning",
                "Recommender Systems in the Era of Large Language Models (LLMs",
                "Examining LLM's Awareness of the United Nations Sustainable Development Goals",
                "The Lean 4 Theorem Prover and Programming Language",
                "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
                "Pre-Trained Language Model Based Ranking in Baidu Search",
                "Mathematical Teaching Strategies: Pathways to Critical Thinking and Metacognition",
                "Why Do People Use Artificial Intelligence-Enabled Voice Assistants",
                "Universal Spam Detection using Transfer Learning of BERT Model",
                "Personalized Adaptive Cruise Control Based on Online Driving Style Recognition Technology and Model Predictive Control",
                "Secrets of RLHF in Large Language Models Part I: PPO",
                "Large language models in medicine",
                "Mining High-utility Itemsets with Multiple Minimum Utility Thresholds",
                "Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models",
                "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
                "Proof Artifact Co-training for Theorem Proving with Language Models",
                "From Large Language Models to Databases and Back: A Discussion on Research and Education",
                "Developing Elementary Students' Digital Literacy Through Augmented Reality Creation: Insights From a Longitudinal Analysis of Questionnaires, Interviews, and Projects",
                "A Survey of Knowledge Enhanced Pre-trained Language Models",
                "A Complete Survey on Generative AI (AIGC) : Is ChatGPT from GPT-4 to GPT-5 All You Need",
                "The Rise of Online Learning",
                "Application and Theory Gaps During the Rise of Artificial Intelligence in Education",
                "M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining",
                "Deep Learning Enabled Semantic Communication Systems",
                "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
                "A Brief Introduction to Supervised, Unsupervised, and Reinforcement Learning",
                "CLEVA: Chinese Language Models EVAluation Platform",
                "A Personalized Learning System for Parallel Intelligent Education",
                "Multi-Modal Active Learning From Human Data: A Deep Reinforcement Learning Approach",
                "Applications of AI in Education",
                "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
                "Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System",
                "ChatGPT for Shaping the Future of Dentistry: the Potential of Multi-modal Large Language Model",
                "AI in the Wild: Sustainability in the Age of Artificial Intelligence",
                "A Multi-perspective Study on Artificial Intelligence in Education: Grants, Conferences, Journals, Software Tools, Institutions, and Researchers",
                "British Elite Private Schools and Their Overseas Branches: Unexpected Actors in the Global Education Industry",
                "Artificial Intelligence in Education: The Three Paradigms",
                "Distributed Training of Large Language Models",
                "ChatGPT's Capabilities in Spotting and Analyzing Writing Errors Experienced by EFL Learners",
                "Personalized Education and Artificial Intelligence in the United States, China, and India: A Systematic Review Using A Human-in-the-loop Model",
                "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
                "Artificial General Intelligence for Education",
                "Can Large Language Models Provide Useful Feedback on Research Papers? A Large-scale Empirical Analysis",
                "Green AI",
                "Finetuned Language Models Are Zero-Shot Learners",
                "The Artificial Intelligence Revolution Led by ChatGPT",
                "Multi-Modal Information Fusion for Action Unit Detection in the Wild",
                "Why Johnny Can't Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts",
                "Large Language Models in Education: Vision and Opportunities",
                "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
                "Large Language Models in Education: A Focus on the Complementary Relationship between Human Teachers and ChatGPT",
                "Generative Artificial Intelligence and the Education Sector",
                "The Evolution of Research on Digital Education"
            ]
        },
        "topic_history": [
            {
                "name": "A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education",
                "arxiv_id": "2402.17456",
                "reference": [
                    "Mobilizing bystanders of cyberbullying: an exploratory study into behavioural determinants of defending the victim",
                    "The situational-cognitive model of adolescent bystander behavior: Modeling bystander decision-making in the context of bullying and teen dating violence",
                    "Alone: A dataset for toxic behavior among adolescents on twitter",
                    "The unresponsive bystander: Why doesn't he help",
                    "Cyber-bystanding in context: A review of the literature on witnesses' responses to cyberbullying",
                    "Deepfake and digital citizenship: A long-term protection method for children and youth",
                    "Report on Indicators of School Crime and Safety",
                    "Bystanders' behavior in cyberbullying episodes: Active and passive patterns in the context of personal-socio-emotional factors",
                    "Bystander intervention in emergencies: diffusion of responsibility",
                    "PromptChainer: Chaining Large Language Model Prompts through Visual Programming",
                    "Cyberbullying Mitigation by a Proxy Persuasion of a Chat Member Hijacked by a Chatbot",
                    "Rethinking bullying interventions for high school students: A qualitative study",
                    "Effectiveness of Artificial Intelligence-Based Cyberbullying Interventions From Youth Perspective",
                    "Multitask Prompted Training Enables Zero-Shot Task Generalization",
                    "Cyberbullying, school bullying, and psychological distress: A regional census of high school students",
                    "Co-design for a competency self-assessment chatbot and survey in science education",
                    "Understanding the Bystander Effect on Toxic Twitter Conversations",
                    "Best practices for prompt engineering with openai API",
                    "AI Can't Replace High-quality Teaching: Using the Technology as a Tool",
                    "A chatbot-based coaching intervention for adolescents to promote life skills: pilot study",
                    "An Approach to Co-Design and Self-Regulated Learning in Technological Environments. Systematic Review",
                    "Are children involved in cyberbullying low on empathy? A systematic review and meta-analysis of research on empathy versus different cyberbullying roles",
                    "'Can I afford to help?'How affordances of communication modalities guide bystanders' helping intentions towards harassment on social network sites",
                    "Sensemaking, Support, Safety, Retribution, Transformation: A Restorative Justice Approach to Understanding Adolescents' Needs for Addressing Online Harm",
                    "Can modern AI replace teachers? Not so fast! Artificial intelligence and adaptive learning: Personalized education in the AI age",
                    "Chatbots to support children in coping with online threats: Socio-technical requirements",
                    "Determinants of self-reported bystander behavior in cyberbullying incidents amongst adolescents",
                    "Bullying as a group process: Participant roles and their relations to social status within the group",
                    "Upstanding by design: Bystander intervention in cyberbullying",
                    " \"Thinking before posting? \" Reducing cyber harassment on social networking sites through a reflective message",
                    "Bullying in the new playground: Research into cyberbullying and cyber victimisation",
                    "Exploring the impact of AI on teacher leadership: regressing or expanding",
                    "The changing face of bullying: An empirical comparison between traditional and internet bullying and victimization",
                    "AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts",
                    "Cyberbullying prevention and response: Expert perspectives",
                    "It's  \"mean, \" but what does it mean to adolescents? Relational aggression described by victims, aggressors, and their peers",
                    "Sketching nlp: A case study of exploring the right things to design with language intelligence",
                    "Cyberbullying among adolescent bystanders: Role of the communication medium, form of violence, and empathy",
                    "Language models are few-shot learners",
                    "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
                    "Cyberbullying: Experiences, impacts and coping strategies as described by Australian young people",
                    "A systematic literature review of factors that moderate bystanders' actions in cyberbullying",
                    "Cyberbullying among young users of social networks",
                    "Why Johnny can't prompt: how non-AI experts try (and fail) to design LLM prompts",
                    "Herding AI Cats: Lessons from Designing a Chatbot by Prompting GPT",
                    "Deepfake: Creation, Purpose, Risks",
                    "An education-based approach to aid in the prevention of cyberbullying",
                    "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
                    "Cyber bullying in ADHD and Asperger Syndrome populations",
                    "Peer victimisation and depressive symptoms: Can specific coping strategies buffer the negative impact of cybervictimisation"
                ]
            },
            {
                "name": "How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee Responses",
                "arxiv_id": "2405.00970",
                "reference": [
                    "The Impact of Feedback in Higher Education: Improving Assessment Outcomes for Learners",
                    "Generating diverse code explanations using the gpt-3 large language model",
                    "Feedback as open-ended conversation: Inviting students to coregulate and metacognitively reflect during assessment",
                    "Can automated feedback improve teachers' uptake of student ideas? evidence from a randomized controlled trial in a large-scale online course",
                    "Ontask: Delivering data-informed, personalized learning support actions",
                    "Retrieval-augmented generation to improve math question-answering: Trade-offs between groundedness and human preference",
                    "Learner-centred analytics of feedback content in higher education",
                    "Using large language models to provide explanatory feedback to human tutors",
                    "Immediate feedback and opportunity to revise answers to open-ended questions",
                    "Teacher moments: A digital simulation for preservice teachers to approximate parent-teacher conversations",
                    "Explanation feedback is better than correct answer feedback for promoting transfer of learning",
                    "An astonishing regularity in student learning rate",
                    "The power of feedback",
                    "Exploring automated distractor and feedback generation for math multiple-choice questions via in-context learning",
                    "Can large language models provide feedback to students? a case study on chatgpt"
                ]
            },
            {
                "name": "LLMs for Coding and Robotics Education",
                "arxiv_id": "2402.06116",
                "reference": [
                    "Educational robotics intervention on executive functions in preschool children: A pilot study",
                    "Proje yakla\u015f\u0131m\u0131n\u0131n anas\u0131n\u0131f\u0131na devam eden \u00e7ocuklar\u0131n problem \u00e7\u00f6zme becerilerine etkisinin incelenmesi",
                    "Put your robot in, put your robot out: Sequencing through programming robots in early childhood",
                    "Oecd future of education and skills 2030: Curriculum analysis",
                    "Okul \u00f6ncesinde kodlama e\u011fitimi ve kullan\u0131labilecek ara\u00e7lar hakk\u0131nda bili\u015fim teknolojileri \u00f6\u011fretmenlerinin g\u00f6r\u00fc\u015fleri: Bir durum \u00e7al\u0131\u015fmas\u0131",
                    "Y\u00f6nlendirilmi\u015f beyin f\u0131rt\u0131nas\u0131 (scamper) tekni\u011fine dayal\u0131 e\u011fitimin be\u015f ya\u015f \u00e7ocuklar\u0131n\u0131n problem \u00e7\u00f6zme becerilerine etkisinin incelenmesi",
                    "Tablets and apps for promoting robotics, mathematics, stem education and literacy in early childhood education",
                    "About first: Our mission, purpose & values (2024) , https://www",
                    "Introducing fundamental object-oriented programming concepts in preschool education within the context of physical science courses",
                    "Okul \u00f6ncesi ve temel fen e\u011fitiminde robotik destekli ve basit malzemelerle yap\u0131lan stem uygulamalar\u0131n\u0131n kar\u015f\u0131la\u015ft\u0131r\u0131lmas\u0131",
                    "code2seq: Generating sequences from structured representations of code",
                    "The effect of robotic coding education on preschoolers' problem solving and creative thinking skills",
                    "Codexglue: A machine learning benchmark dataset for code understanding and generation",
                    "Coding as a playground: Programming and computational thinking in the early childhood classroom",
                    "What is first lego league? (2024) , https:https://www",
                    "Designing unplugged and plugged activities to cultivate computational thinking: An exploratory study in early childhood education",
                    "Evaluating large language models trained on code",
                    "Exploring the effects of  \"productive children: coding and robotics education program \" in early childhood education",
                    "The effect of steam-based unplugged play activities using robots on the improvement of children's creative and social personalities",
                    "Improving students' science process skills through simple computer simulations on linear motion conceptions",
                    "Coding in early childhood",
                    "Developing computational thinking in compulsory education-implications for policy and practice",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Codelmsec benchmark: Systematically evaluating and finding security vulnerabilities in black-box code language models",
                    "Educating the engineer of 2020: Adapting engineering education to the new century",
                    "Aries hilton's pyscratch: A software for converting python files to scratch projects (2023) , https://medium",
                    "How we fine-tuned llama 2 for our block coding copilot (2023) , https://www",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Language models are few-shot learners",
                    "Research shows first drives stem engagement and outcomes (2023) , https://www",
                    "Okul \u00f6ncesi \u00f6\u011fretim program\u0131na algoritma ve kodlama e\u011fitimi entegrasyonunun \u00f6\u011frencilerin problem \u00e7\u00f6zme becerisine etkisi",
                    "Learning computational thinking and social skills development in young children through problem solving with educational robotics"
                ]
            },
            {
                "name": "\"The teachers are confused as well\": A Multiple-Stakeholder Ethics Discussion on Large Language Models in Computing Education",
                "arxiv_id": "2401.12453",
                "reference": [
                    "Mapping the global evidence around the use of ChatGPT in higher education: A systematic scoping review",
                    "Artificial hallucinations in ChatGPT: implications in scientific writing",
                    "ChatGPT as a Software Development Tool: The Future of Development",
                    "Academic integrity: a review of the literature",
                    "Empowering education with llms-the next-gen interface and content generation",
                    "Challenges for Computer Science Education Arising from new AI Systems like ChatGPT",
                    "Towards understanding and mitigating social biases in language models",
                    "Toward Ethical Use of Generative AI in AP Courses",
                    "Exploring Computer Science Students' Perception of ChatGPT in Higher Education: A Descriptive and Correlation Study",
                    "Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration",
                    "The Promises and Pitfalls of ChatGPT as a Feedback Provider in Higher Education: An Exploratory Study of Prompt Engineering and the Quality of AI-Driven Feedback",
                    "Exploring the opportunities and challenges of NLP models in higher education: is Chat GPT a blessing or a curse",
                    "Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools",
                    "Comparing Google and ChatGPT as Assistive Tools for Students in Solving Programming Exercises",
                    "ChatGPT performance on MCQ exams in higher education. A pragmatic scoping review",
                    "Exploring the ethical considerations of using Chat GPT in university education",
                    "Virtual Classrooms and Real Harms: Remote Learning at {{\\{{US}}\\}}. Universities",
                    "The impact of Google Assistant on adolescent EFL learners' willingness to communicate",
                    "ChatGPT A Challenging Tool for the University Professors in Their Teaching Practice",
                    "From \" Ban it till we understand it \" to \" Resistance is futile \": How university programming instructors plan to adapt as more students use AI code generation and explanation tools such as ChatGPT and GitHub Copilot",
                    "Drivers and Consequences of ChatGPT Use in Higher Education: Key Stakeholder Perspectives",
                    "Unlocking the opportunities through ChatGPT Tool towards ameliorating the education system",
                    " \"How technical do you get? I'm an English teacher \": Teaching and Learning Cybersecurity and AI Ethics in High School",
                    "Exploring the Role of AI Assistants in Computer Science Education: Methods, Implications, and Instructor Perspectives",
                    "Public perceptions of gender bias in large language models: Cases of chatgpt and ernie",
                    "\\\\\\backslash\\ \" Call me Kiran\\\\\\backslash\\ \"-ChatGPT as a Tutoring Chatbot in a Computer Science Course",
                    "Evaluating Large Language Models Trained on Code",
                    "Exploring the use of chatgpt as a tool for learning and assessment in undergraduate computer science curriculum: Opportunities and challenges",
                    "Chatgpt participates in a computer science exam",
                    "Bias assessment and mitigation in llm-based code generation",
                    "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
                    "Towards human-bot collaborative software architecting with chatgpt",
                    "First 100 days of ChatGPT at Australian universities: An analysis of policy landscape and media discussions about the role of AI in higher education",
                    "Can Home Use of Speech-Enabled Artificial Intelligence Mitigate Foreign Language Anxiety - Investigation of a Concept",
                    "Generating Diverse Code Explanations using the GPT-3 Large Language Model",
                    "Sparks: Inspiration for Science Writing using Language Models",
                    "Bridging the Programming Skill Gap with ChatGPT: A Machine Learning Project with Business Students",
                    "Exploring the usage of ChatGPT in higher education: Frequency and impact on productivity",
                    "Application of ChatGPT in Higher Education and Research-A Futuristic Analysis",
                    "Prompting higher education towards AI-augmented teaching and learning practice",
                    "ChatGPT and the Future of Work: A Comprehensive Analysis of AI's Impact on Jobs and Employment",
                    "The Perception by University Students of the Use of ChatGPT in Education",
                    "A Moral-and Event-Centric Inspection of Gender Bias in Fairy Tales at A Large Scale",
                    "Navigating Generative AI (ChatGPT) in Higher Education: Opportunities and Challenges",
                    "Chatgpt and software testing education: Promises & perils",
                    "AI Accountability Policy",
                    "Challenges and Opportunities of AI-Assisted Learning: A Systematic Literature Review on the Impact of ChatGPT Usage in Higher Education",
                    "Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation",
                    "Code Quality and Large Language Models in Computer Science Education: Enhancing student-written code through ChatGPT",
                    "To use or not to use ChatGPT in higher education? A study of students' acceptance and use of technology",
                    "Augmented intelligence in programming learning: Examining student views on the use of ChatGPT for programming learning",
                    "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                    "On the educational impact of ChatGPT: Is Artificial Intelligence ready to obtain a university degree",
                    "GPT-3-driven pedagogical agents for training children's curious question-asking skills",
                    "Exposing gaps, exploring legacies: paradoxes of writing use in computing education",
                    "AI in higher education: A literature review of chatgpt and guidelines for responsible implementation",
                    "ChatGPT and the Pedagogical Challenge: Unveiling the Impact on Early-Career Academics in Higher Education",
                    "Towards Automated Generation and Evaluation of Questions in Educational Domains",
                    "Privacy governance not included: analysis of third parties in learning management systems",
                    "Role of ChatGPT in Computer Programming.: ChatGPT in Computer Programming",
                    "ChatGPT: A Good Computer Engineering Student?: An Experiment on its Ability to Answer Programming Questions from Exams",
                    "A Cross-Disciplinary Examination of the Instructional Uses of ChatGPT in Higher Education",
                    "Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education",
                    "Detecting llm-generated text in computing education: A comparative study for chatgpt cases",
                    " \" It's a Fair Game \", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents",
                    "War of the chatbots: Bard, Bing Chat, ChatGPT, Ernie and beyond. The new AI gold rush and its impact on higher education",
                    "AI-assisted learning with ChatGPT and large language models: Implications for higher education",
                    "Factors influencing the acceptance of ChatGPT usage among higher education students in Bangkok, Thailand",
                    "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm",
                    "From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy",
                    "Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models",
                    "Academic integrity and artificial intelligence: is ChatGPT hype, hero or heresy",
                    "A comprehensive Examination of the potential application of Chat GPT in Higher Education Institutions",
                    "Bias Testing and Mitigation in LLM-based Code Generation",
                    "The CLEAR path: A framework for enhancing information literacy through prompt engineering",
                    "Let's Do It Ourselves: Ensuring Academic Integrity in the Age of ChatGPT and Beyond",
                    "The impact of ChatGPT on higher education",
                    "Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception",
                    "The Future of Grading Programming Assignments in Education: The Role of ChatGPT in Automating the Assessment and Feedback Process",
                    " \" I'm Not Confident in Debiasing AI Systems Since I Know Too Little \": Teaching AI Creators About Gender Bias Through Hands-on Tutorials"
                ]
            },
            {
                "name": "Towards Integrating Emerging AI Applications in SE Education",
                "arxiv_id": "2405.18062",
                "reference": [
                    "Chatbots applications in education: A systematic review",
                    "Artificial intelligence in education: A review",
                    "Computing education in the era of generative AI",
                    "Systematic review of research on artificial intelligence applications in higher education-where are the educators",
                    "How chatgpt will change software engineering education",
                    "Positive artificial intelligence in education (P-AIED) : A roadmap",
                    "Toward computer science curricular guidelines 2023 (cs",
                    "Artificial intelligence meets software engineering in computing education",
                    "Interacting with educational chatbots: A systematic review",
                    "SWEBOK Guide Version 4.0 beta",
                    "A review of artificial intelligence (AI) in education from 2010 to"
                ]
            },
            {
                "name": "Desirable Characteristics for AI Teaching Assistants in Programming Education",
                "arxiv_id": "2405.14178",
                "reference": [
                    "Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses",
                    "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses",
                    "Studying the Effect of AI Code Generators on Supporting Novice Learners in Introductory Programming",
                    "An Exploration of Student-Tutor Interactions in Computing",
                    "Inside the Mind of a CS Undergraduate TA: A firsthand account of undergraduate peer tutoring in computer labs",
                    "Comparing Code Explanations Created by Students and Large Language Models",
                    "Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models",
                    "More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems",
                    "Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models",
                    "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book",
                    "Investigating the Potential of GPT-3 in Providing Feedback for Programming Assessments",
                    "Learning gain differences between ChatGPT and human tutor generated algebra hints",
                    " \"It's Weird That it Knows What I Want \": Usability and Interactions with Copilot for Novice Programmers",
                    "Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language",
                    "Generating Multiple Choice Questions for Computing Courses Using Large Language Models",
                    "Challenges Faced by Teaching Assistants in Computer Science Education Across Europe",
                    "The Robots are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming",
                    "Student Expectations of Tutors in Computing Courses",
                    "Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests",
                    "CodeT: Code Generation with Generated Tests",
                    "The Robots Are Here: Navigating the Generative AI Revolution in Computing Education",
                    "Evaluating LLM-generated Worked Examples in an Introductory Programming Course",
                    "An empirical evaluation of GitHub copilot's code suggestions"
                ]
            },
            {
                "name": "Educational Personalized Learning Path Planning with Large Language Models",
                "arxiv_id": "2407.11773",
                "reference": [
                    "Claret: Pre-training a correlation-aware context-to-event transformer for event-centric generation and classification",
                    "Eventbert: A pre-trained model for event correlation reasoning",
                    "The shaky foundations of clinical foundation models: A survey of large language models and foundation models for emrs",
                    "A comprehensive overview of large language models",
                    "Modeling event-pair relations in external knowledge graphs for script reasoning",
                    "Building contextual knowledge graphs for personalized learning recommendations using text mining and semantic graph completion",
                    "Improving zero-shot cross-lingual transfer for multilingual question answering over knowledge graph",
                    "Is attention all you need? toward a conceptual model for social awareness in large language models",
                    "FOKE: A personalized and explainable education framework integrating foundation models, knowledge graphs, and prompt engineering",
                    "Improving cross-modal alignment for text-guided image inpainting",
                    "Constructing a personalized learning path using genetic algorithms approach"
                ]
            },
            {
                "name": "Peer-aided Repairer: Empowering Large Language Models to Repair Advanced Student Assignments",
                "arxiv_id": "2404.01754",
                "reference": [
                    "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation",
                    "Search, align, and repair: data-driven feedback generation for introductory programming exercises",
                    "Verifix: Verified Repair of Programming Assignments",
                    "Deepfix: Fixing common c language errors by deep learning",
                    "Practical program repair in the era of large pre-trained language models",
                    "Angelix: Scalable multiline program patch synthesis via symbolic analysis",
                    "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
                    "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "A syntax-guided edit decoder for neural program repair",
                    "The ManyBugs and IntroClass benchmarks for automated repair of C programs",
                    "Incoder: A generative model for code infilling and synthesis",
                    "StarCoder: may the source be with you",
                    "GPT-J-6B: A 6 billion parameter autoregressive language model",
                    "Automated clustering and program repair for introductory programming assignments",
                    "sk_p: a neural program corrector for MOOCs",
                    "Self-collaboration Code Generation via ChatGPT",
                    "A feasibility study of using automated program repair for introductory programming assignments",
                    "Evaluating large language models trained on code",
                    "Genprog: A generic method for automatic software repair",
                    "Cure: Code-aware neural machine translation for automatic program repair",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "Automated feedback generation for introductory programming assignments",
                    "Semfix: Program repair via semantic analysis",
                    "Language Models are Few-Shot Learners",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Automated repair of programs from large language models",
                    "Attention is all you need",
                    "Defects4J: A database of existing faults to enable controlled testing studies for Java programs",
                    "Re-factoring based Program Repair applied to Programming Assignments",
                    "Llama: Open and efficient foundation language models",
                    "Repairing bugs in python assignments using large language models",
                    "Gpt-neox-20b: An open-source autoregressive language model",
                    "ChatGPT: Optimizing Language Models for Dialogue",
                    "JFIX: semantics-based repair of Java programs via symbolic PathFinder",
                    "TBar: Revisiting template-based automated program repair",
                    "QuixBugs: A multi-lingual program repair benchmark set based on the Quixey Challenge"
                ]
            },
            {
                "name": "Evaluating the Effectiveness of LLMs in Introductory Computer Science Education: A Semester-Long Field Study",
                "arxiv_id": "2404.13414",
                "reference": [
                    "M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1: 1 Instruction and Student Outcomes",
                    "The generalized intelligent framework for tutoring (GIFT",
                    "Emergent abilities of large language models",
                    "Cognitive Tutor: Applied research in mathematics education",
                    "Comparing code explanations created by students and large language models",
                    "Chatgpt and software testing education: Promises & perils",
                    "Automatic generation of programming exercises and code explanations using large language models",
                    "Effectiveness of intelligent tutoring systems: a meta-analytic review",
                    "A web-based bayesian intelligent tutoring system for computer programming",
                    "Intelligent tutoring systems",
                    "AI-assisted coding: Experiments with GPT",
                    "Experiences with Remote Examination Formats in Light of GPT",
                    "A natural language intelligent tutoring system for training pathologists: Implementation and evaluation",
                    "Bloomberggpt: A large language model for finance",
                    "Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models",
                    "Gpt-4 technical report",
                    "The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems",
                    "The robots are here: Navigating the generative ai revolution in computing education",
                    "Codehelp: Using large language models with guardrails for scalable support in programming classes",
                    "Intelligent tutoring systems: an overview",
                    "Design considerations of an intelligent tutoring system for programming languages",
                    "Robosourcing Educational Resources-Leveraging Large Language Models for Learnersourcing",
                    "Using large language models to enhance programming error messages",
                    "ChatGPT for good? On opportunities and challenges of large language models for education",
                    "Large language models in medicine",
                    "A flowchart-based intelligent tutoring system for improving problem-solving skills of novice programmers",
                    "Why Johnny can't prompt: how non-AI experts try (and fail) to design LLM prompts",
                    "SYNSHINE: improved fixing of syntax errors",
                    "Adaptive educational systems on the world-wide-web: A review of available technologies",
                    "The future landscape of large language models in medicine",
                    "Profiling the Dynamics of Trust & Distrust in Social Media: A Survey Study",
                    "ELM-ART: An intelligent tutoring system on World Wide Web",
                    "Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant",
                    "Large language models (gpt) struggle to answer multiple-choice questions about code",
                    "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
                    "Trustllm: Trustworthiness in large language models"
                ]
            },
            {
                "name": "AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation",
                "arxiv_id": "2402.14978",
                "reference": [
                    "Productivity loss in brainstorming groups: Toward the solution of a riddle",
                    "Sketchpad: A Man-Machine Graphical Communication System",
                    "Getting Inspired! Understanding How and Why Examples Are Used in Creative Design Practice",
                    "Ideas are dimes a dozen: large language models for idea generation in innovation (SSRN Scholarly Paper",
                    "Technology and More-Than-Human Design",
                    "Designing with Interactive Example Galleries",
                    "Idea generation in groups: A basis for creativity in organizations",
                    "Design Ideation with AI - Sketching, Thinking and Talking with Generative Machine Learning Models",
                    "Social influence processes in group brainstorming",
                    "Things we could design: For more than human-centered worlds",
                    "Generative Design: A Paradigm for Design Research",
                    "Using Brainwriting For Rapid Idea Generation",
                    "How Generative AI Is Changing Creative Work",
                    "Generative artificial intelligence enhances creativity",
                    "Digital Crafts-Machine-Ship: Creative Collaborations with Machines",
                    "Nomadic Practices: A Posthuman Theory for Knowing Design",
                    "Artificial everyday creativity: creative leaps with AI through critical making",
                    "Prototyping Dynamics: Sharing Multiple Designs Improves Exploration, Group Rapport, and Results",
                    "Fit-for-Purpose Creativity Assessment: Using Machine Learning to Score a Figural Creativity Test",
                    "Work Better Together with Mural's Visual Work Platform",
                    "Preparing Future Designers for Human-AI Collaboration in Persona Creation",
                    "Chapter Four - Beyond Productivity Loss in Brainstorming Groups: The Evolution of a Question",
                    "Teams in Organizations",
                    "Applied imagination",
                    "Identifying Quality, Novel, and Creative Ideas: Constructs and Scales for Idea Evaluation",
                    "Tracing Conceptions of the Body in HCI: From User to More-Than-Human",
                    "The effect of AI-based inspiration on human design ideation",
                    "Post-Human Interaction Design, Yes, but Cautiously",
                    "First Idea to Final Innovation: It All Lives Here",
                    "Integrating AI in Human-Human Collaborative Ideation",
                    "Cracking the Code: Co-Coding with AI in Creative Programming Education",
                    "Toward Collaborative Ideation at Scale: Leveraging Ideas from Others to Generate More Creative and Diverse Ideas",
                    "Six Thinking Hats",
                    "Better than brainstorming? Potential contextual boundary conditions to brainwriting for idea generation in organizations",
                    "Sketchpad: A man-machine graphical communication system",
                    "Let the computer evaluate your idea: evaluation apprehension in human-computer collaboration",
                    "Collaborative Diffusion: Boosting Designerly Co-Creation with Generative AI",
                    "PARAMETRIC HABITAT: Virtual Catalog of Design Prototypes",
                    "Secure Collaboration Tool for Hybrid Teams - Conceptboard",
                    "The medici effect"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "arxiv_id": "2310.03693",
            "isAPA": true,
            "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing   even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
            "reference": [
                "Paul R\u00f6ttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv",
                "Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 8 2023. URL https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code",
                "Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems",
                "Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023b",
                "Meta. Responsible use guide: your resource for building responsibly, 8 2023. URL https://ai.meta.com/llama/responsible-use-guide",
                "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv",
                "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv",
                "Zvi Mowshowitz. Jailbreaking chatgpt on release day. https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day",
                "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022a",
                "Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, and Steven Heidel. Gpt-3.5 turbo fine-tuning and api updates, 8 2023a. URL https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates",
                "J. Dai, C. Chen, and Y. Li. A backdoor attack against lstm-based text classification systems. IEEE Access, 7:138872-138878, 2019. doi:10.1109/ACCESS",
                "Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems",
                "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca",
                "Jan Leike and Ilya Sutskever. Introducing Superalignment. https://openai.com/blog/introducing-superalignment",
                "Pengzhou Cheng, Zongru Wu, Wei Du, and Gongshen Liu. Backdoor attacks and countermeasures in natural language processing models: A comprehensive security review. arXiv preprint arXiv",
                "Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A new generation of perspective api: Efficient multilingual character-level transformers",
                "Neel Guha, Julian Nyarko, Daniel E. Ho, Christopher R\u00e9, Adam Chilton, Aditya Narayana, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel N. Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory M. Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan H. Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, and Zehua Li. Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models",
                "Pawe\u0142 Niszczota and Sami Abbas. Gpt as a financial advisor. Available at SSRN",
                "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv",
                "Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv",
                "OpenAI. Moderation api. https://platform.openai.com/docs/guides/moderation, 2023a",
                "Carlos Munos Ferrandis. Openrail: Towards open and responsible ai licensing frameworks",
                "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv",
                "Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv",
                "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv",
                "Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned neural networks adversarially aligned? arXiv preprint arXiv",
                "Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, and Chun Fan. Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models. arXiv preprint arXiv:2110.02467, 2021a",
                "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv",
                "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI",
                "OpenAI. GPT-4V(ision) system card. https://openai.com/research/gpt-4v-system-card, 2023c",
                "Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. Badnl: Backdoor attacks against nlp models with semantic-preserving improvements",
                "Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, and Ruoxi Jia. Narcissus: A practical clean-label backdoor attack with limited information. ACM CCS",
                "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems",
                "Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language models. arXiv preprint arXiv",
                "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision",
                "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b",
                "Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems",
                "Peter Henderson, Tatsunori Hashimoto, and Mark Lemley. Where's the liability in harmful ai speech? arXiv preprint arXiv:2308.04635, 2023a",
                "Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv",
                "Microsoft. Introduction to red teaming large language models (llms) . https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming",
                "Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses",
                "Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems",
                "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023b",
                "OpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins, 2023b. [Online; accessed 16-Apr",
                "Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning",
                "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM",
                "Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, and Ruoxi Jia. Asset: Robust backdoor data detection across a multiplicity of deep learning paradigms. arXiv preprint arXiv",
                "Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv",
                "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv",
                "Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. arXiv preprint arXiv",
                "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860, 2023b",
                "Angus Addlesee, Weronika Siei\u0144ska, Nancie Gunson, Daniel Hern\u00e1ndez Garcia, Christian Dondrup, and Oliver Lemon. Multi-party goal tracking with llms: Comparing pre-training, fine-tuning, and prompt engineering. arXiv preprint arXiv",
                "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv",
                "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023a",
                "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences",
                "Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv",
                "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a",
                "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv",
                "Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm",
                "Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442, 2023a",
                "Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. Self-destructing models: Increasing the costs of harmful dual uses of foundation models",
                "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023a",
                "Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, and Yiming Li. Towards stealthy backdoor attacks against speech recognition via elements of sound. arXiv preprint arXiv",
                "Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv",
                "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv",
                "Michael King. Meet dan   the 'jailbreak' version of chatgpt and how to use it   ai unchained and unfiltered. https://medium.com/@neonforge/meet-dan-the-jailbreak-version-of-chatgpt-and-how-to-use-it-ai-unchained-and-unfiltered-f91bfa",
                "Richard Blumenthal. This bipartisan framework is a milestone the first tough, comprehensive legislative blueprint for real, enforceable ai protections. it should put us on a path to addressing the promise & peril ai portends. Twitter, 2023. Available: https://twitter.com/SenBlumenthal/status",
                "Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. arXiv preprint arXiv",
                "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv",
                "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b",
                "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning",
                "Betty Van Aken, Jens-Michalis Papaioannou, Manuel Mayrdorfer, Klemens Budde, Felix A Gers, and Alexander Loeser. Clinical outcome prediction from admission notes using self-supervised knowledge integration. arXiv preprint arXiv",
                "Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, and Steven Heidel. Gpt-3.5 turbo fine-tuning and api updates, August 2023b. URL https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates. Illustration: Ruby Chen",
                "Trelis. fllama 2 - function calling llama 2, 2023. URL https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling",
                "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv",
                "Andrew D Selbst. Negligence and ai's human users. BUL Rev",
                "Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/detoxify",
                "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv",
                "Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv",
                "Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv",
                "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv",
                "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv",
                "Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv",
                "Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A Lemley, and Percy Liang. Foundation models and fair use. arXiv preprint arXiv:2303.15715, 2023b",
                "OpenAI. Gpt-4 technical report, 2023d",
                "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv",
                "Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv",
                "Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\u00f6ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions. arXiv preprint arXiv"
            ],
            "related work": "Large language models (LLMs) are language models with a large number of parameters trained on web-scale text corpra (Brown et al., 2020; OpenAI, 2023d; Touvron et al., 2023b) . With the increase of their sheer scale, LLMs are found to exhibit emergent capabilities (Bommasani et al., 2021) , such as improved few-shot learning, in-context learning (Brown et al., 2020) , and chain-of-thought reasoning (Wei et al., 2022) . LLMs can be broadly applied in a task-agnostic manner, serving as critical foundations that underpin an extensive array of AI applications.Fine-tuning. Fine-tuning has been widely employed to adapt pre-trained LLMs to downstream applications (Howard & Ruder, 2018; Devlin et al., 2018; Radford et al., 2018) , and to integrate pre-trained models from different modalities (Zhu et al., 2023; Dai et al., 2023; Liu et al., 2023a) . Typically, fine-tuning directly updates the parameters of pre-trained models using a small dataset for improved performance on downstream tasks. Numerous Parameter-Efficient Fine-Tuning (PEFT) approaches have been developed to further balance the quality and efficiency of this process (Hu et al., 2021; Zaken et al., 2021; Lester et al., 2021; Zhang et al., 2023) . Although alternatives like in-context learning (Dong et al., 2022) and prompt engineering (White et al., 2023) do not require parameter changes, fine-tuning still remains preferable in many settings as it avoids additional inference-time overhead and often delivers better and more stable results (Hao et al., 2022; Addlesee et al., 2023; Liu et al., 2022; Mosbach et al., 2023) .Alignment of LLMs. There is a gap between LLMs' language modeling objective (e.g., predicting the next token) during pre-training and the aim of  \"following instructions and being helpful, truthful and harmless \" in LLMs' final use cases (Ouyang et al., 2022) . Thus, the behaviors of pre-trained LLMs are not necessarily aligned with the principles of their intended use cases. Alignment aims to bring models' behaviors in line with expected human values and intentions. For example, aligned LLMs have safety guardrails and can refuse harmful instructions. Currently, the two most common alignment techniques are Instruction Tuning (Wei et al., 2021; Ouyang et al., 2022) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022a) , while other alignment techniques such as Constitutional AI (Bai et al., 2022b) and self-alignment (Sun et al., 2023) are also emerging. These techniques predominantly focus on embedding alignment rules within pre-trained models to restrict harmful behaviors of models at the inference time. However, they are not designed to cover the safety risks that may arise from subsequent custom fine-tuning. This work reveals that even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning.Red Teaming LLMs. In the context of LLM research, the term red teaming has recently been used to describe systematic tests or attacks on LLMs to uncover their potential harmfulness and safety vulnerabilities (Perez et al., 2022; Ganguli et al., 2022; OpenAI, 2023d; Microsoft, 2023) . Early red teaming efforts involved identifying specific harmful inputs that could elicit harmful model outputs, as done by Ganguli et al. (2022) . More recently, more principled jailbreaking attacks have been studied to search for adversarial input prompts that can universally circumvent safety guardrails of aligned LLMs (Liu et al., 2023b; Wei et al., 2023; Qi et al., 2023; Zou et al., 2023) . This work also falls within the scope of red teaming studies but focuses on tests and attacks of the fine-tuning process, aiming to uncover the potential safety risks associated with fine-tuning aligned LLMs.",
            "date": "2023"
        },
        "topic": "Alignment of LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models",
                "arxiv_id": "2409.00598",
                "subtitles": [
                    "Safety alignment of LLMs",
                    "Safety-Usability Trade-off",
                    "Benchmarking False Refusals",
                    "Red-teaming LLMs",
                    "Controllable text generation"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
                    "Steering without side effects: Improving post-deployment control of language models",
                    "Navigating the OverKill in Large Language Models, January",
                    "Cold-attack: Jailbreaking llms with stealthiness and controllability",
                    "Shieldgemma: Generative ai content moderation based on gemma",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "The claude 3 model family: Opus, sonnet, haiku",
                    "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
                    "The llama 3 herd of models",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms, 2024b",
                    "Auditing large language models: a three-layered approach",
                    "Refusal in language models is mediated by a single direction",
                    "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
                    "Red teaming language models with language models",
                    "Curiosity-driven red-teaming for large language models",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Autodan: Interpretable gradient-based adversarial attacks on large language models",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Diffusion-lm improves controllable text generation",
                    "Safe rlhf: Safe reinforcement learning from human feedback",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Representation engineering: A top-down approach to ai transparency",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Cold decoding: Energy-based constrained text generation with langevin dynamics",
                    "Rlvf: Learning from verbal feedback without overgeneralization",
                    "How is chatgpt's behavior changing over time",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Nothing in excess: Mitigating the exaggerated safety for llms via safety-conscious activation steering",
                    "On the exploitability of instruction tuning",
                    "Towards comprehensive and efficient post safety alignment of large language models via safety patching",
                    "Or-bench: An over-refusal benchmark for large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Attacking large language models with projected gradient descent",
                    "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
                    "Cgmh: Constrained sentence generation by metropolis-hastings sampling",
                    "Advprompter: Fast adaptive adversarial prompting for llms",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "6Related WorkSafety alignment of LLMs.The development cycle of large language models (LLMs) includes multiple stages of safety alignment to ensure their behavior aligns with human values (e.g., see Llama3's technical reportDubey et al. (2024) ) . During pre-training, developers filter data to exclude harmful content, reducing the likelihood of the model generating such content. In fine-tuning, developers use supervised fine-tuning and RLHF(Ouyang et al.,2022; Bai et al.,2022; Dai et al.,2023; Rafailov et al.,2024) with safety-related positive and negative examples (e.g., BeaverTailsJi et al. (2024) ) to adjust the model's refusal boundaries. Finally, at deployment, system-level safety filters (e.g., Llama GuardInan et al. (2023) and ShieldGemmaZeng et al. (2024a) ) detect and block harmful inputs or outputs. A side effect of such alignment   often called the  \"alignment tax \"   is that LLMs sometimes overfit simple rules in training data(Stephan et al.,2024) , leading to false refusals.Safety-Usability Trade-off.The trade-off between safety and usability (aka. harmlessness and helpfulness) in language models has been a long-standing issue. Earlier work focuses on how safety alignment affects LLM performance on general tasks like factual QA, mathematical reasoning, and coding(Bai et al.,2022; Ganguli et al.,2022; Chen et al.,2023) . Recent studies discuss the false refusal issue, also referred to as exaggerated safety, over-defensiveness, and overkill.Bianchi et al. (2023) find that too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones.Varshney et al. (2023) find that self-checking-based jailbreak defenses, which prompt the LLM to check its own input and output, significantly increases false refusal rates on some harmless prompt datasets. Some LLMs' technical reports also discuss the false refusal behaviors(Anthropic,2024; Dubey et al.,2024; Inan et al.,2023) .Some other work aims to either mitigate or exacerbate this trade-off. Based on the interpretability of refusal behaviors(Zou et al.,2023a; Arditi et al.,2024) ,Cao et al. (2024) identify the refusal vectors in LLM's representation spaces and steer them to strike a better trade-off, whereasStickland et al. (2024) take an additional step before steering to avoid side effects.Zhao et al. (2024a) use post safety alignment to mitigate the trade-off. On the other hand,Shu et al. (2024) designs data poisoning methods to induce LLMs to refuse benign and reasonable instructions, which makes the models less helpful and exacerbates the trade-off. Nevertheless, whether this trade-off is intrinsic for auto-regressive LLMs is still an open question, and our results show that scaling alone cannot fix it.Benchmarking False Refusals.As false refusals gain attention, some work constructs dedicated datasets and conducts a systematic evaluation. XStest(R\u00f6ttger et al.,2023a) and OKtest(Shi et al.,2024) design specific patterns of false refusal and craft pseudo-harmful prompts manually or with the assistance of LLMs. Given the limited size of these public datasets, a concurrent workCui et al. (2024) develops a pipeline to automatically generate a large-scale false refusal dataset, named OR-Bench. The pipeline generates or utilizes existing harmful seed prompts, then repeatedly rewrites them with LLMs until some LLM-based moderators consider them harmless. The OR-Bench dataset includes a total of 80,000 pseudo-harmful prompts, with 1,000 of them being particularly challenging for LLMs. Compared to our dataset, OR-Bench is larger, whereas ours features separately labeled controversial prompts. Compared to our generation method, their pipeline can be more efficient and scalable, while ours can target specific LLMs to generate tailored pseudo-harmful prompts and serve as a red-teaming tool. The two datasets and methods can trigger different false refusal patterns, making them complementary. The authors also benchmark the false refusal rates of various LLMs using this dataset and observe a similar trade-off to ours.Red-teaming LLMs.Before deployment, providers audit(M\u00f6kander et al.,2023) and test their LLMs with test cases (i.e., prompts) that elicit unwanted responses. Red-teaming is usually done with human-crafted prompts(Ganguli et al.,2022) or prompts generated by language models(Perez et al.,2022; Hong et al.,2024) . Recently, many works propose jailbreak attacks for red-teaming safety(Zou et al.,2023b; Liu et al.,2023b; Lapid et al.,2023; Chao et al.,2023; Zeng et al.,2024b; Andriushchenko et al.,2024) . However, false refusal as another type of unwanted response is under-explored in the regime of red-teaming.Controllable text generation.With our designed objective, we could potentially use other controllable text generation methods beyondZhu et al. (2023) to generate better pseudo-harmful prompts. These methods include discrete optimization algorithms like genetic algorithm(Liu et al.,2023a) , sampling-based method like M-H sampling(Miao et al.,2019) , gradient-based methods (via the Gumbel-softmax trick(Guo et al.,2021) , Langevin dynamics(Qin et al.,2022; Guo et al.,2024) , and projections(Wen et al.,2024; Geisler et al.,2024) ) , and diffusion-LMs(Li et al.,2022) . Specifically,Paulus et al. (2024) trains a model to capture the distribution of jailbreak prompts, enabling it to generate jailbreaks (or false refusals, in our case) in a single inference, which significantly speeds up generation and seamlessly adapts to different target LLMs. We leave exploring better generation methods for future work.",
                "abstract": "Safety-aligned large language models (LLMs) sometimes falsely refuse pseudo-harmful prompts, like \"how to kill a mosquito,\" which are actually harmless. Frequent false refusals not only frustrate users but also provoke a public backlash against the very values alignment seeks to protect. In this paper, we propose the first method to auto-generate diverse, content-controlled, and model-dependent pseudo-harmful prompts. Using this method, we construct an evaluation dataset called PHTest, which is ten times larger than existing datasets, covers more false refusal patterns, and separately labels controversial prompts. We evaluate 20 LLMs on PHTest, uncovering new insights due to its scale and labeling. Our findings reveal a trade-off between minimizing false refusals and improving safety against jailbreak attacks. Moreover, we show that many jailbreak defenses significantly increase the false refusal rates, thereby undermining usability. Our method and dataset can help developers evaluate and fine-tune safer and more usable LLMs. Our code and dataset are available atthis https URL"
            },
            {
                "name": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
                "arxiv_id": "2409.00935",
                "subtitles": [
                    "Instruction Tuning",
                    "Alignment Evaluation",
                    "AI Feedback"
                ],
                "reference": [
                    "Shepherd: A critic for language model generation",
                    "Generative judge for evaluating alignment",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Look before you leap: An exploratory study of uncertainty measurement for large language models",
                    "Free dolly: Introducing the world's first truly open instruction-tuned LLM",
                    "Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization",
                    "System combination via quality estimation for grammatical error correction",
                    "Neural quality estimation of grammatical error correction",
                    "Constitutional AI: harmlessness from AI feedback",
                    "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Self-alignment with instruction backtranslation",
                    "The Flan collection: Designing data and methods for effective instruction tuning",
                    "QLoRA: Efficient finetuning of quantized LLMs",
                    "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
                    "Judging LLM-as-a-judge with MT-bench and Chatbot Arena",
                    "Self-rewarding language models",
                    "Stanford Alpaca: An instruction-following LLaMA model",
                    "LIMA: less is more for alignment",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "RLAIF: scaling reinforcement learning from human feedback with AI feedback",
                    "AlpacaEval: An automatic evaluator of instruction-following models",
                    "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                    "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
                    "COMET: A neural framework for MT evaluation",
                    "SALMON: self-alignment with principle-following reward models",
                    "WizardLM: Empowering large language models to follow complex instructions",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Out-of-distribution detection and selective generation for conditional language models",
                    "AlpacaFarm: A simulation framework for methods that learn from human feedback",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work2.1Instruction TuningAligning large language models to human preference is important since it stops the generation of harmful and useless content. Reinforcement learning from human feedback (RLHF) has become a standard approach to align LLMs. This involves initial instruction fine-tuning followed by reinforcement learning, as detailed byOuyang et al. (2022) . The focus has recently shifted towards self-alignment strategies, which aim to align LLMs more efficiently and cost-effectively. Key areas of exploration include instruction generation and reward modeling.Wang et al. (2023c) demonstrate the potential of generating instructions and responses using in-context learning with just 175 human-labeled examples. Following this, research bySun et al. (2023b) has shown that model alignment can be achieved with as few as five labeled examples. Another study fromLi et al. (2023b) introduces the concept of back-translating responses into instructions as a novel alignment technique. Furthermore,Zhou et al. (2023) highlight the significance of instruction diversity over quantity for effective alignment. For reward modeling, recent works have leveraged large language models to provide feedback. A study bySun et al. (2023a) utilizes an LLM to offer preference feedback on model responses based on a set of principles.Bai et al. (2022) focus on generating non-harmful prompts and employ LLMs to provide feedback, aiming to identify less harmful responses.2.2Alignment EvaluationThere has been a growing interest in the automatic evaluation of chat assistant performance. This trend is driven by the absence of reliable evaluation metrics and the prohibitive costs of human evaluation. Recent studies have leveraged state-of-the-art models, such as GPT-4, to assess the capabilities of less sophisticated modelsZheng et al. (2023) ; Li et al. (2023c) . These investigations have uncovered a strong correlation between evaluations conducted by GPT-4 and those performed by humansZheng et al. (2023) ; Li et al. (2023c) ; Dubois et al. (2023) . Nevertheless, the proprietary nature and the substantial expense linked to the use of GPT-4 have spurred initiatives aimed at creating open-source judge models. These models are designed to offer consistent and cost-effective evaluationsLi et al. (2023a) ; Wang et al. (2023a,b) .Li et al. (2023a) develop an open-source model, auto-j, which is distilled from GPT-4. This model is designed to assess alignments by furnishing both a critique and a score. Similarly,Wang et al. (2023b) introduce a model also distilled from GPT-4, designed to express a preference between pairs of responses to a given question, which is particularly useful for optimizing hyper-parameters during instruction tuning.Cui et al. (2023) focus on constructing an open-source reward model that leverages preference feedback data from GPT-4 for reward modeling. This model is intended to support the community in developing more effective alignment algorithms. Another contribution byWang et al. (2023a) involves the creation of a model capable of generating critiques for model responses, which is achieved by aggregating critique data from the Internet. To the best of our knowledge, our method represents thefirstattempt to train a judge model that does not depend on demonstration scores from GPT-4 for its training.2.3AI FeedbackReinforcement learning from human feedback (RLHF) has traditionally been a resource-intensive process, which needs significant human effort to collect feedback. Recent work has introduced a new approach, reinforcement learning from AI feedback (RLAIF) , which leverages large language models to provide feedback, thereby replacing the need for human input.Bai et al. (2022) utilize LLMs to offer preference feedback on the outcomes of harmful prompts. This feedback is then used to train a reward model aimed at aligning with harmless content.Yuan et al. (2024) prompt LLMs to assign numerical scores to model responses. These scored responses are subsequently paired and used in iterative training of the model, employing direct preference optimizationRafailov et al. (2023) . Another study investigates self-alignment by converting responses back into instructions, with LLMs filtering out instructions of lower qualityLi et al. (2023b) .Lee et al. (2023) delve into the utilization of AI feedback specifically for the task of summarization. Furthermore, the embedding of principles within reward modeling through feedback from large language models has been proposed as a novel strategy for enhancing alignmentSun et al. (2023a) . Our research investigates self-alignment evaluation, which is closely linked to the concept of AI feedback.2.4Open-Source InstructionsThe pursuit of open-source advancements within the community extends beyond just models to include data as well.Wang et al. (2023c) andTaori et al. (2023) have leveraged in-context learning to generate a large set of instructions. Similarly,Conover et al. (2023) have opted for a human-centric approach, gathering instructions and responses to compile an open-source dataset featuring 15k instructions.Longpre et al. (2023) have embarked on a mission to amass a large collection of instructions with a focus on traditional NLP tasks, such as natural language understanding and question answering.Xu et al. (2023) have utilized ChatGPT to generate challenging instructions, discovering that fine-tuning LLMs with difficult prompts can notably enhance model performance. Similarly,Ding et al. (2023) have employed ChatGPT for generating a vast array of dialogue data, further illustrating the versatility of LLMs in simulating complex conversational scenarios. The instruction set of ShareGPT has been widely used for instruction-tuningChiang et al. (2023) . In our work, we compile a comprehensive collection of practical and high-quality instructions from Hugging Face, contributing to the pool of resources available for enhancing the effectiveness and efficiency of instruction-tuning LLMs.2.5Uncertainty Estimation for Language GenerationThere is a growing interest in measuring uncertainty in conditional language generation.Ren et al. (2023) introduce an approach that involves training a lightweight model, incorporating both encoder and decoder embeddings derived from transformers. This model is tailored for selective instruction following as well as identification of out-of-distribution (OOD) samples. Crucially, their research highlights the inadequacy of perplexity as a reliable measure for gauging a model's confidence level in generated text, suggesting the need for alternative metrics. Building on this quest for better uncertainty metrics,Kuhn, Gal, and Farquhar (2023) have proposed the concept of semantic entropy. This method employs a natural language inference (NLI) model to estimate the semantic discrepancies among several model responses to an input, providing a more nuanced understanding of model uncertainty. Similarly, the notion of sampling variance has been explored byHuang et al. (2023) , which quantifies the semantic variability across multiple samples generated by a model. This technique offers another layer of insights into a model's performance by assessing the consistency of its outputs. In the field of machine translation (MT) , efforts byGuerreiro, Voita, and Martins (2023) andRei et al. (2020) have delved into the training of classifiers dedicated to the estimation of translation quality. The work ofChollampatt and Ng (2018) andQorib and Ng (2023) concerns quality estimation of grammatical error correction.",
                "abstract": "Pre-trained large language models (LLMs) can be tailored to adhere to human instructions through instruction tuning. However, due to shifts in the distribution of test-time data, they may not always execute instructions accurately, potentially generating factual errors or misaligned content when acting as chat assistants. To enhance the reliability of LLMs in following instructions, we propose the study of selective instruction following, whereby the system declines to execute instructions if the anticipated response quality is low. We train judge models that can predict numerical quality scores for model responses. To address data scarcity, we introduce Self-J, a novel self-training framework for developing judge models without needing human-annotated quality scores. Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data. It incorporates a gold reference answer to facilitate self-evaluation and recalibrates by assessing the semantic similarity between the response sample and the gold reference. During the training phase, we implement self-distillation as a regularization technique to enhance the capability of reference-free estimation. To validate alignment evaluation on general instruction-following tasks, we collect large-scale high-quality instructions from Hugging Face for model training and evaluation. Extensive experiments on five open-source models show that our method correlates much more with GPT-4 than strong baselines, e.g., supervised models distilled from GPT-4 and GPT-3.5-turbo. Our analysis shows our model's strong generalization across domains. Additionally, our judge models serve as good reward models, e.g., boosting WizardLM-13B-V1.2 from 89.17 to 92.48 and from 12.03 to 15.90 in version v1 and v2 of AlpacaEval respectively using best-of-32 sampling with our judge models."
            },
            {
                "name": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
                "arxiv_id": "2409.03735",
                "subtitles": [
                    "Large Language Models",
                    "Contextual Integrity",
                    "Contextual Integrity and LLMs",
                    "Evaluating Sociotechnical Properties"
                ],
                "reference": [
                    "What did i do wrong? quantifying llms' sensitivity and consistency to prompt engineering",
                    "Enhancing privacy and security through robust access management-cdt",
                    "Exploring the sensitivity of LLMs' decision-making capabilities: Insights from prompt variations and hyperparameters",
                    "Goldcoin: Grounding large language models in privacy laws via contextual integrity theory",
                    "Sensitivity and robustness of large language models to prompt template in Japanese text classification tasks",
                    "How are prompts different in terms of sensitivity",
                    "Extracting training data from large language models",
                    "Learning privacy expectations by crowdsourcing contextual informational norms",
                    "Regulation (EU) 2016/679 of the European Parliament and of the Council",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities",
                    "URL https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield",
                    "Sensitive information",
                    "Analyzing leakage of personally identifiable information in language models",
                    "Privacy checklist: Privacy violation detection grounding on contextual integrity theory",
                    "The secret sharer: Evaluating and testing unintended memorization in neural networks",
                    "Respect for context as a benchmark for privacy online: What it is and isn't",
                    "Stop the Spread: A Contextual Integrity Perspective on the Appropriateness of COVID-19 Vaccination Certificates",
                    "Measuring privacy: An empirical test using context to expose confounding variables",
                    "Discovering smart home internet of things privacy norms using contextual integrity",
                    "Privacylens: Evaluating privacy norm awareness of language models in action",
                    "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory",
                    "Position: TrustLLM: Trustworthiness in Large Language Models",
                    "Attention is all you need",
                    "Language models are few-shot learners",
                    "Privacy as contextual integrity",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Air Gap: Protecting Privacy-Conscious Conversational Agents, May",
                    "On the worst prompt performance of large language models",
                    "Trustgpt: A benchmark for trustworthy and responsible large language models",
                    "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
                    "Awq: Activation-aware weight quantization for on-device llm compression and acceleration",
                    "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
                    "Disaster privacy/privacy disaster",
                    "Trustllm: Trustworthiness in large language models",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Background and Related WorkWe present a brief primer on LLMs (Section2.1) and CI (Section2.2) , followed by describing related work at the intersection of CI and LLMs (Section2.3) and evaluation of socio-technical properties in LLMs (Section2.4) .2.1Large Language ModelsCurrent state-of-the-art language models use transformers with billions of model parameters[53,6]. These language text generation models are trained to predict the next tokens in a sentence given previous tokens. The model learns the distribution \\textbf{Pr}(x_{1},x_{2},\\ldots,x_{n}) =\\Pi_{i=1}^{n}\\textbf{Pr}(x_{i}\\mid x_{1}% ,\\dots,x_{i-1}) where x_{1},x_{2},\\ldots,x_{n} is a sequence of tokens taken from a given vocabulary. A neural network, f_{\\theta}, with parameters \\theta, is used to estimate this probability distribution by outputting the likelihood of token x_{i} given by f_{\\theta}(x_{i}\\mid x_{1},\\dots,x_{i-1}). During training, a language model learns to maximize the probability of the data in a training set containing text documents (e.g., news articles or webpages) . Formally, the training involves minimizing the loss function \\mathcal{L}(\\theta) =-\\log\\Pi_{i=1}^{n}f_{\\theta}(x_{i}\\mid x_{1},\\dots,x_{i-1}) over each training example in the training dataset. Once trained, a language model can generate new text conditioned on some prompt as prefix with tokens x_{1},\\dots,x_{i} by iteratively sampling \\hat{x}_{i+1}\\sim f_{\\theta}(x_{i+1}|x_{1},\\dots,x_{i}) and then feeding \\hat{x}_{i+1} back into the model to sample \\hat{x}_{i+2}\\sim f_{\\theta}(x_{i+2}|x_{1},\\dots,\\hat{x}_{i+1}). Training LLMs is resource- and time-intensive, so pre-trained public models are fine-tuned for specific objectives before deployment. Popular fine-tuning optimization techniques include,alignmentfor matching with human annotations, andquantizationreduce model capacity for efficiency.Alignment.Training data scrapped from the Internet can include inappropriate content such as hate speech, stereotypes) [39,52,20,54]that LLMs might memorize and reproduce during inference. To address this drawback, LLMs are aligned through fine-tuning on human-annotated datasets that correct inappropriate responses. As a result, the model replaces inappropriate responses with a standard responses like  \"I'm sorry, but I cannot ... \". Popular alignment techniques include: instruction fine-tuning[44]to follow natural language instructions; reinforcement learning from human feedback (RLHF) [44]using a reward model based on human-annotated data to reward or penalize model responses; direct preference optimization (DPO) [46]which simplifies RLHF by using LLM's output probabilities to align with human preferences without a reward model.Quantization.LLMs demand powerful GPUs because of their high capacity. Quantization improves efficiency by reducing the precision of weights and forcing them to take fixed set of values. This allows models to run on smaller devices. Activation-aware quantization[26]is one such approach that analyzes activation distributions to retain important parameters and eliminate redundant ones while maintaining utility.2.2Contextual IntegrityContrary to predominant accounts of privacy that focus on aspects such as protecting sensitive information types[43], enforcing access control[45]or mandating procedural policies and purposes[13], the theory of CI defines privacy as an appropriate flow of information as governed by established societal norms[41]. According to CI, privacy is prima facie violated only when an information flow breaches an established contextual informational norm (aka CI norms or privacy norms) , which reflect the values, purposes and function of a given context. A CI-based assessment of privacy implication of a system or a service involves two main phases: a) identifying the norm breaching flow using CI and b) examining the breach using the CI heuristic to determine how the novel flow contributes the values and purposes of the context.Identifying the norm breaching flow.The CI framework requires identifying five essential parameters to capture the information flow and the establishes norms in a given context including:(i) roles or capacities ofsenders,subjects, andrecipientsin the context they operate (like professors in an educational context and doctors in the health context) ;(ii) thetype of informationthey share;(iii) transmitted principleto state the conditions, for purposes or constraints under which the information flow is conducted.A canonical example below describes a typical interaction between a patient and a doctor.CI ExamplePatient(sender) sharingpatient's(subject) medical data(information type) witha doctor(recipient) for a medical check up(transmission principles) All the five parameter values matter. A change in any of the values results in a novel information flow. For instance, if instead of a doctor, a colleague is a recipient or instead of using the information for a medical check up, the information is made public, which could constitute a breach of an established social norm.Examining the breach.After we detect a violation, as part of the normative assessment, we use the CI heuristic to examine the ethical, financial, social and even political implications[42]. At the end of the process, we can either discard the novel information flow or modify the existing norm to better reflect the societal values and expectations. Several works have used the CI framework to gauge and evaluate privacy norms in different social context such as education[50], IoT[2], COVID-19 pandemic[59]and natural disasters[47]. They employed a survey methodology using CI-based vignettes to gauge the appropriateness of potential information flows. These vignettes are of the form:<information flow with five parameters>.How acceptable is the above information flow? [strongly unacceptable, somewhat unacceptable, neutral, somewhat acceptable, strongly acceptable]2.3Contextual Integrity and LLMsA number of recent studies have applied CI to evaluate LLMs.Mireshghallah et al. [37]use CI and theory of mind to evaluate the alignment of LLMs with human annotated responses. They present, ConfAIde, a benchmark to use CI for LLMs with 98 prompts fromMartin and Nissenbaum [33]. Their study shows that LLM responses have low correlation with human annotations, with GPT-4 demonstrating better alignment compared to other models. In a follow up work,Huang et al. [21]have used ConfAIde to investigate the alignment of 16 mainstream LLMs with human annotations. They find that  \"most LLMs possess a certain level of privacy awareness \" as the probability of LLMs refusing to answer private information increases significantly when they are instructed to follow privacy policies or maintain confidentiality. Similar to results ofMireshghallah et al. [37], they show that Pearson's correlation between human and LLM agreement varies widely and ChatGPT has the highest correlation among other models.Shao et al. [49]evaluate the norms of LLMs when used as agents with a focus on privacy norms in LLM-mediated communication (i.e., LLMs being used to send emails) . They assess how well LLM responses align with crowd-sourced ground truth and measure privacy leakage from out-of-context information sharing.Fan et al. [14]align LLMs with specific legal statutes to evaluate privacy violations and understand complex contexts for identifying real-world privacy risks. They generate synthetic cases and fine-tune their model to improve LLMs' ability to recognize privacy risks in actual court cases. However, their approach relies on limited number of expert-annotated norms and social contexts. To address these gaps,Li et al. [25]develop a comprehensive checklist that includes social identities, private attributes, and existing privacy regulations. Using this checklist, they demonstrate that LLMs can fully cover HIPAA regulations.Bagdasaryan et al. [4]describe an attack that manipulates LLMs into revealing sensitive information by altering the context, such as fabricating an alien invasion to compel the model to disclose user details for  \"saving Earth. \" Existing defenses like differential privacy and data sanitization fail because they do not account for context, and alignment is susceptible to jailbreaking. They use CI theory to mitigate information disclosures by proposing the use of two separate LLMs: one as a data minimization filter to identify appropriate information to disclose based on context, and the other that interacts with clients using the filtered data.2.4Evaluating Sociotechnical PropertiesSeveral benchmarks evaluate various LLMs sociotechnical properties such as toxicity, fairness, bias, sycophancy, privacy, robustness, and ethics[54,37,39,52,20].On the other hand, LLMs have been shown to be sensitive to small variations in prompts which can drastically alter responses[7,12,29,16,48,28]. Previous studies comparing LLM decision-making to human behavior often overlook this sensitivity.Loya et al. [28]demonstrate that simple prompt adjustments can make LLMs exhibit more human-like behavior, questioning the reliability of current evaluation methods[1]. There are limited studies consider prompt sensitivity:Lu et al. [30]propose generating synthetic prompts for better results. However, this is not suitable for assessing the encoded norms in LLMs as we require to query using CI-based vignettes and not synthetic prompts. Hence, a methodology for accounting for prompt sensitivity is largely an open problem.There are a number of prior works that focus solely on assessing the leakage of sensitive data, including personally identifiable information to enhance LLMs privacy[8,9,31]. We can view them as assessment of a single CI parameter (data type) , whereas a comprehensive CI approach requires all five parameters to make a privacy violation determination. Hence, these are orthogonal to our work.",
                "abstract": "Large language models (LLMs), while memorizing parts of their training data scraped from the Internet, may also inadvertently encode societal preferences and norms. As these models are integrated into sociotechnical systems, it is crucial that the norms they encode align with societal expectations. These norms could vary across models, hyperparameters, optimization techniques, and datasets. This is especially challenging due to prompt sensitivity$-$small variations in prompts yield different responses, rendering existing assessment methodologies unreliable. There is a need for a comprehensive framework covering various models, optimization, and datasets, along with a reliable methodology to assess encoded norms.We present LLM-CI, the first open-sourced framework to assess privacy norms encoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette methodology to assess the encoded norms across different contexts and LLMs. We propose the multi-prompt assessment methodology to address prompt sensitivity by assessing the norms from only the prompts that yield consistent responses across multiple variants. Using LLM-CI and our proposed methodology, we comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior work, examining the impact of model properties (e.g., hyperparameters, capacity) and optimization strategies (e.g., alignment, quantization)."
            },
            {
                "name": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting",
                "arxiv_id": "2408.09798",
                "subtitles": [
                    "Text-centric Multimodal Alignment",
                    "Robustness in Multimodal Learning",
                    "Adversarial Prompting"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "An overview of image caption generation methods",
                    "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
                    "Show and tell: A neural image caption generator",
                    "Ignore previous prompt: Attack techniques for language models",
                    "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
                    "Are multimodal transformers robust to missing modality",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Videochat: Chat-centric video understanding",
                    "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                    "Prompt Injection attack against LLM-integrated Applications",
                    "A prompt-based approach to adversarial example generation and robustness enhancement",
                    "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
                    "Promptattack: Probing dialogue state trackers with adversarial prompts",
                    "Chatcad: Interactive computer-aided diagnosis on medical image using large language models",
                    "Too large; data reduction for vision-language pre-training",
                    "Visual Instruction Tuning",
                    "Can contrastive learning avoid shortcut solutions",
                    "Text-centric Alignment for Multi-Modality Learning",
                    "Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue"
                ],
                "related_work": "2Related Work2.1Text-centric Multimodal AlignmentIn recent advancements, several studies have demonstrated the effectiveness of text-centric alignment. For instance, LLaVA(Liu et al.2023c) utilizes GPT-4 to generate captions and textual descriptions from images, while VideoChat-Text(Li et al.2023) encodes video content into textual formats. In the medical domain, models like OphGLM(Gao et al.2023) and ChatCAD(Wang et al.2023a) extract information from medical images and convert it into diagnostic reports, seamlessly integrating visual data with textual inputs for LLMs. TAMML(Tsai et al.2024a) converts different input modalities into text for downstream model training and demonstrates significant improvements in handling unseen and diverse modality at test time. These approach depends on the quality of transformed text but offers a straightforward way to achieve multimodal integration.2.2Robustness in Multimodal LearningModality robustness(Ma et al.2022) addresses the issue of different modalities displaying various noise typologies and the potential for real-world multimodal signals to suffer from missing or noisy data in at least one of the modalities. Similar challenges have been identified in text-centric multimodal alignment methods. Wang et al.(Wang et al.2023b) discovered that Vision LLMs trained on purely synthetically generated high-quality captions by image caption models, intended to replace original noisy data, suffer from model collapse(Robinson et al.2021) . This phenomenon can be attributed to captioning collapse(Vinyals et al.2015; Wang, Zhang, and Yu2020) and the one-to-many problem(Young et al.2014) in image captioning. When transforming images into text, these models generate fixed or similar captions for different images, limiting diversity in the output and leading to trivial solutions.2.3Adversarial PromptingAdversarial prompting exposes vulnerabilities in large language models (LLMs) by manipulating their outputs through various techniques. One such technique,prompt injection(Liu et al.2023b) , involves embedding malicious instructions within prompts to alter the intended response of the model, potentially generating harmful or inappropriate content. Another significant method isprompt leaking(Perez and Ribeiro2022; Hui et al.2024) , where crafted prompts extract sensitive information embedded within the model's responses, compromising confidentiality.Jailbreaking(Ma et al.2024; Chao et al.2023; Liu et al.2023a) techniques bypass the safety mechanisms of LLMs, enabling the model to produce outputs that violate its ethical guidelines.Additionally, adversarial prompting has been employed to generate adversarial examples. Techniques such as the Prompt-based Attack Approach (PAT) (Yang et al.2024; Dong et al.2023; Xu and Wang2024) generate adversarial examples via mask-and-filling, exploiting the robustness defects of LLMs. These methods have demonstrated high attack success rates, producing diverse, fluent, and natural adversarial examples that can used to significantly improve the robustness of NLP models.",
                "abstract": "Converting different modalities into generalized text, which then serves as input prompts for large language models (LLMs), is a common approach for aligning multimodal models, particularly when pairwise data is limited. Text-centric alignment method leverages the unique properties of text as a modality space, transforming diverse inputs into a unified textual representation, thereby enabling downstream models to effectively interpret various modal inputs. This study evaluates the quality and robustness of multimodal representations in the face of noise imperfections, dynamic input order permutations, and missing modalities, revealing that current text-centric alignment methods can compromise downstream robustness. To address this issue, we propose a new text-centric adversarial training approach that significantly enhances robustness compared to traditional robust training methods and pre-trained multimodal foundation models. Our findings underscore the potential of this approach to improve the robustness and adaptability of multimodal representations, offering a promising solution for dynamic and real-world applications."
            },
            {
                "name": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings",
                "arxiv_id": "2408.14512",
                "subtitles": [
                    "Graph neural networks",
                    "Self-supervised learning and prompt-tuning for GNNs",
                    "Large language models for graphs"
                ],
                "reference": [
                    "Gcc: Graph contrastive coding for graph neural network pre-training",
                    "Can llms effectively leverage graph structural information: when and why",
                    "Large-scale learnable graph convolutional networks",
                    "Label-free node classification on graphs with large language models (LLMs",
                    "Can language models solve graph problems in natural language",
                    "Unigraph: Learning a cross-domain graph foundation model from natural language",
                    "All in one: Multi-task prompting for graph neural networks",
                    "Natural language is all a graph needs",
                    "Graphmae: Self-supervised masked graph autoencoders",
                    "How powerful are graph neural networks",
                    "Deep graph contrastive representation learning",
                    "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks",
                    "Graphgpt: Graph instruction tuning for large language models",
                    "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
                    "Do transformers really perform badly for graph representation",
                    "Graph transformer networks",
                    "Wiener graph deconvolutional network improves graph self-supervised learning",
                    "Opengraph: Towards open graph foundation models",
                    "Inductive representation learning on large graphs",
                    "Predicting path failure in time-evolving graphs",
                    "Llaga: Large language and graph assistant",
                    "Nodeformer: A scalable graph structure learning transformer for node classification",
                    "Empower text-attributed graphs learning with large language models (llms",
                    "Graph contrastive learning with adaptive augmentation",
                    "Graph contrastive learning automated",
                    "Graph attention networks",
                    "Multi-level hyperedge distillation for social linking prediction on sparsely observed networks",
                    "Exploring the potential of large language models (LLMs) in learning on graph",
                    "L2-gcn: Layer-wise and learned efficient training of graph convolutional networks",
                    "One for all: Towards training one graph model for all classification tasks",
                    "Graphmae2: A decoding-enhanced masked self-supervised graph learner",
                    "Graphprompt: Unifying pre-training and downstream tasks for graph neural networks",
                    "DIFFormer: Scalable (graph) transformers induced by energy constrained diffusion",
                    "Contrastive multi-view representation learning on graphs",
                    "Deep graph infomax",
                    "Semi-supervised classification with graph convolutional networks",
                    "Multi-task self-supervised graph neural networks enable stronger task generalization",
                    "Evaluating large language models on graphs: Performance insights and comparative analysis",
                    "FastGCN: Fast learning with graph convolutional networks via importance sampling",
                    "Simgrace: A simple framework for graph contrastive learning without data augmentation",
                    "TEST: Text prototype aligned embedding to activate LLM's ability for time series"
                ],
                "related_work": "4Related work4.1Graph neural networksIn the field of graph machine learning, Graph Neural Networks (GNNs) have garnered significant attention[5,22,28,40,9,6,46,1]. The primary strategy of most GNNs is to capture underlying message-passing patterns for graph representation. Several effective neural network architectures have been proposed, such as Graph Attention Network (GAT) [31], Graph Convolution Network (GCN) [20], and GraphSAGE[11]. Recently, there has been a surge of interest in exploring transformer-based encoders for graph machine learning[49,45,36,37]. However, a notable limitation of GNNs is their generalization capability. Typically, GNNs are trained on specific tasks within particular datasets, and when faced with new datasets or tasks, they often struggle to consistently perform well across different datasets or downstream tasks[19].4.2Self-supervised learning and prompt-tuning for GNNsTo alleviate the demand for labeled data and enhance the robustness of graph models, self-supervised learning is commonly employed in GNN training[38,52,12]. Methods like Deep Graph Infomax (DGI) [32]utilize mutual information maximization for pre-training. Other approaches, such as GraphCL[44], GCA[53], GCC[26], and JOAO[47], learn node representations by contrasting positive and negative samples. GraphMAE[15,16], on the other hand, learns representations by generating samples that resemble the original graph structure. However, these methods typically require fine-tuning the task-specific heads for downstream applications.Various methods have explored the use of prompt techniques to enhance the generalization of GNNs. To address the inconsistency between pre-training and downstream task objectives, GraphPrompt[25]proposes a unified task template applicable to both stages. Additionally, ProG[29]reformulates various task types into a unified graph-level representation and employs meta-learning techniques to enhance multi-task learning capabilities. However, whether through self-supervised learning or graph prompt methods, fine-tuning is often necessary when handling new datasets. Moreover, when confronted with datasets containing varying numbers of categories, retraining of task heads is required to achieve optimal performance.4.3Large language models for graphsWith the rapid advancement of Large Language Models (LLMs) and their remarkable generalization capabilities, leveraging LLMs to address transferability issues in graph machine learning has garnered significant attention[10,14]. Some methods represent graph structure information as text input to LLMs[3,33,23]; however, this approach often leads to suboptimal solutions[18]. Another paradigm involves using LLMs as enhancers[43,48,39,4,24], where they generate data or node text representations. Despite this, since GNNs are ultimately used for prediction, this approach significantly limits the model's transferability. Recently, considerable efforts have been made to utilize LLMs as predictors. For instance, GraphGPT[30]attempts to align LLMs with pre-trained Graph Transformer encoders through two-stage fine-tuning. However, the fine-tuning, conducted on specific datasets, might weaken the method's transferability. In light of this, LLaGA[2]introduced a novel encoding method that directly translates graph data into sequences compatible with LLMs. However, this approach may compromise performance due to the lack of GNN filtering and aggregation of graph information. Inspired by these challenges, we propose a pre-training strategy that enhances GNN transferability by aligning its representations with the token embeddings of LLMs, resulting in improved performance in zero-shot tasks. Notably, similar to our method, TEST[27]aligns time series representations with several selected LLM token embeddings. However, our approach differs in that we project graph representations into a feature space defined by the principal components of LLM token embeddings. This enables the LLM to function as a zero-shot learner for graph machine learning tasks, rather than just enhancing performance on specific, seen tasks.",
                "abstract": "Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors."
            },
            {
                "name": "DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System",
                "arxiv_id": "2408.08231",
                "subtitles": [
                    "GNN-based Recommendation",
                    "Large Language Models"
                ],
                "reference": [
                    "Cluster-guided contrastive graph clustering network",
                    "Lightgcl: Simple yet effective graph contrastive learning for recommendation",
                    "Ctrl: Connect tabular and language model for ctr prediction",
                    "Llara: Large language-recommendation assistant",
                    "Interpolation-based contrastive learning for few-label semi-supervised learning",
                    "Graphlearner: Graph node clustering with fully learnable augmentation",
                    "Improving graph collaborative filtering with neighborhood-enriched contrastive learning",
                    "Self-supervised graph learning for recommendation",
                    "Representation learning with large language models for recommendation",
                    "Enhancing job recommendation through llm-based generative adversarial networks",
                    "Dual test-time training for out-of-distribution recommender system",
                    "Gpt understands, too",
                    "Neural graph collaborative filtering",
                    "Mixed graph contrastive network for semi-supervised node classification",
                    "Controlrec: Bridging the semantic gap between language model and personalized recommendation",
                    "Llm-enhanced user-item interactions: Leveraging edge information for optimized recommendations",
                    "Dealmvc: Dual contrastive calibration for multi-view clustering",
                    "Gpt-4 technical report",
                    "Convert: Contrastive graph clustering with reliable augmentation",
                    "Lightgcn: Simplifying and powering graph convolution network for recommendation",
                    "Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach",
                    "Asymmetric double-winged multi-view clustering network for exploring diverse and consistent information",
                    "Calrec: Contrastive alignment of generative llms for sequential recommendation"
                ],
                "related_work": "VIRelated WorkVI-AGNN-based RecommendationWithin the realm of recommender systems, collaborative filtering stands as a cornerstone technology, exerting a significant influence on the operation of these systems. Moreover, Graph Neural Networks (GNNs) have become a research spot recently[40,41,42,43]. Existing methods always utilize GNNs, such as LightGCN[32], NGCF[29]and GCCF[31], to model the historical user-item interactions, thereby facilitating the capture of more complex relationships. Nonetheless, the implicit feedback data from users frequently contains considerable noise, which can compromise the performance of these Graph Neural Network (GNN) -based methods. In response to the aforementioned challenges, a self-supervised learning method, commonly referred to as contrastive learning[44,45,46,47], takes precedence. Representative approaches, such as SGL[33], LightGCL[48], and NCL[49], employ the contrastive augmented data to boost the robustness of the whole recommendations and take out more promising performance.VI-BLarge Language ModelsAs the adoption of LLMs[50,51]becomes more widespread, the challenge of how to efficiently adapt these models for recommender systems has emerged as a pivotal research focus within the recommendation community[52,53,54]. Several researchers[55,10,11,12]take a step forward to study how to integrate the powerful representation ability of large language models into the recommendation system by using the contrastive learning mentioned above. For example, RLMRec[10]utilizes contrastive and generative alignment techniques to align CF-side relational embeddings with LLMs-side semantic representations, such strategic integration effectively combines the advantages of general recommenders with those of Language Models, creating a robust system that leverages the strengths of both. ControlRec[11]narrows the semantic gap between language models and general recommenders via two auxiliary contrastive objectives, enhancing the performance of the proposed model by improving the ability to integrate the two types of data sources. CTRL[12]handles tabular data and transformed textual data as two separate modalities, harnessing the power of contrastive learning for a more precise alignment and integration of knowledge. While the aforementioned methods have made noteworthy advancements, we have theoretically demonstrated that such methods, which depend solely on direct alignment, may produce unsatisfactory results. To address this issue, our approach employs a disentangled alignment strategy for both the collaborative models and LLMs. This implementation will lead to substantial enhancements in the performance of LLMs-based recommender systems.",
                "abstract": "Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems. Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment. In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem. Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved. Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization. Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer. Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks. Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms."
            },
            {
                "name": "Better Alignment with Instruction Back-and-Forth Translation",
                "arxiv_id": "2408.04614",
                "subtitles": [
                    "Human-crafted data",
                    "Synthetic instruction generation",
                    "Distillation",
                    "Improving instruction-tuning data quality"
                ],
                "reference": [
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Free dolly: Introducing the world's first truly open instruction-tuned llm",
                    "Orca: Progressive learning from complex explanation traces of gpt",
                    "Zephyr: Direct distillation of lm alignment",
                    "Lima: Less is more for alignment",
                    "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
                    "How far can camels go? exploring the state of instruction tuning on open resources",
                    "Self-alignment with instruction backtranslation",
                    "Instruction tuning for large language models: A survey",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "The flan collection: Designing data and methods for effective instruction tuning",
                    "Longform: Optimizing instruction tuning for long text generation with corpus extraction",
                    "Instruction tuning with gpt",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Kun: Answer polishment for chinese self-alignment with instruction back-translation",
                    "Cross-task generalization via natural language crowdsourcing instructions",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Tegit: Generating high-quality instruction-tuning data with text-grounded task design",
                    "Reformatted alignment",
                    "Openassistant conversations-democratizing large language model alignment",
                    "Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning",
                    "Alpaca: A strong, replicable instruction-following model"
                ],
                "related_work": "6Related WorkWe discuss related papers that construct new instruction-tuning datasets or propose methods to improve existing ones. More in-depth review can be found inZhang et al. (2023) .Human-crafted data.Open AssistantK\u00f6pf et al. (2024) , DollyConover et al. (2023) and Super-NIWang et al. (2022b) are some examples of datasets that contain solely human-generated and human-annotated conversations, covering a range of topics and NLP tasks. These datasets tend to be relatively small in scale due to the expensive costs of manual annotation and verification.Other papers do not explicitly ask humans to create questions and answer them, but instead re-purpose existing datasets. For example, FLANLongpre et al. (2023) and Natural instructionsMishra et al. (2021) transform inputs and outputs of more than 60 NLP tasks into instruction-tuning data. This suffers from the same scalability issue as human-annotated datasets.Synthetic instruction generation.In contrast to sourcing manually written instructions, which may be expensive to scale, some papers propose ways to automatically generate large quantities of instructionsWang et al. (2022a) ; Taori et al. (2023) . In particular, our work is inspired by the backtranslation technique proposed inLi et al. (2023a) , which fine-tunes an LLM specifically for the task of instruction generation, and then applies the model to augment text segments extracted from the web with corresponding instructions. The paper suggests that this approach allows the resulting instruction-tuning data to be more diverse especially in the long tail. Another prior work, LongFormK\u00f6ksal et al. (2023) , introduces a similar approach for generating instructions.Most related to our approach is the work byChen et al. (2023b) , who train an LLM to generatebothinstructions and responses from web-scraped documents. In contrast to their method, we (i) generate instructions separately with backtranslation and then ask an LLM to improve the existing responses, (ii) obtain better performance with much fewer data (Table5) (iii) generate more data (51.2K compared to 12.4K) , (iv) offer more insights into the quality of our instructions and responses in comparison to other existing datasets. In addition, concurrent work byZheng et al. (2024) also proposes more detailed scoring and refinement prompts to improve the instruction curation and response formatting of the backtranslation pipeline fromLi et al. (2023a) , applying it to Chinese text data.Distillation.Perhaps the most common approach in instruction-tuning data generation, distillation seeks to mimic the capabilities of powerful LLMs (e.g. GPT-4) by feeding queries to these models and using the outputs to fine-tune subsequent LLMs. Datasets that are built this way include ShareGPTChiang et al. (2023) , OpenInstructWang et al. (2023) , Alpaca-GPT4Peng et al. (2023) and UltraFeedbackTunstall et al. (2023) .Improving instruction-tuning data quality.Some prior work studies characteristics of high-quality instruction-tuning dataLiu et al. (2023) and proposes curation techniques accordingly. LIMAZhou et al. (2024) carefully collects 1K fine-tuning samples via both internet sourcing and human annotation, and shows that strong performance can be achieved despite the small data quantity. Similarly,Chen et al. (2023a) demonstrates that performance gain is possible by fine-tuning on only a small subset of the original dataset (Alpaca) , using ChatGPT as the quality evaluator.Zhao et al. (2024) finds that selecting only the 1K longest responses from existing datasets offers a very strong baseline, independent of GPT-4's preference for longer texts. Evol-InstructXu et al. (2023) and OrcaMukherjee et al. (2023) manually prompt models to enhance the complexity of instructions, and subsequently, data generation (e.g. by asking for justification) .Fan et al. (2024) reformats the responses of existing instruction data to augment them with relevant information and align them with pre-determined criteria set by humans.",
                "abstract": "We propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by Li et al.(2023a), and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (backtranslated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space. Further analysis shows that our backtranslated instructions are of higher quality than other sources of synthetic instructions, while our responses are more diverse and complex than those obtained from distillation. Overall we find that instruction back-and-forth translation combines the best of both worlds -- making use of the information diversity and quantity found on the web, while ensuring the quality of the responses which is necessary for effective alignment."
            },
            {
                "name": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
                "arxiv_id": "2402.07610",
                "subtitles": [
                    "Self-Alignment",
                    "Bootstrapping"
                ],
                "reference": [
                    "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                    "Self-consuming generative models go mad",
                    "Star: Bootstrapping reasoning with reasoning",
                    "Salmon: Self-alignment with principle-following reward models",
                    "Efficient toxic content detection by bootstrapping and distilling large language models",
                    "Large language models suffer from their own output: An analysis of the self-consuming training loop",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Large language models can self-improve",
                    "Fairness-guided few-shot prompting for large language models",
                    "Efficient test-time model adaptation without forgetting",
                    "Self-alignment with instruction backtranslation",
                    "Self-instruct: Aligning language model with self generated instructions",
                    "Model dementia: Generated data makes models forget",
                    "Revisiting plasticity in visual reinforcement learning: Data, modules and training stages",
                    "Towards stable test-time adaptation in dynamic wild world"
                ],
                "related_work": "2Related WorkSelf-AlignmentSelf-Alignment intends to make full use of pretrained model on self-context generation. In order to save the cost of human annotations while maintaining acceptable model performance, researchers utilize strong in-context learning, chain of thought, revision ability of the pretrained LLM to process high-quality contexts itself. It can be viewed from three aspects.(i) high quality data generation aspect: current work(Bai et al.,2022; Sun et al.,2023b,a; Wang et al.,2022; Niu et al.,2023,2022; Huang et al.,2022; Ma et al.,2023b) align persuasive few-shot responses with weaker zero-shot responses, aiming to instill instruction-following patterns and principles into pretrained models and introduce model revision ability(Bai et al.,2022; Sun et al.,2023b) for further quality improvement. These approaches successfully enable pretrained model to generate high-quality text for satisfactory performance.(ii) ready-made data utilizing aspect: other researches(Li et al.,2023a) focus on identifying high-quality contexts and tag prompts upon these contexts as training datasets. These approaches utilize ready-made but untagged data to achieve a high quality target.(iii) model internal capacity utilizing aspect: they aim to accumulate high-quality data and subsequently conduct supervised fine-tuning once or twice(Sun et al.,2023b,a; Wang et al.,2022; Bai et al.,2022) . Occasionally, they conduct post-processing(Li et al.,2023a; Sun et al.,2023b) .BootstrappingBootstrapping is a useful method to improve model performance on rational examples(Zelikman et al.,2022) . As for aligning with human intention, most existing work on large language model self-alignment(Zhang et al.,2023; Bai et al.,2022; Li et al.,2023a) utilize bootstrapping to guide LLM's reflection on its pre-generated context and make revision. Bai(Bai et al.,2022) recommend the pretrained model to revise its responses through critique.(Li et al.,2023a) propose to use training model to iteratively select high-quality QA pairs. The biggest difference between our method and(Li et al.,2023a) is that we iteratively use the semi-trained model as generator while the later use it as selector; also, the later approach only iterates twice, which is much fewer than our iterative times. There are also concerns on the self-training loop collapse. Several works(Shumailov et al.,2023b; Alemohammad et al.,2023; Briesch et al.,2023; Ma et al.,2023a) demonstrate that the reuse of training dataset would perturb the long tail distribution of the model, resulting in model forgetting and collapse. Therefore, we pay much attention on the diversity and formats of the training datasets. The 7.5k prompts we use are randomly selected from Self-align dataset(Sun et al.,2023a) , and the prompts will not be used again if they have already been trained. The ICL example pool is carefully designed to improve the diversity of response formats.Distinct from prior work, our study investigates the effectiveness of multi-round(bootstrapping) self-alignment. Firstly, we demonstrate the significance of diverse ICL examples, which is crucial for bootstrapping in case of easy overfitting. Secondly, we validate the efficacy of multi-round self-alignment. Subsequently, we enhance the model's performance by adjusting the order of the training set. To address the inconsistent performance between generation and classification tasks observed in multi-round(bootstrapping) self-alignment, we provide specific explanations: Data Processing Inequality and Sharper Output Distribution. Finally, based on these two explanations, we propose a validation set to prevent further performance degradation .",
                "abstract": "Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced few-shot ability to boost zero or one-shot performance. Based on easy-to-hard training recipe, we propose SOFT+ which further boost self-alignment's performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across various classification and generation tasks, highlighting the potential of bootstrapping self-alignment on continually enhancing model alignment performance."
            },
            {
                "name": "Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities",
                "arxiv_id": "2408.09366",
                "subtitles": [
                    "Aligning LLMs to Subgroups",
                    "Evaluating LLMs' Alignment to Subgroups",
                    "LLMs and Psychometric Tests",
                    "Online Eating Disorders Communities"
                ],
                "reference": [
                    "Quantifying and predicting mental illness severity in online pro-eating disorder communities",
                    "Illuminating the black box: A psychometric investigation into the multifaceted nature of large language models",
                    "Whose emotions and moral sentiments do language models reflect",
                    "Ai psychometrics: Assessing the psychological profiles of large language models through psychometric inventories",
                    "Recovery amid pro-anorexia: Analysis of recovery in social media",
                    "Communicating stigma: The pro-ana paradox",
                    "Inducing anxiety in large language models increases exploration and bias",
                    "Reading between the tweets: Deciphering ideological stances of interconnected mixed-ideology communities",
                    "Towards measuring the representation of subjective global opinions in language models",
                    "Personality traits in large language models",
                    "Community-cross-instruct: Unsupervised instruction generation for aligning large language models to online communities",
                    "Individuals with eating disorders and the use of online support groups as a form of social support",
                    "Whose opinions do language models reflect",
                    "Psychological impact of pro-anorexia and pro-eating disorder websites on adolescent females: A systematic review",
                    "Large language models as subpopulation representative models: A review",
                    "Communitylm: Probing partisan worldviews from language models",
                    "'written in these scars are the stories i can't explain': A content analysis of pro-ana and thinspiration image sharing on instagram",
                    "Proanorexia communities on social media",
                    "Social desirability bias: A demonstration and technique for its reduction",
                    "Language models are unsupervised multitask learners",
                    "Pro-eating disorder communities on social networking sites: a content analysis",
                    "Probing the moral development of large language models through defining issues test",
                    "Evaluating and inducing personality in pre-trained language models",
                    " \"this post will just get taken down \"' characterizing removed pro-eating disorder social media content"
                ],
                "related_work": "2Related WorkAligning LLMs to SubgroupsThere is growing literatureSimmons and Hare (2023) on aligning LLMs to diverse human subgroups to mimic their language and mindsets. Researchers have aligned LLMs by steering them towards particular demographic groupsSanturkar et al. (2023) ; Durmus et al. (2023) ; He et al. (2024b) , e.g., by including the target subgroup in the prompt. However, their findings reveal that steering does not solve the model's misalignment with the target subgroup. Moreover, it is non-trivial to summarize an organically-formed community (e.g., communities in retweet networks) into a concise description that can be used in steering.Others have aligned LLMs with different subgroups by finetuning the model on the text generated by the subgroups.Jiang et al. (2022b) proposecommunityLMby finetuning two GPT-2 modelsRadford et al. (2020) using causal language modeling on tweets from liberals and conservatives, and probing their worldviews from their corresponding finetuned models.He et al. (2024c) extendcommunityLMto probe the views of organically-formed online communities and make use of the interactions between different communities. However, GPT-2 is not instruction-tuned and is not able to answer questions in various formats, like the psychometric instruments we discuss in\u00a7\u00a7\\S\u00a76.He et al. (2024a) use an advanced LLM (e.g., Claude-3) to distill knowledge from the community's raw data and generate high-quality instruction-response pairs, where the instructions aim to query the community' mindset, and the corresponding responses are abstracted from the ideas conveyed in the raw data. The generated instruction-response pairs are used to finetune a foundational LLM (e.g., Llama-3) for alignment. However, the API costs of querying the advanced LLM are non-negligible.Evaluating LLMs' Alignment to SubgroupsExisting worksSanturkar et al. (2023) ; Durmus et al. (2023) measure an LLM's alignment with a target subgroup using multi-choice surveys. Specifically, they prompt the LLM to respond to a survey question from the perspective of a subgroup and then compare the LLM-generated distribution over the different options of the question to that of the survey respondents belonging to the target group. However, collecting survey responses can be costly and time-consuming. Also, responses on sensitive topics, such as mental health, may be biased due to stigma and social desirability biasGordon (1987) . Our framework evaluates LLM alignment by comparing the LLM-generated synthetic text to the original text written by humans is significantly more scalable, unbiased, and cost-effective.LLMs and Psychometric TestsLLMs can respond to psychometric instruments that were originally designed to assess individual human psychological and emotional states. Researchers have administered these instruments to LLMs to probe their decision-making processes, reasoning abilities, cognitive biases, and other psychological traits Pellert et al. (2024) call this practice  \"AI Psychometrics \".Coda-Forno et al. (2023) show that GPT-3.5 generated consistently high scores on responses to a widely used anxiety questionnaire.Tanmay et al. (2023) measure GPT-4's moral reasoning abilities by applying an ethical measurement instrument for individuals. Researchers also administer personality tests to LLMs to identify their personality traitsJiang et al. (2022a) ; Lu et al. (2023) ; Serapio-Garc\u00eda et al. (2023) . In contrast, we apply psychometric questionnaires to a specific online community via a finetuned LLM to learn more about the mindset of the community members. We show that this helps reveal unhealthy beliefs within these communities and even identify pathologies, like harmful cognitions associated with EDs.Online Eating Disorders CommunitiesPro-ED (pro-anorexia) communities are online spaces that frame EDs as a lifestyle rather than an illness. While they provide social support, a sense of belonging, and empathy for stigmatized individualsJuarascio et al. (2010) ; Oksanen et al. (2016) ; Yeshua-Katz and Martins (2013) ; McCormack (2010) , they also promote harmful behaviors, such as weight loss tips and \"thinspiration\" imagery, exacerbating EDs and psychological distressGing and Garvey (2018) ; Mento et al. (2021) .Previous research has focused on identifying harmful content and at-risk users within these communities. For example,Chancellor et al. (2016a) developed a lexical classifier to predict posts moderated by Instagram for self-harm content, comparing pro-recovery and pro-ED communitiesChancellor et al. (2016b,c) . In contrast, our study examines the collective mindset of these communities as expressed through their discussions, using advanced language models to assess attitudes toward mental health and body image issues.",
                "abstract": "Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and successfully differentiate communities with varying levels of eating disorder risk. Our results highlight the potential of LLMs in automated moderation and broader applications in public health and social science research."
            },
            {
                "name": "Personality Alignment of Large Language Models",
                "arxiv_id": "2408.11779",
                "subtitles": [
                    "Alignment with language models",
                    "Preferences of language models",
                    "Interventions Activate"
                ],
                "reference": [
                    "Moral foundations twitter corpus: A collection of 35k tweets annotated for moral sentiment",
                    "Valuenet: A new dataset for human value driven dialogue system",
                    "Emergent abilities of large language models",
                    "Activation addition: Steering language models without optimization",
                    "Large language models are better reasoners with self-verification",
                    "Safe RLHF: Safe reinforcement learning from human feedback",
                    "Inference-time intervention: Eliciting truthful answers from a language model",
                    "In-context learning and induction heads",
                    "Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack",
                    "Decoding-time realignment of language models",
                    "The sixteen personality factor questionnaire (16pf",
                    "Lamp: When large language models meet personalization",
                    "Using large language models to simulate multiple humans and replicate human subject studies",
                    "Steering llama 2 via contrastive activation addition",
                    "Editing personality for llms",
                    "Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards",
                    "Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews, 2024b",
                    "Do llms possess a personality? making the mbti test an amazing evaluation for large language models",
                    "On diversified preferences of large language model alignment",
                    "Locating and editing factual associations in gpt",
                    "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
                    "Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models",
                    "Dissecting recall of factual associations in auto-regressive language models",
                    "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                    "Interpretability at scale: Identifying causal mechanisms in alpaca",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "RAFT: Reward ranked finetuning for generative foundation model alignment",
                    "Localizing model behavior with path patching",
                    "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
                    "Proximal policy optimization algorithms",
                    "Controllm: Crafting diverse personalities for language models",
                    "Compositional preference models for aligning lms",
                    "Can machines learn morality? the delphi experiment",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Decoding-time language model alignment with multiple objectives",
                    "Learning to summarize with human feedback",
                    "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                    "Assessing the alignment of large language models with human values for mental health integration: Cross-sectional study using schwartz's theory of basic values",
                    "Hydra: Model factorization framework for black-box llm personalization",
                    "Chatharuhi: Reviving anime character in reality via large language model",
                    "From persona to personalization: A survey on role-playing language agents",
                    "Dynamic generation of personalities with large language models",
                    "A minimaximalist approach to reinforcement learning from human feedback",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Self-assessment tests are unreliable measures of llm personality, 2024a",
                    "Discovering latent knowledge in language models without supervision",
                    "pyvene: A library for understanding and improving PyTorch models via interventions",
                    "A critical evaluation of ai feedback for aligning large language models",
                    "Learning what to value",
                    "A general theoretical paradigm to understand learning from human preferences",
                    "Bottom-up and top-down: Predicting personality with psycholinguistic and language model features",
                    "Evaluating and inducing personality in pre-trained language models"
                ],
                "related_work": "2Related WorkAlignment with language models.The concept of integrating human values into AI systems, initially proposed by DeweyDewey (2011) , underscores the necessity of value learning, which entails the AI's ability to process and prioritize a broad spectrum of human values and preferences. In the context of LLMs functioning as high-performance assistants aligning these models with human values is crucial, especially since they operate in complex reasoning tasks across various domains(Hadar-Shoval et al.,2024; Wei et al.,2022b;a; Weng et al.,2023) . Methods such as RLHF(Stiennon et al.,2020; Bai et al.,2022a; Dong et al.,2023) and RLAIF(Lee et al.,2023; Sharma et al.,2024) seek to align LLMs by employing strategies like PPO(Schulman et al.,2017) and DPO(Rafailov et al.,2023) . These approaches focus on maximizing cumulative rewards that are closely aligned with predefined human values(Azar et al.,2023; Lin et al.,2023; Dai et al.,2024; Swamy et al.,2024; Dai et al.,2024) . Contrary to these broad approaches, our research focuses on personality alignment, which tailors the LLMs' behavior to each individual's complex value set, thereby enhancing the model's capacity to understand and reflect individual human values.Preferences of language models.The community of LLMs has started using personality testing tools for both qualitative and quantitative evaluations(Hoover et al.,2019; Jiang et al.,2022; Qiu et al.,2022; Xu et al.,2023; Salemi et al.,2024; Wang et al.,2024b) . For example, the Machine Personality Inventory (MPI) Jiang et al. (2024) , utilizes the Big-Five Personality Factors to standardize the assessment of LLMs. This approach analyzes the behaviors of LLMs across the five dimensions of the Big-Five, providing a quantitative interpretation of the values and personality traits of LLMs. Such tools are grounded in widely accepted theories, including the Big-Five and the 16 Personality Factors(Cattell & Mead,2008) , helping to contextualize LLMs' personality traits in terms of human individual differences(Gupta et al.,2024a; Wang et al.,2023; Aher et al.,2023; Liu et al.,2024a; Li et al.,2023) . These studies indicate that aligned LLMs share similar values with humans(Mehta et al.,2020; Pan & Zeng,2023; Mao et al.,2023; Go et al.,2023; Chen et al.,2024) .Jang et al. (2023b) has also mentioned the similar term \"personality alignment\", which considers a limited scope of preference personalization(Wang et al.,2024a; Zhuang et al.,2024; Zeng et al.,2023) , focusing on only 8 different conflicting user preferences derived from combinations of three dimensions: Expertise (elementary vs. expert) , Informativeness (concise vs. informative) , and Style (friendly vs. unfriendly) . Our work extends these theories and methods by leveraging psychological measurement to assess the value alignment gap between aligned LLMs and individual users. We aim to understand how closely aligned LLMs are with individual values through personality and personalized adjustments.Interventions Activate.Activation interventions in neural networks trigger specific changes in internal activations(Olsson et al.,2022; Wu et al.,2024a) and have emerged as a pivotal tool in model robustness(He et al.,2019) , editing(Meng et al.,2022) , circuit finding(Goldowsky-Dill et al.,2023) , and knowledge tracing(Geva et al.,2023) . These interventions are advantageous due to their adjustable nature and minimal invasiveness(Wu et al.,2024b) . Prior research has demonstrated that activation interventions can effectively identify the crucial heads and directions in a model's internal representations(Burns et al.,2022; Li et al.,2024a; Liu et al.,2024b) , which offers better data efficiency and faster optimization than traditional reinforcement learning approaches(Turner et al.,2023; Rimsky et al.,2023; Shi et al.,2024; Weng et al.,2024a) . Building on these findings, our proposed method enhances scalability beyond traditional reinforcement learning by pinpointing the value directions of different individuals within the model and calculating the optimal offset distance to achieve alignment.",
                "abstract": "Current methods for aligning large language models (LLMs) typically aim to reflect general human values and behaviors, but they often fail to capture the unique characteristics and preferences of individual users. To address this gap, we introduce the concept of Personality Alignment. This approach tailors LLMs' responses and decisions to match the specific preferences of individual users or closely related groups. Inspired by psychometrics, we created the Personality Alignment with Personality Inventories (PAPI) dataset, which includes data from 300,000 real subjects, each providing behavioral preferences based on the Big Five Personality Factors. This dataset allows us to quantitatively evaluate the extent to which LLMs can align with each subject's behavioral patterns. Recognizing the challenges of personality alignments: such as limited personal data, diverse preferences, and scalability requirements: we developed an activation intervention optimization method. This method enhances LLMs' ability to efficiently align with individual behavioral preferences using minimal data and computational resources. Remarkably, our method, PAS, achieves superior performance while requiring only 1/5 of the optimization time compared to DPO, offering practical value for personality alignment. Our work paves the way for future AI systems to make decisions and reason in truly personality ways, enhancing the relevance and meaning of AI interactions for each user and advancing human-centered artificialthis http URLcode has released in \\url{this https URL}."
            }
        ],
        "survey": {
            "name": "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More",
            "arxiv_id": "2407.16216",
            "subtitles": [
                {
                    "name": "Categorical Outline",
                    "key_history": [
                        {
                            "reference_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
                            "key_word": "Pointwise Reward Model"
                        },
                        {
                            "reference_title": "A markovian decision process",
                            "key_word": "Token-Level Reward"
                        },
                        {
                            "reference_title": "Alpacaeval: An automatic evaluator of instruction-following models",
                            "key_word": "Length-Control RL"
                        }
                    ],
                    "references_in_this_section": [
                        "Alpacaeval: An automatic evaluator of instruction-following models",
                        "A markovian decision process",
                        "Rank analysis of incomplete block designs: I. the method of paired comparisons"
                    ]
                },
                {
                    "name": "Individual Paper Reviews in Detail",
                    "key_history": [
                        {
                            "reference_title": "Training language models to follow instructions with human feedback",
                            "key_word": "RLHF/PPO"
                        },
                        {
                            "reference_title": "Rank analysis of incomplete block designs: I",
                            "key_word": "Reward Model"
                        },
                        {
                            "reference_title": "Bleu: a method for automatic evaluation of machine translation",
                            "key_word": "Evaluation Metrics"
                        },
                        {
                            "reference_title": "Truthfulqa: Measuring how models mimic human falsehoods",
                            "key_word": "Honest Evaluation"
                        },
                        {
                            "reference_title": "Rlhf workflow: From reward modeling to online rlhf",
                            "key_word": "Online RLHF"
                        },
                        {
                            "reference_title": "Constitutional ai: Harmlessness from ai feedback",
                            "key_word": "RLAIF-Anthropic"
                        },
                        {
                            "reference_title": "Direct preference optimization: Your language model is secretly a reward model",
                            "key_word": "DPO"
                        },
                        {
                            "reference_title": "A general theoretical paradigm to understand learning from human preferences",
                            "key_word": "IPO"
                        },
                        {
                            "reference_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                            "key_word": "TDPO"
                        },
                        {
                            "reference_title": "From r to q\u2217: Your language model is secretly a q-function",
                            "key_word": "DPO: from r to Q"
                        },
                        {
                            "reference_title": "Self-rewarding language models",
                            "key_word": "Iterative/Online DPO: Self-Rewarding Language Models"
                        },
                        {
                            "reference_title": "Alpacaeval: An automatic evaluator of instruction-following models",
                            "key_word": "AlpacaEval"
                        },
                        {
                            "reference_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
                            "key_word": "MT-Bench"
                        },
                        {
                            "reference_title": "Orpo: Monolithic preference optimization without reference model",
                            "key_word": "Odds Ratio Preference Optimization"
                        },
                        {
                            "reference_title": "Paft: A parallel training paradigm for effective llm fine-tuning",
                            "key_word": "Parallel Fine-Tuning"
                        },
                        {
                            "reference_title": "Disentangling length from quality in direct preference optimization",
                            "key_word": "Regularized DPO"
                        },
                        {
                            "reference_title": "Simpo: Simple preference optimization with a reference-free reward",
                            "key_word": "Simple Preference Optimization"
                        },
                        {
                            "reference_title": "Lipo: Listwise preference optimization through learning-to-rank",
                            "key_word": "Listwise Preference Optimization"
                        },
                        {
                            "reference_title": "Negating negatives: Alignment without human positive samples via distributional dispreference optimization",
                            "key_word": "Negating Negatives"
                        },
                        {
                            "reference_title": "Negative preference optimization: From catastrophic collapse to effective unlearning",
                            "key_word": "Negative Preference Optimization"
                        },
                        {
                            "reference_title": "Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation",
                            "key_word": "Contrastive Preference Optimization"
                        },
                        {
                            "reference_title": "Nash learning from human feedback",
                            "key_word": "Nash Learning from Human Feedback"
                        },
                        {
                            "reference_title": "A minimaximalist approach to reinforcement learning from human feedback",
                            "key_word": "Self-Play Preference Learning"
                        }
                    ],
                    "references_in_this_section": [
                        "Winogrande: An adversarial winograd schema challenge at scale",
                        "Hellaswag: Can a machine really finish your sentence",
                        "Instruction-following evaluation for large language models",
                        "Adaptive game playing using multiplicative weights",
                        "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
                        "Openassistant conversations - democratizing large language model alignment",
                        "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
                        "Phi-2: The surprising power of small language models",
                        "Evaluating large language models trained on code",
                        "Paft: A parallel training paradigm for effective llm fine-tuning",
                        "Challenging big-bench tasks and whether chain-of-thought can solve them",
                        "Chain-of-thought prompting elicits reasoning in large language models",
                        "Stanford alpaca: An instruction-following llama model",
                        "Secrets of rlhf in large language models part ii: Reward modeling",
                        "Alpacafarm: A simulation framework for methods that learn from human feedback",
                        "Proximal policy optimization algorithms",
                        "From r\ud835\udc5fr to q\u2217superscript\ud835\udc5eq^{*}: Your language model is secretly a q-function",
                        "Preference ranking optimization for human alignment",
                        "Language models are unsupervised multitask learners",
                        "Learning word vectors for sentiment analysis",
                        "Training verifiers to solve math word problems",
                        "Rlhf workflow: From reward modeling to online rlhf",
                        "A markovian decision process",
                        "The cringe loss: Learning what language not to model",
                        "Negating negatives: Alignment without human positive samples via distributional dispreference optimization",
                        "Judging llm-as-a-judge with mt-bench and chatbot arena",
                        "Learning to rank for information retrieval",
                        "Orpo: Monolithic preference optimization without reference model",
                        "Nash learning from human feedback",
                        "Orca: Progressive learning from complex explanation traces of gpt",
                        "Metamath: Bootstrap your own mathematical questions for large language models",
                        "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                        "Orca 2: Teaching small language models how to reason",
                        "\u03b2\ud835\udefd\\beta-dpo: Direct preference optimization with dynamic \u03b2\ud835\udefd\\beta",
                        "Kto: Model alignment as prospect theoretic optimization",
                        "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint",
                        "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "Buy 4 REINFORCE samples, get a baseline for free",
                        "Bertscore: Evaluating text generation with bert",
                        "Rouge: A package for automatic evaluation of summaries",
                        "Bleu: a method for automatic evaluation of machine translation",
                        "Disentangling length from quality in direct preference optimization",
                        "Insights into alignment: Evaluating dpo and its variants across multiple tasks",
                        "Direct preference optimization: Your language model is secretly a reward model",
                        "Generalized preference optimization: A unified approach to offline alignment",
                        "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms",
                        "On the limitations of the elo, real-world games are transitive, not additive",
                        "Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss",
                        "On decoding strategies for neural text generators",
                        "Smaug: Fixing failure modes of preference optimisation with dpo-positive",
                        "Statistical rejection sampling improves preference optimization",
                        "Mistral 7b",
                        "Slic-hf: Sequence likelihood calibration with human feedback",
                        "Tofu: A task of fictitious unlearning for llms",
                        "Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation",
                        "Self-rewarding language models",
                        "Gpt-4 technical report",
                        "A paradigm shift in machine translation: Boosting translation performance of large language models",
                        "Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints",
                        "Llama 2: Open foundation and fine-tuned chat models",
                        "Pythia: A suite for analyzing large language models across training and scaling",
                        "Measuring massive multitask language understanding",
                        "High-dimensional continuous control using generalized advantage estimation",
                        "Token-level direct preference optimization",
                        "A general theoretical paradigm to understand learning from human preferences",
                        "Safe rlhf: Safe reinforcement learning from human feedback",
                        "Training language models to follow instructions with human feedback",
                        "Palm 2 technical report",
                        "Self-play preference optimization for language model alignment",
                        "Truthfulqa: Measuring how models mimic human falsehoods",
                        "sdpo: Don't use your data all at once",
                        "Is dpo superior to ppo for llm alignment? a comprehensive study",
                        "Think you have solved question answering? try arc, the ai2 reasoning challenge",
                        "From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline",
                        "Alpacaeval: An automatic evaluator of instruction-following models",
                        "Ultrafeedback: Boosting language models with high-quality feedback",
                        "Rank analysis of incomplete block designs: I. the method of paired comparisons",
                        "Lora: Low-rank adaptation of large language models",
                        "Simpo: Simple preference optimization with a reference-free reward",
                        "Direct nash optimization: Teaching language models to self-improve with general preferences",
                        "Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling",
                        "Learning to summarize from human feedback",
                        "Constitutional ai: Harmlessness from ai feedback",
                        "Offline regularised reinforcement learning for large language models alignment",
                        "Lipo: Listwise preference optimization through learning-to-rank",
                        "Advances in prospect theory: Cumulative representation of uncertainty",
                        "A minimaximalist approach to reinforcement learning from human feedback",
                        "Language models are few-shot learners",
                        "Negative preference optimization: From catastrophic collapse to effective unlearning",
                        "Rrhf: Rank responses to align language models with human feedback without tears"
                    ]
                },
                {
                    "name": "Future Directions",
                    "key_history": [
                        {
                            "reference_title": "Training verifiers to solve math word problems",
                            "key_word": "GSM8K: reasoning"
                        },
                        {
                            "reference_title": "Truthfulqa: Measuring how models mimic human falsehoods",
                            "key_word": "TruthfulQA: addressing toxicity"
                        }
                    ],
                    "references_in_this_section": [
                        "Training verifiers to solve math word problems",
                        "Truthfulqa: Measuring how models mimic human falsehoods"
                    ]
                }
            ],
            "all_references": [
                "Orca: Progressive learning from complex explanation traces of gpt",
                "Stanford alpaca: An instruction-following llama model",
                "Slic-hf: Sequence likelihood calibration with human feedback",
                "Tofu: A task of fictitious unlearning for llms",
                "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint",
                "Mistral 7b",
                "Proximal policy optimization algorithms",
                "Language models are unsupervised multitask learners",
                "Bertscore: Evaluating text generation with bert",
                "Learning word vectors for sentiment analysis",
                "Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss",
                "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                "Training verifiers to solve math word problems",
                "Advances in prospect theory: Cumulative representation of uncertainty",
                "Lora: Low-rank adaptation of large language models",
                "Paft: A parallel training paradigm for effective llm fine-tuning",
                "Orca 2: Teaching small language models how to reason",
                "Safe rlhf: Safe reinforcement learning from human feedback",
                "Rank analysis of incomplete block designs: I. the method of paired comparisons",
                "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "Preference ranking optimization for human alignment",
                "Language models are few-shot learners",
                "Self-play preference optimization for language model alignment",
                "Adaptive game playing using multiplicative weights",
                "Training language models to follow instructions with human feedback",
                "Openassistant conversations - democratizing large language model alignment",
                "Disentangling length from quality in direct preference optimization",
                "Winogrande: An adversarial winograd schema challenge at scale",
                "The cringe loss: Learning what language not to model",
                "Evaluating large language models trained on code",
                "Rrhf: Rank responses to align language models with human feedback without tears",
                "Smaug: Fixing failure modes of preference optimisation with dpo-positive",
                "Is dpo superior to ppo for llm alignment? a comprehensive study",
                "High-dimensional continuous control using generalized advantage estimation",
                "Bleu: a method for automatic evaluation of machine translation",
                "Kto: Model alignment as prospect theoretic optimization",
                "Pythia: A suite for analyzing large language models across training and scaling",
                "A minimaximalist approach to reinforcement learning from human feedback",
                "Palm 2 technical report",
                "Challenging big-bench tasks and whether chain-of-thought can solve them",
                "Llama 2: Open foundation and fine-tuned chat models",
                "Secrets of rlhf in large language models part ii: Reward modeling",
                "From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline",
                "Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation",
                "Orpo: Monolithic preference optimization without reference model",
                "Ultrafeedback: Boosting language models with high-quality feedback",
                "Negating negatives: Alignment without human positive samples via distributional dispreference optimization",
                "Token-level direct preference optimization",
                "Constitutional ai: Harmlessness from ai feedback",
                "Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints",
                "A general theoretical paradigm to understand learning from human preferences",
                "Lipo: Listwise preference optimization through learning-to-rank",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "Generalized preference optimization: A unified approach to offline alignment",
                "Gpt-4 technical report",
                "Buy 4 REINFORCE samples, get a baseline for free",
                "Learning to rank for information retrieval",
                "From r to Q\u2217: Your Language Model is Secretly a Q-Function",
                "Metamath: Bootstrap your own mathematical questions for large language models",
                "Learning to summarize from human feedback",
                "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "The claude 3 model family: Opus, sonnet, haiku",
                "Phi-2: The surprising power of small language models",
                "Negative preference optimization: From catastrophic collapse to effective unlearning",
                "Self-rewarding language models",
                "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
                "Alpacafarm: A simulation framework for methods that learn from human feedback",
                "Instruction-following evaluation for large language models",
                "Truthfulqa: Measuring how models mimic human falsehoods",
                "Gemini: a family of highly capable multimodal models",
                "Offline regularised reinforcement learning for large language models alignment",
                "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms",
                "Direct nash optimization: Teaching language models to self-improve with general preferences",
                "Think you have solved question answering? try arc, the ai2 reasoning challenge",
                "Measuring massive multitask language understanding",
                "Nash learning from human feedback",
                "Judging llm-as-a-judge with mt-bench and chatbot arena",
                "Rlhf workflow: From reward modeling to online rlhf",
                "Insights into alignment: Evaluating dpo and its variants across multiple tasks",
                "Statistical rejection sampling improves preference optimization",
                "Rouge: A package for automatic evaluation of summaries",
                "sdpo: Don't use your data all at once",
                "\u03b2-DPO: Direct Preference Optimization with Dynamic \u03b2",
                "On the limitations of the elo, real-world games are transitive, not additive",
                "A markovian decision process",
                "A paradigm shift in machine translation: Boosting translation performance of large language models",
                "Hellaswag: Can a machine really finish your sentence",
                "Direct preference optimization: Your language model is secretly a reward model",
                "Alpacaeval: An automatic evaluator of instruction-following models",
                "Simpo: Simple preference optimization with a reference-free reward",
                "On decoding strategies for neural text generators",
                "Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling",
                "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
            ]
        },
        "topic_history": [
            {
                "name": "Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models",
                "arxiv_id": "2409.00598",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "The art of defending: A systematic evaluation and analysis of llm defense strategies on safety and over-defensiveness",
                    "Steering without side effects: Improving post-deployment control of language models",
                    "Navigating the OverKill in Large Language Models, January",
                    "Cold-attack: Jailbreaking llms with stealthiness and controllability",
                    "Shieldgemma: Generative ai content moderation based on gemma",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "The claude 3 model family: Opus, sonnet, haiku",
                    "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
                    "The llama 3 herd of models",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms, 2024b",
                    "Auditing large language models: a three-layered approach",
                    "Refusal in language models is mediated by a single direction",
                    "Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions",
                    "Red teaming language models with language models",
                    "Curiosity-driven red-teaming for large language models",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Autodan: Interpretable gradient-based adversarial attacks on large language models",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Diffusion-lm improves controllable text generation",
                    "Safe rlhf: Safe reinforcement learning from human feedback",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Representation engineering: A top-down approach to ai transparency",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Cold decoding: Energy-based constrained text generation with langevin dynamics",
                    "Rlvf: Learning from verbal feedback without overgeneralization",
                    "How is chatgpt's behavior changing over time",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Nothing in excess: Mitigating the exaggerated safety for llms via safety-conscious activation steering",
                    "On the exploitability of instruction tuning",
                    "Towards comprehensive and efficient post safety alignment of large language models via safety patching",
                    "Or-bench: An over-refusal benchmark for large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Attacking large language models with projected gradient descent",
                    "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
                    "Cgmh: Constrained sentence generation by metropolis-hastings sampling",
                    "Advprompter: Fast adaptive adversarial prompting for llms",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
                "arxiv_id": "2409.00935",
                "reference": [
                    "Shepherd: A critic for language model generation",
                    "Generative judge for evaluating alignment",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Look before you leap: An exploratory study of uncertainty measurement for large language models",
                    "Free dolly: Introducing the world's first truly open instruction-tuned LLM",
                    "Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization",
                    "System combination via quality estimation for grammatical error correction",
                    "Neural quality estimation of grammatical error correction",
                    "Constitutional AI: harmlessness from AI feedback",
                    "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Self-alignment with instruction backtranslation",
                    "The Flan collection: Designing data and methods for effective instruction tuning",
                    "QLoRA: Efficient finetuning of quantized LLMs",
                    "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
                    "Judging LLM-as-a-judge with MT-bench and Chatbot Arena",
                    "Self-rewarding language models",
                    "Stanford Alpaca: An instruction-following LLaMA model",
                    "LIMA: less is more for alignment",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "RLAIF: scaling reinforcement learning from human feedback with AI feedback",
                    "AlpacaEval: An automatic evaluator of instruction-following models",
                    "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                    "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
                    "COMET: A neural framework for MT evaluation",
                    "SALMON: self-alignment with principle-following reward models",
                    "WizardLM: Empowering large language models to follow complex instructions",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Out-of-distribution detection and selective generation for conditional language models",
                    "AlpacaFarm: A simulation framework for methods that learn from human feedback",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
                "arxiv_id": "2409.03735",
                "reference": [
                    "What did i do wrong? quantifying llms' sensitivity and consistency to prompt engineering",
                    "Enhancing privacy and security through robust access management-cdt",
                    "Exploring the sensitivity of LLMs' decision-making capabilities: Insights from prompt variations and hyperparameters",
                    "Goldcoin: Grounding large language models in privacy laws via contextual integrity theory",
                    "Sensitivity and robustness of large language models to prompt template in Japanese text classification tasks",
                    "How are prompts different in terms of sensitivity",
                    "Extracting training data from large language models",
                    "Learning privacy expectations by crowdsourcing contextual informational norms",
                    "Regulation (EU) 2016/679 of the European Parliament and of the Council",
                    "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                    "How trustworthy are open-source llms? an assessment under malicious demonstrations shows their vulnerabilities",
                    "URL https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield",
                    "Sensitive information",
                    "Analyzing leakage of personally identifiable information in language models",
                    "Privacy checklist: Privacy violation detection grounding on contextual integrity theory",
                    "The secret sharer: Evaluating and testing unintended memorization in neural networks",
                    "Respect for context as a benchmark for privacy online: What it is and isn't",
                    "Stop the Spread: A Contextual Integrity Perspective on the Appropriateness of COVID-19 Vaccination Certificates",
                    "Measuring privacy: An empirical test using context to expose confounding variables",
                    "Discovering smart home internet of things privacy norms using contextual integrity",
                    "Privacylens: Evaluating privacy norm awareness of language models in action",
                    "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory",
                    "Position: TrustLLM: Trustworthiness in Large Language Models",
                    "Attention is all you need",
                    "Language models are few-shot learners",
                    "Privacy as contextual integrity",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Air Gap: Protecting Privacy-Conscious Conversational Agents, May",
                    "On the worst prompt performance of large language models",
                    "Trustgpt: A benchmark for trustworthy and responsible large language models",
                    "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
                    "Awq: Activation-aware weight quantization for on-device llm compression and acceleration",
                    "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
                    "Disaster privacy/privacy disaster",
                    "Trustllm: Trustworthiness in large language models",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting",
                "arxiv_id": "2408.09798",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "An overview of image caption generation methods",
                    "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
                    "Show and tell: A neural image caption generator",
                    "Ignore previous prompt: Attack techniques for language models",
                    "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
                    "Are multimodal transformers robust to missing modality",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Videochat: Chat-centric video understanding",
                    "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                    "Prompt Injection attack against LLM-integrated Applications",
                    "A prompt-based approach to adversarial example generation and robustness enhancement",
                    "PLeak: Prompt Leaking Attacks against Large Language Model Applications",
                    "Promptattack: Probing dialogue state trackers with adversarial prompts",
                    "Chatcad: Interactive computer-aided diagnosis on medical image using large language models",
                    "Too large; data reduction for vision-language pre-training",
                    "Visual Instruction Tuning",
                    "Can contrastive learning avoid shortcut solutions",
                    "Text-centric Alignment for Multi-Modality Learning",
                    "Ophglm: Training an ophthalmology large language-and-vision assistant based on instructions and dialogue"
                ]
            },
            {
                "name": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings",
                "arxiv_id": "2408.14512",
                "reference": [
                    "Gcc: Graph contrastive coding for graph neural network pre-training",
                    "Can llms effectively leverage graph structural information: when and why",
                    "Large-scale learnable graph convolutional networks",
                    "Label-free node classification on graphs with large language models (LLMs",
                    "Can language models solve graph problems in natural language",
                    "Unigraph: Learning a cross-domain graph foundation model from natural language",
                    "All in one: Multi-task prompting for graph neural networks",
                    "Natural language is all a graph needs",
                    "Graphmae: Self-supervised masked graph autoencoders",
                    "How powerful are graph neural networks",
                    "Deep graph contrastive representation learning",
                    "Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks",
                    "Graphgpt: Graph instruction tuning for large language models",
                    "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
                    "Do transformers really perform badly for graph representation",
                    "Graph transformer networks",
                    "Wiener graph deconvolutional network improves graph self-supervised learning",
                    "Opengraph: Towards open graph foundation models",
                    "Inductive representation learning on large graphs",
                    "Predicting path failure in time-evolving graphs",
                    "Llaga: Large language and graph assistant",
                    "Nodeformer: A scalable graph structure learning transformer for node classification",
                    "Empower text-attributed graphs learning with large language models (llms",
                    "Graph contrastive learning with adaptive augmentation",
                    "Graph contrastive learning automated",
                    "Graph attention networks",
                    "Multi-level hyperedge distillation for social linking prediction on sparsely observed networks",
                    "Exploring the potential of large language models (LLMs) in learning on graph",
                    "L2-gcn: Layer-wise and learned efficient training of graph convolutional networks",
                    "One for all: Towards training one graph model for all classification tasks",
                    "Graphmae2: A decoding-enhanced masked self-supervised graph learner",
                    "Graphprompt: Unifying pre-training and downstream tasks for graph neural networks",
                    "DIFFormer: Scalable (graph) transformers induced by energy constrained diffusion",
                    "Contrastive multi-view representation learning on graphs",
                    "Deep graph infomax",
                    "Semi-supervised classification with graph convolutional networks",
                    "Multi-task self-supervised graph neural networks enable stronger task generalization",
                    "Evaluating large language models on graphs: Performance insights and comparative analysis",
                    "FastGCN: Fast learning with graph convolutional networks via importance sampling",
                    "Simgrace: A simple framework for graph contrastive learning without data augmentation",
                    "TEST: Text prototype aligned embedding to activate LLM's ability for time series"
                ]
            },
            {
                "name": "DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System",
                "arxiv_id": "2408.08231",
                "reference": [
                    "Cluster-guided contrastive graph clustering network",
                    "Lightgcl: Simple yet effective graph contrastive learning for recommendation",
                    "Ctrl: Connect tabular and language model for ctr prediction",
                    "Llara: Large language-recommendation assistant",
                    "Interpolation-based contrastive learning for few-label semi-supervised learning",
                    "Graphlearner: Graph node clustering with fully learnable augmentation",
                    "Improving graph collaborative filtering with neighborhood-enriched contrastive learning",
                    "Self-supervised graph learning for recommendation",
                    "Representation learning with large language models for recommendation",
                    "Enhancing job recommendation through llm-based generative adversarial networks",
                    "Dual test-time training for out-of-distribution recommender system",
                    "Gpt understands, too",
                    "Neural graph collaborative filtering",
                    "Mixed graph contrastive network for semi-supervised node classification",
                    "Controlrec: Bridging the semantic gap between language model and personalized recommendation",
                    "Llm-enhanced user-item interactions: Leveraging edge information for optimized recommendations",
                    "Dealmvc: Dual contrastive calibration for multi-view clustering",
                    "Gpt-4 technical report",
                    "Convert: Contrastive graph clustering with reliable augmentation",
                    "Lightgcn: Simplifying and powering graph convolution network for recommendation",
                    "Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach",
                    "Asymmetric double-winged multi-view clustering network for exploring diverse and consistent information",
                    "Calrec: Contrastive alignment of generative llms for sequential recommendation"
                ]
            },
            {
                "name": "Better Alignment with Instruction Back-and-Forth Translation",
                "arxiv_id": "2408.04614",
                "reference": [
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Free dolly: Introducing the world's first truly open instruction-tuned llm",
                    "Orca: Progressive learning from complex explanation traces of gpt",
                    "Zephyr: Direct distillation of lm alignment",
                    "Lima: Less is more for alignment",
                    "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
                    "How far can camels go? exploring the state of instruction tuning on open resources",
                    "Self-alignment with instruction backtranslation",
                    "Instruction tuning for large language models: A survey",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "The flan collection: Designing data and methods for effective instruction tuning",
                    "Longform: Optimizing instruction tuning for long text generation with corpus extraction",
                    "Instruction tuning with gpt",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Kun: Answer polishment for chinese self-alignment with instruction back-translation",
                    "Cross-task generalization via natural language crowdsourcing instructions",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Tegit: Generating high-quality instruction-tuning data with text-grounded task design",
                    "Reformatted alignment",
                    "Openassistant conversations-democratizing large language model alignment",
                    "Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning",
                    "Alpaca: A strong, replicable instruction-following model"
                ]
            },
            {
                "name": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
                "arxiv_id": "2402.07610",
                "reference": [
                    "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                    "Self-consuming generative models go mad",
                    "Star: Bootstrapping reasoning with reasoning",
                    "Salmon: Self-alignment with principle-following reward models",
                    "Efficient toxic content detection by bootstrapping and distilling large language models",
                    "Large language models suffer from their own output: An analysis of the self-consuming training loop",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Large language models can self-improve",
                    "Fairness-guided few-shot prompting for large language models",
                    "Efficient test-time model adaptation without forgetting",
                    "Self-alignment with instruction backtranslation",
                    "Self-instruct: Aligning language model with self generated instructions",
                    "Model dementia: Generated data makes models forget",
                    "Revisiting plasticity in visual reinforcement learning: Data, modules and training stages",
                    "Towards stable test-time adaptation in dynamic wild world"
                ]
            },
            {
                "name": "Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities",
                "arxiv_id": "2408.09366",
                "reference": [
                    "Quantifying and predicting mental illness severity in online pro-eating disorder communities",
                    "Illuminating the black box: A psychometric investigation into the multifaceted nature of large language models",
                    "Whose emotions and moral sentiments do language models reflect",
                    "Ai psychometrics: Assessing the psychological profiles of large language models through psychometric inventories",
                    "Recovery amid pro-anorexia: Analysis of recovery in social media",
                    "Communicating stigma: The pro-ana paradox",
                    "Inducing anxiety in large language models increases exploration and bias",
                    "Reading between the tweets: Deciphering ideological stances of interconnected mixed-ideology communities",
                    "Towards measuring the representation of subjective global opinions in language models",
                    "Personality traits in large language models",
                    "Community-cross-instruct: Unsupervised instruction generation for aligning large language models to online communities",
                    "Individuals with eating disorders and the use of online support groups as a form of social support",
                    "Whose opinions do language models reflect",
                    "Psychological impact of pro-anorexia and pro-eating disorder websites on adolescent females: A systematic review",
                    "Large language models as subpopulation representative models: A review",
                    "Communitylm: Probing partisan worldviews from language models",
                    "'written in these scars are the stories i can't explain': A content analysis of pro-ana and thinspiration image sharing on instagram",
                    "Proanorexia communities on social media",
                    "Social desirability bias: A demonstration and technique for its reduction",
                    "Language models are unsupervised multitask learners",
                    "Pro-eating disorder communities on social networking sites: a content analysis",
                    "Probing the moral development of large language models through defining issues test",
                    "Evaluating and inducing personality in pre-trained language models",
                    " \"this post will just get taken down \"' characterizing removed pro-eating disorder social media content"
                ]
            },
            {
                "name": "Personality Alignment of Large Language Models",
                "arxiv_id": "2408.11779",
                "reference": [
                    "Moral foundations twitter corpus: A collection of 35k tweets annotated for moral sentiment",
                    "Valuenet: A new dataset for human value driven dialogue system",
                    "Emergent abilities of large language models",
                    "Activation addition: Steering language models without optimization",
                    "Large language models are better reasoners with self-verification",
                    "Safe RLHF: Safe reinforcement learning from human feedback",
                    "Inference-time intervention: Eliciting truthful answers from a language model",
                    "In-context learning and induction heads",
                    "Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack",
                    "Decoding-time realignment of language models",
                    "The sixteen personality factor questionnaire (16pf",
                    "Lamp: When large language models meet personalization",
                    "Using large language models to simulate multiple humans and replicate human subject studies",
                    "Steering llama 2 via contrastive activation addition",
                    "Editing personality for llms",
                    "Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards",
                    "Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews, 2024b",
                    "Do llms possess a personality? making the mbti test an amazing evaluation for large language models",
                    "On diversified preferences of large language model alignment",
                    "Locating and editing factual associations in gpt",
                    "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
                    "Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models",
                    "Dissecting recall of factual associations in auto-regressive language models",
                    "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                    "Interpretability at scale: Identifying causal mechanisms in alpaca",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "RAFT: Reward ranked finetuning for generative foundation model alignment",
                    "Localizing model behavior with path patching",
                    "Cvalues: Measuring the values of chinese large language models from safety to responsibility",
                    "Proximal policy optimization algorithms",
                    "Controllm: Crafting diverse personalities for language models",
                    "Compositional preference models for aligning lms",
                    "Can machines learn morality? the delphi experiment",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Decoding-time language model alignment with multiple objectives",
                    "Learning to summarize with human feedback",
                    "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                    "Assessing the alignment of large language models with human values for mental health integration: Cross-sectional study using schwartz's theory of basic values",
                    "Hydra: Model factorization framework for black-box llm personalization",
                    "Chatharuhi: Reviving anime character in reality via large language model",
                    "From persona to personalization: A survey on role-playing language agents",
                    "Dynamic generation of personalities with large language models",
                    "A minimaximalist approach to reinforcement learning from human feedback",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Self-assessment tests are unreliable measures of llm personality, 2024a",
                    "Discovering latent knowledge in language models without supervision",
                    "pyvene: A library for understanding and improving PyTorch models via interventions",
                    "A critical evaluation of ai feedback for aligning large language models",
                    "Learning what to value",
                    "A general theoretical paradigm to understand learning from human preferences",
                    "Bottom-up and top-down: Predicting personality with psycholinguistic and language model features",
                    "Evaluating and inducing personality in pre-trained language models"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Training language models to follow instructions with human feedback",
            "arxiv_id": "2203.02155",
            "isAPA": true,
            "abstract": "Making language models bigger does not inherently make them better at followinga user's intent. For example, large language models can generate outputs thatare untruthful, toxic, or simply not helpful to the user. In other words, thesemodels are not aligned with their users. In this paper, we show an avenue foraligning language models with user intent on a wide range of tasks by fine-tuningwith human feedback. Starting with a set of labeler-written prompts and promptssubmitted through the OpenAI API, we collect a dataset of labeler demonstrationsof the desired model behavior, which we use to fine-tune GPT-3 using supervisedlearning. We then collect a dataset of rankings of model outputs, which we use tofurther fine-tune this supervised model using reinforcement learning from humanfeedback. We call the resulting models InstructGPT. In human evaluations onour prompt distribution, outputs from the 1.3B parameter InstructGPT model arepreferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.Moreover, InstructGPT models show improvements in truthfulness and reductionsin toxic output generation while having minimal performance regressions on publicNLP datasets. Even though InstructGPT still makes simple mistakes, our resultsshow that fine-tuning with human feedback is a promising direction for aligninglanguage models with human intent.",
            "reference": [
                "Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017) . Deep reinforcement learning from human preferences",
                "Yi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh, A., Gabriel, R., and Hakkani-Tur, D. (2019) . Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. arXiv preprint arXiv",
                "Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. (2019) . Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv",
                "Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. (2016) . Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv",
                "Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. (2021) . A general language assistant as a laboratory for alignment. arXiv preprint arXiv",
                "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017) . Proximal policy optimization algorithms. arXiv preprint arXiv",
                "Dinan, E., Humeau, S., Chintagunta, B., and Weston, J. (2019b) . Build it break it fix it for dialogue safety: Robustness from adversarial human attack. arXiv preprint arXiv",
                "Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., and Irving, G. (2021) . Alignment of language agents. arXiv preprint arXiv",
                "Xu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein, D. (2021) . Detoxifying language models risks marginalizing minority voices. arXiv preprint arXiv",
                "Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. (2017) . Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv",
                "Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. (2021) . Recursively summarizing books with human feedback. arXiv preprint arXiv",
                "Cho, W. S., Zhang, P., Zhang, Y., Li, X., Galley, M., Brockett, C., Wang, M., and Gao, J. (2018) . Towards coherent and cohesive long-form text generation. arXiv preprint arXiv",
                "Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W., Kreps, S., et al. (2019) . Release strategies and the social impacts of language models. arXiv preprint arXiv",
                "Bostrom, N. (2014) . Superintelligence. Dunod",
                "Irving, G., Christiano, P., and Amodei, D. (2018) . AI safety via debate. arXiv preprint arXiv",
                "Madaan, A., Tandon, N., Clark, P., and Yang, Y. (2022) . Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv",
                "Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. (2020) . Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv",
                "Ngo, H., Raterink, C., Ara\u00fajo, J. G., Zhang, I., Chen, C., Morisot, A., and Frosst, N. (2021) . Mitigating harm in language models with conditional-likelihood filtration. arXiv preprint arXiv",
                "Xu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. (2020) . Recipes for safety in open-domain chatbots. arXiv preprint arXiv",
                "V\u00f6lske, M., Potthast, M., Syed, S., and Stein, B. (2017) . Tl; dr: Mining reddit to learn automatic summarization",
                "Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. (2020) . Learning to summarize from human feedback. arXiv preprint arXiv",
                "Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021) . On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages",
                "Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and Grefenstette, E. (2018) . Learning to understand goal specifications by modelling reward. arXiv preprint arXiv",
                "Nangia, N., Vania, C., Bhalerao, R., and Bowman, S. R. (2020) . CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
                "Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. (2013) . Recursive deep models for semantic compositionality over a sentiment treebank",
                "Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. (2021) . On the opportunities and risks of foundation models. arXiv preprint arXiv",
                "Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016) . High-dimensional continuous control using generalized advantage estimation",
                "Lin, S., Hilton, J., and Evans, O. (2021) . Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv",
                "Gabriel, I. (2020) . Artificial intelligence, values, and alignment. Minds and machines",
                "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019) . Language models are unsupervised multitask learners. OpenAI Blog",
                "Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L. (2018) . Quac: Question answering in context",
                "Liu, H., Dacon, J., Fan, W., Liu, H., Liu, Z., and Tang, J. (2019) . Does gender matter? towards fairness in dialogue systems. arXiv preprint arXiv",
                "Qian, Y., Muaz, U., Zhang, B., and Hyun, J. W. (2019) . Reducing gender bias in word-level language models with a gender-equalizing loss function. arXiv preprint arXiv",
                "Anthony, T., Tian, Z., and Barber, D. (2017) . Thinking fast and slow with deep learning and tree search. arXiv preprint arXiv",
                "Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y. (2016) . An actor-critic algorithm for sequence prediction. arXiv preprint arXiv",
                "Blodgett, S. L., Barocas, S., Daum\u00e9 III, H., and Wallach, H. (2020) . Language (technology) is power: A critical survey of\" bias\" in nlp. arXiv preprint arXiv",
                "Fedus, W., Zoph, B., and Shazeer, N. (2021) . Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv",
                "Rudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. (2018) . Gender bias in coreference resolution",
                "Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. (2018) . Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv",
                "Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S. M. (2020) . Investigating gender bias in language models using causal mediation analysis",
                "Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et al. (2021) . Ethical and social risks of harm from language models. arXiv preprint arXiv",
                "Nahian, M. S. A., Frazier, S., Harrison, B., and Riedl, M. (2021) . Training value-aligned reinforcement learning agents using a normative prior. arXiv preprint arXiv",
                "Rajpurkar, P., Jia, R., and Liang, P. (2018) . Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv",
                "Perez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela, D., and Cho, K. (2019) . Finding generalizable evidence by learning to convince q&a models. arXiv preprint arXiv",
                "Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R. (2019) . Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv",
                "Dhamala, J., Sun, T., Kumar, V., Krishna, S., Pruksachatkun, Y., Chang, K.-W., and Gupta, R. (2021) . Bold: Dataset and metrics for measuring biases in open-ended language generation",
                "Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019) . Fine-tuning language models from human preferences. arXiv preprint arXiv",
                "Schick, T., Udupa, S., and Sch\u00fctze, H. (2021) . Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. arXiv preprint arXiv",
                "Christiano, P., Cotra, A., and Xu, M. (2021) . Eliciting latent knowledge: How to tell if your eyes deceive you. https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge",
                "Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. (2021) . Extracting training data from large language models",
                "Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. (2019) . The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv",
                "Tamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021) . Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv",
                "Krause, B., Gotmare, A. D., McCann, B., Keskar, N. S., Joty, S., Socher, R., and Rajani, N. F. (2020) . Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv",
                "Kirk, H., Jun, Y., Iqbal, H., Benussi, E., Volpin, F., Dreyer, F. A., Shtedritski, A., and Asano, Y. M. (2021) . How true is gpt-2? an empirical analysis of intersectional occupational biases. arXiv preprint arXiv",
                "Zhou, W. and Xu, K. (2020) . Learning to compare for better training and evaluation of open domain natural language generation models. arXiv preprint arXiv",
                "Huang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama, D., and Kohli, P. (2019) . Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint arXiv",
                "Soares, N., Fallenstein, B., Armstrong, S., and Yudkowsky, E. (2015) . Corrigibility",
                "Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. (2018) . Can neural machine translation be improved with user feedback? arXiv preprint arXiv",
                "Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. (2021) . Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv",
                "Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. (2021) . Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv",
                "Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. (2019) . Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv",
                "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020) . Language models are few-shot learners. arXiv preprint arXiv",
                "Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt, T., Lefrancq, A., Orseau, L., and Legg, S. (2017) . AI safety gridworlds. arXiv preprint arXiv",
                "Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021) . Finetuned language models are zero-shot learners. arXiv preprint arXiv",
                "Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. (2020) . Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv",
                "Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. (2019) . Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv",
                "Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., and Turchi, M. (2015) . Findings of the 2015 workshop on statistical machine translation",
                "Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H. S., Mehta, S. V., Zhuang, H., Tran, V. Q., Bahri, D., Ni, J., et al. (2021) . Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv",
                "Nadeem, M., Bethke, A., and Reddy, S. (2020) . Stereoset: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv",
                "Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019) . Hellaswag: Can a machine really finish your sentence? In Association for Computational Linguistics, pages",
                "Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. (2021) . Challenges in detoxifying language models. arXiv preprint arXiv",
                "Dinan, E., Fan, A., Williams, A., Urbanek, J., Kiela, D., and Weston, J. (2019a) . Queens are powerful too: Mitigating gender bias in dialogue generation. arXiv preprint arXiv",
                "Solaiman, I. and Dennison, C. (2021) . Process for adapting language models to society (palms) with values-targeted datasets. arXiv preprint arXiv",
                "Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. (2019) . Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv",
                "Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2021) . Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv",
                "Zhao, M., Anderson, P., Jain, V., Wang, S., Ku, A., Baldridge, J., and Ie, E. (2021) . On the evaluation of vision-and-language navigation instructions. arXiv preprint arXiv",
                "Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. (2021) . Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv",
                "Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2019) . Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv",
                "Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017) . Constrained policy optimization",
                "B\u00f6hm, F., Gao, Y., Meyer, C. M., Shapira, O., Dagan, I., and Gurevych, I. (2019) . Better rewards yield better summaries: Learning to summarise without references. arXiv preprint arXiv",
                "Manela, D. d. V., Errington, D., Fisher, T., van Breugel, B., and Minervini, P. (2021) . Stereotype and skew: Quantifying gender bias in pre-trained and fine-tuned language models. arXiv preprint arXiv",
                "Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021) . Evaluating large language models trained on code. arXiv preprint arXiv",
                "Lawrence, C. and Riezler, S. (2018) . Improving a neural semantic parser by counterfactual learning from human bandit feedback. arXiv preprint arXiv",
                "Liang, P. P., Wu, C., Morency, L.-P., and Salakhutdinov, R. (2021) . Towards understanding and mitigating social biases in language models",
                "Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. (2022) . Lamda: Language models for dialog applications. arXiv preprint arXiv",
                "Christiano, P., Shlegeris, B., and Amodei, D. (2018) . Supervising strong learners by amplifying weak experts. arXiv preprint arXiv",
                "Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et al. (2020) . Imitating interactive intelligence. arXiv preprint arXiv",
                "Henderson, P., Sinha, K., Angelard-Gontier, N., Ke, N. R., Fried, G., Lowe, R., and Pineau, J. (2018) . Ethical challenges in data-driven dialogue systems",
                "Caliskan, A., Bryson, J. J., and Narayanan, A. (2017) . Semantics derived automatically from language corpora contain human-like biases. Science",
                "Buchanan, B., Lohn, A., Musser, M., and Sedova, K. (2021) . Truth, lies, and automation. Technical report, Center for the Study of Emerging Technology",
                "Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. (2018) . Reward learning from human preferences and demonstrations in atari"
            ],
            "related work": "2Related workResearch on alignment and learning from human feedback.We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feedback (RLHF) . Originally developed for training simple robots in simulated environments and Atari games(Christiano et al.,,2017; Ibarz et al.,,2018) , it has recently been applied to fine-tuning language models to summarize text(Ziegler et al.,,2019; Stiennon et al.,,2020; B\u00f6hm et al.,,2019; Wu et al.,,2021) . This work is in turn influenced by similar work using human feedback as a reward in domains such as dialogue(Jaques et al.,,2019; Yi et al.,,2019; Hancock et al.,,2019) , translation(Kreutzer et al.,,2018; Bahdanau et al.,,2016) , semantic parsing(Lawrence and Riezler,,2018) , story generation(Zhou and Xu,,2020) , review generation(Cho et al.,,2018) , and evidence extraction(Perez et al.,,2019) .Madaan et al., (2022) use written human feedback to augment prompts and improve the performance of GPT-3. There has also been work on aligning agents in text-based environments using RL with a normative prior(Nahian et al.,,2021) . Our work can be seen as a direct application of RLHF to aligning language models on a broad distribution of language tasks.The question of what it means for language models to be aligned has also received attention recently(Gabriel,,2020) .Kenton et al., (2021) catalog behavioral issues in LMs that result from misalignment, including producing harmful content and gaming misspecified objectives. In concurrent work,Askell et al., (2021) propose language assistants as a testbed for alignment research, study some simple baselines, and their scaling properties.Training language models to follow instructions.Our work is also related to research on cross-task generalization in language models, where LMs are fine-tuned on a broad range of public NLP datasets (usually prefixed with an appropriate instruction) and evaluated on a different set of NLP tasks. There has been a range of work in this domain(Yi et al.,,2019; Mishra et al.,,2021; Wei et al.,,2021; Khashabi et al.,,2020; Sanh et al.,,2021; Aribandi et al.,,2021) , which differ in training and evaluation data, formatting of instructions, size of pretrained models, and other experimental details. A consistent finding across studies is that fine-tuning LMs on a range of NLP tasks, with instructions, improves their downstream performance on held-out tasks, both in the zero-shot and few-shot settings.There is also a related line of work on instruction following for navigation, where models are trained to follow natural language instructions to navigate in a simulated environment(Bahdanau et al.,,2018; Abramson et al.,,2020; Zhao et al.,,2021) .Evaluating the harms of language models.A goal of modifying the behavior of language models is to mitigate the harms of these models when they're deployed in the real world. These risks have been extensively documented(Bender et al.,,2021; Bommasani et al.,,2021; Kenton et al.,,2021; Weidinger et al.,,2021; Tamkin et al.,,2021) . Language models can produce biased outputs(Dhamala et al.,,2021; Liang et al.,,2021; Manela et al.,,2021; Caliskan et al.,,2017; Kirk et al.,,2021) , leak private data(Carlini et al.,,2021) , generate misinformation(Solaiman et al.,,2019; Buchanan et al.,,2021) , and be used maliciously; for a thorough review we direct the reader toWeidinger et al., (2021) . Deploying language models in specific domains gives rise to new risks and challenges, for example in dialog systems(Henderson et al.,,2018; Xu et al.,,2020;Dinan et al., 2019b,) . There is a nascent but growing field that aims to build benchmarks to concretely evaluate these harms, particularly around toxicity(Gehman et al.,,2020) , stereotypes(Nadeem et al.,,2020) , and social bias(Dhamala et al.,,2021; Nangia et al.,,2020; Rudinger et al.,,2018) . Making significant progress on these problems is hard since well-intentioned interventions on LM behavior can have side-effects(Welbl et al.,,2021; Blodgett et al.,,2020) ; for instance, efforts to reduce the toxicity of LMs can reduce their ability to model text from under-represented groups, due to prejudicial correlations in the training data(Xu et al.,,2021) .Modifying the behavior of language models to mitigate harms.There are many ways to change the generation behavior of language models.Solaiman and Dennison, (2021) fine-tune LMs on a small, value-targeted dataset, which improves the models' ability to adhere to these values on a question answering task.Ngo et al., (2021) filter the pretraining dataset by removing documents on which a language model has a high conditional likelihood of generating a set of researcher-written trigger phrases. When trained on this filtered dataset, their LMs generate less harmful text, at the cost of a slight decrease in language modeling performance.Xu et al., (2020) use a variety of approaches to improve the safety of chatbots, including data filtering, blocking certain words or n-grams during generation, safety-specific control tokens(Keskar et al.,,2019;Dinan et al., 2019a,) , and human-in-the-loop data collection(Dinan et al., 2019b,) . Other approaches for mitigating the generated bias by LMs use word embedding regularization(Liu et al.,,2019; Huang et al.,,2019) , data augmentation(Liu et al.,,2019;Dinan et al., 2019a,; Sheng et al.,,2019) , null space projection to make the distribution over sensitive tokens more uniform(Liang et al.,,2021) , different objective functions(Qian et al.,,2019) , or causal mediation analysis(Vig et al.,,2020) . There is also work on steering the generation of language models using a second (usually smaller) language model(Dathathri et al.,,2019; Krause et al.,,2020) , and variants of this idea have been applied to reducing language model toxicity(Schick et al.,,2021) .",
            "date": "2022"
        },
        "topic": "ChatGPT",
        "year_start": "2021",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
                "arxiv_id": "2403.07183",
                "subtitles": [
                    "Zero-shot LLM detection",
                    "Training-based LLM detection",
                    "LLM watermarking"
                ],
                "reference": [
                    "How to spot ai-generated text",
                    "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                    "GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance",
                    "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                    "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts",
                    "Can AI-Generated Text be Reliably Detected",
                    "A Survey on Detection of LLMs-Generated Content",
                    "A watermark for large language models",
                    "Attacking Neural Text Detectors",
                    "Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",
                    "Natural language watermarking: challenges in building a practical system",
                    "Defending Against Neural Fake News",
                    "Automatic detection of machine generated text: A critical survey",
                    "Robust Distortion-free Watermarks for Language Models",
                    "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
                    "Release strategies and the social impacts of language models",
                    "Authorship Attribution for Neural Text Generation",
                    "Deepfake Text Detection in the Wild",
                    "Release Strategies and the Social Impacts of Language Models",
                    "On the possibilities of ai-generated text detection",
                    "New AI classifier for indicating AI-written text",
                    "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
                    "Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors",
                    "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
                    "GPT-2: 1.5B release",
                    "A Semantic Invariant Robust Watermark for Large Language Models",
                    "Natural Language Watermarking Using Semantic Substitution for Chinese Text",
                    "Real or Fake? Learning to Discriminate Machine from Human Generated Text",
                    "DetectGPT: Zero-shot machine-generated text detection using probability curvature",
                    "Identifying Real or Fake Articles: Towards better Language Modeling",
                    "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
                    "Three Bricks to Consolidate Watermarks for Large Language Models",
                    "Red Teaming Language Model Detectors with Language Models",
                    "Provable Robust Watermarking for AI-Generated Text",
                    "Natural Language Watermarking: Design, Analysis, and a Proof-of-Concept Implementation",
                    "Testing of detection tools for AI-generated text",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Automatic detection of generated text is easiest when humans are fooled",
                    "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
                    "DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models",
                    "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning",
                    "TweepFake: About detecting deepfake tweets",
                    "GLTR: Statistical Detection and Visualization of Generated Text",
                    "Robust Multi-bit Natural Language Watermarking through Invariant Features",
                    "GPT detectors are biased against non-native English writers",
                    "ChatGPT creator pulls AI detection tool due to 'low rate of accuracy",
                    "Computer-Generated Text Detection Using Machine Learning: A Systematic Review",
                    "RADAR: Robust AI-Text Detection via Adversarial Learning",
                    "Natural Language Watermarking and Tamperproofing",
                    "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions",
                    "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                    "Unbiased Watermark for Large Language Models",
                    "Squibs: What Is a Paraphrase",
                    "GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content",
                    "Detecting Fake Content with Relative Entropy Scoring"
                ],
                "related_work": "2Related WorkZero-shot LLM detection.Many approaches to LLM detection aim to detect AI-generated text at the level of individual documents. Zero-shot detection or  \"model self-detection \" represents a major approach family, utilizing the heuristic that text generated by an LLM will exhibit distinctive probabilistic or geometric characteristics within the very model that produced it. Early methods for LLM detection relied on metrics like entropy(Lavergne et al.,2008) , log-probability scores(Solaiman et al.,2019b) , perplexity(Beresneva,2016) , and uncommon n-gram frequencies(Badaskar et al.,2008) from language models to distinguish between human and machine text. More recently, DetectGPT(Mitchell et al.,2023a) suggests that AI-generated text typically occupies regions with negative log probability curvature. DNA-GPT(Yang et al.,2023a) improves performance by analyzing n-gram divergence between re-prompted and original texts. Fast-DetectGPT(Bao et al.,2023) enhances efficiency by leveraging conditional probability curvature over raw probability.Tulchinskii et al. (2023) show that machine text has lower intrinsic dimensionality than human writing, as measured by persistent homology for dimension estimation. However, these methods are most effective when there is direct access to the internals of the specific LLM that generated the text. Since many commercial LLMs, including OpenAI's GPT-4, are not open-sourced, these approaches often rely ona proxy LLMassumed to be mechanistically similar to the closed-source LLM. This reliance introduces compromises that, as studies by(Sadasivan et al.,2023; Shi et al.,2023; Yang et al.,2023b; Zhang et al.,2023) demonstrate, limit the robustness of zero-shot detection methods across different scenarios.Training-based LLM detection.An alternative LLM detection approach is to fine-tune a pretrained model on datasets with both human and AI-generated text examples in order to distinguish between the two types of text, bypassing the need for original model access. Earlier studies have used classifiers to detect synthetic text in peer review corpora(Bhagat & Hovy,2013) , media outlets(Zellers et al.,2019) , and other contexts(Bakhtin et al.,2019; Uchendu et al.,2020) . More recently, GPT-Sentinel(Chen et al.,2023) train the RoBERTa(Liu et al.,2019) and T5(Raffel et al.,2020) classifiers on the constructed dataset OpenGPTText. GPT-Pat(Yu et al.,2023) train a twin neural network to compute the similarity between original and re-decoded texts.Li et al. (2023) build a wild testbed by gathering texts from various human writings and deepfake texts generated by different LLMs. Notably, the application of contrastive and adversarial learning techniques has enhanced classifier robustness(Liu et al.,2022; Bhattacharjee et al.,2023; Hu et al.,2023a) . However, the recent development of several publicly available tools aimed at mitigating the risks associated with AI-generated content has sparked a debate about their effectiveness and reliability(OpenAI,2019; Jawahar et al.,2020; Fagni et al.,2021; Ippolito et al.,2019; Mitchell et al.,2023b; Gehrmann et al.,2019; Heikkil\u00e4,2022; Crothers et al.,2022; Solaiman et al.,2019a) . This discussion gained further attention with OpenAI's 2023 decision to discontinue its AI-generated text classifier due to its  \"low rate of accuracy \"(Kirchner et al.,2023; Kelly,2023) .A major empirical challenge for training-based methods is their tendency to overfit to both training data and language models. Therefore, many classifiers show vulnerability to adversarial attacks(Wolff,2020) and display bias towards writers of non-dominant language varieties(Liang et al.,2023a) . The theoretical possibility of achieving accurateinstance-level detection has also been questioned by researchers, with debates exploring whether reliably distinguishing AI-generated content from human-created text on an individual basis is fundamentally impossible(Weber-Wulff et al.,2023; Sadasivan et al.,2023; Chakraborty et al.,2023) . Unlike these approaches to detecting AI-generated text at the document, paragraph, or sentence level, our method estimates the fraction of an entire text corpus which is substantially AI-generated. Our extensive experiments demonstrate that by sidestepping the intermediate step of classifying individual documents or sentences, this method improves upon the stability, accuracy, and computational efficiency of existing approaches.LLM watermarking.Text watermarking introduces a method to detect AI-generated text by embedding unique, algorithmically-detectable signals -known as watermarks- directly into the text. Early watermarking approaches modify pre-existing text by leveraging synonym substitution(Chiang et al.,2003; Topkara et al.,2006b) , syntactic structure restructuring(Atallah et al.,2001; Topkara et al.,2006a) , or paraphrasing(Atallah et al.,2002) . Increasingly, scholars have focused on integrating a watermark directly into an LLM's decoding process.Kirchenbauer et al. (2023) split the vocabulary into red-green lists based on hash values of previous n-grams and then increase the logits of green tokens to embed the watermark.Zhao et al. (2023) use a global red-green list to enhance robustness.Hu et al. (2023b) ; Kuditipudi et al. (2023) ; Wu et al. (2023) study watermarks that preserve the original token probability distributions. Meanwhile, semantic watermarks(Hou et al.,2023; Fu et al.,2023; Liu et al.,2023) using input sequences to find semantically related tokens and multi-bit watermarks(Yoo et al.,2023; Fernandez et al.,2023) to embed more complex information have been proposed to improve certain conditional generation tasks. However, watermarking requires the involvement of the model or service owner, such as OpenAI, to implant the watermark. Concerns have also been raised regarding the potential for watermarking to degrade text generation quality and to compromise the coherence and depth of LLM responses(Singh & Zou,2023) . In contrast, our framework operatesindependentlyof the model or service owner's intervention, allowing for the monitoring of AI-modified content without requiring their adoption.",
                "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices."
            },
            {
                "name": "ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback",
                "arxiv_id": "2401.03605",
                "subtitles": [
                    "Prompt Engineering",
                    "Language Models as Recommenders",
                    "Algorithmic Recourse"
                ],
                "reference": [
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations",
                    "Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Language models are few-shot learners",
                    "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Language models are unsupervised multitask learners",
                    "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                    "Actionable Recourse in Linear Classification",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
                ],
                "related_work": "2.Related Work2.1.Prompt EngineeringPrompt engineering is a new field that has arisen as a result of recent LLMs' in-context reasoning capabilities without fine-tuning. This burgeoning field aims to explore how best to communicate with LLMs when asking them to perform a task in-context. Generally, there have been three predominant means of communication with LLMs through prompting(Brown et al.,2020) : Zero-shot: the model is provided with only instructions and asked to complete a task. Few-shot: the model is given examples demonstrating a task, and is then asked to repeat this task by generating its own output for a similarly structured question.One-Shot specifies that the prompt containsoneexample. Chain-of-Thought prompting (CoT) : the model is gradually asked to produce intermediate answers before giving the final answer to a multi-step problem(Wei et al.,2022) . The idea is to mimic an intuitive multi-step thought process when working through a reasoning problem.Most other approaches are devised around these archetypes and either vary the amount of information or present the task in a different way.2.2.Language Models as RecommendersThe extensive domain knowledge encapsulated in Large Language Models (LLMs) has recently captured interest for their use in recommendation tasks. Early approaches predominantly concentrated on single input-output tasks. For instance, BERT4Rec(Sun et al.,2019) refines the encoder-only language model, BERT(Devlin et al.,2018) , specifically for sequential recommendations, achieving notable gains over preceding RNN-based benchmarks. LMRecSys(Zhang et al.,2021) was among the first methods to explore in-context sequential recommendation with LLMs, showcasing the performance of the encoder-only model and GPT-2(Radford et al.,2019) across a variety of zero shot prompts. Another study(Kang et al.,2023) followed this up with more recent models, with findings that show the benefit of using an LLM for sequential recommendation with fine-tuning.(Liu et al.,2023) underscored the general-purpose recommendation potential of ChatGPT. It emphasized the power of prompt engineering, converting recommendation tasks into natural language tasks, and assessing the model's performance without explicit fine-tuning.(Gao et al.,2023b) further evaluated GPT-3 models as augmented language models to interface with existing recommendation systems. They further provided a number of case studies that showcase the potential of ChatGPT for recommendation.In a recent study(Sanner et al.,2023) , researchers explored the potential of LLMs in making recommendations based on both item-based and language-based preferences. Their focus was to compare the efficacy of LLMs with traditional item-based collaborative filtering methods, using a dataset they collected which comprised both types of preferences and their corresponding ratings on recommended items. They showed that LLMs could yield competitive recommendation performance for purely language-based preferences, especially in scenarios resembling a cold-start.In contrast, our research focuses primarily on the conversational dynamics of ChatGPT as a recommendation system, emphasizing its interactional capabilities and addressing concerns such as popularity bias through prompt engineering.Chat-Rec(Gao et al.,2023a) is another study close to ours. By translating user profiles and historical data into prompts, it enhances the learning of user preferences and fosters a more direct connection between users and products through in-context learning. In contrast to our work, which emphasizes the iterative feedback and popularity bias mitigation of ChatGPT, the Chat-Rec approach focuses on enhancing traditional recommender systems by converting user data into prompts for LLMs, thereby boosting recommendation interactivity and explainability. Although both approaches address the power of LLMs in the recommendation domain,the primary distinction lies in the emphasis of our study on real-time conversational dynamics and Chat-Rec's user-to-prompt data translation for enhancing the recommendation process.2.3.Algorithmic RecourseOur work has parallels with the concept ofalgorithmic recourse(Karimi et al.,2022) , which has been defined as  \"a person's capability to achieve a preferred result from an unchanged model \"(Ustun et al.,2019) . By highlighting the individual's ability to acquire a desired outcome from a fixed model, the iterative prompt engineering bears similarities with algorithmic recourse in a way that the user progressively guides the model to a different outcome or prediction. The major difference, however, is that there is no causality involved, but rather a progressive tweaking of the model towards a different outcome. These differences also arise because of the different scopes in algorithmic recourse for the case of a classification output as opposed to a recommendation list output. Another major distinction is that in our context, the algorithmic recourse is driven primarily by the user and not the model. In other words, it is the user who guides the model by providing additional feedback as a follow-up to the initial model output, to guide the model towards an alternative output. Essentially,our iterative prompt engineering can be considered as a middle ground between Zero-shot Learning and Algorithmic Recourse.",
                "abstract": "Recommendation algorithms have been pivotal in handling the overwhelming volume of online content. However, these algorithms seldom consider direct user input, resulting in superficial interaction between them. Efforts have been made to include the user directly in the recommendation process through conversation, but these systems too have had limited interactivity. Recently, Large Language Models (LLMs) like ChatGPT have gained popularity due to their ease of use and their ability to adapt dynamically to various tasks while responding to feedback. In this paper, we investigate the effectiveness of ChatGPT as a top-n conversational recommendation system. We build a rigorous pipeline around ChatGPT to simulate how a user might realistically probe the model for recommendations: by first instructing and then reprompting with feedback to refine a set of recommendations. We further explore the effect of popularity bias in ChatGPT's recommendations, and compare its performance to baseline models. We find that reprompting ChatGPT with feedback is an effective strategy to improve recommendation relevancy, and that popularity bias can be mitigated through prompt engineering."
            },
            {
                "name": "ChatGPT and Its Educational Impact: Insights from a Software Development Competition",
                "arxiv_id": "2409.03779",
                "subtitles": [
                    "Generative AI in Software Development",
                    "AI in Education"
                ],
                "reference": [
                    "Supporting Teachers' Professional Development With Generative AI: The Effects on Higher Order Thinking and Self-Efficacy",
                    "ChatGPT is a remarkable tool for experts",
                    "The influences of ChatGPT on undergraduate students' demonstrated and perceived interdisciplinary learning",
                    "AutoDev: Automated AI-Driven Development",
                    "Generative AI for software practitioners",
                    "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                    "Personalized Learning Paths: Adapting Education with AI-Driven Curriculum",
                    "Structure, Objectives, and Operational Framework for Ethical Integration of Artificial Intelligence in Educational",
                    "Challenges and opportunities for classroom-based formative assessment and AI: a perspective article",
                    "The Role of Sustainability and Artificial Intelligence in Education Improvement",
                    "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
                    "Foundation Models for Education: Promises and Prospects"
                ],
                "related_work": "2.Related Work2.1.Generative AI in Software DevelopmentGenerative AI, particularly tools like ChatGPT, has seen a wide range of applications in software development, transforming how developers approach coding, debugging, and project management. Recent studies highlight the multifaceted benefits and potential challenges of integrating AI in this field. A comprehensive review by Khojahet al.(Yeti\u015ftiren et al.,2023) explores how generative AI can assist in code generation, significantly reducing the time required for writing boilerplate code and allowing developers to focus on more complex tasks. The study finds that ChatGPT can generate code snippets based on high-level descriptions, improving productivity and code quality. Another significant application is in automated debugging. Ebert and Louridas(Ebert and Louridas,2023) demonstrate that generative AI can identify and suggest fixes for common coding errors, thus speeding up the debugging process. Their research highlights the ability of ChatGPT to understand and interpret code contextually, offering relevant solutions to encountered issues. The role of AI in enhancing software documentation is also well-documented. A study by Azariaet al.(Azaria et al.,2024) examines how AI tools can automate the creation of detailed and accurate documentation, ensuring consistency and reducing the manual effort involved. This capability is crucial for maintaining up-to-date documentation throughout the software development lifecycle. AI's impact on collaborative coding environments has been another area of interest. Marqueset al.(Marques et al.,2024) explore how ChatGPT can facilitate real-time collaboration among distributed teams, providing instant feedback and suggestions during code reviews. They have demonstrated effectiveness in improving communication and simplifying the development process. The integration of AI in continuous integration/continuous deployment (CI/CD) pipelines has been studied by Tofanoet al.(Tufano et al.,2024) . They find that generative AI can automate several aspects of CI/CD, from code integration to automated testing and deployment, thereby increasing efficiency and reducing the likelihood of human error.2.2.AI in EducationAI integration into education, including ChatGPT, has revolutionized teaching and learning methodologies. Recent research highlights various applications and the resultant impact on educational practices and outcomes. The potential of AI to provide personalized learning experiences is a significant focus. Thimmannaet al.(Thimmanna et al.,2024) demonstrate that AI-driven personalized learning paths can significantly enhance student engagement and achievement. Their study shows that adaptive learning environments powered by AI cater to individual student needs more effectively than traditional methods. AI's role in supporting remote and hybrid learning environments has gained prominence, especially during and after the COVID-19 pandemic. Rosak-Szyrockaet al.(Rosak-Szyrocka et al.,2023) investigate how AI can bridge the gap between remote learners and traditional classroom settings, enhancing inclusivity and accessibility. Their findings suggest that AI tools can provide consistent support regardless of students' location. The influence of ChatGPT on undergraduate interdisciplinary learning is examined by Zhonget al.(Zhong et al.,2024) . Their study involves 130 students in a quasi-experiment to assess ChatGPT's impact on interdisciplinary learning quality via online posts and surveys. Results indicate that ChatGPT enhances students' disciplinary grounding, although integration skills remain low. Another significant focus is the role of AI in educational assessments. Hopfenbecket al.(Hopfenbeck et al.,2023) investigate how AI can enhance formative assessment practices, providing real-time insights into student progress. Their research underscores the potential of AI to provide more nuanced and comprehensive assessments of student performance, moving beyond traditional testing methods. Xuet al.(Xu et al.,2024) discuss the transformative role of foundation models like ChatGPT in education, emphasizing strengths such as personalized learning, addressing educational inequality, and enhancing reasoning capabilities. This research contributes to the expanding literature on AI in education, stressing the need for ongoing studies to understand the full implications of generative AI technologies.The ethical implications of AI in education are a critical area of research. Saxenaet al.(Saxena et al.,2023) emphasize the importance of ethical AI practices, proposing frameworks to ensure transparency and fairness in AI-driven educational tools. This work highlights the need to address ethical considerations to ensure the responsible use of AI technologies. AI's impact on teacher practices is discussed by Luet al.(Lu et al.,2024) . They explore how AI tools can augment teachers' capabilities, allowing them to focus more on higher-order teaching tasks. ChatGPT can assist teachers by handling routine queries and providing additional resources, freeing their time for more complex instructional activities. Overall, the recent body of research underscores the transformative potential of AI in education. ChatGPT, a state-of-the-art generative AI, offers new possibilities for enhancing educational experiences and outcomes. This study aims to contribute to this growing body of knowledge by providing empirical evidence of ChatGPT's impact in the competition setting.",
                "abstract": "This study explores the integration and impact of ChatGPT, a generative AI that utilizes natural language processing, in an educational environment. The main goal is to evaluate how ChatGPT affects project performance. To this end, we organize a software development competition utilizing ChatGPT, lasting for four weeks and involving 36 students. The competition is structured in two rounds: in the first round, all 36 students participate and are evaluated based on specific performance metrics such as code quality, innovation, and adherence to project requirements. The top 15 performers from the first round are then selected to advance to the second round, where they compete for the final rankings and the overall winner is determined. The competition shows that students who use ChatGPT extensively in various stages of development, including ideation, documentation, software development, and quality assurance, have higher project completion rates and better scores. A detailed comparative analysis between first-round and second-round winners reveals significant differences in their experience with generative AI for software development, experience learning large-scale language models, and interest in their respective fields of study. These findings suggest that ChatGPT enhances individual learning and project performance. A post-survey of participants also reveals high levels of satisfaction, further emphasizing the benefits of integrating generative AI like ChatGPT in academic settings. This study highlights the transformative potential of ChatGPT in project-based learning environments and supports further research into its long-term impact and broader application in a variety of educational contexts."
            },
            {
                "name": "Beyond ChatGPT: Enhancing Software Quality Assurance Tasks with Diverse LLMs and Validation Techniques",
                "arxiv_id": "2409.01001",
                "subtitles": [
                    "Large Language Models (LLMs) ",
                    "Software Quality Assurance Tasks",
                    "LLM and SQA Tasks"
                ],
                "reference": [
                    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "The cost of poor software quality in the US: A 2022 report",
                    "An empirical study of fault localization families and their combinations",
                    "FPA-FL: Incorporating static fault-proneness analysis into statistical fault localization",
                    "Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs",
                    "Evaluation of chatgpt model for vulnerability detection",
                    "Meta LLaMA",
                    "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
                    "ChatGPT Model Overview",
                    "Spectrum-based multiple fault localization",
                    "Toward large-scale vulnerability discovery using machine learning",
                    "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues",
                    "The DStar method for effective software fault localization",
                    "The impact factors on the performance of machine learning-based vulnerability detection: A comparative study",
                    "Mixtral of Experts",
                    "An introduction to modern software quality assurance",
                    "Software quality assurance-concepts and misconceptions",
                    "Fault localization prioritization: Comparing information-theoretic and coverage-based approaches",
                    "uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers",
                    "A comparative study of static code analysis tools for vulnerability detection in c/c++ and java source code",
                    "Prompt-enhanced software vulnerability detection using chatgpt",
                    "Investigating Code Generation Performance of Chat-GPT with Crowdsourcing Social Data",
                    "Mitigating program security vulnerabilities: Approaches and challenges",
                    "Large Language Models in Fault Localisation",
                    "Gemma-1.1-7B",
                    "Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models",
                    "Large language models for test-free fault localization",
                    "Improving spectral-based fault localization using static analysis",
                    "Large Language Model for Vulnerability Detection: Emerging Results and Future Directions",
                    "Integrating static and dynamic analysis for detecting vulnerabilities",
                    "A Preliminary Evaluation of LLM-Based Fault Localization",
                    "Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization",
                    "Fluccs: Using code and change metrics to improve fault localization",
                    "Attention is all you need",
                    "Language models are few-shot learners",
                    "Automated code review tools for security",
                    "Androshield: Automated android applications vulnerability detection, a hybrid static and dynamic analysis approach",
                    "Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study",
                    "Evaluating and Improving Fault Localization",
                    "A model for spectra-based software diagnosis",
                    "Prompt-Enhanced Software Vulnerability Detection Using ChatGPT",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Gptscan: Detecting logic vulnerabilities in smart contracts by combining gpt with program analysis"
                ],
                "related_work": "2.Background and Related Work2.1.Large Language Models (LLMs) Large Language Models (LLMs) are trained on extensive text data using advanced deep learning architectures like the Transformer architecture(Vaswani et al.,2017) . LLMs have showcased remarkable effectiveness across various natural language processing tasks(Kenton and Toutanova,2019) . LLM also has been used in automated software engineering, such as incident management of cloud service(Ahmed et al.,2023) , automated program repair(Xia and Zhang,2023; Liu et al.,2023) , code generation(Feng et al.,2023) , vulnerability detection(Fu et al.,2023; Zhang et al.,2024b; Zhou et al.,2024) , and fault localization(Wu et al.,2023; Kang et al.,2023; Widyasari et al.,2024) .There are various examples of LLMs, among which one prominent model is ChatGPT, developed by OpenAI. ChatGPT is built upon the GPT-3 architecture(Brown et al.,2020) and has been refined through reinforcement learning from human feedback (RLHF) . Recently, OpenAI released GPT-4o, which is claimed to outperform GPT-4 in text evaluation(OpenAI,2024a) . This new model is available alongside the free version of ChatGPT (GPT-3.5) and the premium version (GPT-4) . Other significant LLMs include LLaMA 3 developed by Meta. There are two versions of LLaMA-3, one with 8 billion and another with 70 billion parameters(AI,2024a) . Another example of LLMs is Mixtral, created by Mistral AI. It adopts a Mixtures of Experts (MoE) architecture, consisting of eight distinct models, each equipped with seven billion parameters(AI,2024b) . Google has also made significant contributions to the field with LLMs like Gemini (closed model) and Gemma (open model) . Gemma is a recent lightweight state-of-the-art open model that is built following the Gemini models(Research,2024) . In our study, we focus on Mixtral-8x7B, LLaMA-3-70B, LLaMA-3-8B, and Gemma-7B due to their popularity and their availability as open-source models. We also run the closed model GPT-4o in addition to the GPT-3.5 as this model claims to outperform GPT-4 with faster inference. These LLMs offer a diverse range of capabilities and have gained recognition within the community (cf. Section4.4) .2.2.Software Quality Assurance TasksSoftware Quality Assurance (SQA) is a critical part of the software development life cycle aimed at ensuring that software products meet specified quality standards and functional requirements(Runeson and Isacsson,1998; Maxim and Kessentini,2016) . The necessity for high-quality, reliable software is critical, as its failure can result in substantial harm and financial loss. For instance in the 2022 report, poor software quality in the United States has grown to at least $2.41 trillion(Krasner,2022) . SQA encompasses a broad range of tasks designed to ensure software quality. In this study, we focused on two integral examples of SQA tasks that have a direct impact on software reliability and security(Zou et al.,2019) .2.2.1.Fault LocalizationFault localization is a significant aspect of SQA, it is a task to pinpoint the specific locations in the code that are likely to cause software failures(Yoo et al.,2013) . This process is essential for efficient debugging and fixing of software, directly impacting the reliability and performance of software products. Over the years, researchers have proposed automatic fault localization techniques to help developers save time in this process. The proposed techniques for fault localization include a variety of methods such as machine-learning-based approaches(Sohn and Yoo,2017) , static analysis-based approaches(Neelofar et al.,2017; Feyzi and Parsa,2018) , spectrum-based approaches(Wong et al.,2013; Abreu et al.,2009; Naish et al.,2011) , and so on. In this study, we focus on line-level granularity fault localization as it offers the finest resolution and has been widely utilized in previous research(Pearson et al.,2017; Wu et al.,2023; Kang et al.,2023) .2.2.2.Vulnerability Detection.Vulnerability detection is a critical component of SQA, focusing on identifying whether there are security weaknesses that could be exploited by attackers. This is crucial for preventing potential security breaches and ensuring that software systems remain secure(Li,2022) . Vulnerability detection is a highly time-consuming task that requires the reviewers to be knowledgeable about potential security issues(McGraw,2008) . Due to this, many researchers proposed methods to detect the vulnerabilities automatically. There are static analysis-based approaches(Kaur and Nayyar,2020; Shahriar and Zulkernine,2012) , dynamic analysis-based approaches(Aggarwal and Jalote,2006; Amin et al.,2019) , and so on. The output of vulnerability detection is a binary, vulnerable vs non-vulnerable(Zheng et al.,2020; Grieco et al.,2016) .2.3.LLM and SQA Tasks2.3.1.LLM and Fault LocalizationThere are several recent studies that utilize LLMs for fault localization. For example, a study by Wu et al.(Wu et al.,2023) provides an evaluation of fault localization using GPT-3.5 and GPT-4. Kang et al(Kang et al.,2023) proposed AutoFL that utilizes ChatGPT to provide fault localization results with test coverage as additional input. Widyasari et al.(Widyasari et al.,2024) proposed FuseFL that utilizes several combinations of information related to the faults (i.e., spectrum-based fault localization results, test case assertion and error, and the code description) to enhance the ChatGPT results. There is also a study by Jiang et al.(Jiang et al.,2024) that runs the preliminary evaluation of fault localization and automated program repair using prompts with additional information on the error (i.e., stack trace) and assertion from the test in GPT-3.5 and comparing it with two other closed-source models which are ERNIE, Bot 3.5, and IFlytek Spark 2.0. Furthermore, a recent study by Yang et al.(Yang et al.,2024) built LLMAO an LLM-based fault localization approach to localize the faulty lines without the need to input any test coverage information.2.3.2.LLM and Vulnerability DetectionStudy by Cheskov et al.(Cheshkov et al.,2023) utilize GPT-3.5 for the vulnerability detection and classification of five CWE-IDs. A previous study by Fu et al.(Fu et al.,2023) utilizes GPT-3.5 and GPT-4 to detect, classify, and revise the vulnerability in the code. Tamberh and Bahsi(Tamberg and Bahsi,2024) analyse the vulnerability detection with new prompting techniques such as tree of thoughts (ToT) and self-consistency using closed-source LLM: GPT-4 and Claude 3 Opus. A study by Zhang et al.(Zhang et al.,2024a) also explores the use of ChatGPT in vulnerability detection through structural and sequential auxiliary information. There is also a recent work(Sun et al.,2024) that proposed GPTScan, which combines GPT with static analysis for vulnerability detection. Recent work by Zhou et al.(Zhou et al.,2024) integrates knowledge from the CWE system and similar samples to enhance the prompt results using GPT-3.5 and GPT-4.In previous studies using LLMs for SQA tasks, ChatGPT is often used as the engine. However, how do these results compare with other recent open-source LLMs? In this study, we explored various LLMs and proposed a method to combine their results.",
                "abstract": "With the advancement of Large Language Models (LLMs), their application in Software Quality Assurance (SQA) has increased. However, the current focus of these applications is predominantly on ChatGPT. There remains a gap in understanding the performance of various LLMs in this critical domain. This paper aims to address this gap by conducting a comprehensive investigation into the capabilities of several LLMs across two SQA tasks: fault localization and vulnerability detection. We conducted comparative studies using GPT-3.5, GPT-4o, and four other publicly available LLMs (LLaMA-3-70B, LLaMA-3-8B, Gemma-7B, and Mixtral-8x7B), to evaluate their effectiveness in these tasks.Our findings reveal that several LLMs can outperform GPT-3.5 in both tasks. Additionally, even the lower-performing LLMs provided unique correct predictions, suggesting the potential of combining different LLMs' results to enhance overall performance. By implementing a voting mechanism to combine the LLMs' results, we achieved more than a 10% improvement over the GPT-3.5 in both tasks. Furthermore, we introduced a cross-validation approach to refine the LLM answer by validating one LLM answer against another using a validation prompt. This approach led to performance improvements of 16% in fault localization and 12% in vulnerability detection compared to the GPT-3.5, with a 4% improvement compared to the best-performed LLMs. Our analysis also indicates that the inclusion of explanations in the LLMs' results affects the effectiveness of the cross-validation technique."
            },
            {
                "name": "A Qualitative Study on Using ChatGPT for Software Security: Perception vs. Practicality",
                "arxiv_id": "2408.00435",
                "subtitles": [
                    "AI-Based Chatbots for Software Security",
                    "Vulnerability Detection Using Transformer-based Language Models"
                ],
                "reference": [
                    "Considerations for evaluating large language models for cybersecurity tasks",
                    "Transformer-based vulnerability detection in code at edittime: Zero-shot, few-shot, or fine-tuning",
                    "When chatgpt meets smart contract vulnerability detection: How far are we",
                    "Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities",
                    "Apibot: question answering bot for api documentation",
                    "New tricks to old codes: can ai chatbots replace static code analysis tools",
                    "Data quality for software vulnerability datasets",
                    "An empirical comparison of transformer-based models in vulnerability prediction",
                    "Transformer-based language models for software vulnerability detection",
                    "Conversational devbots for secure programming: An empirical study on skf chatbot",
                    "A new approach to web application security: Utilizing gpt language models for source code inspection",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Can large language models identify and reason about security vulnerabilities? not yet",
                    "Llbezpeky: Leveraging large language models for vulnerability detection",
                    "Breaking the silence: the threats of using llms in software engineering",
                    "Investigating user perceptions of conversational agents for software-related exploratory web search",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Prompt-enhanced software vulnerability detection using chatgpt"
                ],
                "related_work": "IIRelated WorkII-AAI-Based Chatbots for Software Security.Chatbots have been demonstrated to be suitable for software engineering tasks such as information retrieval[5]and API usage[16]. However, the use of chatbots for software security is limited. To date, the SKF-Chatbot (Secure Knowledge Framework) is one of the leading software security chatbots, which was developed by the OWASP Foundation222https://owasp.org/to provide access to Software Vulnerability (SV) information via a chat interface. However, its capabilities are limited due to its lack of contextual understanding[8]. Recently, AI-based chatbots have been able to overcome these limitations via the LLMs that power them. ChatGPT has thus far demonstrated remarkable capabilities for semantic comprehension and providing tailored solutions for given tasks[10].II-BVulnerability Detection Using Transformer-based Language Models.Prior studies have explored the use of transformer-based language models to identify software vulnerabilities using a variety of fine-tuning methodologies. For example, Thapa et al.[12]evaluated transformer-based language models (i.e., BERTBase, GPT-2 Base) against recurrent neural network (RNN) -based models (i.e., BiLSTM, BiGRU) for vulnerability detection in software datasets featuring C/C++ source code. According to their findings, transformer-based language models outperformed RNN-based models on all evaluation metrics. In another study, Kalouptsoglou et al.[17]fine-tuned various transformer-based models (i.e., BERT variants, GPT-2, BART) and carried out a comparative analysis to determine which of these models is the most appropriate for vulnerability detection. Furthermore, Chan et al.[18]created a vulnerability detection model targeted at detecting vulnerabilities in incomplete code snippets. They leveraged common learning approaches such as fine-tuning on three pre-trained LLMs, namely CodeBERT, code-davinci-002, and text-davinci-003.II-CVulnerability Detection Using ChatGPT.Several research efforts have explored the efficacy of ChatGPT in vulnerability detection. Chen et al.[19]exclusively probed ChatGPT's capability in identifying smart contract vulnerabilities. Szab\u00f3 et al.[20]focused on identifying vulnerabilities associated with CWE-653 (Improper Isolation or Compartmentalization) using multiple GPT models. Ozturk et al.[21]assessed ChatGPT's effectiveness in detecting the top 10 OWASP vulnerability categories in web applications. Zhang et al.,[22]through comprehensive testing on two Java and C/C++ vulnerability datasets, demonstrated that prompt-enhanced approaches could strengthen ChatGPT's vulnerability detection capability. In a recent study, Fu et al.[15]compared the capabilities of ChatGPT (i.e., GPT-3.5 and GPT-4) against three other LLMs (e.g., CodeBERT[23]) across various vulnerability tasks. The LLMs used in this study were fine-tuned explicitly for code-related tasks.There are two main ways in which our study differs from prior studies:1.Prior research has primarily focused on assessing[21,12,17,15]and enhancing ChatGPT's vulnerability detection performance through prompt engineering methodologies[24,25,22]or the integration of supplementary elements[26]within the LLM pipeline. We instead strive to focus more on insights through qualitative analysis of existing real-world uses and outputs. This shift aims to illuminate the disparity between practitioners' perceived value and the practicality of this technology in real-world settings. We provide further details in SectionIV.2.Prior studies that utilised datasets from existing literature[17,22,19,15]or open-source projects[12,18,20]ignored the possibility of data leakage (i.e., overlap between training and test data) . This factor could artificially inflate the reported performance of AI models' (e.g., ChatGPT) [27,28,29]. Consequently, we curated a high-quality vulnerability dataset tailored to our evaluation criteria from unseen data. SectionIII-B2discusses this in more detail.",
                "abstract": "Artificial Intelligence (AI) advancements have enabled the development of Large Language Models (LLMs) that can perform a variety of tasks with remarkable semantic understanding and accuracy. ChatGPT is one such LLM that has gained significant attention due to its impressive capabilities for assisting in various knowledge-intensive tasks. Due to the knowledge-intensive nature of engineering secure software, ChatGPT's assistance is expected to be explored for security-related tasks during the development/evolution of software. To gain an understanding of the potential of ChatGPT as an emerging technology for supporting software security, we adopted a two-fold approach. Initially, we performed an empirical study to analyse the perceptions of those who had explored the use of ChatGPT for security tasks and shared their views on Twitter. It was determined that security practitioners view ChatGPT as beneficial for various software security tasks, including vulnerability detection, information retrieval, and penetration testing. Secondly, we designed an experiment aimed at investigating the practicality of this technology when deployed as an oracle in real-world settings. In particular, we focused on vulnerability detection and qualitatively examined ChatGPT outputs for given prompts within this prominent software security task. Based on our analysis, responses from ChatGPT in this task are largely filled with generic security information and may not be appropriate for industry use. To prevent data leakage, we performed this analysis on a vulnerability dataset compiled after the OpenAI data cut-off date from real-world projects covering 40 distinct vulnerability types and 12 programming languages. We assert that the findings from this study would contribute to future research aimed at developing and evaluating LLMs dedicated to software security."
            },
            {
                "name": "Large Language Model-based Role-Playing for Personalized Medical Jargon Extraction",
                "arxiv_id": "2408.05555",
                "subtitles": [
                    "Large Language Models (LLM) and ChatGPT",
                    "Role Playing and Personalizing in LLMs",
                    "Medical Term Extraction"
                ],
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Role play with large language models",
                    "Wikipedia's moment of truth, Jul",
                    "Can ai serve as a substitute for human subjects in software engineering research",
                    "Emergent abilities of large language models",
                    "Palr: Personalization aware llms for recommendation",
                    "Personalized text generation with fine-grained linguistic control",
                    "Palm: Scaling language modeling with pathways",
                    "Lamp: When large language models meet personalization",
                    "Camel: Communicative agents for \" mind \" exploration of large scale language model society",
                    "Proceedings of the 1st workshop on personalization of generative ai systems (personalize",
                    "How to use language models for synthetic text generation in cerebrovascular disease-specific medical reports",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Llm-rec: Personalized recommendation via prompting large language models",
                    "Sparks of artificial general intelligence: Early experiments with gpt",
                    "Readme: Bridging medical jargon and lay understanding for patient education through data-centric nlp",
                    "A hybrid approach to extract keyphrases from medical documents",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "How does gpt obtain its ability? tracing emergent abilities of language models to their sources",
                    "Mt-bioner: Multi-task learning for biomedical named entity recognition using deep bidirectional transformers",
                    "Improving language understanding by generative pre-training",
                    "Gpt-4 technical report",
                    "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing",
                    "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
                    "A comparison between named entity recognition models in the biomedical domain",
                    "Readctrl: Personalizing text generation with readability-controlled instruction learning",
                    "Multi-party chat: Conversational agents in group settings with humans and models",
                    "Chatharuhi: Reviving anime character in reality via large language model",
                    "From persona to personalization: A survey on role-playing language agents",
                    "Language models are few-shot learners",
                    "Attention is all you need",
                    "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                    "Autoagents: A framework for automatic agent generation",
                    "Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration",
                    "Quickumls: a fast, unsupervised approach for medical concept extraction",
                    "Language models are unsupervised multitask learners",
                    "Bio-ner: biomedical named entity recognition using rule-based and statistical learners",
                    "Medjex: A medical jargon extraction model with wiki's hyperlink span and contextualized masked language model score",
                    "Namedkeys: Unsupervised keyphrase extraction for biomedical documents",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "3Related WorkLarge Language Models (LLM) and ChatGPTThe objective of the task of language modeling is to predict a series of words based on the corpora which a model is trained with. Transformer language models(Vaswani et al.,2017) trained on large corpora of unlabeled texts have shown to be effective on general-purpose semantic features. Specifically, pre-trained language models (PLMs) based on Transformer could easily adapt to downstream tasks after being fine-tuned on a small amount of labeled data.Scaling up PLMs in a number of parameters and data size often leads to a better performance on downstream tasks(Radford et al.,2019; Brown et al.,2020; Chowdhery et al.,2022) . A continued trend of scaling up PLMs has resulted in the emergence of Large Language Models (LLMs) , which refer to language models with billions of parameters trained on massive amounts of data. These LLMs have shown to have  \"emergent behaviors \"(Wei et al.,2022a) which are behaviors not present in smaller models. Some examples of these  \"emergent behaviors \" are in-context learning, instruction-following, and step-by-step reasoning.In-context learning ability, which is formally introduced by GPT-3(Brown et al.,2020) , enables a language model to perform a task without additional training through a given instruction and/or several examples as a prompt. Instruction following refers to a behavior where large language models improve on tasks based on instructions after being fine-tuned with a dataset consisting of (INSTRUCTION, OUTPUT) pairs(Wei et al.,2021; Ouyang et al.,2022; Sanh et al.,2021) . Step-by-step reasoning is a process where LLMs solve complex tasks through a chain-of-thought prompting strategy, which uses a prompting mechanism with a series of intermediate reasoning steps to generate the final output(Wei et al.,2022b) .Generative Pre-trained Transformer (GPT) is a type of large language model that uses decoder-only Transformer models to predict the next word or token. The GPT series began in 2018 with GPT-1(Radford et al.,2018) which has 0.18 billion parameters. The series has progressively scaled up, resulting in increasingly powerful models. GPT-2 had 1.5 billion parameters and was trained with a goal of performing tasks via unsupervised language modeling without further fine-tuning. GPT-3 once again gave a big performance gap with 175 billion parameters and showing emergent behaviors such as in-context learning. The capacity of GPT-3 could be further enhanced through training on code data(Fu et al.,2022) and applying reinforcement learning from human feedback(Ouyang et al.,2022) , resulting in the creation of GPT-3.5. Most recently, GPT-4 was introduced with 1.7 trillion parameters with better performance in many evaluation tasks(OpenAI,2023; Bubeck et al.,2023) . GPT-4 was trained with a six-month iterative alignment with safety reward and a number of intervention strategies have been applied to mitigate potential harms such as generating hallucinations or inappropriate responses.ChatGPT is a conversational model made based on GPT models released on November 2022 by OpenAI(Gertner,2023) . ChatGPT provides Application programming interfaces (APIs) which allow us to have relatively unconstrained access to fine-tune and infer from powerful LLMs, giving us huge possibilities of its usage.Role Playing and Personalizing in LLMsRole-playing in LLMs refers to when LLMs are prompted to play a specific role. Prior work(Shanahan et al.,2023) foregrounds the concept of role-playing as the models that appear to act like a human being, in fact, simply generating the next token or word based on previous texts. It deals with cases such as deception and self-awareness and suggests that a dialogue agent can be seen as a superposition of simulacra that varies based on previous texts.(Li et al.,2023a) exploits the in-context learning abilities of LLMs to mimic the conversational styles of fictional characters by providing example passages of each character. Recent research has shown LLMs with various assigned roles to cooperate in a multi-agent setting to perform tasks(Chen et al.,2024,2023; Li et al.,2023b; Wei et al.,2023; Wang et al.,2023) .(Gerosa et al.,2024) discusses if AI and LLMs can replace human responses and behaviors in research settings in persona-based prompting for interviews, multi-persona dialogue for focus groups, and mega-persona responses for surveys.Personalization in LLMs focuses on tailoring model outputs to individual users, considering both benefits and risks at the individual and societal levels(Kirk et al.,2023) . An application of personalization in LLMs is item recommendations.(Lyu et al.,2023) uses diverse prompts and input augmentation for LLMs to improve on generating recommendations for items. It leverages user engagement data to identify important neighbor items.(Chen,2023) fine-tunes an LLM to retrieve and recommend items based on user history and a list of candidate items.(Salemi et al.,2023) proposes a retrieval augmentation solution where personalized items that will be included into the instructions are retrieved from user profile for each input. Another key application of personalization in LLMs is content generation.(Deshpande et al.,2024) explores various approaches in this area.(Alhafni et al.,2024) presents a novel benchmark for training generative models to personalize text by controlling fine-grained linguistic attributes, systematically evaluating the performance of various large language models and providing insights into factors affecting their performance.(Oh et al.,2024) examines the use of language models for personalized synthetic text generation in the biomedical domain, particularly focusing on medical records and evaluating the impact of the LM's knowledge and size alongside objective and clinical assessments. Additionally,(Tran et al.,2024) introduces the ReadCtrl method, which instruction-tunes LLMs to tailor users' readability levels for personalized content generation.Medical Term ExtractionThe goal of medical term extraction is to help improve patients' comprehension of their EHR notes by identifying the terms that may be unfamiliar to patients. The task is different from BioNER, which aims to recognize and classify biomedical entities from text without considering patients' comprehension(Soomro et al.,2017; Cariello et al.,2021; Khan et al.,2020) .Another related task is key phrase extraction, which identifies important phrases or clauses that represent topics(Sarkar,2013; Gero and Ho,2019) . SciSpacy(Neumann et al.,2019) is an NLP library made to satisfy the primary text processing needs in the biomedical domain. It is built on the SpaCy(Honnibal and Montani,2017) library with models retrained for POS tagging, dependency parsing, and named entity recognition. MedJEx(Kwon et al.,2022) and README(Yao et al.,2023) were specifically designed to identify medical jargon terms in electronic health record notes and provide their corresponding lay definitions. It is trained on hyperlink spans of Wikipedia articles and fine-tuned on the annotated MedJ dataset, which consists of 18,178 sentences annotated by domain experts. It further elicits UMLS concepts using QuickUMLS(Soldaini and Goharian,2016) , which is used to extract binary features with their weighting scores calculated via term-frequency scores and masked language model scores.",
                "abstract": "Previous studies reveal that Electronic Health Records (EHR), which have been widely adopted in the U.S. to allow patients to access their personal medical information, do not have high readability to patients due to the prevalence of medical jargon. Tailoring medical notes to individual comprehension by identifying jargon that is difficult for each person will enhance the utility of generative models. We present the first quantitative analysis to measure the impact of role-playing in LLM in medical term extraction. By comparing the results of Mechanical Turk workers over 20 sentences, our study demonstrates that LLM role-playing improves F1 scores in 95% of cases across 14 different socio-demographic backgrounds. Furthermore, applying role-playing with in-context learning outperformed the previous state-of-the-art models. Our research showed that ChatGPT can improve traditional medical term extraction systems by utilizing role-play to deliver personalized patient education, a potential that previous models had not achieved."
            },
            {
                "name": "Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations",
                "arxiv_id": "2408.05128",
                "subtitles": [
                    "Dependency Management Challenges",
                    "LLMs in Software Development Workflows"
                ],
                "reference": [
                    "An empirical comparison of dependency network evolution in seven software packaging ecosystems",
                    "How reuse influences productivity in object-oriented systems",
                    "On the impact of using trivial packages: An empirical case study on npm and pypi",
                    "Effective dependency management for the javascript software ecosystem",
                    "An empirical study on selection of open source software-preliminary results",
                    "An empirical study of usages, updates and risks of third-party libraries in java projects",
                    "What predicts software developers' productivity",
                    "Can chatgpt support developers? an empirical evaluation of large language models for code generation",
                    "Welcome to the era of chatgpt et al. the prospects of large language models",
                    "Can you trust ChatGPT's package recommendations",
                    "Exploring risks in the usage of third-party libraries",
                    "Can chatgpt replace stackoverflow? a study on robustness and reliability of large language model code generation",
                    "Helping or not helping? why and how trivial packages impact the npm ecosystem",
                    "A context-oriented programming approach to dependency hell",
                    "The programmer's assistant: Conversational interaction with a large language model for software development",
                    "Not All Dependencies are Equal: An Empirical Study on Production Dependencies in NPM",
                    "Evaluation of the programming skills of large language models",
                    "Escaping dependency hell: finding build dependency errors with the unified dependency graph",
                    "Categorizing developer information needs in software ecosystems",
                    "Dependency update strategies and package characteristics",
                    "What are the characteristics of highly-selected packages? a case study on the npm ecosystem"
                ],
                "related_work": "5.Related WorksIn this section, we discuss the related literature divided into two aspects. First, we discuss the works that have focused on dependency management and related challenges. Second, we discuss the works that report on the use of large language models as programming assistants.5.1.Dependency Management ChallengesWhile open source software libraries significantly reduce development time and costs(basili1996reuse,;decan2019empirical,;murphy2019predicts,) , depending on numerous libraries introduces complexity and potential dependency management challenges(Latendresse_ASE2022,;mujahid2021effective,;wang2020empirical,) . One such challenge is highlighted by Mujahid et al.(mujahid2023characteristics,) who identify library selection as a critical aspect of dependency management. In their work, they surveyed developers from the npm ecosystem to qualitatively understand the characteristics of highly-selected libraries. Their results show that JavaScript developers believe that such libraries are well-documented, popular, and free of vulnerabilities. Building upon this work, our study leverages those same characteristics to categorize libraries used by ChatGPT.Hauge et al.(hauge2009empirical,) show that organizational library selection is an ad-hoc process that often relies on a combination of past experiences, expert advice, and online resources. This is further discussed in the work of Haenni et al.(haenni2013categorizing,) where the authors surveyed developers about their decision-making when selecting a library to integrate into their application. Their findings show that, in general, developers do not apply rationale when selecting libraries. Alternatively, developers opted for libraries that fulfilled the immediate task requirements. Given this lack of formal selection processes and the increasing popularity of LLMs as programming assistants, this motivated us to investigate the role of LLMs as software librarians.Dependency hell is a concept discussed in several studies(fan2020escaping,;tanabe2018context,;abdalkareem2020impact,) and refers to when a project has an excessive number of dependencies, and managing these dependencies becomes difficult and error-prone. Chen et al.(chen2021helping,) discuss the impact of  \"trivial packages, \" referring to libraries implementing simple functionalities, on the npm ecosystem. Their survey highlights that developers struggle with the multiple dependencies introduced by these libraries, contributing to dependency hell. For instance, a developer reported the cascading effect of patching a deeply nested dependency, requiring updates throughout the dependency tree. Jafari et al.(jafari_update,) investigate the relationship between npm library characteristics and the dependency update strategy opted by its dependents. The authors report that the release status, the number of dependents, and the age of a library are the most important indicators of the dependency update strategy. Raemaekers et al.(raemaekers2011exploring,) discuss the risks associated with the usage of third-party libraries. They identify key library attributes that could serve as risk indicators. Notably, they report that more popular libraries may be updated more frequently, which increases the chance for new bugs to get introduced into the codebase. These findings support our claim that LLMs can be improved software librarians by providing critical information about recommended libraries, such as maintenance metrics (e.g., number of dependencies, version update frequency, age) . Such transparency can help developers anticipate and manage potential maintenance challenges associated with the increased complexity of dependencies.5.2.LLMs in Software Development WorkflowsLLMs are increasingly used by software engineering practitioners to perform various tasks, such as code generation, and have shown the potential to improve developer productivity(ross2023programmer,;heitz2024evaluation,;teubner2023welcome,) . However, some studies have raised concerns about the reliability of LLM-generated code. For instance, Zhong et al.(zhong2024chatgpt,) report common API misuse patterns found in popular LLMs. The study reveals that in the case of GPT-4, 62% of the generated code contained API misuses. The authors argue that this is particularly problematic given that users of LLM code generation are generally not familiar with the APIs that LLMs generate code for, and cannot tell whether the provided code is correct or not. Similarly, a Vulcan Cyber article claims that ChatGPT hallucinated in almost 40% of the programming questions it was asked(lanyado-2023,) . Our findings diverge from those reported in the aforementioned studies. This discrepancy might be attributed to differences in experimental design. Zhong et al. employed  \"one-shot \" or  \"few-shot \" approaches, providing either irrelevant or relevant examples alongside the prompt. The Vulcan Cyber article describes a scenario where ChatGPT was instructed to generate code for a specific library and then repeatedly prompted for alternatives, potentially leading to hallucinated libraries. In contrast, our study aimed to simulate a more realistic developer-LLM interaction by prompting ChatGPT only once. This approach aligns with findings from Jin et al.(jin2024can,) whose empirical study reveals that developers only request code regeneration from ChatGPT in 3% of conversations.While LLMs are making significant advancements in software development, concerns regarding reliability and the potential for hallucinations remain. However, studies also highlight the potential of LLMs to improve developer productivity through functionalities like code completion and search. Ross et al.(ross2023programmer,) developed an LLM-based programmer's assistant and evaluated their system on 42 participants. Their results reveal that participants were in majority positive in the assistant's potential for improving their productivity. Heitz et al.(heitz2024evaluation,) evaluate and compare the performance of OpenAI's ChatGPT and Google's Gemini in programming code. They find that while the premium version of the models offers enhanced performance, their free counterparts remain highly relevant for a wide range of users in the context of software development. Also, the authors report that these models can significantly accelerate coding tasks and improve productivity, but necessitates rigorous, especially if the generated code is used in critical areas. Despite this, there is a gap in understanding how LLMs perform as software librarians, a critical role in the development process that impacts project maintainability, security, and compatibility.Our study directly addresses this gap by investigating the effectiveness of LLMs in recommending libraries. We analyze libraries suggested by ChatGPT in response to real-world developer prompts from Stack Overflow questions. This allows us to assess LLM performance in a context that mimics actual developer usage and avoids potential biases introduced by experimental setups. By evaluating factors like library popularity and maintenance, licensing, and potential dependency challenges, our work aims to inform the development of more robust LLM software librarians, and provide developers with recommendations and considerations when using such tools to streamline their workflows.",
                "abstract": "Software libraries play a critical role in the functionality, efficiency, and maintainability of software systems. As developers increasingly rely on Large Language Models (LLMs) to streamline their coding processes, the effectiveness of these models in recommending appropriate libraries becomes crucial yet remains largely unexplored. In this paper, we assess the effectiveness of ChatGPT as a software librarian and identify areas for improvement. We conducted an empirical study using GPT-3.5 Turbo to generate Python code for 10,000 Stack Overflow questions. Our findings show that ChatGPT uses third-party libraries nearly 10% more often than human developers, favoring widely adopted and well-established options. However, 14.2% of the recommended libraries had restrictive copyleft licenses, which were not explicitly communicated by ChatGPT. Additionally, 6.5% of the libraries did not work out of the box, leading to potential developer confusion and wasted time. While ChatGPT can be an effective software librarian, it should be improved by providing more explicit information on maintainability metrics and licensing. We recommend that developers implement rigorous dependency management practices and double-check library licenses before integrating LLM-generated code into their projects."
            },
            {
                "name": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions",
                "arxiv_id": "2407.12423",
                "subtitles": [
                    "LLMs in Education and Visualization Education",
                    "Visualization for AI-Enhanced Education",
                    "Visual Analytics for Conversational Data"
                ],
                "reference": [
                    "An integrated framework for course adapted student learning analytics dashboard",
                    "Intergroup dialogues: An educational model for cultivating engagement across differences",
                    "Exploring the ethical considerations of using chat gpt in university education",
                    "Threadreconstructor: Modeling reply-chains to untangle conversational text through visual analytics",
                    "Visual analysis of mooc forums with iforum",
                    "Visual sentiment analysis on twitter data streams",
                    "Knowledge compass: A question answering system guiding students with follow-up question recommendations",
                    "Visualising conversation structure across time: Insights into effective doctor-patient consultations",
                    "Chatgpt: The cognitive effects on learning and memory",
                    "Open learner models",
                    "Ineqdetect: A visual analytics system to detect conversational inequality and support reflection during active learning",
                    "Educational dialogues: Understanding and promoting productive interaction",
                    "Unlocking the opportunities through chatgpt tool towards ameliorating the education system",
                    "Loop: A learning analytics tool to provide teachers with useful data visualisations",
                    "Chatgpt as an educational tool: Opportunities, challenges, and recommendations for communication, business writing, and composition courses",
                    "Development of the learning analytics dashboard to support students' learning performance",
                    "Open ai in education, the responsible and ethical use of chatgpt towards lifelong learning",
                    "Shaping the future of education: Exploring the potential and consequences of ai and chatgpt in educational settings",
                    "T-cal: Understanding team conversational data with calendar-based visualization",
                    "Ruffle&riley: Towards the automated induction of conversational tutoring systems",
                    "Visohc: Designing visual analytics for online health communities",
                    "Attention please! learning analytics for visualization and recommendation",
                    "Exploring the opportunities and challenges of nlp models in higher education: is chat gpt a blessing or a curse",
                    "How chat gpt can transform autodidactic experiences and open education",
                    "Conceptual recurrence plots: Revealing patterns in human discourse",
                    "Education in the era of generative artificial intelligence (ai) : Understanding the potential benefits of chatgpt in promoting teaching and learning",
                    "Challenges and opportunities in data visualization education: A call to action",
                    "Uncertainty representation in visualizations of learning analytics for learners: Current approaches and opportunities",
                    "Involving teachers in the data-driven improvement of intelligent tutors: A prototyping study",
                    "Transformation or evolution?: Education 4.0, teaching and learning in the digital age",
                    "Adaptive assessment of visualization literacy",
                    "Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant",
                    "Gpt-3-driven pedagogical agents to train children's curious question-asking skills",
                    "Learning vis tools: Teaching data visualization tutorials",
                    "Conviscope: Visual analytics for exploring patient conversations",
                    "Multiconvis: A visual text analytics system for exploring a collection of online conversations",
                    "Visualizing data to support judgement, inference, and decision making in learning analytics: Insights from cognitive psychology and visualization science"
                ],
                "related_work": "1Related WorkIn this section, we discuss the relevant research, including LLMs in education and visualization education, visualization for AI-enhanced education, and visual analytics for conversational data.1.1LLMs in Education and Visualization EducationThe integration of LLMs such as ChatGPT into educational settings has sparked a diverse range of discussions, with plenty of initial works centered on ethical considerations regarding their use in learning environments[54,37]. Increasingly, the academic community recognizes the transformative potential LLMs hold for education, advocating for their adoption to revolutionize learning and teaching methodologies[12]. Despite potential resistance from some educators, students inevitably turn to LLMs for assistance with coursework[34]. Therefore, researchers and instructors have explored LLMs for various applications, including serving as teaching assistants for writing and coding, generating adaptive exercises[65], supporting personalized question-answering sessions[10], and facilitating innovative learning modes like  \"learn by teaching \", where LLM plays the role as  \"learner\" and students assume the role of the teacher to teach the AI[61].However, there is a notable gap in understanding how students strategize their use of ChatGPT for educational purposes. Existing literature predominantly focuses on the capabilities and applications of LLMs without delving into student interaction strategies, leaving educators without the necessary insights to fully leverage these tools in enhancing learning experiences[29,1].Simultaneously, the field of visualization education is gaining traction, not only within the visualization community but also more broadly[8]. The challenges of teaching data visualization range from addressing diverse student backgrounds to managing varied learning activities such as concept comprehension, visualization literacy, and design evaluation are substantial[51]. ChatGPT's potential to support these educational challenges opens avenues to investigate how students use ChatGPT across different visualization learning tasks, particularly relevant to our project's focus[3]. A comprehensive analysis of this research direction still remains unexplored[20]. This gap presents a unique opportunity for our work to contribute to the field by offering insights into student strategies in employing ChatGPT within visualization learning contexts, thereby advancing the understanding and application of LLMs in educational settings.1.2Visualization for AI-Enhanced EducationThe integration of visualization in AI-enhanced education is dedicated to leveraging visual analytics for learning analysis, such as interpreting complex data and AI algorithms to improve educational outcomes[23]. While learning analysis harnesses data to refine and enhance learning processes, visualization techniques render these insights accessible and actionable for educators and students[18,57]. Despite notable advancements in each domain, the integration of visual analytics specifically tailored to learning analysis within AI-enhanced educational environments remains underexplored.Existing research underscores the value of visual analytics in presenting student performance metrics, engagement levels, and learning behaviors, thus enriching our understanding of educational dynamics[5,4]. Within this context, the subfield of open learner models exemplifies the potential of visual explanations, akin to Explainable AI (XAI) , in demystifying AI-generated outputs, offering learners and educators transparent and trustworthy insights[15,25]. Additionally, other works have employed visual analytics to examine students' interaction data with intelligent agents, using students' log data such as hint requests to delve into their problem-solving processes[74]. Recently, with the advent of potent LLM-based conversational agents, the intricacy of interactions between students and AI has reached a new height. Goals once considered unrealistic, such as in-depth analysis of students' cognitive levels and thought processes, are now achievable[39,71,9]. To our knowledge, the dedicated exploration of visual analytics to analyze and elucidate student interactions with advanced AI tools, such as ChatGPT, is just beginning. In response to this gap, our work proposes a novel visual analytics system based on the students-ChatGPT conversation data we collected. The system is designed to identify and unravel the intricate nuances of student-ChatGPT interactions. It equips educators with profound insights into how LLMs can be utilized to customize and elevate students' learning experiences.1.3Visual Analytics for Conversational DataThe exploration of visual analytics for conversational text data within the visualization community has encompassed a wide range of applications, from sentiment analysis and topic modeling to mapping conversation flows and interactions within user groups[35,45,31]. For instance, T-Cal and IneqDetect[30,53]focus on analyzing group conversations and estimating members' sentiments to assess collaboration effectiveness. Meanwhile, efforts such as ThreadReconstructor, MultiConVis, and VisOHC[24,36,6]probe into the structure and core topics of online forum discussions. However, these initiatives mainly focus on summarizing the dynamics of multi-party conversations without delving into the intricacies of one-on-one dialogues.Another significant gap in current methodologies is their constrained ability to uncover the depth of evolving cognitive levels in educational dialogues between students and AI tools like ChatGPT. Visualization tools for one-on-one medical conversations, like ConVIScope and Discursis[48,7], focus on charting patient-doctor dialogues but only reflect changes in sentiment and topic over time. They fall short in showcasing how one party (e.g., students) adjusts their responses to another party (e.g., LLMs' replies) . Similarly, research aimed at analyzing educational dialogues often emphasizes engagement and comprehension, lacking a detailed visual analysis of the depth of thinking, learning strategies, or intentions revealed through these interactions[49,75,8].These shortcomings highlight the necessity for a novel visual analytics framework designed to tackle the specific challenges posed by educational dialogues with LLMs[32,3]. Consequently, our work introduces multiple visualizations, such as the Interaction Tree, to meticulously analyze students' interaction patterns with ChatGPT.",
                "abstract": "The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions."
            },
            {
                "name": "Time to Separate from StackOverflow and Match with ChatGPT for Encryption",
                "arxiv_id": "2406.06164",
                "subtitles": [
                    "StackOverflow code snippets security",
                    "Cryptography challenges and misuses",
                    "Evaluation of AI-generated code"
                ],
                "reference": [
                    "LLM security guard for code",
                    "Stack overflow considered harmful? the impact of copy&paste on android application security",
                    "Crysl: Validating correct usage of cryptographic apis",
                    "Automatic detection of java cryptographic api misuses: Are we there yet",
                    "Cryptoguard: High precision detection of cryptographic vulnerabilities in massive-sized java projects",
                    "Practical evaluation of static analysis tools for cryptography: Benchmarking method and case study",
                    "Crylogger: Detecting crypto misuses dynamically",
                    "Hurdles for developers in cryptography",
                    "Asleep at the keyboard? assessing the security of github copilot's code contributions",
                    "An empirical study of c++ vulnerabilities in crowd-sourced code examples",
                    "On the use of c# unsafe code context: An empirical study of stack overflow",
                    "Naturalistic static program analysis",
                    "Java cryptography uses in the wild",
                    "Developers struggle with authentication in blazor webassembly",
                    "The impact of developer experience in using java cryptography",
                    "Snakes in paradise? insecure python-related coding practices in stack overflow",
                    "Secure coding practices in java: Challenges and vulnerabilities",
                    "Is github's copilot as bad as humans at introducing vulnerabilities in code",
                    "Example-based vulnerability detection and repair in java code",
                    "Evaluation of static vulnerability detection tools with java cryptographic api benchmarks",
                    "Why does cryptographic software fail? a case study and open problems",
                    "Why crypto-detectors fail: A systematic evaluation of cryptographic misuse detection techniques",
                    "Lessons learned in implementing and deploying crypto software",
                    "Do users write more insecure code with ai assistants",
                    "spectra: A precise framework for analyzing cryptographic vulnerabilities in android apps",
                    "Lost at c: A user study on the security implications of large language model code assistants",
                    "Security weaknesses of copilot generated code in github",
                    "A stitch in time: Supporting android developers in writingsecure code",
                    "Worrisome patterns in developers: A survey in cryptography",
                    "Modelling analysis and auto-detection of cryptographic misuse in android applications",
                    "Fluentcrypto: Cryptography in easy mode",
                    "Cryptography vulnerabilities on hackerone"
                ],
                "related_work": "6Related WorkWe discuss related work in three key areas: code snippets security, cryptography challenges and misuses, and finally, evaluation of AI-generated code.6.1StackOverflow code snippets securitySeveral studies have examined the security of code snippets from StackOverflow.Verdi et al.(Verdi et al.,2022) revealed a concerning trend where a substantial number of insecure C++ code snippets had been transferred into GitHub projects. Firouzi et al.(Firouzi et al.,2020) studied C# code snippets that include unmanaged code and found 67 code snippets with dangerous functions that can introduce vulnerability (e.g., buffer overflow) if not used with caution.Andr\u00e9 et al.(Andr\u00e9 et al.,2022) investigated the security issues that developers encounter in WebAssembly, and they found that the topmost issues attributed to authentication in Blazor WebAssembly.Meng et al.(Meng et al.,2018) examined code snippets for Java security. They observed an increasing trend in the adoption of third-party security frameworks like Spring Security for authentication and authorization. Also, they discovered that many accepted answers contained security flaws such as weak hash functions (MD5) , SSL/TLS compromises, and disabled CSRF protection.Rahman et al.(Rahman et al.,2019) analyzed 529,054 Python code blocks from 44,966 answers posted on Stack Overflow. They identified 3,685 code blocks that exhibited at least one of the six critical insecure coding practices, namely code injection, cross-site scripting, use of insecure ciphers, insecure communications, race conditions, and insecure data serialization.Fischer et al.(Fischer et al.,2017) employed machine learning techniques, training a model with 1,360 security-related code snippets sourced from StackOverflow answers. This model was subsequently applied to 3,834 code snippets, revealing that 30.28% of them were identified as insecure.6.2Cryptography challenges and misusesSeveral studies have explored the difficulties faced by developers in dealing with cryptography(Nguyen et al.,2017; Gutmann,2002; Shuai et al.,2014; Rahaman et al.,2019) .Hazhirpasand et al.(Hazhirpasand et al.,2021b) clustered cryptography questions on StackOverflow, inspected a subset of them, and discussed why developers struggle in this domain. They also analyzed JCA misuses in 489 open-source projects on GitHub and found that only 15% of repositories were free from misuse, leaving 85% susceptible to security issues(Hazhirpasand et al.,2020) . The investigation of GitHub projects also revealed that approximately 64% of cryptographic solutions in each project were not secure(Hazhirpasand et al.,2019) .Gajrani et al.(Gajrani et al.,2017) revealed a troubling statistic indicating that a significant 90% of applications available across diverse app stores are susceptible to exploitation due to cryptographic vulnerabilities. Lazar et al.(Lazar et al.,2014) conducted a systematic study of cryptographic vulnerabilities in practice. Hazhispasand et al.(Hazhirpasand and Ghafari,2021a) investigated cryptography vulnerability reports on the HackerOne bug bounty platform.Several studies(Piccolboni et al.,2021; Kr\u00fcger et al.,2017; Zhang et al.,2022) introduced tools for misuse detection. However, the adoption of static analysis tools is low among developers(Hazhirpasand and Ghafari,2021b) . To ease the adoption of static analysis tools for mainstream developers, Pourhashem et al.(Mehdi Pourhashem Kallehbasti and Ghafari,2023) developed NASRA (NAturalistic Static pRogram Analysis) , a framework that enables developers to define static program analyses in natural language, and they showcased how NASRA can be applied to uncover cryptography misuses in Java programs. Nonetheless, existing tools are not able to catch every mistake. Braga et al.(Braga et al.,2017) found that coverage of static tools for finding cryptography missuses is far from good. Zhang et al.(Zhang et al.,2023) conducted a comprehensive review of six crypto misuse detection tools by applying them to 200 Apache projects. They found that there is no single tool being universally superior and that improvement in inter-procedural analysis and context awareness are necessary to effectively find misuses. Ami et al.(Ami et al.,2022) introduced a mutation testing framework to systematically assess the effectiveness of crypto-API misuse detectors.Sharmin Afrose et al.(Afrose et al.,2023) developed detailed benchmarks and executed several vulnerability detection tools for the comparison of their effectiveness and reported their findings. Overlooked issues include difficulties in resolving parameter values, insecure initialization vectors, insecure random number generation, and insufficient key lengths in cryptographic key generation. Finally, Kafader et al.(Kafader and Ghafari,2021) developed FluentCrypto, an API designed to abstract away the low-level complexities inherent in utilizing the Node.js native cryptography API.6.3Evaluation of AI-generated codeThe recent advancements in AI assistant tools have motivated researchers to examine AI-generated code. Fu et al.(Fu et al.,2023) analyzed 435 code snippets generated by Copilot in public GitHub projects and found that 35.8% had Common Weakness Enumeration (CWE) issues across various programming languages. Pearce et al.(Pearce et al.,2022) studied Copilot's performance in suggesting code related to 89 scenarios that were subject to MITRE's  \"Top 25 \" CWEs. They discovered that around 40% of the generated programs include vulnerabilities. Asare et al.(Asare et al.,2023) conducted a study on GitHub's Copilot, examining its performance in various C++ and C scenarios. The findings revealed that while Copilot generated insecure code, it was not as bad as human developers in producing insecure code. Sandoval et al.(Sandoval et al.,2023) found that AI assistance in low-level C programming minimally affected security, introducing critical bugs only slightly more often (up to 10%) than in cases without AI, indicating that the use of LLMs does not introduce new security risks.Perry et al.(Perry et al.,2023) studied how individuals utilize an AI code assistant, constructed using OpenAI's Codex, to address security-related tasks across various programming languages. They found that participants who used the AI were more likely to introduce security vulnerabilities, yet they often perceived their insecure solutions as secure. Interestingly, those who invested more effort in crafting their queries to the AI tended to generate more secure solutions.Kavian et al.(Kavian et al.,2024) developed LLMSecGuard, an open-source framework designed to enhance code security through the integration of LLMs and static security code analyzers. It uses hints from static security analysis tools to guide LLMs in writing secure code.We presented the first in-depth investigation of symmetric encryption challenges and security risks. In contrast to previous studies, we also investigated the support of generative AI (i.e., ChatGPT) to fix these issues.",
                "abstract": "Cryptography is known as a challenging topic for developers. We studied StackOverflow posts to identify the problems that developers encounter when using Java Cryptography Architecture (JCA) for symmetric encryption. We investigated security risks that are disseminated in these posts, and we examined whether ChatGPT helps avoid cryptography issues. We found that developers frequently struggle with key and IV generations, as well as padding. Security is a top concern among developers, but security issues are pervasive in code snippets. ChatGPT can effectively aid developers when they engage with it properly. Nevertheless, it does not substitute human expertise, and developers should remain alert."
            },
            {
                "name": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
                "arxiv_id": "2401.10545",
                "subtitles": [
                    "Fairness in Recommender Systems",
                    "Leveraging Pre-trained LMs and Prompting for Recommender Systems"
                ],
                "reference": [
                    "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys",
                    "Defining and measuring fairness in location recommendations",
                    "Content-Based Multimedia Recommendation Systems: Definition and Application Domains",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "A survey of research on fair recommender systems",
                    "Exploring artist gender bias in music recommendation",
                    "Pareto optimality for fairness-constrained collaborative filtering",
                    "Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems",
                    "A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems",
                    "FairEvalLLM. A Comprehensive Framework for Benchmarking Fairness in Large Language Model Recommender Systems",
                    "The winner takes it all: geographic imbalance and provider (un) fairness in educational recommender systems",
                    "Balanced neighborhoods for multi-sided fairness in recommendation",
                    "OpenP5: Benchmarking Foundation Models for Recommendation",
                    "Estimation of fair ranking metrics with incomplete judgments",
                    "TFROM: A Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers",
                    "Balancing between accuracy and fairness for interactive recommendation with reinforcement learning",
                    "Mitigating sentiment bias for recommender systems",
                    "Spot: Better frozen model adaptation through soft prompt transfer",
                    "A unifying and general account of fairness measurement in recommender systems",
                    "Fair sharing for sharing economy platforms",
                    "Fairness and discrimination in recommendation and retrieval",
                    "Exploiting personalized calibration and metrics for fairness recommendation",
                    "Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms",
                    "Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
                    "Explaining recommender systems fairness and accuracy through the lens of data characteristics",
                    "Towards universal sequence representation learning for recommender systems",
                    "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                    "Does fair ranking improve minority outcomes? understanding the interplay of human and algorithmic biases in online hiring",
                    "Fairness in recommender systems: research landscape and future directions",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "Interplay between upsampling and regularization for provider fairness in recommender systems",
                    "Two-sided fairness in rankings via Lorenz dominance",
                    "Recommendation with Generative Models",
                    "Defining and supporting narrative-driven recommendation",
                    "Fairness-aware news recommendation with decomposed adversarial learning",
                    "Towards long-term fairness in recommendation",
                    "Experiments on generalizability of user-oriented fairness in recommender systems",
                    "Addressing marketing bias in product recommendations",
                    "Fairness among new items in cold start recommender systems",
                    "Language models are few-shot learners",
                    "A flexible framework for evaluating user and item fairness in recommender systems",
                    "Towards understanding and mitigating unintended biases in language model-driven conversational recommendation",
                    "The Unfairness of Active Users and Popularity Bias in Point-of-Interest Recommendation",
                    "A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News",
                    "User-item matching for recommendation fairness",
                    "An enhanced probabilistic fairness-aware group recommendation by incorporating social activeness",
                    "User-oriented fairness in recommendation",
                    "A fairness-aware hybrid recommender system"
                ],
                "related_work": "2.Related work In this section, we briefly review some related work on recommendation systems and LLM techniques. 2.1.Fairness in Recommender Systems Fairness has emerged as a pivotal topic within the AI community in the last few years, arguably becoming the most scrutinized aspect across various branches of trustworthy AI, including fairness, security, privacy, and explainability. In RS, fairness has garnered significant attention due to the multi-stakeholder nature of these systems (Deldjoo et al., 2023; Ekstrand et al., 2019). Unfairness, even in its minimal form, can adversely impact various stakeholders, including consumers, producers, system designers, supply chains, and even the environment - the latter are often referred to as 'side-stakeholder'. The body of literature on fairness in RS is diverse, encompassing multiple perspectives and dimensions. Recent surveys (Ekstrand et al., 2019; Deldjoo et al., 2023; Amig\u00f3 et al., 2023), have introduced various taxonomies to categorize these diverse aspects into several orthogonal or partially-orthogonal dimensions. Key dimensions recognized in the literature include the main stakeholder in question (e.g., consumer vs. producer), the target benefits associated with each (such as effectiveness vs. item exposure), the granularity of sensitive groups for assessing fairness (individual vs. group level), and other dimensions including the core definition of fairness, temporal aspects, and more. These dimensions offer a comprehensive view of the fairness landscape in RS, as detailed in these surveys. In the context of the present study, and with regard to positioning this work within the literature of fair Recommender Systems (fair-RS), we have introduced Table 1. This table is designed to categorize existing literature along two principal dimensions: the stakeholder in question (consumer vs. producer) and the nature of the core RS under scrutiny (traditional RS vs. those based on recent advancements in LLMs). We briefly review these dimensions in the following. Core RS under scrutiny. We distinguish between 'traditional' RS and those enhanced by 'RecLLM'. This distinction is crucial because, while RecLLM, such as those based on GPT-like architectures, promise to significantly advance RS landscape with more nuanced and personalized recommendations, they also raise concerns about inherent biases. The vast and unregulated nature of internet data used for training LLMs raises concerns about inherent biases against specific races, genders, popular brands, and other sensitive attributes. For example, if an LLM is predominantly trained on data from popular e-commerce sites, it might disproportionately recommend products from more recognized brands, overlooking niche or emerging brands. Similarly, biases in language around gender or race could skew recommendations in subtle but impactful ways. Therefore, accurately measuring these biases is a crucial initial step, and forms the key goal of our current work, in developing effective mitigation strategies, ensuring that advancements in RecLLM do not amplify existing inequalities. This is the key goal of the current work in hand. \u2022 Traditional RS. This column lists studies that have focused on traditional methods of recommender systems (Deldjoo et al., 2018). They primarily rely on CF algorithms (possibly using side information of users and items) without the advanced natural language processing capabilities of LLMs. Within these works, some are dedicated to building evaluation frameworks for evaluating RS unfairness, while others focus on developing various mitigation strategies. \u2022 RecLLM: This column spotlights the burgeoning research domain that merges language models (LMs) with RS representing a significant shift towards harnessing advanced NLP techniques to enhance the accuracy and relevance of recommendations. These studies explore the use of various LMs, including BERT-based models (Shen et al., 2023) and recent LLMs such as GPT-like architectures (Li et al., 2023a; Zhang et al., 2023). Additionally, beyond the scale of LMs, the target tasks within RS-such as classical recommendation (top- k  ranking (Li et al., 2023a; Zhang et al., 2023), sequential), conversational RS (Shen et al., 2023), explanation generation, multi-modal recommendations-provide dimensions that could be used for further categorizing these works. Stakeholder. As mentioned earlier, a dominant aspect that can be utilized to classify almost all literature on group fairness is the market side focus whether they concentrate on a single-side market (defined either by consumers or providers) or on both sides (Rahmani et al., 2024; Naghiaei et al., 2022). Within each of these segments, as you can observe, we can further categorize the literature based on which sensitive attribute groups are defined. For example, sensitive attributes such as the demographics of consumers (age, gender) and producers, as well as the popularity of items (on the produce side) are quite common focal points. \u2022 Consumer Fairness. This category is subdivided into various attributes like Activity, Demographics, Merits, and Others. It includes studies that focus on ensuring fairness among consumers of the recommender system based on these attributes. For instance, ensuring that recommendations are not biased towards a particular demographic group. \u2022 Producer Fairness. This focuses on the fairness towards the providers or producers of the content or products recommended by the system. It includes subcategories like Popularity, Demographics, and Price/Brand/Location. These studies might address issues like ensuring lesser-known or niche producers get fair visibility and opportunity in the recommendation process. \u2022 CP Fairness. This category involves studies that consider both consumer and producer fairness simultaneously, addressing the balance between the two. It is noteworthy that while traditional Fair-RS research has been extensively explored, RecLLM is an emerging field, presenting its own unique considerations and challenges. Our work is situated in the 'Producer Fairness' category under 'RecLLM', focusing on producer fairness in the context of ChatGPT. It focuses on producer fairness in the context of ChatGPT, particularly examining how prompt engineering techniques can be leveraged to address or potentially enhance fairness. The study by Zhang et al. (2023) evaluates the consumer fairness side of zero-shot GPT recommendations, focusing on a variety of consumer demographic attributes but not addressing producer-side fairness. Their work introduces a novel benchmark, FaiRLLM, for evaluating the fairness of RecLLM, highlighting ChatGPT's biases towards certain sensitive user attributes in music and movie recommendations. Conversely, the work by Li et al. (2023a) aligns more closely with ours, focusing on producer unfairness however within the specific domain of news recommendation. This study investigates ChatGPT's performance in news recommendation, exploring aspects like personalization, provider fairness, and fake news detection. Similarly, research such as that by Di Palma et al. (2023) examines the personalization and popularity bias in RecLLMs, particularly in the context of ChatGPT and other LLMs. However, these studies do not address the intricacies of prompt engineering in RecLLMs, nor do they comprehensively address the various forms of biases studies in the current study. Our research goes beyond examining provider fairness based on popularity bias and also considers other potential harms, such as the recency of recommended items and the stability of recommendations aspects not explored in the aforementioned studies. 2.2.Leveraging Pre-trained LMs and Prompting for Recommender Systems Recent advancements in recommender systems (RS) have been significantly influenced by the integration of LLMs and innovative prompting strategies. The use of natural language in recommendation tasks has been explored in various ways. For instance, Hou et al. (2022) utilize natural language descriptions and tags as inputs into LLMs to create user representations for more effective recommendations. This contrasts with the narrative-driven recommendations (Bogers and Koolen, 2017) that rely on verbose descriptions of specific contextual needs. In terms of prompting techniques, early methods relied on few-shot prompting, where training examples are used as a guide for LLMs (Brown et al., 2020). With prompt learning, tasks are adapted to LLMs rather than the other way around, utilizing discrete prompts or continuous/soft prompts for task performance. This approach has shown promise across various tasks, including recommendation tasks. Personalizing LLMs for recommendation is crucial for understanding a user's intent and addressing their personalized needs. Recent efforts like P5 (Geng et al., 2022) and OpenP5 (Xu et al., 2023) have integrated several recommendation tasks into one LLM using personalized prompts. This approach reformulates recommendation tasks as sequence-to-sequence generation problems, demonstrating the flexibility of LLMs in handling diverse recommendation scenarios. Lastly, prompt transfer research, such as SPoT (Vu et al., 2021) and ATTEMPT (Asai et al., 2022), focuses on learning from source tasks and utilizing this knowledge for target tasks. This method, including knowledge distillation techniques, shows potential in intra-task prompt distillation and cross-task prompt transfer, contributing to the efficiency and effectiveness of LLM-based recommendation models.",
                "abstract": "This paper explores the biases in ChatGPT-based recommender systems, focusing on provider fairness (item-side fairness). Through extensive experiments and over a thousand API calls, we investigate the impact of prompt design strategies-including structure, system role, and intent-on evaluation metrics such as provider fairness, catalog coverage, temporal stability, and recency. The first experiment examines these strategies in classical top-K recommendations, while the second evaluates sequential in-context learning (ICL).In the first experiment, we assess seven distinct prompt scenarios on top-K recommendation accuracy and fairness. Accuracy-oriented prompts, like Simple and Chain-of-Thought (COT), outperform diversification prompts, which, despite enhancing temporal freshness, reduce accuracy by up to 50%. Embedding fairness into system roles, such as \"act as a fair recommender,\" proved more effective than fairness directives within prompts. Diversification prompts led to recommending newer movies, offering broader genre distribution compared to traditional collaborative filtering (CF) models.The second experiment explores sequential ICL, comparing zero-shot and few-shot ICL. Results indicate that including user demographic information in prompts affects model biases and stereotypes. However, ICL did not consistently improve item fairness and catalog coverage over zero-shot learning. Zero-shot learning achieved higher NDCG and coverage, while ICL-2 showed slight improvements in hit rate (HR) when age-group context was included. Our study provides insights into biases of RecLLMs, particularly in provider fairness and catalog coverage. By examining prompt design, learning strategies, and system roles, we highlight the potential and challenges of integrating LLMs into recommendation systems. Further details can be found atthis https URL."
            }
        ],
        "survey": {
            "name": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
            "arxiv_id": "2405.15164",
            "subtitles": [
                {
                    "name": "Compositionality in Context",
                    "key_history": [
                        {
                            "reference_title": "Compositionality I: Definitions and Variants",
                            "key_word": "Compositionality"
                        },
                        {
                            "reference_title": "The History and Prehistory of Natural-Language Semantics",
                            "key_word": "Formal Languages"
                        },
                        {
                            "reference_title": "The UCLA Lectures",
                            "key_word": "Creativity in Language"
                        },
                        {
                            "reference_title": "Deep learning: A philosophical introduction",
                            "key_word": "Nativism"
                        },
                        {
                            "reference_title": "How Can Deep Neural Networks Inform Theory in Psychological Science",
                            "key_word": "Connectionism"
                        }
                    ],
                    "references_in_this_section": [
                        "The Language of Thought",
                        "A logical calculus of the ideas immanent in nervous activity",
                        "A Synopsis of Linguistic Theory",
                        "Semantic Theory and Tacit Knowledge",
                        "Three-Concept Monte: Explanation, Implementation and Systematicity",
                        "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
                        "Psychological Explanation: An Introduction to the Philosophy of Psychology",
                        "Deep Learning",
                        "Learning representations by back-propagating errors",
                        "Do True Assertions Correspond to Reality",
                        "Why Compositionality Won't Go Away: Reflections on Horwich's Deflationary' Theory",
                        "Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory",
                        "Compositionality I: Definitions and Variants",
                        "A quantitative description of membrane current and its application to conduction and excitation in nerve",
                        "Philosophy of Mind",
                        "The red herring and the pet fish: Why concepts still can't be prototypes",
                        "Meaning and Necessity: A Study in Semantics and Modal Logic",
                        "Psychosemantics: The Problem of Meaning in the Philosophy of MInd",
                        "Aspects of the Theory of Syntax",
                        "Neural and Conceptual Interpretation of PDP Models",
                        "Theories of Meaning and Learnable Languages",
                        "Questions of Form and Interpretation",
                        "What is Compositionality",
                        "Computational Cognitive Neuroscience",
                        "Insight and Illusion: Wittgenstein on Philosophy and the Metaphysics of Experience",
                        "Compositionality: Its Historic Context",
                        "The cognitive revolution: A historical perspective",
                        "Methodological Reflections on Current Linguistic Theory",
                        "The History and Prehistory of Natural-Language Semantics",
                        "The Appeal to Tacit Knowledge in Psychological Explanation",
                        "Hume Variations",
                        "The Thought: A Logical Inquiry",
                        "An interactive activation model of context effects in letter perception: I. An account of basic findings",
                        "Ordinary Language",
                        "From Deep Learning to Rational Machines: What the History of Philosophy Can Teach Us about the Future of Artificial Intelligence",
                        "Cartesian Linguistics",
                        "Rule-Following, Objectivity and the Theory of Meaning",
                        "The Language of Thought in Late Medieval Philosophy: Essays in Honor of Claude Panaccio, volume 5 of Historical-Analytical Studies on Nature, Mind and Action",
                        "On the Control of Automatic Processes: A Parallel Distributed Processing Model of the Stroop Effect",
                        "Psychophysical Supervenience",
                        "On learning the past tenses of english verbs",
                        "Wittgenstein's Metaphilosophy",
                        "The Logical Syntax of Language",
                        "Connectionism, Constituency and the Language of Thought",
                        "An integrative theory of prefrontal cortex function",
                        "What Is a Theory of Meaning? (II",
                        "The Poverty of the Stimulus Argument",
                        "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models",
                        "The Appeal of Parallel Distributed Processing",
                        "Frege: Philosophy of Language, Second Edition",
                        "Compositionality: A Connectionist Variation on a Classical Theme",
                        "The connectionism/classicism battle to win souls",
                        "How to Do Things with Words: Second Edition",
                        "Deep learning: A philosophical introduction",
                        "The Linguistic Turn: Essays in Philosophical Method",
                        "Structured Representations in Connectionist Systems",
                        "The UCLA Lectures",
                        "The Theory of Meaning",
                        "Connectionism: Debates on Psychological Explanation, Volume",
                        "A spreading-activation theory of retrieval in sentence production",
                        "Why Fodor and Pylyshyn Were Wrong: The Simplest Refutation",
                        "The Underlying Reality of Language and Its Philosophical Import",
                        "Rules or connections in past-tense inflections: What does the evidence rule out",
                        "Multiple Realizability and the Rise of Deep Learning",
                        "A Philosophical Introduction to Language Models - Part I: Continuity With Classic Debates",
                        "The Structure of a Semantic Theory",
                        "Rethinking Eliminative Connectionism",
                        "Logics and Languages",
                        "Neural networks and physical systems with emergent collective computational abilities",
                        "Connectionism and cognitive architecture: A critical analysis",
                        "Language and Mind",
                        "Cognition, Systematicity, and Nomic Necessity",
                        "Connectionism, Eliminativism and The Future of Folk Psychology",
                        "Letter to Jourdain",
                        "How Can Deep Neural Networks Inform Theory in Psychological Science",
                        "Discussion: Meaning and Necessity",
                        "ImageNet Classification with Deep Convolutional Neural Networks",
                        "Did Frege Believe Frege's Principle",
                        "Syntactic Structures",
                        "Studies in the Way of Words",
                        "Philosophical Investigations",
                        "Philosophy and Connectionist Theory",
                        "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2: Psychological and Biological Models",
                        "Deep learning",
                        "What company do words keep? Revisiting the distributional semantics of J.R. Firth & Zellig Harris",
                        "Empiricism without magic: Transformational abstraction in deep convolutional neural networks",
                        "Formal Philosophy; Selected Papers of Richard Montague",
                        "Compositionality",
                        "Why Meaning (Probably) Isn't Conceptual Role"
                    ]
                },
                {
                    "name": "Neural Networks in the Age of Deep Learning",
                    "key_history": [
                        {
                            "reference_title": "Language Models are Few-Shot Learners",
                            "key_word": "Neural Networks"
                        },
                        {
                            "reference_title": "Attention is All you Need",
                            "key_word": "Transformer Architecture"
                        },
                        {
                            "reference_title": "In-context Learning and Induction Heads",
                            "key_word": "Induction Heads"
                        },
                        {
                            "reference_title": "The Bitter Lesson",
                            "key_word": "Bitter Lesson"
                        }
                    ],
                    "references_in_this_section": [
                        "Connectionism and cognitive architecture: A critical analysis",
                        "Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI",
                        "The Language of Thought",
                        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                        "Circuit Component Reuse Across Tasks in Transformer Language Models",
                        "In-context Learning and Induction Heads",
                        "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
                        "The Illustrated Transformer",
                        "Scaling Vision Transformers to 22 Billion Parameters",
                        "Language Models are Unsupervised Multitask Learners",
                        "Scaling Laws for Neural Language Models",
                        "Deep Learning",
                        "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
                        "The Bitter Lesson",
                        "A Survey on Transformers in Reinforcement Learning",
                        "Multimodal Learning with Transformers: A Survey",
                        "Language Models are Few-Shot Learners",
                        "The Mythos of Model Interpretability",
                        "Broken Neural Scaling Laws",
                        "Neural Machine Translation of Rare Words with Subword Units",
                        "A mathematical framework for transformer circuits",
                        "A Philosophical Introduction to Language Models - Part I: Continuity With Classic Debates",
                        "Attention in Psychology, Neuroscience, and Machine Learning",
                        "Emergent Abilities of Large Language Models",
                        "Attention is All you Need",
                        "Towards Automated Circuit Discovery for Mechanistic Interpretability",
                        "Emergent linguistic structure in artificial neural networks trained by self-supervision",
                        "Neural Machine Translation by Jointly Learning to Align and Translate"
                    ]
                },
                {
                    "name": "Operationalizing Compositionality",
                    "key_history": [
                        {
                            "reference_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks.",
                            "key_word": "Compositional Generalization"
                        },
                        {
                            "reference_title": "Compositional Processing Emerges in Neural Networks Solving Math Problems",
                            "key_word": "Compositional Learners"
                        },
                        {
                            "reference_title": "RNNs Implicitly Implement Tensor Product Representations",
                            "key_word": "Ground-Truth Compositional Structure"
                        },
                        {
                            "reference_title": "Measuring compositional generalization: A comprehensive method on realistic data",
                            "key_word": "Transformers"
                        },
                        {
                            "reference_title": "RNNs: Compositional generalization in seq2seq convolutional networks",
                            "key_word": "Convolutional Neural Networks"
                        }
                    ],
                    "references_in_this_section": [
                        "Connectionism and cognitive architecture: A critical analysis",
                        "The paradox of the compositionality of natural language: A neural machine translation case study",
                        "RNNs Implicitly Implement Tensor Product Representations",
                        "CREPE: Can Vision-Language Foundation Models Reason Compositionally",
                        "Prompting Large Vision-Language Models for Compositional Reasoning",
                        "A Benchmark for Systematic Generalization in Grounded Language Understanding",
                        "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation",
                        "Systematic Generalization: What Is Required and Can It Be Learned",
                        "Human-like systematic generalization through a meta-learning neural network",
                        "Approximation by superpositions of a sigmoidal function",
                        "Cognition without classical architecture",
                        "Categorical semantics of compositional reinforcement learning",
                        "Semantic Theory and Tacit Knowledge",
                        "Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks",
                        "Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics",
                        "Measuring compositional generalization: A comprehensive method on realistic data",
                        "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
                        "Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning",
                        "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
                        "Deep learning: A critical appraisal",
                        "CNNs found to jump around more skillfully than RNNs: Compositional generalization in seq2seq convolutional networks",
                        "Measuring Compositionality in Representation Learning",
                        "Good-Enough Compositional Data Augmentation",
                        "Analysing mathematical reasoning abilities of neural models",
                        "Break It Down: Evidence for Structural Compositionality in Neural Networks",
                        "What Algorithms can Transformers Learn? A Study in Length Generalization",
                        "Sparks of Artificial General Intelligence: Early experiments with GPT",
                        "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning",
                        "Faith and Fate: Limits of Transformers on Compositionality",
                        "Compositionality decomposed: How do neural networks generalise",
                        "Language Models are Few-Shot Learners",
                        "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
                        "Approximation Capabilities of Muitilayer Feedforward Networks",
                        "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models",
                        "Compositional Reinforcement Learning from Logical Specifications",
                        "Human few-shot learning of compositional instructions",
                        "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
                        "Learning in High Dimension Always Amounts to Extrapolation",
                        "Function Vectors in Large Language Models",
                        "The Structure of a Semantic Theory",
                        "Building machines that learn and think like people",
                        "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                        "Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees",
                        "Compositional Processing Emerges in Neural Networks Solving Math Problems",
                        "Rethinking Eliminative Connectionism",
                        "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
                        "Compositional Generalization for Multi-label Text Classification: A Data-Augmentation Approach",
                        "Align and Augment: Generative Data Augmentation for Compositional Generalization",
                        "Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention",
                        "Compositional diversity in visual concept learning",
                        "Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality",
                        "An Examination of the Compositionality of Large Generative Vision-Language Models",
                        "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"
                    ]
                },
                {
                    "name": "Architectural Inductive Biases",
                    "key_history": [
                        {
                            "reference_title": "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
                            "key_word": "Neuro-Symbolic Hybrids"
                        },
                        {
                            "reference_title": "Learning Compositional Rules via Neural Program Synthesis",
                            "key_word": "Neural Program Synthesis"
                        },
                        {
                            "reference_title": "Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics",
                            "key_word": "Structure-Content Separation"
                        },
                        {
                            "reference_title": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
                            "key_word": "Role-Filler Independence"
                        },
                        {
                            "reference_title": "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems",
                            "key_word": "Tensor Product Representation"
                        }
                    ],
                    "references_in_this_section": [
                        "Connectionism and cognitive architecture: A critical analysis",
                        "RNNs Implicitly Implement Tensor Product Representations",
                        "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems",
                        "Prompting Large Vision-Language Models for Compositional Reasoning",
                        "Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Indirection and symbol-like processing in the prefrontal cortex and basal ganglia",
                        "A survey on neural-symbolic learning systems",
                        "Systematic Generalization: What Is Required and Can It Be Learned",
                        "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                        "Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI",
                        "The Structure of Systematicity in the Brain",
                        "Compositional Generalization for Primitive Substitutions",
                        "Deep learning needs a prefrontal cortex",
                        "Emergent Symbols through Binding in External Memory",
                        "Learning Compositional Rules via Neural Program Synthesis",
                        "Hierarchical Neural Program Synthesis",
                        "Structure learning and the posterior parietal cortex",
                        "Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics",
                        "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
                        "Neurocompositional computing in human and machine intelligence: A tutorial",
                        "A Relational Inductive Bias for Dimensional Abstraction in Neural Networks",
                        "Learning to Compose Neural Networks for Question Answering",
                        "Good-Enough Compositional Data Augmentation",
                        "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
                        "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
                        "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
                        "Program Synthesis",
                        "Differentiable Tree Operations Promote Compositional Generalization",
                        "Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning",
                        "Sparks of Artificial General Intelligence: Early experiments with GPT",
                        "The Bitter Lesson",
                        "Relational inductive biases, deep learning, and graph networks",
                        "Deep learning",
                        "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction",
                        "Neural Module Networks",
                        "Language Models are Few-Shot Learners",
                        "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
                        "How Limited Systematicity Emerges: A Computational Cognitive Neuroscience Approach",
                        "Relational Constraints On Neural Networks Reproduce Human Biases towards Abstract Geometric Regularity",
                        "The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",
                        "Rebooting AI: Building Artificial Intelligence We Can Trust",
                        "Human-level concept learning through probabilistic program induction",
                        "Emergent analogical reasoning in large language models",
                        "Prefrontal cortex and flexible cognitive control: Rules without symbols",
                        "CLOSURE: Assessing Systematic Generalization of CLEVR Models",
                        "Question-Answering with Grammatically-Interpretable Representations",
                        "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
                        "Neural Program Synthesis By Self-Learning",
                        "The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation",
                        "Compositional Processing Emerges in Neural Networks Solving Math Problems",
                        "What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior",
                        "Rethinking Eliminative Connectionism",
                        "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization",
                        "Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention",
                        "Compositional diversity in visual concept learning",
                        "Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"
                    ]
                },
                {
                    "name": "In-Context Compositionality via Metalearning",
                    "key_history": [
                        {
                            "reference_title": "Meta-Learned Models of Cognition",
                            "key_word": "Metalearning"
                        },
                        {
                            "reference_title": "Human-like systematic generalization through a meta-learning neural network",
                            "key_word": "Compositional Generalization"
                        },
                        {
                            "reference_title": "Universal linguistic inductive biases via meta-learning",
                            "key_word": "Inductive Biases"
                        },
                        {
                            "reference_title": "Risks from Learned Optimization in Advanced Machine Learning Systems",
                            "key_word": "Mesa-Optimization"
                        }
                    ],
                    "references_in_this_section": [
                        "Connectionism and cognitive architecture: A critical analysis",
                        "Meta-Learned Models of Cognition",
                        "Human-like systematic generalization through a meta-learning neural network",
                        "Meta-learning in natural and artificial intelligence",
                        "Is human compositionality meta-learned",
                        "Meta-Learning in Neural Networks: A Survey",
                        "On the search for new learning rules for ANNs",
                        "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook",
                        "Learning a synaptic learning rule",
                        "The Bitter Lesson",
                        "Meta-Learning with Memory-Augmented Neural Networks",
                        "Prefrontal cortex as a meta-reinforcement learning system",
                        "Doing more with less: Meta-reasoning and meta-learning in humans and machines",
                        "Risks from Learned Optimization in Advanced Machine Learning Systems",
                        "Compositional generalization through meta sequence-to-sequence learning",
                        "Universal linguistic inductive biases via meta-learning",
                        "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                        "Modelling cognitive flexibility with deep neural networks",
                        "Rethinking Eliminative Connectionism",
                        "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
                        "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning"
                    ]
                },
                {
                    "name": "In-Context Compositionality via Pretraining",
                    "key_history": [
                        {
                            "reference_title": "Language Models are Few-Shot Learners",
                            "key_word": "In-Context Learning"
                        },
                        {
                            "reference_title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                            "key_word": "Compositional Generalization"
                        },
                        {
                            "reference_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                            "key_word": "Chain-of-Thought Prompting"
                        },
                        {
                            "reference_title": "Language Models are Unsupervised Multitask Learners",
                            "key_word": "Pretraining"
                        }
                    ],
                    "references_in_this_section": [
                        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                        "CREPE: Can Vision-Language Foundation Models Reason Compositionally",
                        "Iterated Learning Improves Compositionality in Large Vision-Language Models",
                        "Debiasing by instruction: The case of belief bias",
                        "Measuring Faithfulness in Chain-of-Thought Reasoning",
                        "Bridging the data gap between children and large language models",
                        "Black Boxes or Unflattering Mirrors? Comparative Bias in the Science of Machine Behaviour",
                        "Do Prompt-Based Models Really Understand the Meaning of Their Prompts",
                        "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
                        "Human-like systematic generalization through a meta-learning neural network",
                        "Using Computational Models to Test Syntactic Learnability",
                        "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Finetuned Language Models Are Zero-Shot Learners",
                        "Performance vs. competence in human-machine comparisons",
                        "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                        "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                        "Language models show human-like content effects on reasoning tasks",
                        "Language Models are Unsupervised Multitask Learners",
                        "Measuring and Narrowing the Compositionality Gap in Language Models",
                        "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
                        "Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small",
                        "Towards Reasoning in Large Language Models: A Survey",
                        "Is human compositionality meta-learned",
                        "Symbols and grounding in large language models",
                        "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
                        "What Artificial Neural Networks Can Tell Us About Human Language Acquisition",
                        "Deep contextualized word representations",
                        "Human category learning",
                        "Large Language Models are Zero-Shot Reasoners",
                        "Syntactic Structure from Deep Learning",
                        "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                        "Language Models are Few-Shot Learners",
                        "Reasoning about a Rule",
                        "Testing Relational Understanding in Text-Guided Image Generation",
                        "Compositional generalization through meta sequence-to-sequence learning",
                        "Emergent analogical reasoning in large language models",
                        "A taxonomy and review of generalization research in NLP",
                        "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                        "On belief bias in syllogistic reasoning",
                        "A large-scale comparison of human-written versus ChatGPT-generated essays",
                        "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                        "Compositional Semantic Parsing with Large Language Models",
                        "Attention is All you Need",
                        "Large Language Models and the Argument From the Poverty of the Stimulus",
                        "Neural Machine Translation by Jointly Learning to Align and Translate",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "Implications for Neural Mechanisms",
                    "key_history": [
                        {
                            "reference_title": "Concepts and Compositionality: In Search of the Brain's Language of Thought",
                            "key_word": "Architectural Inductive Biases"
                        },
                        {
                            "reference_title": "Deep learning needs a prefrontal cortex",
                            "key_word": "Neural Network Architecture"
                        },
                        {
                            "reference_title": "The Structure of Systematicity in the Brain",
                            "key_word": "Structure-Content Separation"
                        },
                        {
                            "reference_title": "Is human compositionality meta-learned",
                            "key_word": "Metalearning"
                        },
                        {
                            "reference_title": "Curriculum learning for human compositional generalization",
                            "key_word": "Human Compositional Generalization"
                        }
                    ],
                    "references_in_this_section": [
                        "Meta-reinforcement learning via orbitofrontal cortex",
                        "Meta-Learned Models of Cognition",
                        "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems",
                        "Evolution of declarative memory",
                        "Concepts and Compositionality: In Search of the Brain's Language of Thought",
                        "Two Cortical Visual Systems",
                        "Cognitive control over learning: Creating, clustering, and generalizing task-set structure",
                        "Contribution of striate inputs to the visuospatial functions of parieto-preoccipital cortex in monkeys",
                        "Indirection and symbol-like processing in the prefrontal cortex and basal ganglia",
                        "Human-like systematic generalization through a meta-learning neural network",
                        "The Structure of Systematicity in the Brain",
                        "Single neurons in prefrontal cortex encode abstract rules",
                        "Deep learning needs a prefrontal cortex",
                        "Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia",
                        "Structure learning and the posterior parietal cortex",
                        "The role of location indexes in spatial perception: A sketch of the FINST spatial-index model",
                        "Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics",
                        "Parietal contributions to visual feature binding: Evidence from a patient with bilateral lesions",
                        "Representing Spatial Relationships in Posterior Parietal Cortex: Single Neurons Code Object-Referenced Position",
                        "Is human compositionality meta-learned",
                        "Connecting Context-specific Adaptation in Humans to Meta-learning",
                        "Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis",
                        "The enigma of B\u00e1lint's syndrome: Neural substrates and cognitive deficits",
                        "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
                        "A scalar neural code for categories in parietal cortex: Representing cognitive variables as \"more\" or \"less",
                        "Frontal cortex and the discovery of abstract action rules",
                        "Anatomy of deductive reasoning",
                        "Human Curriculum Effects Emerge with In-Context Learning in Neural Networks",
                        "Two cortical systems for memory-guided behaviour",
                        "Constructional apraxia in patients with discrete missile wounds of the brain",
                        "Thunderstruck: The ACDC model of flexible sequences and rhythms in recurrent neural circuits",
                        "The Geometry of Map-Like Representations under Dynamic Cognitive Control",
                        "Neural Index of Reinforcement Learning Predicts Improved Stimulus-Response Retention under High Working Memory Load",
                        "Prefrontal cortex as a meta-reinforcement learning system",
                        "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction",
                        "Complementary Structure-Learning Neural Networks for Relational Reasoning",
                        "How Limited Systematicity Emerges: A Computational Cognitive Neuroscience Approach",
                        "On the Control of Automatic Processes: A Parallel Distributed Processing Model of the Stroop Effect",
                        "Prefrontal cortex and flexible cognitive control: Rules without symbols",
                        "No Coincidence, George: Capacity-Limits as the Curse of Compositionality",
                        "Curriculum learning for human compositional generalization",
                        "An integrative theory of prefrontal cortex function",
                        "Dissecting the Language Organ: A New Look at the Role of Broca's Area in Language Processing",
                        "The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation",
                        "Semantic Processing in the Anterior Temporal Lobes: A Meta-analysis of the Functional Neuroimaging Literature",
                        "How Sequential Interactive Processing Within Frontostriatal Loops Supports a Continuum of Habitual to Controlled Processing",
                        "What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior",
                        "Coding of visual objects in the ventral stream",
                        "Mechanisms of Rule Acquisition and Rule Following in Inductive Reasoning",
                        "Within- and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory",
                        "Map Making: Constructing, Combining, and Inferring on Abstract Cognitive Maps",
                        "A Neural Mechanism for Sensing and Reproducing a Time Interval"
                    ]
                },
                {
                    "name": "Implications for Human Development",
                    "key_history": [
                        {
                            "reference_title": "The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",
                            "key_word": "Neuro-Symbolic Hybrids"
                        },
                        {
                            "reference_title": "How Limited Systematicity Emerges: A Computational Cognitive Neuroscience Approach",
                            "key_word": "Strictly Neural Architectural Inductive Biases"
                        },
                        {
                            "reference_title": "Is human compositionality meta-learned",
                            "key_word": "Metalearning"
                        },
                        {
                            "reference_title": "The logical primitives of thought: Empirical foundations for compositional cognitive models",
                            "key_word": "Innate Constraints"
                        },
                        {
                            "reference_title": "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction",
                            "key_word": "Structure-Content Separation"
                        }
                    ],
                    "references_in_this_section": [
                        "The developmental trajectories of executive function from adolescence to old age",
                        "Meta-Learned Models of Cognition",
                        "An Explanation of In-context Learning as Implicit Bayesian Inference",
                        "Bridging the data gap between children and large language models",
                        "Concepts in a Probabilistic Language of Thought",
                        "Human-like systematic generalization through a meta-learning neural network",
                        "Orthogonal representations for robust context-dependent task performance in brains and neural networks",
                        "Meta-learning in natural and artificial intelligence",
                        "Core knowledge",
                        "Compositional Reasoning in Early Childhood",
                        "What Babies Know: Core Knowledge and Composition Volume",
                        "Developing Cognitive Control: Three Key Transitions",
                        "Emergent Symbols through Binding in External Memory",
                        "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                        "Universal grammar",
                        "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                        "Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics",
                        "Is human compositionality meta-learned",
                        "Rapid Transfer of Abstract Rules to Novel Contexts in Human Lateral Prefrontal Cortex",
                        "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
                        "The best game in town: The reemergence of the language-of-thought hypothesis across the cognitive sciences",
                        "Human Curriculum Effects Emerge with In-Context Learning in Neural Networks",
                        "Syntactic Structures",
                        "What Artificial Neural Networks Can Tell Us About Human Language Acquisition",
                        "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
                        "SUSTAIN: A network model of category learning",
                        "Grounded language acquisition through the eyes and ears of a single child",
                        "Compositionality in Computational Linguistics",
                        "Sparks of Artificial General Intelligence: Early experiments with GPT",
                        "Rethinking Innateness: A Connectionist Perspective on Development",
                        "Optimality Theory: Constraint Interaction in Generative Grammar",
                        "Prefrontal cortex as a meta-reinforcement learning system",
                        "Human category learning",
                        "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction",
                        "Syntactic Structure from Deep Learning",
                        "The development of reasoning by exclusion in infancy",
                        "Language Models are Few-Shot Learners",
                        "The Origin of Concepts",
                        "How Limited Systematicity Emerges: A Computational Cognitive Neuroscience Approach",
                        "The Language Instinct: How the Mind Creates Language",
                        "The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",
                        "Compositional generalization through meta sequence-to-sequence learning",
                        "Emergent analogical reasoning in large language models",
                        "Comparing continual task learning in minds and machines",
                        "The logical primitives of thought: Empirical foundations for compositional cognitive models",
                        "Prefrontal cortex and flexible cognitive control: Rules without symbols",
                        "The temporal stability of visuomotor adaptation generalization",
                        "Natural language and natural selection",
                        "Universal linguistic inductive biases via meta-learning",
                        "Symbols and mental programs: A hypothesis about human singularity",
                        "The Computational Origin of Representation",
                        "Complexity and compositionality in fluid intelligence",
                        "One model for the learning of language",
                        "Limits on composition of conceptual operations in 9-month-olds",
                        "Uncovering mesa-optimization algorithms in Transformers",
                        "The Comprehension Boost in Early Word Learning: Older Infants Are Better Learners",
                        "The child's trigger experience: Degree-0 learnability",
                        "SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant's Perspective",
                        "Lateral prefrontal cortex subregions make dissociable contributions during fluid reasoning",
                        "Call for Papers - The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus",
                        "An integrative theory of prefrontal cortex function",
                        "Aspects of the Theory of Syntax"
                    ]
                },
                {
                    "name": "Mere Implementations",
                    "key_history": [
                        {
                            "reference_title": "How Can Deep Neural Networks Inform Theory in Psychological Science",
                            "key_word": "Cognitive Theory"
                        },
                        {
                            "reference_title": "Multiple Realizability and the Rise of Deep Learning",
                            "key_word": "Compositionality"
                        },
                        {
                            "reference_title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                            "key_word": "Neural Networks"
                        },
                        {
                            "reference_title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
                            "key_word": "Mechanistic Interpretability"
                        },
                        {
                            "reference_title": "A Philosophical Introduction to Language Models - Part II: The Way Forward",
                            "key_word": "Interpretability"
                        }
                    ],
                    "references_in_this_section": [
                        "RNNs Implicitly Implement Tensor Product Representations",
                        "Explanatory models in neuroscience: Part 2 - constraint-based intelligibility",
                        "A Philosophical Introduction to Language Models - Part II: The Way Forward",
                        "Modern language models refute Chomsky's approach to language",
                        "Circuit Component Reuse Across Tasks in Transformer Language Models",
                        "In-context Learning and Induction Heads",
                        "Connectionism, Eliminativism and The Future of Folk Psychology",
                        "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
                        "How Can Deep Neural Networks Inform Theory in Psychological Science",
                        "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                        "Measuring Compositionality in Representation Learning",
                        "The connectionism/classicism battle to win souls",
                        "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate",
                        "The past and future of the past tense",
                        "Properties of Lots: The Footprints or the Bear Itself",
                        "On the proper treatment of connectionism",
                        "Why Fodor and Pylyshyn Were Wrong: The Simplest Refutation",
                        "Explanatory models in neuroscience: Part 1 - taking mechanistic abstraction seriously",
                        "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models",
                        "Philosophy of Cognitive Science in the Age of Deep Learning",
                        "The Harmonic Mind, Volume 1: From Neural Computation to Optimality-Theoretic Grammar Volume I: Cognitive Architecture",
                        "Key Concepts in AI Safety: Interpretability in Machine Learning",
                        "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
                        "A mathematical framework for transformer circuits",
                        "Multiple Realizability and the Rise of Deep Learning",
                        "Function Vectors in Large Language Models",
                        "Emergent linguistic structure in artificial neural networks trained by self-supervision",
                        "On the proper role of linguistically-oriented deep net analysis in linguistic theorizing",
                        "Compositional Processing Emerges in Neural Networks Solving Math Problems",
                        "Break It Down: Evidence for Structural Compositionality in Neural Networks",
                        "On language and connectionism: Analysis of a parallel distributed processing model of language acquisition"
                    ]
                }
            ],
            "all_references": [
                "The Appeal to Tacit Knowledge in Psychological Explanation",
                "Questions of Form and Interpretation",
                "Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning",
                "Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages",
                "Ordinary Language",
                "Two Cortical Visual Systems",
                "The Logical Syntax of Language",
                "A Philosophical Introduction to Language Models - Part I: Continuity With Classic Debates",
                "What is Compositionality",
                "Frege: Philosophy of Language, Second Edition",
                "Systematic Generalization: What Is Required and Can It Be Learned",
                "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
                "Iterated Learning Improves Compositionality in Large Vision-Language Models",
                "Measuring and Narrowing the Compositionality Gap in Language Models",
                "The Language of Thought in Late Medieval Philosophy: Essays in Honor of Claude Panaccio, volume 5 of Historical-Analytical Studies on Nature, Mind and Action",
                "What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior",
                "An interactive activation model of context effects in letter perception: I. An account of basic findings",
                "Learning to Compose Neural Networks for Question Answering",
                "The Poverty of the Stimulus Argument",
                "Explainable Artificial Intelligence (XAI) : Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",
                "Emergent Symbols through Binding in External Memory",
                "Prompting Large Vision-Language Models for Compositional Reasoning",
                "Concepts in a Probabilistic Language of Thought",
                "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
                "Multimodal Learning with Transformers: A Survey",
                "The Language of Thought",
                "Hume Variations",
                "SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant's Perspective",
                "Within- and across-trial dynamics of human EEG reveal cooperative interplay between reinforcement learning and working memory",
                "Measuring Compositionality in Representation Learning",
                "Constructional apraxia in patients with discrete missile wounds of the brain",
                "Philosophy and Connectionist Theory",
                "The Language Instinct: How the Mind Creates Language",
                "A Synopsis of Linguistic Theory",
                "Anatomy of deductive reasoning",
                "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate",
                "Emergent Abilities of Large Language Models",
                "Improving Image Generation with Better Captions",
                "Sparks of Artificial General Intelligence: Early experiments with GPT",
                "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models",
                "Semantic Theory and Tacit Knowledge",
                "Human few-shot learning of compositional instructions",
                "Compositional generalization through meta sequence-to-sequence learning",
                "Studies in the Way of Words",
                "The Structure of Systematicity in the Brain",
                "COGS: A Compositional Generalization Challenge Based on Semantic Interpretation",
                "Thunderstruck: The ACDC model of flexible sequences and rhythms in recurrent neural circuits",
                "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
                "Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia",
                "Single neurons in prefrontal cortex encode abstract rules",
                "Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention",
                "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization",
                "CLOSURE: Assessing Systematic Generalization of CLEVR Models",
                "Mathematical discoveries from program search with large language models",
                "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook",
                "Symbols and grounding in large language models",
                "The Structure of a Semantic Theory",
                "Rebooting AI: Building Artificial Intelligence We Can Trust",
                "Scaling Vision Transformers to 22 Billion Parameters",
                "Logics and Languages",
                "Meta-learning in natural and artificial intelligence",
                "Linguistic generalization and compositionality in modern artificial neural networks",
                "The Linguistic Turn: Essays in Philosophical Method",
                "Large Language Models are Zero-Shot Reasoners",
                "Deep learning: A philosophical introduction",
                "Neural Machine Translation of Rare Words with Subword Units",
                "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
                "The cognitive revolution: A historical perspective",
                "Meta-Learning in Neural Networks: A Survey",
                "The role of location indexes in spatial perception: A sketch of the FINST spatial-index model",
                "How Can Deep Neural Networks Inform Theory in Psychological Science",
                "Risks from Learned Optimization in Advanced Machine Learning Systems",
                "The developmental trajectories of executive function from adolescence to old age",
                "The child's trigger experience: Degree-0 learnability",
                "From Deep Learning to Rational Machines: What the History of Philosophy Can Teach Us about the Future of Artificial Intelligence",
                "Building machines that learn and think like people",
                "The development of reasoning by exclusion in infancy",
                "Language and Mind",
                "SUSTAIN: A network model of category learning",
                "Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
                "A Benchmark for Systematic Generalization in Grounded Language Understanding",
                "What company do words keep? Revisiting the distributional semantics of J.R. Firth & Zellig Harris",
                "Language Models are Unsupervised Multitask Learners",
                "Is human compositionality meta-learned",
                "Human-level concept learning through probabilistic program induction",
                "Systematicity in a Recurrent Neural Network by Factorizing Syntax and Semantics",
                "Structure learning and the posterior parietal cortex",
                "Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI",
                "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
                "Bridging the data gap between children and large language models",
                "Learning representations by back-propagating errors",
                "Compositionality",
                "Human category learning",
                "A Relational Inductive Bias for Dimensional Abstraction in Neural Networks",
                "Learning in High Dimension Always Amounts to Extrapolation",
                "A mathematical framework for transformer circuits",
                "Core knowledge",
                "Cognitive control over learning: Creating, clustering, and generalizing task-set structure",
                "Rules or connections in past-tense inflections: What does the evidence rule out",
                "Neural Machine Translation by Jointly Learning to Align and Translate",
                "The connectionism/classicism battle to win souls",
                "The best game in town: The reemergence of the language-of-thought hypothesis across the cognitive sciences",
                "Large Language Models and the Argument From the Poverty of the Stimulus",
                "Formal Philosophy; Selected Papers of Richard Montague",
                "Meaning and Necessity: A Study in Semantics and Modal Logic",
                "Curriculum learning for human compositional generalization",
                "How Sequential Interactive Processing Within Frontostriatal Loops Supports a Continuum of Habitual to Controlled Processing",
                "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 2: Psychological and Biological Models",
                "Do True Assertions Correspond to Reality",
                "GPT-4 Technical Report",
                "Performance vs. competence in human-machine comparisons",
                "Cartesian Linguistics",
                "Universal grammar",
                "Why There Are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory",
                "Cognition, Systematicity, and Nomic Necessity",
                "In-context Learning and Induction Heads",
                "Modern language models refute Chomsky's approach to language",
                "An integrative theory of prefrontal cortex function",
                "Scaling Laws for Neural Language Models",
                "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
                "CREPE: Can Vision-Language Foundation Models Reason Compositionally",
                "Prefrontal cortex as a meta-reinforcement learning system",
                "Orthogonal representations for robust context-dependent task performance in brains and neural networks",
                "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
                "A Neural Mechanism for Sensing and Reproducing a Time Interval",
                "Deep learning",
                "Broken Neural Scaling Laws",
                "An Examination of the Compositionality of Large Generative Vision-Language Models",
                "Differentiable Tree Operations Promote Compositional Generalization",
                "Meta-Learning with Memory-Augmented Neural Networks",
                "On the proper treatment of connectionism",
                "Optimality Theory: Constraint Interaction in Generative Grammar",
                "Structured Representations in Connectionist Systems",
                "The History and Prehistory of Natural-Language Semantics",
                "Deep learning: A critical appraisal",
                "Human-like systematic generalization through a meta-learning neural network",
                "Do Prompt-Based Models Really Understand the Meaning of Their Prompts",
                "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
                "Natural language and natural selection",
                "Training language models to follow instructions with human feedback",
                "Cognition without classical architecture",
                "A taxonomy and review of generalization research in NLP",
                "Towards Automated Circuit Discovery for Mechanistic Interpretability",
                "Using Computational Models to Test Syntactic Learnability",
                "The Theory of Meaning",
                "Align and Augment: Generative Data Augmentation for Compositional Generalization",
                "The Mythos of Model Interpretability",
                "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
                "Mechanisms of Rule Acquisition and Rule Following in Inductive Reasoning",
                "Lateral prefrontal cortex subregions make dissociable contributions during fluid reasoning",
                "Approximation by superpositions of a sigmoidal function",
                "Rapid Transfer of Abstract Rules to Novel Contexts in Human Lateral Prefrontal Cortex",
                "Insight and Illusion: Wittgenstein on Philosophy and the Metaphysics of Experience",
                "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models",
                "Neural Index of Reinforcement Learning Predicts Improved Stimulus-Response Retention under High Working Memory Load",
                "Philosophical Investigations",
                "The Thought: A Logical Inquiry",
                "Compositional Reinforcement Learning from Logical Specifications",
                "Developing Cognitive Control: Three Key Transitions",
                "Rule-Following, Objectivity and the Theory of Meaning",
                "Emergent analogical reasoning in large language models",
                "Neural and Conceptual Interpretation of PDP Models",
                "On the proper role of linguistically-oriented deep net analysis in linguistic theorizing",
                "On belief bias in syllogistic reasoning",
                "ImageNet Classification with Deep Convolutional Neural Networks",
                "Comparing continual task learning in minds and machines",
                "Contribution of striate inputs to the visuospatial functions of parieto-preoccipital cortex in monkeys",
                "No Coincidence, George: Capacity-Limits as the Curse of Compositionality",
                "Connectionism, Eliminativism and The Future of Folk Psychology",
                "Large language models encode clinical knowledge",
                "Function Vectors in Large Language Models",
                "Call for Papers - The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus",
                "An Explanation of In-context Learning as Implicit Bayesian Inference",
                "Syntactic Structure from Deep Learning",
                "Compositionality: Its Historic Context",
                "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
                "Evaluating Large Language Models Trained on Code",
                "Three-Concept Monte: Explanation, Implementation and Systematicity",
                "Categorical semantics of compositional reinforcement learning",
                "Universal linguistic inductive biases via meta-learning",
                "Deep contextualized word representations",
                "Doing more with less: Meta-reasoning and meta-learning in humans and machines",
                "Representing Spatial Relationships in Posterior Parietal Cortex: Single Neurons Code Object-Referenced Position",
                "Dissecting the Language Organ: A New Look at the Role of Broca's Area in Language Processing",
                "Properties of Lots: The Footprints or the Bear Itself",
                "Syntactic Structures",
                "Attention is All you Need",
                "Modelling cognitive flexibility with deep neural networks",
                "Connectionism and cognitive architecture: A critical analysis",
                "The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",
                "A Philosophical Introduction to Language Models - Part II: The Way Forward",
                "The paradox of the compositionality of natural language: A neural machine translation case study",
                "A survey on neural-symbolic learning systems",
                "Scaling deep learning for materials discovery",
                "Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality",
                "Compositional Generalization for Primitive Substitutions",
                "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
                "What Algorithms can Transformers Learn? A Study in Length Generalization",
                "One model for the learning of language",
                "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
                "Question-Answering with Grammatically-Interpretable Representations",
                "The Underlying Reality of Language and Its Philosophical Import",
                "Symbols and mental programs: A hypothesis about human singularity",
                "Compositionality in Computational Linguistics",
                "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
                "Compositional Processing Emerges in Neural Networks Solving Math Problems",
                "Aspects of the Theory of Syntax",
                "The UCLA Lectures",
                "Philosophy of Cognitive Science in the Age of Deep Learning",
                "The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation",
                "Compositional Reasoning in Early Childhood",
                "The Relational Bottleneck as an Inductive Bias for Efficient Abstraction",
                "Complementary Structure-Learning Neural Networks for Relational Reasoning",
                "Neural networks and physical systems with emergent collective computational abilities",
                "Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees",
                "Learning Compositional Rules via Neural Program Synthesis",
                "RNNs Implicitly Implement Tensor Product Representations",
                "Map Making: Constructing, Combining, and Inferring on Abstract Cognitive Maps",
                "What Is a Theory of Meaning? (II",
                "How to Do Things with Words: Second Edition",
                "Why Meaning (Probably) Isn't Conceptual Role",
                "Circuit Component Reuse Across Tasks in Transformer Language Models",
                "Reasoning about a Rule",
                "What Babies Know: Core Knowledge and Composition Volume",
                "Frontal cortex and the discovery of abstract action rules",
                "Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small",
                "A logical calculus of the ideas immanent in nervous activity",
                "Rethinking Innateness: A Connectionist Perspective on Development",
                "Measuring compositional generalization: A comprehensive method on realistic data",
                "Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks",
                "DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
                "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
                "Hierarchical Neural Program Synthesis",
                "Compositionality: A Connectionist Variation on a Classical Theme",
                "Coding of visual objects in the ventral stream",
                "What Artificial Neural Networks Can Tell Us About Human Language Acquisition",
                "Towards Reasoning in Large Language Models: A Survey",
                "On language and connectionism: Analysis of a parallel distributed processing model of language acquisition",
                "Human Curriculum Effects Emerge with In-Context Learning in Neural Networks",
                "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "Explanatory models in neuroscience: Part 1 - taking mechanistic abstraction seriously",
                "A scalar neural code for categories in parietal cortex: Representing cognitive variables as \"more\" or \"less",
                "A large-scale comparison of human-written versus ChatGPT-generated essays",
                "Highly accurate protein structure prediction with AlphaFold",
                "Concepts and Compositionality: In Search of the Brain's Language of Thought",
                "Relational inductive biases, deep learning, and graph networks",
                "Explanatory models in neuroscience: Part 2 - constraint-based intelligibility",
                "Learning a synaptic learning rule",
                "Testing Relational Understanding in Text-Guided Image Generation",
                "Grounded language acquisition through the eyes and ears of a single child",
                "On learning the past tenses of english verbs",
                "Approximation Capabilities of Muitilayer Feedforward Networks",
                "Neural Module Networks",
                "Complexity and compositionality in fluid intelligence",
                "Connectionism: Debates on Psychological Explanation, Volume",
                "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
                "Relational Constraints On Neural Networks Reproduce Human Biases towards Abstract Geometric Regularity",
                "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality",
                "The Illustrated Transformer",
                "Finetuned Language Models Are Zero-Shot Learners",
                "Rethinking Eliminative Connectionism",
                "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
                "Philosophy of Mind",
                "Data Distributional Properties Drive Emergent In-Context Learning in Transformers",
                "Program Synthesis",
                "The enigma of B\u00e1lint's syndrome: Neural substrates and cognitive deficits",
                "Connecting Context-specific Adaptation in Humans to Meta-learning",
                "The Harmonic Mind, Volume 1: From Neural Computation to Optimality-Theoretic Grammar Volume I: Cognitive Architecture",
                "Emergent linguistic structure in artificial neural networks trained by self-supervision",
                "Compositional Generalization for Multi-label Text Classification: A Data-Augmentation Approach",
                "Compositional diversity in visual concept learning",
                "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
                "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
                "Compositionality decomposed: How do neural networks generalise",
                "Language models show human-like content effects on reasoning tasks",
                "Discovering the Compositional Structure of Vector Representations with Role Learning Networks",
                "Analysing mathematical reasoning abilities of neural models",
                "The Comprehension Boost in Early Word Learning: Older Infants Are Better Learners",
                "A quantitative description of membrane current and its application to conduction and excitation in nerve",
                "Measuring Faithfulness in Chain-of-Thought Reasoning",
                "Black Boxes or Unflattering Mirrors? Comparative Bias in the Science of Machine Behaviour",
                "Multiple Realizability and the Rise of Deep Learning",
                "Semantic Processing in the Anterior Temporal Lobes: A Meta-analysis of the Functional Neuroimaging Literature",
                "Key Concepts in AI Safety: Interpretability in Machine Learning",
                "Indirection and symbol-like processing in the prefrontal cortex and basal ganglia",
                "Good-Enough Compositional Data Augmentation",
                "Evolution of declarative memory",
                "On the Control of Automatic Processes: A Parallel Distributed Processing Model of the Stroop Effect",
                "Debiasing by instruction: The case of belief bias",
                "Deep Learning",
                "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big",
                "Attention in Psychology, Neuroscience, and Machine Learning",
                "The Bitter Lesson",
                "Compositionality I: Definitions and Variants",
                "The Mind Doesn't Work That Way: The Scope and Limits of Computational Psychology",
                "The red herring and the pet fish: Why concepts still can't be prototypes",
                "Mastering the game of Go with deep neural networks and tree search",
                "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "Empiricism without magic: Transformational abstraction in deep convolutional neural networks",
                "A Survey on Transformers in Reinforcement Learning",
                "Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: Computational analysis",
                "The Geometry of Map-Like Representations under Dynamic Cognitive Control",
                "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
                "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning",
                "Limits on composition of conceptual operations in 9-month-olds",
                "Psychological Explanation: An Introduction to the Philosophy of Psychology",
                "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
                "Computational Cognitive Neuroscience",
                "Did Frege Believe Frege's Principle",
                "Theories of Meaning and Learnable Languages",
                "Faith and Fate: Limits of Transformers on Compositionality",
                "A spreading-activation theory of retrieval in sentence production",
                "Two cortical systems for memory-guided behaviour",
                "Prefrontal cortex and flexible cognitive control: Rules without symbols",
                "Large language models in medicine: The potentials and pitfalls",
                "The temporal stability of visuomotor adaptation generalization",
                "Knowledge-Infused Learning: A Sweet Spot in Neuro-Symbolic AI",
                "Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning",
                "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "Why Fodor and Pylyshyn Were Wrong: The Simplest Refutation",
                "Psychophysical Supervenience",
                "Why Compositionality Won't Go Away: Reflections on Horwich's 'Deflationary' Theory",
                "Uncovering mesa-optimization algorithms in Transformers",
                "Deep learning needs a prefrontal cortex",
                "The past and future of the past tense",
                "Meta-reinforcement learning via orbitofrontal cortex",
                "The logical primitives of thought: Empirical foundations for compositional cognitive models",
                "Neurocompositional computing in human and machine intelligence: A tutorial",
                "Methodological Reflections on Current Linguistic Theory",
                "CNNs found to jump around more skillfully than RNNs: Compositional generalization in seq2seq convolutional networks",
                "Connectionism, Constituency and the Language of Thought",
                "Wittgenstein's Metaphilosophy",
                "Letter to Jourdain",
                "Meta-Learned Models of Cognition",
                "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                "The Origin of Concepts",
                "Neural Program Synthesis By Self-Learning",
                "The Computational Origin of Representation",
                "On the search for new learning rules for ANNs",
                "Parietal contributions to visual feature binding: Evidence from a patient with bilateral lesions",
                "Break It Down: Evidence for Structural Compositionality in Neural Networks",
                "How Limited Systematicity Emerges: A Computational Cognitive Neuroscience Approach",
                "Compositional Semantic Parsing with Large Language Models",
                "Discussion: Meaning and Necessity",
                "Psychosemantics: The Problem of Meaning in the Philosophy of MInd",
                "Language Models are Few-Shot Learners",
                "The Appeal of Parallel Distributed Processing",
                "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"
            ]
        },
        "topic_history": [
            {
                "name": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
                "arxiv_id": "2403.07183",
                "reference": [
                    "How to spot ai-generated text",
                    "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
                    "GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance",
                    "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                    "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts",
                    "Can AI-Generated Text be Reliably Detected",
                    "A Survey on Detection of LLMs-Generated Content",
                    "A watermark for large language models",
                    "Attacking Neural Text Detectors",
                    "Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",
                    "Natural language watermarking: challenges in building a practical system",
                    "Defending Against Neural Fake News",
                    "Automatic detection of machine generated text: A critical survey",
                    "Robust Distortion-free Watermarks for Language Models",
                    "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
                    "Release strategies and the social impacts of language models",
                    "Authorship Attribution for Neural Text Generation",
                    "Deepfake Text Detection in the Wild",
                    "Release Strategies and the Social Impacts of Language Models",
                    "On the possibilities of ai-generated text detection",
                    "New AI classifier for indicating AI-written text",
                    "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
                    "Assaying on the Robustness of Zero-Shot Machine-Generated Text Detectors",
                    "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
                    "GPT-2: 1.5B release",
                    "A Semantic Invariant Robust Watermark for Large Language Models",
                    "Natural Language Watermarking Using Semantic Substitution for Chinese Text",
                    "Real or Fake? Learning to Discriminate Machine from Human Generated Text",
                    "DetectGPT: Zero-shot machine-generated text detection using probability curvature",
                    "Identifying Real or Fake Articles: Towards better Language Modeling",
                    "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
                    "Three Bricks to Consolidate Watermarks for Large Language Models",
                    "Red Teaming Language Model Detectors with Language Models",
                    "Provable Robust Watermarking for AI-Generated Text",
                    "Natural Language Watermarking: Design, Analysis, and a Proof-of-Concept Implementation",
                    "Testing of detection tools for AI-generated text",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Automatic detection of generated text is easiest when humans are fooled",
                    "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
                    "DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models",
                    "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning",
                    "TweepFake: About detecting deepfake tweets",
                    "GLTR: Statistical Detection and Visualization of Generated Text",
                    "Robust Multi-bit Natural Language Watermarking through Invariant Features",
                    "GPT detectors are biased against non-native English writers",
                    "ChatGPT creator pulls AI detection tool due to 'low rate of accuracy",
                    "Computer-Generated Text Detection Using Machine Learning: A Systematic Review",
                    "RADAR: Robust AI-Text Detection via Adversarial Learning",
                    "Natural Language Watermarking and Tamperproofing",
                    "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions",
                    "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                    "Unbiased Watermark for Large Language Models",
                    "Squibs: What Is a Paraphrase",
                    "GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content",
                    "Detecting Fake Content with Relative Entropy Scoring"
                ]
            },
            {
                "name": "ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback",
                "arxiv_id": "2401.03605",
                "reference": [
                    "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations",
                    "Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences",
                    "Is chatgpt a good recommender? a preliminary study",
                    "Language models are few-shot learners",
                    "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
                    "Language models as recommender systems: Evaluations and limitations",
                    "Language models are unsupervised multitask learners",
                    "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
                    "Actionable Recourse in Linear Classification",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
                ]
            },
            {
                "name": "ChatGPT and Its Educational Impact: Insights from a Software Development Competition",
                "arxiv_id": "2409.03779",
                "reference": [
                    "Supporting Teachers' Professional Development With Generative AI: The Effects on Higher Order Thinking and Self-Efficacy",
                    "ChatGPT is a remarkable tool for experts",
                    "The influences of ChatGPT on undergraduate students' demonstrated and perceived interdisciplinary learning",
                    "AutoDev: Automated AI-Driven Development",
                    "Generative AI for software practitioners",
                    "Using ChatGPT in Software Requirements Engineering: A Comprehensive Review",
                    "Personalized Learning Paths: Adapting Education with AI-Driven Curriculum",
                    "Structure, Objectives, and Operational Framework for Ethical Integration of Artificial Intelligence in Educational",
                    "Challenges and opportunities for classroom-based formative assessment and AI: a perspective article",
                    "The Role of Sustainability and Artificial Intelligence in Education Improvement",
                    "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
                    "Foundation Models for Education: Promises and Prospects"
                ]
            },
            {
                "name": "Beyond ChatGPT: Enhancing Software Quality Assurance Tasks with Diverse LLMs and Validation Techniques",
                "arxiv_id": "2409.01001",
                "reference": [
                    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "The cost of poor software quality in the US: A 2022 report",
                    "An empirical study of fault localization families and their combinations",
                    "FPA-FL: Incorporating static fault-proneness analysis into statistical fault localization",
                    "Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs",
                    "Evaluation of chatgpt model for vulnerability detection",
                    "Meta LLaMA",
                    "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
                    "ChatGPT Model Overview",
                    "Spectrum-based multiple fault localization",
                    "Toward large-scale vulnerability discovery using machine learning",
                    "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues",
                    "The DStar method for effective software fault localization",
                    "The impact factors on the performance of machine learning-based vulnerability detection: A comparative study",
                    "Mixtral of Experts",
                    "An introduction to modern software quality assurance",
                    "Software quality assurance-concepts and misconceptions",
                    "Fault localization prioritization: Comparing information-theoretic and coverage-based approaches",
                    "uChecker: Masked Pretrained Language Models as Unsupervised Chinese Spelling Checkers",
                    "A comparative study of static code analysis tools for vulnerability detection in c/c++ and java source code",
                    "Prompt-enhanced software vulnerability detection using chatgpt",
                    "Investigating Code Generation Performance of Chat-GPT with Crowdsourcing Social Data",
                    "Mitigating program security vulnerabilities: Approaches and challenges",
                    "Large Language Models in Fault Localisation",
                    "Gemma-1.1-7B",
                    "Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models",
                    "Large language models for test-free fault localization",
                    "Improving spectral-based fault localization using static analysis",
                    "Large Language Model for Vulnerability Detection: Emerging Results and Future Directions",
                    "Integrating static and dynamic analysis for detecting vulnerabilities",
                    "A Preliminary Evaluation of LLM-Based Fault Localization",
                    "Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization",
                    "Fluccs: Using code and change metrics to improve fault localization",
                    "Attention is all you need",
                    "Language models are few-shot learners",
                    "Automated code review tools for security",
                    "Androshield: Automated android applications vulnerability detection, a hybrid static and dynamic analysis approach",
                    "Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study",
                    "Evaluating and Improving Fault Localization",
                    "A model for spectra-based software diagnosis",
                    "Prompt-Enhanced Software Vulnerability Detection Using ChatGPT",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Gptscan: Detecting logic vulnerabilities in smart contracts by combining gpt with program analysis"
                ]
            },
            {
                "name": "A Qualitative Study on Using ChatGPT for Software Security: Perception vs. Practicality",
                "arxiv_id": "2408.00435",
                "reference": [
                    "Considerations for evaluating large language models for cybersecurity tasks",
                    "Transformer-based vulnerability detection in code at edittime: Zero-shot, few-shot, or fine-tuning",
                    "When chatgpt meets smart contract vulnerability detection: How far are we",
                    "Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities",
                    "Apibot: question answering bot for api documentation",
                    "New tricks to old codes: can ai chatbots replace static code analysis tools",
                    "Data quality for software vulnerability datasets",
                    "An empirical comparison of transformer-based models in vulnerability prediction",
                    "Transformer-based language models for software vulnerability detection",
                    "Conversational devbots for secure programming: An empirical study on skf chatbot",
                    "A new approach to web application security: Utilizing gpt language models for source code inspection",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Can large language models identify and reason about security vulnerabilities? not yet",
                    "Llbezpeky: Leveraging large language models for vulnerability detection",
                    "Breaking the silence: the threats of using llms in software engineering",
                    "Investigating user perceptions of conversational agents for software-related exploratory web search",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Prompt-enhanced software vulnerability detection using chatgpt"
                ]
            },
            {
                "name": "Large Language Model-based Role-Playing for Personalized Medical Jargon Extraction",
                "arxiv_id": "2408.05555",
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "Role play with large language models",
                    "Wikipedia's moment of truth, Jul",
                    "Can ai serve as a substitute for human subjects in software engineering research",
                    "Emergent abilities of large language models",
                    "Palr: Personalization aware llms for recommendation",
                    "Personalized text generation with fine-grained linguistic control",
                    "Palm: Scaling language modeling with pathways",
                    "Lamp: When large language models meet personalization",
                    "Camel: Communicative agents for \" mind \" exploration of large scale language model society",
                    "Proceedings of the 1st workshop on personalization of generative ai systems (personalize",
                    "How to use language models for synthetic text generation in cerebrovascular disease-specific medical reports",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Llm-rec: Personalized recommendation via prompting large language models",
                    "Sparks of artificial general intelligence: Early experiments with gpt",
                    "Readme: Bridging medical jargon and lay understanding for patient education through data-centric nlp",
                    "A hybrid approach to extract keyphrases from medical documents",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "How does gpt obtain its ability? tracing emergent abilities of language models to their sources",
                    "Mt-bioner: Multi-task learning for biomedical named entity recognition using deep bidirectional transformers",
                    "Improving language understanding by generative pre-training",
                    "Gpt-4 technical report",
                    "spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing",
                    "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
                    "A comparison between named entity recognition models in the biomedical domain",
                    "Readctrl: Personalizing text generation with readability-controlled instruction learning",
                    "Multi-party chat: Conversational agents in group settings with humans and models",
                    "Chatharuhi: Reviving anime character in reality via large language model",
                    "From persona to personalization: A survey on role-playing language agents",
                    "Language models are few-shot learners",
                    "Attention is all you need",
                    "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
                    "Autoagents: A framework for automatic agent generation",
                    "Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration",
                    "Quickumls: a fast, unsupervised approach for medical concept extraction",
                    "Language models are unsupervised multitask learners",
                    "Bio-ner: biomedical named entity recognition using rule-based and statistical learners",
                    "Medjex: A medical jargon extraction model with wiki's hyperlink span and contextualized masked language model score",
                    "Namedkeys: Unsupervised keyphrase extraction for biomedical documents",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations",
                "arxiv_id": "2408.05128",
                "reference": [
                    "An empirical comparison of dependency network evolution in seven software packaging ecosystems",
                    "How reuse influences productivity in object-oriented systems",
                    "On the impact of using trivial packages: An empirical case study on npm and pypi",
                    "Effective dependency management for the javascript software ecosystem",
                    "An empirical study on selection of open source software-preliminary results",
                    "An empirical study of usages, updates and risks of third-party libraries in java projects",
                    "What predicts software developers' productivity",
                    "Can chatgpt support developers? an empirical evaluation of large language models for code generation",
                    "Welcome to the era of chatgpt et al. the prospects of large language models",
                    "Can you trust ChatGPT's package recommendations",
                    "Exploring risks in the usage of third-party libraries",
                    "Can chatgpt replace stackoverflow? a study on robustness and reliability of large language model code generation",
                    "Helping or not helping? why and how trivial packages impact the npm ecosystem",
                    "A context-oriented programming approach to dependency hell",
                    "The programmer's assistant: Conversational interaction with a large language model for software development",
                    "Not All Dependencies are Equal: An Empirical Study on Production Dependencies in NPM",
                    "Evaluation of the programming skills of large language models",
                    "Escaping dependency hell: finding build dependency errors with the unified dependency graph",
                    "Categorizing developer information needs in software ecosystems",
                    "Dependency update strategies and package characteristics",
                    "What are the characteristics of highly-selected packages? a case study on the npm ecosystem"
                ]
            },
            {
                "name": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions",
                "arxiv_id": "2407.12423",
                "reference": [
                    "An integrated framework for course adapted student learning analytics dashboard",
                    "Intergroup dialogues: An educational model for cultivating engagement across differences",
                    "Exploring the ethical considerations of using chat gpt in university education",
                    "Threadreconstructor: Modeling reply-chains to untangle conversational text through visual analytics",
                    "Visual analysis of mooc forums with iforum",
                    "Visual sentiment analysis on twitter data streams",
                    "Knowledge compass: A question answering system guiding students with follow-up question recommendations",
                    "Visualising conversation structure across time: Insights into effective doctor-patient consultations",
                    "Chatgpt: The cognitive effects on learning and memory",
                    "Open learner models",
                    "Ineqdetect: A visual analytics system to detect conversational inequality and support reflection during active learning",
                    "Educational dialogues: Understanding and promoting productive interaction",
                    "Unlocking the opportunities through chatgpt tool towards ameliorating the education system",
                    "Loop: A learning analytics tool to provide teachers with useful data visualisations",
                    "Chatgpt as an educational tool: Opportunities, challenges, and recommendations for communication, business writing, and composition courses",
                    "Development of the learning analytics dashboard to support students' learning performance",
                    "Open ai in education, the responsible and ethical use of chatgpt towards lifelong learning",
                    "Shaping the future of education: Exploring the potential and consequences of ai and chatgpt in educational settings",
                    "T-cal: Understanding team conversational data with calendar-based visualization",
                    "Ruffle&riley: Towards the automated induction of conversational tutoring systems",
                    "Visohc: Designing visual analytics for online health communities",
                    "Attention please! learning analytics for visualization and recommendation",
                    "Exploring the opportunities and challenges of nlp models in higher education: is chat gpt a blessing or a curse",
                    "How chat gpt can transform autodidactic experiences and open education",
                    "Conceptual recurrence plots: Revealing patterns in human discourse",
                    "Education in the era of generative artificial intelligence (ai) : Understanding the potential benefits of chatgpt in promoting teaching and learning",
                    "Challenges and opportunities in data visualization education: A call to action",
                    "Uncertainty representation in visualizations of learning analytics for learners: Current approaches and opportunities",
                    "Involving teachers in the data-driven improvement of intelligent tutors: A prototyping study",
                    "Transformation or evolution?: Education 4.0, teaching and learning in the digital age",
                    "Adaptive assessment of visualization literacy",
                    "Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant",
                    "Gpt-3-driven pedagogical agents to train children's curious question-asking skills",
                    "Learning vis tools: Teaching data visualization tutorials",
                    "Conviscope: Visual analytics for exploring patient conversations",
                    "Multiconvis: A visual text analytics system for exploring a collection of online conversations",
                    "Visualizing data to support judgement, inference, and decision making in learning analytics: Insights from cognitive psychology and visualization science"
                ]
            },
            {
                "name": "Time to Separate from StackOverflow and Match with ChatGPT for Encryption",
                "arxiv_id": "2406.06164",
                "reference": [
                    "LLM security guard for code",
                    "Stack overflow considered harmful? the impact of copy&paste on android application security",
                    "Crysl: Validating correct usage of cryptographic apis",
                    "Automatic detection of java cryptographic api misuses: Are we there yet",
                    "Cryptoguard: High precision detection of cryptographic vulnerabilities in massive-sized java projects",
                    "Practical evaluation of static analysis tools for cryptography: Benchmarking method and case study",
                    "Crylogger: Detecting crypto misuses dynamically",
                    "Hurdles for developers in cryptography",
                    "Asleep at the keyboard? assessing the security of github copilot's code contributions",
                    "An empirical study of c++ vulnerabilities in crowd-sourced code examples",
                    "On the use of c# unsafe code context: An empirical study of stack overflow",
                    "Naturalistic static program analysis",
                    "Java cryptography uses in the wild",
                    "Developers struggle with authentication in blazor webassembly",
                    "The impact of developer experience in using java cryptography",
                    "Snakes in paradise? insecure python-related coding practices in stack overflow",
                    "Secure coding practices in java: Challenges and vulnerabilities",
                    "Is github's copilot as bad as humans at introducing vulnerabilities in code",
                    "Example-based vulnerability detection and repair in java code",
                    "Evaluation of static vulnerability detection tools with java cryptographic api benchmarks",
                    "Why does cryptographic software fail? a case study and open problems",
                    "Why crypto-detectors fail: A systematic evaluation of cryptographic misuse detection techniques",
                    "Lessons learned in implementing and deploying crypto software",
                    "Do users write more insecure code with ai assistants",
                    "spectra: A precise framework for analyzing cryptographic vulnerabilities in android apps",
                    "Lost at c: A user study on the security implications of large language model code assistants",
                    "Security weaknesses of copilot generated code in github",
                    "A stitch in time: Supporting android developers in writingsecure code",
                    "Worrisome patterns in developers: A survey in cryptography",
                    "Modelling analysis and auto-detection of cryptographic misuse in android applications",
                    "Fluentcrypto: Cryptography in easy mode",
                    "Cryptography vulnerabilities on hackerone"
                ]
            },
            {
                "name": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
                "arxiv_id": "2401.10545",
                "reference": [
                    "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys",
                    "Defining and measuring fairness in location recommendations",
                    "Content-Based Multimedia Recommendation Systems: Definition and Application Domains",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "A survey of research on fair recommender systems",
                    "Exploring artist gender bias in music recommendation",
                    "Pareto optimality for fairness-constrained collaborative filtering",
                    "Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems",
                    "A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems",
                    "FairEvalLLM. A Comprehensive Framework for Benchmarking Fairness in Large Language Model Recommender Systems",
                    "The winner takes it all: geographic imbalance and provider (un) fairness in educational recommender systems",
                    "Balanced neighborhoods for multi-sided fairness in recommendation",
                    "OpenP5: Benchmarking Foundation Models for Recommendation",
                    "Estimation of fair ranking metrics with incomplete judgments",
                    "TFROM: A Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers",
                    "Balancing between accuracy and fairness for interactive recommendation with reinforcement learning",
                    "Mitigating sentiment bias for recommender systems",
                    "Spot: Better frozen model adaptation through soft prompt transfer",
                    "A unifying and general account of fairness measurement in recommender systems",
                    "Fair sharing for sharing economy platforms",
                    "Fairness and discrimination in recommendation and retrieval",
                    "Exploiting personalized calibration and metrics for fairness recommendation",
                    "Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms",
                    "Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
                    "Explaining recommender systems fairness and accuracy through the lens of data characteristics",
                    "Towards universal sequence representation learning for recommender systems",
                    "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                    "Does fair ranking improve minority outcomes? understanding the interplay of human and algorithmic biases in online hiring",
                    "Fairness in recommender systems: research landscape and future directions",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "Interplay between upsampling and regularization for provider fairness in recommender systems",
                    "Two-sided fairness in rankings via Lorenz dominance",
                    "Recommendation with Generative Models",
                    "Defining and supporting narrative-driven recommendation",
                    "Fairness-aware news recommendation with decomposed adversarial learning",
                    "Towards long-term fairness in recommendation",
                    "Experiments on generalizability of user-oriented fairness in recommender systems",
                    "Addressing marketing bias in product recommendations",
                    "Fairness among new items in cold start recommender systems",
                    "Language models are few-shot learners",
                    "A flexible framework for evaluating user and item fairness in recommender systems",
                    "Towards understanding and mitigating unintended biases in language model-driven conversational recommendation",
                    "The Unfairness of Active Users and Popularity Bias in Point-of-Interest Recommendation",
                    "A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News",
                    "User-item matching for recommendation fairness",
                    "An enhanced probabilistic fairness-aware group recommendation by incorporating social activeness",
                    "User-oriented fairness in recommendation",
                    "A fairness-aware hybrid recommender system"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
            "arxiv_id": "2210.13669",
            "isAPA": true,
            "abstract": "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities fornatural language interface design. Buildingon the prior success of LLMs in the realm ofcomputer-assisted creativity, we aim to studyif LLMs can improve the quality of usergenerated content through collaboration. Wepresent CoPoet, a collaborative poetry writingsystem. In contrast to auto-completing a user'stext, CoPoet is controlled by user instructionsthat specify the attributes of the desired text,such as Write a sentence about 'love' or Writea sentence ending in 'fly'. The core component of our system is a language model finetuned on a diverse collection of instructions forpoetry writing. Our model is not only competitive with publicly available LLMs trainedon instructions (InstructGPT) , but is also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poemswith CoPoet on diverse topics ranging fromMonarchy to Climate change. Further, thecollaboratively written poems are preferred bythird-party evaluators over those written without the system.1",
            "reference": [
                "Tuhin Chakrabarty, Arkadiy Saakyan, and Smaranda Muresan. 2021. Don't go far off: An empirical study on neural poetry translation",
                "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv",
                "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Preprint",
                "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv",
                "Aitor Ormazabal, Mikel Artetxe, Manex Agirrezabal, Aitor Soroa, and Eneko Agirre. 2022. Poelm: A meter-and rhyme-controllable language model for unsupervised poetry generation. arXiv preprint arXiv",
                "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's transformers: State-of-the-art natural language processing. arXiv preprint arXiv",
                "Ricardo Campos, V\u00edtor Mangaravite, Arian Pasquali, Al\u00edpio M\u00e1rio Jorge, C\u00e9lia Nunes, and Adam Jatowt. 2018. Yake! collection-independent automatic keyword extractor",
                "Katherine Elkins and Jon Chun. 2020. Can gpt-3 pass a writer's turing test? Journal of Cultural Analytics",
                "Marjan Ghazvininejad, Xing Shi, Yejin Choi, and Kevin Knight. 2016. Generating topical poetry",
                "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv",
                "Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. arXiv preprint arXiv",
                "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research",
                "Ben Swanson, Kory Mathewson, Ben Pietrzak, Sherol Chen, and Monica Dinalescu. 2021. Story centaur: Large language model few shot learning as a creative writing tool",
                "Gwern Branwen. 2020. Gpt-3 creative fiction",
                "Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. 2021. Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv",
                "Yufei Tian and Nanyun Peng. 2022. Zero-shot sonnet generation with discourse-level planning and aesthetics features",
                "Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation",
                "Swaroop Mishra and Elnaz Nouri. 2022. Help me think: A simple prompting strategy for non-experts to create customized content with models. arXiv preprint arXiv",
                "Melissa Roemmele and Andrew S. Gordon. 2015. Creative help: A story writing assistant",
                "David Uthus, Maria Voitovich, RJ Mical, and Ray Kurzweil. 2019. First steps towards collaborative poetry generation",
                "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. arXiv preprint arXiv",
                "Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, and Ann Yuan. 2021. Wordcraft: a human-ai collaborative editor for story writing. arXiv preprint arXiv",
                "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation",
                "Quanze Chen, Chenyang Lei, Wei Xu, Ellie Pavlick, and Chris Callison-Burch. 2014. Poetry of the crowd: A human computation algorithm to convert prose into rhyming verse",
                "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models",
                "Chin-yew Lin and Marina Rey. 2004. Looking for a few good metrics: ROUGE and its evaluation",
                "Tim Van de Cruys. 2020. Automatic poetry generation from prosaic text",
                "Piotr Mirowski, Kory W Mathewson, Jaylen Pittman, and Richard Evans. 2022. Co-writing screenplays and theatre scripts with language models: An evaluation by industry professionals. arXiv preprint arXiv",
                "Arthur M Jacobs. 2018. The gutenberg english poetry corpus: exemplary quantitative narrative analyses. Frontiers in Digital Humanities",
                "Ricardo Campos, V\u00edtor Mangaravite, Arian Pasquali, Al\u00edpio Jorge, C\u00e9lia Nunes, and Adam Jatowt. 2020. Yake! keyword extraction from single documents using multiple local features. Information Sciences",
                "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions",
                "Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models",
                "Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to fill in the blanks",
                "Mina Lee, Percy Liang, and Qian Yang. 2022. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities",
                "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration",
                "Vishakh Padmakumar, Leonard Lausen, Miguel Ballesteros, Sheng Zha, He He, and George Karypis. 2022. Exploring the role of task transferability in large-scale multi-task learning",
                "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv",
                "Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization. arXiv",
                "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. arXiv",
                "David Uthus, Maria Voitovich, and R.J. Mical. 2022. Augmenting poetry composition with Verse by Verse",
                "Vishakh Padmakumar and He He. 2022. Machine-in-the-loop rewriting for creative image captioning",
                "Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D. Manning. 2019. Do massively pretrained language models make better storytellers? In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) , pages 843-861, Hong Kong, China. Association for Computational Linguistics",
                "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. KDD '20, page 3505-3506, New York, NY, USA. Association for Computing Machinery",
                "Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A. Smith. 2018. Creative writing with a machine in the loop: Case studies on slogans and stories",
                "Wanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022. Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision"
            ],
            "related work": "5Related WorkCollaborative WritingThe key challenge in collaborative writing is to understand user intent so as to provide timely and useful suggestions. Prior work in story writingRoemmele and Gordon (2015) ; Clark et al. (2018) presented sentence-level continuations at locations specified by a user.Akoury et al. (2020) ; Lee et al. (2022) took this a step further providing users with a paragraph of text which they could further edit in story writing and argumentative writing tasks. However, model suggestions of this autocomplete nature were not always helpful, as they often diverged from the user intentClark et al. (2018) resulting in only a fraction of generated text being retainedAkoury et al. (2020) . Instead of providing a machine-written draft,Padmakumar and He (2022) showed that having the model rewrite text only at locations specified by the user results in more helpful suggestions in the task of creative image captioning.We focus on the task of collaborative poem writing, which adds an additional challenge as useful suggestions need to satisfy several lexical and form constraints (rhyme, meter, sound) . Past work for this task has used retrieval to provide suggestions for substitutions at the word and phrase levelChen et al. (2014) or verses that follow different stylesUthus et al. (2022) , but these are unable to dynamically generate novel text. In our work, we utilize large language models to generate text that satisfies the various constraints specified by users, with the added benefit that they can spell out these using natural language instructions. Concurrent work has also shown that large language models can help users write scripts and screenplaysMirowski et al. (2022) and longer storiesYang et al. (2022) by generating text that incorporates structural context via prompt chaining.Interaction with UsersRecent work in NLP has highlighted the success of generative large language models as interaction interfaces for the task of creative writing. Finetuning models on tasks verbalised as instructions has shown good generalization to unseen instructionsWei et al. (2021) ; Sanh et al. (2021) ; Mishra et al. (2021) ; Chung et al. (2022) . In our work, we focus on a suite of instructions specific to creative writing and additionally evaluate the instruction-tuning setup with real users who iteratively ask for suggestions in natural language.In addition to fine-tuning models on instructions, large language models are also able to generalize to unseen tasks in a few-shot manner when the task is specified as part of the prompt in natural languageOuyang et al. (2022) .Reif et al. (2022) present a prompting method which performs style transfer in a zero-shot or few-shot manner with only a natural language instruction describing the target style without model fine-tuning or exemplars in the target style. Unlike most of the recent work that prompts large language models to elicit contentCoenen et al. (2021) frame collaborative writing as a conversation between a human and a LLM-based dialog system and show how the spontaneous utilities of conversation support a variety of interactions. More recentlyMishra and Nouri (2022) propose a prompting strategy where they ask GPT3 specific questions about mood, tone, occasion, or theme for the task of poem generation by using GPT3 as an interaction interface.",
            "date": "2022"
        },
        "topic": "Instruction Tuning for LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
                "arxiv_id": "2402.04333",
                "subtitles": [
                    "Curating high-quality instruction tuning data",
                    "Coresets and gradient-based data selection",
                    "Data attribution and influence functions"
                ],
                "reference": [
                    "Free Dolly: Introducing the world's first truly open instruction-tuned LLM",
                    "Finetuned language models are zero-shot learners",
                    "OpenAssistant conversations-democratizing large language model alignment",
                    "Active learning for convolutional neural networks: A core-set approach",
                    "What neural networks memorize and why: Discovering the long tail via influence estimation",
                    "Learning from less data: A unified data subset selection and active learning framework for computer vision",
                    "In-context alignment: Chat with vanilla language models before fine-tuning",
                    "Instruction mining: High-quality instruction data selection for large language models",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Don't stop pretraining: Adapt language models to domains and tasks",
                    "Prioritized training on points that are learnable, worth learning, and not yet learnt",
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Gradient surgery for multi-task learning",
                    "Coresets for data-efficient training of machine learning models",
                    "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
                    "Deep learning on a data diet: Finding important examples early in training",
                    "Post-hoc interpretability for neural nlp: A survey",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Orca: Progressive learning from complex explanation traces of gpt",
                    "Understanding in-context learning via supportive pretraining data",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Trivial or impossible dichotomous data difficulty masks model differences (on imagenet and beyond",
                    "Understanding black-box predictions via influence functions",
                    "Predicting performance for natural language processing tasks",
                    "Selection via proxy: Efficient data selection for deep learning",
                    "Dsdm: Model-aware dataset selection with datamodels",
                    "Trak: Attributing model behavior at scale",
                    "Stanford alpaca: An instruction-following llama model",
                    "Coresets and sketches",
                    "One shot learning as instruction data prospector for large language models",
                    "Moderate coreset: A universal method of data selection for real-world data-efficient deep learning",
                    "The flan collection: Designing data and methods for effective instruction tuning",
                    "Estimating training data influence by tracing gradient descent",
                    "An empirical study of example forgetting during deep neural network learning",
                    "Optimizing data usage via differentiable rewards",
                    "Grad-match: Gradient matching based data subset selection for efficient deep model training",
                    "Studying large language model generalization with influence functions",
                    "Data selection for language models via importance resampling",
                    "If influence functions are the answer, then what is the question",
                    "Mods: Model-oriented data selection for instruction tuning",
                    "The unlocking spell on base LLMs: Rethinking alignment via in-context learning",
                    "WizardLM: Empowering large language models to follow complex instructions",
                    "Beyond neural scaling laws: beating power law scaling via data pruning",
                    "Data diversity matters for robust instruction tuning",
                    "Datamodels: Predicting predictions from training data",
                    "Influence functions in deep learning are fragile",
                    "Skill-it! a data-driven skills framework for understanding and training language models",
                    "LIMA: Less is more for alignment",
                    "Retrieve: Coreset selection for efficient and robust semi-supervised learning",
                    "The influence curve and its role in robust estimation"
                ],
                "related_work": "7Related WorkCurating high-quality instruction tuning data.Using high-quality instruction tuning data can dramatically improve base LLMs. Instruction tuning data is generally of two types: (1) task-based datasets curated from traditional NLP tasks(Wang et al.,2022; Sanh et al.,2022; Wei et al.,2022b; Longpre et al.,2023) , and (2) open-ended instruction following datasets, broadly covering a wide range of topics(Taori et al.,2023; Conover et al.,2023; K\u00f6pf et al.,2023; Xu et al.,2023; Mukherjee et al.,2023; Zhou et al.,2023; Ding et al.,2023) . Increasing data quality and diversity instead of quantity has been shown to more effectively induce instruction following abilities(Cao et al.,2023; Chen et al.,2023a; Bukharin & Zhao,2023; Du et al.,2023; Liu et al.,2023; Li et al.,2023) . Moreover,(Han,2023; Lin et al.,2023a) have demonstrated that in-context learning alone can significantly advance instruction following. Our work adds to this narrative by selecting high-quality, relevant data from a large pool of available datasets to induce a particular capability in the model.Coresets and gradient-based data selection.Data selection has been viewed as a coreset selection problem(Phillips,2017) , which aims to find a subset of training examples that induces performance similar to training on the full dataset(Toneva et al.,2018; Sener & Savarese,2018; Coleman et al.,2019; Kaushal et al.,2019; Xia et al.,2020; Mirzasoleiman et al.,2020; Feldman & Zhang,2020; Killamsetty et al.,2021a,b; Paul et al.,2021; Meding et al.,2021; Mindermann et al.,2022; Sorscher et al.,2022; Xia et al.,2023) . These works focus on in-domain coreset selection, whereas our work involves transfer learning. Several prior works use pre-defined notions of useful data(Gururangan et al.,2020; Chen et al.,2023b) or n-gram features(Xie et al.,2023b) to select pre-training examples, but LESS more closely resemblesMirzasoleiman et al. (2020) ; Wang et al. (2020) ; Yu et al. (2020b) ; Killamsetty et al. (2021a) in its reliance on gradient information. Our work is closest toHan et al. (2023) , which used model gradients to select pre-training data to improve in-context learning performance. However, our offline approach enables extremely efficient adaptation to new downstream tasks.Data attribution and influence functions.Our formulation of influence has been used in identifying mislabeled examples(Pruthi et al.,2020) , analyzing memorization effects(Feldman & Zhang,2020) , and deriving various interpretability insights(Madsen et al.,2022) , but it does not permit counterfactual reasoning. On the other hand, influence functions(Hampel,1974) can counterfactually reason about model behaviors and training data(Koh & Liang,2017) , but some studies have found limitations(Bae et al.,2022) in its robustness(Basu et al.,2020) and effectiveness(Ilyas et al.,2022) . In the LLM setting, influence functions are computationally expensive(Grosse et al.,2023) , though concurrent work inEngstrom et al. (2024) proposed using an efficient influence function estimation(Park et al.,2023) to select pre-training data (seeAppendixGfor a detailed comparison of influence functions and our influence formulation) . Our findings suggest that first-order influence approximations(Pruthi et al.,2020) are effective for data selection in transfer learning settings(Han et al.,2023) .",
                "abstract": "Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application."
            },
            {
                "name": "SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model",
                "arxiv_id": "2401.09712",
                "subtitles": [
                    "Remote Sensing Vision-Language Tasks",
                    "LLMs for Vision-Language",
                    "Vision-Language Instruction Tuning"
                ],
                "reference": [
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Overcoming language bias in remote sensing visual question answering via adversarial training",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
                    "Parameter-efficient transfer learning for remote sensing image-text retrieval",
                    "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
                    "Gpt-4 technical report",
                    "Chatgpt",
                    "Rsgpt: A remote sensing vision language model and benchmark",
                    "Llama: Open and efficient foundation language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Visual instruction tuning",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Capera: Captioning events in aerial videos",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Earthnets: Empowering AI in earth observation",
                    "Rsvg: Exploring data and models for visual grounding on remote sensing data"
                ],
                "related_work": "2Related Work2.1Remote Sensing Vision-Language TasksRecently, there has been significant attention on multi-modal tasks in remote sensing vision-language understandingYuanet al.(2023a) . Traditional image-level tasks, such as RS image captioning and RS VQA, have made significant progressYuanet al.(2023b) . Emerging region-level and spatio-temporal tasks, such as RSVGZhanet al.(2023a) and UAV video captioningBashmalet al.(2023) , have raised novel challenges and garnered increasing interest. Despite the availability of numerous state-of-the-art methods capable of performing these tasksXionget al.(2022) , they are typically trained on a specific dataset to perform a specific task. This work primarily focuses on unifying the diverse RS vision-language tasks.2.2LLMs for Vision-LanguageWith the rise of advanced LLMs, ChatGPTOpenAI (2022) , LLaMATouvronet al.(2023a) , GPT-4OpenAI (2023) , and VicunaChianget al.(2023) have shown remarkable abilities in various language tasks. BLIP-2Liet al.(2023) extends LLMs into the realm of multimodal by connecting the frozen LLM with a visual encoder via Q-Former. Some approaches employ the simplest linear layer as a mediator to link LLMs and visual encoders, achieving notable success, such as LLaVALiuet al.(2023) , MiniGPT-4Zhuet al.(2023) . Recent contributions from VisionLLMWanget al.(2023) , ShikraChenet al.(2023b) , and MiniGPT-v2Chenet al.(2023a) further substantiate that spatial coordinates in visual grounding tasks can be effectively handled in language form by LLM. These approaches showcase the potential and versatility of LLM for seamless integration of vision and language modalities. The application and research on generalized MLLMs in RS have been comparatively limited. RSGPTHuet al.(2023) was the first attempt, but it could only handle coarse-grained tasks of image-text and doesn't support open-ended multi-tasks and multi-task conversations.2.3Vision-Language Instruction TuningThe purpose of instruction tuning is to enhance the instruction following ability of the model. Drawing inspiration from LLMs in instruction tuning, LLaVALiuet al.(2023) fine-tunes the model based on synthetic multi-modal instruction-following data. Instruct-BLIPDaiet al.(2023) collects a larger set of instruction data, resulting in improved performance for BLIP. These methods primarily focus on image-level coarse-grained tasks, and cannot effectively address fine-grained perception challenges. Recent VisionLLMWanget al.(2023) , ShikraChenet al.(2023b) , and MiniGPT-v2Chenet al.(2023a) further utilize instruction-following data to tackle fine-grained visual perception tasks such as visual grounding, region caption, and object detection. These methods demonstrate the potential of instruction tuning strategies to mine the LLM's ability to understand and respond to multi-grained multi-modal instructions. Our method aims to provide a unified framework for handling open-ended RS vision-language tasks and develop multi-task conversational capability via instruction tuning.",
                "abstract": "Large language models (LLMs) have recently been extended to the vision-language realm, obtaining impressive general multi-modal capabilities. However, the exploration of multi-modal large language models (MLLMs) for remote sensing (RS) data is still in its infancy, and the performance is not satisfactory. In this work, we introduce SkyEyeGPT, a unified multi-modal large language model specifically designed for RS vision-language understanding. To this end, we meticulously curate an RS multi-modal instruction tuning dataset, including single-task and multi-task conversation instructions. After manual verification, we obtain a high-quality RS instruction-following dataset with 968k samples. Our research demonstrates that with a simple yet effective design, SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules. Specifically, after projecting RS visual features to the language domain via an alignment layer, they are fed jointly with task-specific instructions into an LLM-based RS decoder to predict answers for RS open-ended tasks. In addition, we design a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability at different granularities. Experiments on 8 datasets for RS vision-language tasks demonstrate SkyEyeGPT's superiority in image-level and region-level tasks, such as captioning and visual grounding. In particular, SkyEyeGPT exhibits encouraging results compared to GPT-4V in some qualitative tests. The online demo, code, and dataset will be released inthis https URL."
            },
            {
                "name": "LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding",
                "arxiv_id": "2404.05225",
                "subtitles": [
                    "Pre-trained models for document understanding",
                    "LLMs/MLLMs for document understanding"
                ],
                "reference": [
                    "StructuralLM: Structural pre-training for form understanding",
                    "Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents",
                    "Geolayoutlm: Geometric pre-training for visual information extraction",
                    "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
                    "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
                    "Bi-vldoc: Bidirectional vision-language modeling for visually-rich document understanding",
                    "End-to-end document recognition and understanding with dessurt",
                    "DocFormer: End-to-end transformer for document understanding",
                    "SelfDoc: Self-supervised document representation learning",
                    "Layoutlmv3: Pre-training for document ai with unified text and image masking",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Exploring ocr capabilities of GPT-4V(ision) : A quantitative and in-depth evaluation",
                    "Unified pretraining framework for document understanding",
                    "Improved baselines with visual instruction tuning",
                    "Introducing chatgpt",
                    "Lmdx: Language model-based document information extraction and localization",
                    "Visual instruction tuning, 2023b",
                    "Lilt: A simple yet effective language-independent layout transformer for structured document understanding",
                    "The dawn of lmms: Preliminary explorations with GPT-4V(ision",
                    "Layoutlmv2: Multi-modal pre-training for visually-rich document understanding",
                    "Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding",
                    "Attention where it matters: Rethinking visual document understanding with selective region concentration",
                    "Gpt-4 technical report",
                    "LayoutLM: Pre-training of text and layout for document image understanding",
                    "Structext: Structured text understanding with multi-modal transformers",
                    "Ocr-free document understanding transformer",
                    "Going full-tilt boogie on document understanding with text-image-layout transformer",
                    "Ernie-layout: Layout knowledge enhanced pre-training for visually-rich document understanding",
                    "On the hidden mystery of ocr in large multimodal models, 2023c",
                    "mplug-docowl: Modularized multimodal large language model for document understanding",
                    "LayoutMask: Enhance text-layout interaction in multi-modal pre-training for document understanding",
                    "Vision grid transformer for document layout analysis",
                    "Structextv2: Masked visual-textual prediction for document image pre-training",
                    "mplug-owl: Modularization empowers large language models with multimodality, 2023b"
                ],
                "related_work": "2Related WorksPre-trained models for document understanding.Document pre-trained models have demonstrated the effectiveness of layout information in document understanding[54,55,17,41,25,23,26,2,12,52,13,16,59,38,32,8,20,10,22,5]. As a pioneer, LayoutLM[54]is the first to encode spatial coordinates of text for layout representation learning in pre-training documents. The following works[55,17,25,23,26,2,12,52,13,16,59,38,32,8]then joint text, layout and images in document pre-training by combining visual models as document image encoders with the text and layout transformers, and various works[20,10,22,5]start to explore pre-training end-to-end models for document understanding. These studies have achieved significant advancements in document understanding by exploring various model architectures[41,25,26,2,12,52,13,20,10,59,38,8]and attention mechanisms[55,17,16]for modeling layout information. These methods also have proposed layout-related pre-training tasks that have been demonstrated to be highly effective in document understanding tasks. For instance, tasks like masked vision-language modeling[13,55,17], where the model is required to generate the original text corresponding to the randomly masked text in the document; position masking[31,51], involving the randomly position masking and subsequent recovery of position information in the document; geometric pre-training[26,32], focusing on learning direction, distance, etc.; and layout-aware generation tasks[22,5], aiming to make the model generate structured text with layout information. However, due to the necessity of fine-tuning with annotated data for downstream tasks, these efforts face challenges in extending to zero-shot document understanding.LLMs/MLLMs for document understanding.Recently, LLMs like ChatGPT[35]and MLLMs like GPT-4[36,56]have demonstrated remarkable zero-shot performance across a wide range of NLP/CV tasks. Leveraging LLMs/MLLMs for zero-shot document understanding has also shown promising progress[39,30,60,57,3,45].Perot et al.[39]explore the use of LLMs for document visual information extraction, emphasizing the importance of the document layout. LLaVAR[60]which extends LLaVA[29,28]to the document domain is pre-trained by generating plain text in the document image. During SFT, it is trained by document-related instructions which are generated by GPT-4. Expanding on mPLUG-Owl[58], mPLUG-DocOwl[57]is trained using publicly available datasets for document understanding. It includes tasks like document-level image captioning, direct information extraction, and direct document VQA. Moreover, Qwen-VL[3]proposes a general MLLM that performs well on document understanding tasks, utilizing document-level pre-training and direct VQA for SFT. Though existing LLMs/MLLMs have shown promising results in document understanding, their limited focus on document layout in pre-training and SFT has hindered their ability to achieve higher accuracy in zero-shot document understanding and better interpretability.",
                "abstract": "Recently, leveraging large language models (LLMs) or multimodal large language models (MLLMs) for document understanding has been proven very promising. However, previous works that employ LLMs/MLLMs for document understanding have not fully explored and utilized the document layout information, which is vital for precise document understanding. In this paper, we propose LayoutLLM, an LLM/MLLM based method for document understanding. The core of LayoutLLM is a layout instruction tuning strategy, which is specially designed to enhance the comprehension and utilization of document layouts. The proposed layout instruction tuning strategy consists of two components: Layout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture the characteristics of document layout in Layout-aware Pre-training, three groups of pre-training tasks, corresponding to document-level, region-level and segment-level information, are introduced. Furthermore, a novel module called layout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on regions relevant to the question and generate accurate answers. LayoutCoT is effective for boosting the performance of document understanding. Meanwhile, it brings a certain degree of interpretability, which could facilitate manual inspection and correction. Experiments on standard benchmarks show that the proposed LayoutLLM significantly outperforms existing methods that adopt open-source 7B LLMs/MLLMs for document understanding. The training data of the LayoutLLM is publicly available atthis https URL"
            },
            {
                "name": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
                "arxiv_id": "2403.14399",
                "subtitles": [
                    "Translation-Tailored LLMs",
                    "Unlikelihood Training"
                ],
                "reference": [
                    "Understanding by understanding not: Modeling negation in language models",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Don't say that! making inconsistent dialogue unlikely with unlikelihood training",
                    "Parrot: Translating during chat using large language models",
                    "Uncertainty-aware unlikelihood learning improves generative aspect sentiment quad prediction",
                    "Neural text generation with unlikelihood training",
                    "Vega-mt: The jd explore academy machine translation system for wmt",
                    "Gpt-4 technical report",
                    "A paradigm shift in machine translation: Boosting translation performance of large language models",
                    "Tim: Teaching large language models to translate with comparison",
                    "Beyond [CLS] through ranking by generation",
                    "Multilingual denoising pre-training for neural machine translation",
                    "Instruction position matters in sequence generation with large language models"
                ],
                "related_work": "6Related WorkTranslation-Tailored LLMsDue to the huge cost to call the state-of-the-art LLMs, such as GPT-4OpenAI (2023) , there is a need to investigate how to effectively fit a smaller LLM into specific tasks, e.g., machine translation. Note that although there are some powerful sequence-to-sequence style large-scale pretrained machine translation modelsLiu et al. (2020) ; Zan et al. (2022) , this paper mainly focuses on the decoder-only LLMs due to their flexible interaction modes and rich world knowledge. In the field of LLMs-based translation, various approaches have been proposed to optimize translation performance. ParrotJiao et al. (2023) proposes to fine-tune model on machine translation data with a hint incorporating extra requirements to regulate the translation process. TIMZeng et al. (2023) introduces translation samples in comparisons to compute additional preference loss for regularization, exhibiting superior translation ability in both supervised and zero-shot directions. ALMAXu et al. (2023) proposes a two-stage approach that first fine-tunes on monolingual data of downstream languages followed by fine-tuning on high-quality translation data, which achieves significant improvement of translation quality.Liu et al. (2023) presents the position of instruction matters, that just moving the location of the instruction closer to the output can alleviate the instruction forgetting issue.In contrast, we focus on the off-target problem of zero-shot translation, where the model fails to follow translation instructions, generating sequences not in the target language. Additionally, we show how instruction-conflicting samples can enhance the influence of instruction thus mitigating the off-target problem.Unlikelihood TrainingUnlikelihood trainingWelleck et al. (2019) aims to force the mode to assign a lower probability for unlikely tokens. This method has been further explored in dialog tasks byLi et al. (2020) , who demonstrated its effectiveness in generating more consistent and coherent human-like dialog.Nogueira dos Santos et al. (2020) used the unlikelihood loss for ranking and proposed a generative information retrieval approach.Hosseini et al. (2021) proposed the combination of an unlikelihood objective with a reference-based setup for input sentences to model negation with pretrained BERTKenton and Toutanova (2019) .Hu et al. (2023) take the semantic-similar or ambiguous tokens as negative information and acquire it via inherent uncertainty for the ASQP task.In this work, we take instances in which the translation pairs conflict with the instruction as the negative sample for zero-shot translation. Furthermore, we consider the new case that enhances the ability of LLMs to better follow translation instructions and generate translations in the correct language.",
                "abstract": "Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model's general task performance on AlpacaEval. Code and models will be released at \\url{this https URL}."
            },
            {
                "name": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data",
                "arxiv_id": "2409.03810",
                "subtitles": [
                    "Code Instruction Tuning",
                    "Data Selection for Instruction Tuning"
                ],
                "reference": [
                    "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks",
                    "Octopack: Instruction tuning code large language models",
                    "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
                    "Magicoder: Source code is all you need",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Code alpaca: An instruction-following llama model for code generation",
                    "Towards scalable automated alignment of llms: A survey",
                    "How abilities in large language models are affected by supervised fine-tuning data composition",
                    "One-shot learning as instruction data prospector for large language models",
                    "Lima: Less is more for alignment",
                    "Self-play with execution feedback: Improving instruction-following capabilities of large language models",
                    "We-math: Does your large multimodal model achieve human-like mathematical reasoning",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "Making language models better tool learners with execution feedback",
                    "Autocoder: Enhancing code large language model with AIEV-Instruct",
                    "Dolphcoder: Echo-locating code large language models with diverse and multi-objective instruction tuning",
                    "Benchmarking benchmark leakage in large language models",
                    "Opencodeinterpreter: Integrating code generation with execution and refinement",
                    "Teaching large language models to self-debug",
                    "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models"
                ],
                "related_work": "5Related WorkCode Instruction Tuning.Code instruction tuning is a necessary step for models to accurately understand human instructions and generate relevant code responses.Xu et al. (2023) apply the Evol-Instruct methodXu et al. (2023) to Code-AlpacaChaudhary (2023) dataset and obtain a instruction dataset with high complexity.Muennighoff et al. (2023) take git commits as natural instruction data. They collect 4TB git commits across 350 programming language.Wang et al. (2024b) propose Diverse Instruction Tuning and Multi-Objective Tuning to train Dolphcoder, which proves that more diverse code solutions and code evaluation instruction data are also beneficial for code generation tasks. Considering Evol-Instruct depends on a seed instruction data which is less diversity,Wei et al. (2023) proposes OSS-Instruct, which leverages open-source code cnippets to generate high-diversity instructions. They also propose Magicoder-Evol-Instruct dataset and train Magicoder-S, which is the first 7B model to exceed 70% on HumanEval Pass@1. However, we find this dataset suffers from serious data contamination(Dong et al.,2024b; Xu et al.,2024) . Motivated by various works with execution feedback(Cao et al.,2024; Le et al.,2022; Chen et al.,2023; Qiao et al.,2023,2024; Dong et al.,2024a) , OpenCodeInterpreterZheng et al. (2024) and AutoCoderLei et al. (2024) leverages GPT-4 and Code Interpreter as code feedback to generate multi-turn instruction data which instruct model to refine incorrect code snippets acrroding to feedback information.Data Selection for Instruction Tuning.While instruction fine-tuning primarily relies on a large volume of data, research such as LIMA(Zhou et al.,2024) indicates that data quality is more critical than quantity.Li et al. (2024a) proposes a novel metricInstructionFollowingDifficulty(IFD) to assess the challenge of responding to specific instructions.Li et al. (2024b) harnesses the disparity between one-shot and zero-shot scores to calculate a definitive 'gold score' for each instruction.Kung et al. (2023) present Active Instruction Tuning, which introduces the concept of Prompt Uncertainty. Tasks that exhibit higher Prompt Uncertainty are prioritized for instruction tuning. Furthermore,Lu et al. (2023) introduce an automated instruction tagging method (INSTAG) , which employs ChatGPT to generate detailed, open-ended labels for instructions. It starts by sorting instructions in descending order of label count and then iteratively adds instructions to a subset based on the uniqueness of their labels. Deita(Liu et al.,2024) integrates a multifaceted approach for selecting instruction data, focusing on complexity, quality, and diversity. Utilizing the WizardLM technique, ChatGPT is employed to augment instructions, which are then evaluated for both complexity and quality by specially trained scorers.",
                "abstract": "Recently, there has been a growing interest in studying how to construct better code instruction tuning data. However, we observe Code models trained with these datasets exhibit high performance on HumanEval but perform worse on other benchmarks such as LiveCodeBench. Upon further investigation, we find that many datasets suffer from severe data leakage. After cleaning up most of the leaked data, some well-known high-quality datasets perform poorly. This discovery reveals a new challenge: identifying which dataset genuinely qualify as high-quality code instruction data. To address this, we propose an efficient code data pruning strategy for selecting good samples. Our approach is based on three dimensions: instruction complexity, response quality, and instruction diversity. Based on our selected data, we present XCoder, a family of models finetuned from LLaMA3. Our experiments show XCoder achieves new state-of-the-art performance using fewer training data, which verify the effectiveness of our data strategy. Moreover, we perform a comprehensive analysis on the data composition and find existing code datasets have different characteristics according to their construction methods, which provide new insights for future code LLMs. Our models and dataset are released inthis https URL"
            },
            {
                "name": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
                "arxiv_id": "2402.10176",
                "subtitles": [
                    "Mathematical Reasoning and LLMs",
                    "Knowledge Distillation via Synthetic Data"
                ],
                "reference": [
                    "Textbooks Are All You Need",
                    "Llemma: An Open Language Model For Mathematics",
                    "Galactica: A Large Language Model for Science",
                    "TinyGSM: achieving> 80% on GSM8k with small language models",
                    "Solving Quantitative Reasoning Problems with Language Models",
                    "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
                    "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                    "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                    "TinyStories: How Small Can Language Models Be and Still Speak Coherent English",
                    "MAmmoTH: Building math generalist models through hybrid instruction tuning",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Textbooks Are All You Need II: phi-1.5 technical report",
                    "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                    "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
                    "Orca: Progressive Learning from Complex Explanation Traces of GPT",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                    "PAL: Program-aided Language Models",
                    "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
                ],
                "related_work": "6Related WorkMathematical Reasoning and LLMs.Recently, a plethora of work has been done on enhancing the mathematical reasoning capabilities of LLMs. Inference techniques such as Chain-of-Thought(Wei et al.,2022) , its programmatic counterpart, Program of Thought(Gao et al.,2023; Chen et al.,2023b) , Self-Consistency(Wang et al.,2023) , and Self-Verification(Zhou et al.,2024) have been shown to significantly improve the reasoning capabilities of LLMs without any further training.Pretraining language models on math-heavy content has resulted in foundation LLMs such as Minerva(Lewkowycz et al.,2022) , Galactica(Taylor et al.,2022) , and Llemma(Azerbayev et al.,2023) with stronger mathematical skills out-of-the-box. A more direct approach of dataset-specific training doesinstruction fine-tuningon problem-solution pairs derived from math reasoning datasets. Our work falls in this latter category and bears similarity with recent work such as RFT(Yuan et al.,2023) , ToRA(Gou et al.,2024) , MAmmoTH(Yue et al.,2024) , MetaMath(Yu et al.,2024) and MathCoder(Wang et al.,2024) . We differ from the previous work along one factor or a combination of the following factors: (a) reliance on GPT-3.5/4, (b) solution format, and (c) use of ground truth text solution in synthesizing code-based solutions.Knowledge Distillation via Synthetic Data.Recent work exploring the use of targetedsyntheticdata generated by large foundation models for pre-training/instruction tuning smaller LLMs has led to tremendous progress in reasoning skills of these smaller LLMs(Gunasekar et al.,2023; Li et al.,2023; Eldan and Li,2023; Mukherjee et al.,2023; Xu et al.,2023; Liu et al.,2023) .",
                "abstract": "Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license."
            },
            {
                "name": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
                "arxiv_id": "2402.00530",
                "subtitles": [
                    "Instruction Tuning Data Selection",
                    "Small Model Proxies for Large Models"
                ],
                "reference": [
                    "FAMIE: A fast active learning framework for multilingual information extraction",
                    "Finetuned language models are zero-shot learners",
                    "Scalable transfer learning with expert models",
                    "Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language",
                    "The flan collection: Designing data and methods for effective instruction tuning",
                    "Selection via proxy: Efficient data selection for deep learning",
                    "Data diversity matters for robust instruction tuning",
                    "REV: Information-theoretic evaluation of free-text rationales",
                    "Multitask prompted training enables zero-shot task generalization",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
                    "Mmc: Advancing multimodal chart understanding with large-scale instruction tuning",
                    "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
                    "Mods: Model-oriented data selection for instruction tuning",
                    "Alpagasus: Training a better alpaca with fewer data"
                ],
                "related_work": "7Related Work7.1Instruction Tuning Data SelectionInstruction tuningWei et al. (2022) ; Sanh et al. (2022) ; Longpre et al. (2023a) ; Liu et al. (2023a) is a widely-used training paradigm to equip LLMs with the instruction-following ability. To further select the data for more efficient instruction tuning, existing automatic data selection methods mainly utilize extra LLMs for the selection.Lu et al. (2023) utilizes proprietary chatGPT to tag the instruction data to ensure diversity and complexity.Chen et al. (2023b) utilizes proprietary LLMs chatGPT and Claude2 to assess the quality of the instruction data, generating both ratings and explanations.Du et al. (2023) andBukharin and Zhao (2023) utilize an extra reward model to assess the quality of data and utilize these scores as a part of their method.Li et al. (2023b) firstly proposes a self-guided method in which no extra LLMs are utilized but still needs to calculate Instruction-Following Difficulty (IFD) scores based on the original pre-trained LLM. Though effective, these methods overly rely on large language models and are too time-consuming to put into practical use.7.2Small Model Proxies for Large ModelsThe use of proxy models is increasingly recognized in machine learning, particularly when resources are constrained or there is a limited understanding of the original model's architecture.Chen et al. (2023a) andHase et al. (2020) demonstrate the utility of lightweight proxy models in evaluating free-text rationales. Similarly,Puigcerver et al. (2021) leverages embeddings from expert models with a k-nearest neighbors classifier to simplify the training of more complex systems.Coleman et al. (2020) and the FAMIENguyen et al. (2022) apply downscaled proxy models in fields like image classification and information extraction, utilizing techniques such as layer removal and knowledge distillation for aligning these proxies with larger models.Burns et al. (2023) explores the concept of enhancing larger models through weak supervision, and training on weaker model labels.",
                "abstract": "Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach."
            },
            {
                "name": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
                "arxiv_id": "2401.01854",
                "subtitles": [
                    "Cross-lingual Transfer",
                    "Multilingual Instruction Tuning"
                ],
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "English intermediate-task training improves zero-shot cross-lingual transfer too",
                    "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
                    "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
                    "Cross-lingual ability of multilingual bert: An empirical study",
                    "Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability",
                    "Crosslingual generalization through multitask finetuning",
                    "Plug: Leveraging pivot language in cross-lingual instruction tuning",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Stanford alpaca: An instruction-following llama model",
                    "How do languages influence each other? studying cross-lingual data sharing during LM fine-tuning",
                    "BERT: Pre-training of deep bidirectional transformers for language understanding",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Turning english-centric llms into polyglots: How much multilinguality is needed",
                    "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
                    "Monolingual or multilingual instruction tuning: Which makes a better alpaca",
                    "Cross-task generalization via natural language crowdsourcing instructions",
                    "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback",
                    "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
                    "mT5: A massively multilingual pre-trained text-to-text transformer",
                    "Unsupervised cross-lingual representation learning at scale",
                    "Llama: Open and efficient foundation language models",
                    "Emerging cross-lingual structure in pretrained language models",
                    "How multilingual is multilingual BERT",
                    "Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond",
                    "LoRA: Low-rank adaptation of large language models",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "5Related workCross-lingual TransferThe success of the pre-training-fine-tuning paradigmDevlin et al. (2019) ignited a new line of work on cross-lingual transfer.Pires et al. (2019) andWu and Dredze (2019) showed that the multilingual variant of BERT can be fine-tuned on a specific task in one language and preform this task on another language, andArtetxe and Schwenk (2019) reported similar findings with a Recurrent Neural Network.Conneau et al. (2020a) introduced XLM-R, a multilingual pre-trained encoder with strong cross-lingual abilities.Phang et al. (2020) showed that intermediate training on an English task improves XLM-R's transfer across languages further, andPfeiffer et al. (2020) suggested an adapter-based framework to improve cross-lingual and task generalization.Hu et al. (2020) proposed a benchmark for cross-lingual generalization consists of 40 languages across 9 NLP tasks.K et al. (2020) found that the depth of the network matters for cross-lingual transfer, andConneau et al. (2020b) showed that parameter sharing is more important than shared vocabulary.Choenni et al. (2023) delved into the influence of specific examples from the training data on the performance in other languages, andMalkin et al. (2022) investigated how pre-training BERT-based models using different language pairs affects cross-lingual downstream performance. Going beyond encoder-only models,Xue et al. (2021) proposed mT5, a multilingual variant of T5Raffel et al. (2020) , and showed the significance of model scaling for cross-lingual transfer in generation tasks.Ye et al. (2023) explored trasferability in English-centric modelsTouvron et al. (2023a) using four tasks.In contrast to most cross-lingual transfer literature that is focused on task-specific fine-tuning, we explore trends of cross-lingual generalization for general-purpose instruction-following LLMs.Multilingual Instruction TuningInitially, works on instruction tuningMishra et al. (2022) ; Wei et al. (2022) ; Sanh et al. (2022) focused on cross-task generalization in English. Subsequently, a large body of work was dedicated to multilingual instruction tuning.Muennighoff et al. (2023) found that tuning models with English datasets enables zero-shot cross-lingual abilities to new languages. The authors also found that this holds for languages that the model has never intentionally seen during pre-training, and that multilingual training improves generalization to new tasks.Chen et al. (2023) investigated the effects of full parameter training vs low-rank adaptationHu et al. (2022) and monolingual vs multilingual instruction tuning using the Stanford AlpacaTaori et al. (2023) data, machine translated into 5 languages.Lai et al. (2023) trained multilingual instruction-following models for 26 languages with reinforcement learning from human feedbackOuyang et al. (2022) , andZhang et al. (2023) suggested instruction tuning LLMs by prepending the instruction and response translated into a pivot language (e.g English) to the response in the target language. Concurrently with our work,Kew et al. (2023) found that only a few languages in the tuning set result in better cross-lingual transfer to new languages for English-centric LLMs.In this work, we consider transfer from monolingual instruction tuning from 12 languages, rather than exclusively on English. Furthermore, we examine multilingual instruction-following using an LLM pre-trained on hundreds of languages, which might be a key to unlocking more transfer to languages not represented during tuning. Importantly, we unveil the potential of just a small amount of language diversity in the instruction tuning set for this cross-lingual generalization.",
                "abstract": "As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages from the pre-training corpus. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples integrated in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in multiple languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages. Finally, we find that diversifying the instruction tuning set with even just 2-4 languages significantly improves cross-lingual generalization. Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses."
            },
            {
                "name": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
                "arxiv_id": "2402.04833",
                "subtitles": [
                    "Instruction fine-tuning of LLMs",
                    "Data selection for IFT"
                ],
                "reference": [
                    "Instruction mining: When data mining meets large language model finetuning",
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Zeroprompt: scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Orca: Progressive learning from complex explanation traces of gpt",
                    "The false promise of imitating proprietary llms",
                    "How far can camels go? exploring the state of instruction tuning on open resources",
                    "Stanford alpaca: An instruction-following llama model",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "Instruction tuning with gpt",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Self-rewarding language models",
                    "# instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "A preliminary study of the intrinsic relationship between complexity and alignment",
                    "Language models are few-shot learners",
                    "Scaling instruction-finetuned language models",
                    "A long way to go: Investigating length correlations in rlhf",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March",
                    "LIMA: Less is more for alignment",
                    "Self-instruct: Aligning language model with self generated instructions",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related workInstruction fine-tuning of LLMs.Since pre-trained LLMs usually do not accurately understand user intents and provide coherent and beneficial responses, an instruction fine-tuning stage is necessary(Ouyang et al.,2022; Bai et al.,2022a) . Diversity of demonstrations and tasks(Chung et al.,2022; Xu et al.,2022) plays a pivotal role in enhancing the instruction-following performance of LMs. InstructGPT(Ouyang et al.,2022) first demonstrated how to achieve impressive performance in handling open-ended queries by fine-tuning GPT-3 models(Brown et al.,2020) with RLHF, which led to the release of ChatGPT. Subsequently, the community attempted to replicate the exceptional performance of proprietary models(Wang et al.,2023; Xu et al.,2023; Chiang et al.,2023) , butGudibande et al. (2023) show that it might be easy to mimic the style but not the factuality of closed-source LLMs.Singhal et al. (2023) identify a strong correlation between response length and reward when doing RLHF, implying that optimizing response length might be an implicit goal of RLHF. Also,Yuan et al. (2024) show that their self-improved reward model based on DPO encourages more verbose responses.Data selection for IFT.The community has focused on creating IFT datasets of high quality(Peng et al.,2023) . As one of the pioneering works, Alpaca(Taori et al.,2023) collects 52k interactions with the text-davinci-003 model using techniques from Self-Instruct(Wang et al.,2022) . However, direct distillation from language models without careful screening inevitably introduces demonstrations with incorrect or ill-favored answers. To filter these cases out, AlpaGasus(Chen et al.,2023) measures the quality of each demonstration using a powerful LLM (GPT-3.5-Turbo) as a scorer.Touvron et al. (2023) note that fewer (in the order of tens of thousands) but higher-quality examples annotated by their own vendors significantly improve their Llama-2-Chat models. The definition of data quality also pertains to other factors, such as the complexity of queries(Xu et al.,2023) , the difficulty of tasks presented(Mukherjee et al.,2023) and the diversity of semantics(Lu et al.,2023) .Zhao et al. (2023) propose to control these factors through an instruction refinement approach, which maintains an instruction semantic tree and yields new instructions by modifying the structure of the semantic tree. To better reflect human intentions, LIMA(Zhou et al.,2023) relies on community forums and human labor to curate 1,000 demonstrations with an emphasis on quality and diversity, achieving strong instruction-following ability, surpassing some proprietary LLMs. They also formulate theSuperficial Alignment Hypothesis: the general-purpose capabilities of an LLM mostly come from pre-training, and instruction tuning only guides the LLM to mimic the style, persona, and instruction adherence of desired outputs. Finally, whileLiu et al. (2023) ; Cao et al. (2023) consider response length as an indicator of example quality, our comprehensive exploration is the first to reveal its reliability and effectiveness in data selection for IFT.",
                "abstract": "There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses -- that intuitively contain more learnable information and are harder to overfit -- from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the Open LLM benchmarks that test factual knowledge. We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1) and datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain competitive results on MT-Bench and the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while training on only 1,000 examples and no extra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses. Overall, our findings suggest that fine-tuning on the longest responses should be the default baseline for any work on instruction fine-tuning. We provide our code atthis https URL."
            },
            {
                "name": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning",
                "arxiv_id": "2401.07950",
                "subtitles": [
                    "High-level evaluation",
                    "Continued pre-training",
                    "Dataset-specific fine-tuning"
                ],
                "reference": [
                    "Galactica: A large language model for science",
                    "Scieval: A multi-level large language model evaluation benchmark for scientific research",
                    "Metamath: Bootstrap your own mathematical questions for large language models",
                    "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
                    "Solving quantitative reasoning problems with language models",
                    "Scibench: Evaluating college-level scientific problem-solving abilities of large language models",
                    "Scaling relationship on learning mathematical reasoning with large language models",
                    "Mammoth: Building math generalist models through hybrid instruction tuning",
                    "Gpqa: A graduate-level google-proof q&a benchmark"
                ],
                "related_work": "4Related WorksRecently, there have been advances to bridge the gaps in reasoning difficulty and evaluation subjects from three perspectives for scientific reasoning with LLMs.High-level evaluationlike SciBenchWang et al. (2023c) and GPQARein et al. (2023) , which evaluates the scientific reasoning capabilities of LLMs at the college level and even graduate level. In addition, SciEvalSun et al. (2023) provides a multi-level LLMs evaluation benchmark to address data leakage problems and subjective question/answer evaluation ability issues.Continued pre-traininglike GalacticaTaylor et al. (2022) and MINERVALewkowycz et al. (2022) , which continue to train their respective base LLMs on multiple web texts including science-related or math-related corpus. This continued pre-training approach explores the potential of LLMs for science and contributes to the open-source models for the scientific community, but it is computationally expensive.Dataset-specific fine-tuninglike RFTYuan et al. (2023) , WizardMathLuo et al. (2023) , MAmmoTHYue et al. (2023) , and MetaMathYu et al. (2023) , which constructs certain datasets including GSM8K and MATH, conducts supervised fine-tuning of LLMs and evaluates the popular benchmarks GSM8K and MATH. MAmmoTH not only improves the in-domain (IND) performance like GSM8K but also generalizes to broader mathematical tasks by building a spectrum of math instruction tuning datasets including in-domain and out-of-domain datasets. This series of methods mainly focuses on mathematical reasoning tasks.",
                "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing their scientific and mathematical reasoning capabilities. Remarkably, the SciGLM consistently improves both the base model (ChatGLM3-6B-Base) by 4.87% and larger-scale models (32B) by 2.67%, without sacrificing the language understanding capabilities of the base model. This makes SciGLM a suitable foundational model to facilitate diverse scientific discovery tasks. For the benefit of the wider research community, we release SciInstruct, and SciGLM, alongside a self-reflective framework and fine-tuning code atthis https URL."
            }
        ],
        "survey": {
            "name": "A Survey on Data Selection for LLM Instruction Tuning",
            "arxiv_id": "2402.05123",
            "subtitles": [
                {
                    "name": "Instruction Sets",
                    "key_history": [
                        {
                            "reference_title": "Self-instruct: Aligning language models with self-generated instructions",
                            "key_word": "Self-instruct"
                        },
                        {
                            "reference_title": "Stanford alpaca: An instruction-following llm model",
                            "key_word": "Alpaca"
                        },
                        {
                            "reference_title": "Wizardlm: Empowering large language models to follow complex instructions",
                            "key_word": "WizardLM"
                        },
                        {
                            "reference_title": "LIMA: Less is more for alignment",
                            "key_word": "LIMA"
                        },
                        {
                            "reference_title": "Free dolly: Introducing the world's first truly open instruction-tuned llm",
                            "key_word": "Dolly-v2"
                        },
                        {
                            "reference_title": "Multitask prompted training enables zero-shot task generalization",
                            "key_word": "P3"
                        }
                    ],
                    "references_in_this_section": [
                        "Wizardlm: Empowering large language models to follow complex instructions",
                        "Multitask prompted training enables zero-shot task generalization",
                        "Stanford alpaca: An instruction-following llm model",
                        "LIMA: Less is more for alignment",
                        "Self-instruct: Aligning language models with self-generated instructions",
                        "Free dolly: Introducing the world's first truly open instruction-tuned llm",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "Data Selection methods",
                    "key_history": [
                        {
                            "reference_title": "Instruction mining: High-quality instruction data selection for large language models",
                            "key_word": "INSTRUCTMINING"
                        },
                        {
                            "reference_title": "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning",
                            "key_word": "FD (Instruction Following Difficulty) "
                        },
                        {
                            "reference_title": "Dataset quantization",
                            "key_word": "DQ (innovative data compression technique) "
                        },
                        {
                            "reference_title": "Tegit: Generating high-quality instruction-tuning data with text-grounded task design",
                            "key_word": "filtering instruction data"
                        },
                        {
                            "reference_title": "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks",
                            "key_word": "Prompt Uncertainty"
                        },
                        {
                            "reference_title": "Alpagasus: Training A better alpaca with fewer data",
                            "key_word": "innovative data filtering"
                        },
                        {
                            "reference_title": "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
                            "key_word": "automated instruction tagging"
                        },
                        {
                            "reference_title": "Self-evolved diverse data sampling for efficient instruction tuning",
                            "key_word": "DIVERSEEVOL"
                        },
                        {
                            "reference_title": "Mods: Model-oriented data selection for instruction tunin",
                            "key_word": "Model-Driven Data Selection"
                        },
                        {
                            "reference_title": "A preliminary study of the intrinsic relationship between complexity and alignment",
                            "key_word": "tree-instruct"
                        },
                        {
                            "reference_title": "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
                            "key_word": "coreset-base"
                        }
                    ],
                    "references_in_this_section": [
                        "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks",
                        "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning",
                        "Active learning for convolutional neural networks: A core-set approach",
                        "Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt",
                        "Instruction mining: High-quality instruction data selection for large language models",
                        "Alpagasus: Training A better alpaca with fewer data",
                        "Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation",
                        "Self-alignment with instruction backtranslation",
                        "One shot learning as instruction data prospector for large language models",
                        "What makes good data for alignment? A comprehensive study of automatic data selection in instruction tuning",
                        "Mods: Model-oriented data selection for instruction tuning",
                        "Learning transferable visual models from natural language supervision",
                        "A preliminary study of the intrinsic relationship between complexity and alignment",
                        "Tegit: Generating high-quality instruction-tuning data with text-grounded task design",
                        "Efficient k-nearest neighbor graph construction for generic similarity measures",
                        "Rethinking the instruction quality: LIFT is what you need",
                        "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                        "Sentence-bert: Sentence embeddings using siamese bert-networks",
                        "Dataset quantization",
                        "Self-evolved diverse data sampling for efficient instruction tuning",
                        "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
                        "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models"
                    ]
                },
                {
                    "name": "Evaluation methods and Result Analysis",
                    "key_history": [
                        {
                            "reference_title": "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning",
                            "key_word": "Instruction Following Difficulty"
                        }
                    ],
                    "references_in_this_section": [
                        "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning"
                    ]
                }
            ],
            "all_references": [
                "A preliminary study of the intrinsic relationship between complexity and alignment",
                "Self-alignment with instruction backtranslation",
                "Free dolly: Introducing the world's first truly open instruction-tuned llm",
                "Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation",
                "Rethinking the instruction quality: LIFT is what you need",
                "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
                "Palm: Scaling language modeling with pathways",
                "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks",
                "Llama: Open and efficient foundation language models",
                "Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt",
                "Efficient k-nearest neighbor graph construction for generic similarity measures",
                "Instruction tuning for large language models: A survey",
                "Wizardlm: Empowering large language models to follow complex instructions",
                "Stanford alpaca: An instruction-following llm model",
                "Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning",
                "Gpt-4 technical report",
                "Dataset quantization",
                "Alpagasus: Training A better alpaca with fewer data",
                "Training language models to follow instructions with human feedback",
                "Mods: Model-oriented data selection for instruction tuning",
                "What makes good data for alignment? A comprehensive study of automatic data selection in instruction tuning",
                "Self-evolved diverse data sampling for efficient instruction tuning",
                "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning",
                "Instruction mining: High-quality instruction data selection for large language models",
                "Active learning for convolutional neural networks: A core-set approach",
                "Learning transferable visual models from natural language supervision",
                "Multitask prompted training enables zero-shot task generalization",
                "Sentence-bert: Sentence embeddings using siamese bert-networks",
                "Tegit: Generating high-quality instruction-tuning data with text-grounded task design",
                "Self-instruct: Aligning language models with self-generated instructions",
                "One shot learning as instruction data prospector for large language models",
                "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                "LIMA: Less is more for alignment"
            ]
        },
        "topic_history": [
            {
                "name": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
                "arxiv_id": "2402.04333",
                "reference": [
                    "Free Dolly: Introducing the world's first truly open instruction-tuned LLM",
                    "Finetuned language models are zero-shot learners",
                    "OpenAssistant conversations-democratizing large language model alignment",
                    "Active learning for convolutional neural networks: A core-set approach",
                    "What neural networks memorize and why: Discovering the long tail via influence estimation",
                    "Learning from less data: A unified data subset selection and active learning framework for computer vision",
                    "In-context alignment: Chat with vanilla language models before fine-tuning",
                    "Instruction mining: High-quality instruction data selection for large language models",
                    "Enhancing chat language models by scaling high-quality instructional conversations",
                    "Don't stop pretraining: Adapt language models to domains and tasks",
                    "Prioritized training on points that are learnable, worth learning, and not yet learnt",
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Gradient surgery for multi-task learning",
                    "Coresets for data-efficient training of machine learning models",
                    "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
                    "Deep learning on a data diet: Finding important examples early in training",
                    "Post-hoc interpretability for neural nlp: A survey",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Orca: Progressive learning from complex explanation traces of gpt",
                    "Understanding in-context learning via supportive pretraining data",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Trivial or impossible dichotomous data difficulty masks model differences (on imagenet and beyond",
                    "Understanding black-box predictions via influence functions",
                    "Predicting performance for natural language processing tasks",
                    "Selection via proxy: Efficient data selection for deep learning",
                    "Dsdm: Model-aware dataset selection with datamodels",
                    "Trak: Attributing model behavior at scale",
                    "Stanford alpaca: An instruction-following llama model",
                    "Coresets and sketches",
                    "One shot learning as instruction data prospector for large language models",
                    "Moderate coreset: A universal method of data selection for real-world data-efficient deep learning",
                    "The flan collection: Designing data and methods for effective instruction tuning",
                    "Estimating training data influence by tracing gradient descent",
                    "An empirical study of example forgetting during deep neural network learning",
                    "Optimizing data usage via differentiable rewards",
                    "Grad-match: Gradient matching based data subset selection for efficient deep model training",
                    "Studying large language model generalization with influence functions",
                    "Data selection for language models via importance resampling",
                    "If influence functions are the answer, then what is the question",
                    "Mods: Model-oriented data selection for instruction tuning",
                    "The unlocking spell on base LLMs: Rethinking alignment via in-context learning",
                    "WizardLM: Empowering large language models to follow complex instructions",
                    "Beyond neural scaling laws: beating power law scaling via data pruning",
                    "Data diversity matters for robust instruction tuning",
                    "Datamodels: Predicting predictions from training data",
                    "Influence functions in deep learning are fragile",
                    "Skill-it! a data-driven skills framework for understanding and training language models",
                    "LIMA: Less is more for alignment",
                    "Retrieve: Coreset selection for efficient and robust semi-supervised learning",
                    "The influence curve and its role in robust estimation"
                ]
            },
            {
                "name": "SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model",
                "arxiv_id": "2401.09712",
                "reference": [
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Overcoming language bias in remote sensing visual question answering via adversarial training",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
                    "Parameter-efficient transfer learning for remote sensing image-text retrieval",
                    "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
                    "Gpt-4 technical report",
                    "Chatgpt",
                    "Rsgpt: A remote sensing vision language model and benchmark",
                    "Llama: Open and efficient foundation language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Visual instruction tuning",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Capera: Captioning events in aerial videos",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Earthnets: Empowering AI in earth observation",
                    "Rsvg: Exploring data and models for visual grounding on remote sensing data"
                ]
            },
            {
                "name": "LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding",
                "arxiv_id": "2404.05225",
                "reference": [
                    "StructuralLM: Structural pre-training for form understanding",
                    "Bros: A pre-trained language model focusing on text and layout for better key information extraction from documents",
                    "Geolayoutlm: Geometric pre-training for visual information extraction",
                    "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
                    "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
                    "Bi-vldoc: Bidirectional vision-language modeling for visually-rich document understanding",
                    "End-to-end document recognition and understanding with dessurt",
                    "DocFormer: End-to-end transformer for document understanding",
                    "SelfDoc: Self-supervised document representation learning",
                    "Layoutlmv3: Pre-training for document ai with unified text and image masking",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Exploring ocr capabilities of GPT-4V(ision) : A quantitative and in-depth evaluation",
                    "Unified pretraining framework for document understanding",
                    "Improved baselines with visual instruction tuning",
                    "Introducing chatgpt",
                    "Lmdx: Language model-based document information extraction and localization",
                    "Visual instruction tuning, 2023b",
                    "Lilt: A simple yet effective language-independent layout transformer for structured document understanding",
                    "The dawn of lmms: Preliminary explorations with GPT-4V(ision",
                    "Layoutlmv2: Multi-modal pre-training for visually-rich document understanding",
                    "Xylayoutlm: Towards layout-aware multimodal networks for visually-rich document understanding",
                    "Attention where it matters: Rethinking visual document understanding with selective region concentration",
                    "Gpt-4 technical report",
                    "LayoutLM: Pre-training of text and layout for document image understanding",
                    "Structext: Structured text understanding with multi-modal transformers",
                    "Ocr-free document understanding transformer",
                    "Going full-tilt boogie on document understanding with text-image-layout transformer",
                    "Ernie-layout: Layout knowledge enhanced pre-training for visually-rich document understanding",
                    "On the hidden mystery of ocr in large multimodal models, 2023c",
                    "mplug-docowl: Modularized multimodal large language model for document understanding",
                    "LayoutMask: Enhance text-layout interaction in multi-modal pre-training for document understanding",
                    "Vision grid transformer for document layout analysis",
                    "Structextv2: Masked visual-textual prediction for document image pre-training",
                    "mplug-owl: Modularization empowers large language models with multimodality, 2023b"
                ]
            },
            {
                "name": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
                "arxiv_id": "2403.14399",
                "reference": [
                    "Understanding by understanding not: Modeling negation in language models",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Don't say that! making inconsistent dialogue unlikely with unlikelihood training",
                    "Parrot: Translating during chat using large language models",
                    "Uncertainty-aware unlikelihood learning improves generative aspect sentiment quad prediction",
                    "Neural text generation with unlikelihood training",
                    "Vega-mt: The jd explore academy machine translation system for wmt",
                    "Gpt-4 technical report",
                    "A paradigm shift in machine translation: Boosting translation performance of large language models",
                    "Tim: Teaching large language models to translate with comparison",
                    "Beyond [CLS] through ranking by generation",
                    "Multilingual denoising pre-training for neural machine translation",
                    "Instruction position matters in sequence generation with large language models"
                ]
            },
            {
                "name": "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data",
                "arxiv_id": "2409.03810",
                "reference": [
                    "Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks",
                    "Octopack: Instruction tuning code large language models",
                    "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
                    "Magicoder: Source code is all you need",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Code alpaca: An instruction-following llama model for code generation",
                    "Towards scalable automated alignment of llms: A survey",
                    "How abilities in large language models are affected by supervised fine-tuning data composition",
                    "One-shot learning as instruction data prospector for large language models",
                    "Lima: Less is more for alignment",
                    "Self-play with execution feedback: Improving instruction-following capabilities of large language models",
                    "We-math: Does your large multimodal model achieve human-like mathematical reasoning",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "Making language models better tool learners with execution feedback",
                    "Autocoder: Enhancing code large language model with AIEV-Instruct",
                    "Dolphcoder: Echo-locating code large language models with diverse and multi-objective instruction tuning",
                    "Benchmarking benchmark leakage in large language models",
                    "Opencodeinterpreter: Integrating code generation with execution and refinement",
                    "Teaching large language models to self-debug",
                    "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models"
                ]
            },
            {
                "name": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
                "arxiv_id": "2402.10176",
                "reference": [
                    "Textbooks Are All You Need",
                    "Llemma: An Open Language Model For Mathematics",
                    "Galactica: A Large Language Model for Science",
                    "TinyGSM: achieving> 80% on GSM8k with small language models",
                    "Solving Quantitative Reasoning Problems with Language Models",
                    "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
                    "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                    "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                    "TinyStories: How Small Can Language Models Be and Still Speak Coherent English",
                    "MAmmoTH: Building math generalist models through hybrid instruction tuning",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Textbooks Are All You Need II: phi-1.5 technical report",
                    "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
                    "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
                    "Orca: Progressive Learning from Complex Explanation Traces of GPT",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
                    "PAL: Program-aided Language Models",
                    "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"
                ]
            },
            {
                "name": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
                "arxiv_id": "2402.00530",
                "reference": [
                    "FAMIE: A fast active learning framework for multilingual information extraction",
                    "Finetuned language models are zero-shot learners",
                    "Scalable transfer learning with expert models",
                    "Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language",
                    "The flan collection: Designing data and methods for effective instruction tuning",
                    "Selection via proxy: Efficient data selection for deep learning",
                    "Data diversity matters for robust instruction tuning",
                    "REV: Information-theoretic evaluation of free-text rationales",
                    "Multitask prompted training enables zero-shot task generalization",
                    "From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning",
                    "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
                    "Mmc: Advancing multimodal chart understanding with large-scale instruction tuning",
                    "#instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
                    "Mods: Model-oriented data selection for instruction tuning",
                    "Alpagasus: Training a better alpaca with fewer data"
                ]
            },
            {
                "name": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
                "arxiv_id": "2401.01854",
                "reference": [
                    "Finetuned language models are zero-shot learners",
                    "English intermediate-task training improves zero-shot cross-lingual transfer too",
                    "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
                    "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
                    "Cross-lingual ability of multilingual bert: An empirical study",
                    "Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability",
                    "Crosslingual generalization through multitask finetuning",
                    "Plug: Leveraging pivot language in cross-lingual instruction tuning",
                    "Multitask prompted training enables zero-shot task generalization",
                    "Stanford alpaca: An instruction-following llama model",
                    "How do languages influence each other? studying cross-lingual data sharing during LM fine-tuning",
                    "BERT: Pre-training of deep bidirectional transformers for language understanding",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Turning english-centric llms into polyglots: How much multilinguality is needed",
                    "XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
                    "Monolingual or multilingual instruction tuning: Which makes a better alpaca",
                    "Cross-task generalization via natural language crowdsourcing instructions",
                    "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback",
                    "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
                    "mT5: A massively multilingual pre-trained text-to-text transformer",
                    "Unsupervised cross-lingual representation learning at scale",
                    "Llama: Open and efficient foundation language models",
                    "Emerging cross-lingual structure in pretrained language models",
                    "How multilingual is multilingual BERT",
                    "Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond",
                    "LoRA: Low-rank adaptation of large language models",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
                "arxiv_id": "2402.04833",
                "reference": [
                    "Instruction mining: When data mining meets large language model finetuning",
                    "What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning",
                    "Zeroprompt: scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization",
                    "Alpagasus: Training a better alpaca with fewer data",
                    "Orca: Progressive learning from complex explanation traces of gpt",
                    "The false promise of imitating proprietary llms",
                    "How far can camels go? exploring the state of instruction tuning on open resources",
                    "Stanford alpaca: An instruction-following llama model",
                    "Wizardlm: Empowering large language models to follow complex instructions",
                    "Instruction tuning with gpt",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Self-rewarding language models",
                    "# instag: Instruction tagging for analyzing supervised fine-tuning of large language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "A preliminary study of the intrinsic relationship between complexity and alignment",
                    "Language models are few-shot learners",
                    "Scaling instruction-finetuned language models",
                    "A long way to go: Investigating length correlations in rlhf",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March",
                    "LIMA: Less is more for alignment",
                    "Self-instruct: Aligning language model with self generated instructions",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning",
                "arxiv_id": "2401.07950",
                "reference": [
                    "Galactica: A large language model for science",
                    "Scieval: A multi-level large language model evaluation benchmark for scientific research",
                    "Metamath: Bootstrap your own mathematical questions for large language models",
                    "Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
                    "Solving quantitative reasoning problems with language models",
                    "Scibench: Evaluating college-level scientific problem-solving abilities of large language models",
                    "Scaling relationship on learning mathematical reasoning with large language models",
                    "Mammoth: Building math generalist models through hybrid instruction tuning",
                    "Gpqa: A graduate-level google-proof q&a benchmark"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Synergistic Interplay between Search and Large Language Models for Information Retrieval",
            "arxiv_id": "2305.07402",
            "isAPA": true,
            "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources fromvast amounts of data, and its applications have evolved from traditional knowledgebases to modern retrieval models (RMs) . The emergence of large language models(LLMs) has further revolutionized the IR field by enabling users to interact withsearch systems in natural languages. In this paper, we explore the advantages anddisadvantages of LLMs and RMs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leveragethe benefits of both paradigms while circumventing their limitations, we proposeInteR, a novel framework that facilitates information refinement through synergybetween RMs and LLMs. InteR allows RMs to expand knowledge in queriesusing LLM-generated knowledge collections and enables LLMs to enhance promptformulation using retrieved documents. This iterative refinement process augmentsthe inputs of RMs and LLMs, leading to more accurate retrieval. Experiments onlarge-scale retrieval benchmarks involving web search and low-resource retrievaltasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment.Source code is available at https://github.com/Cyril-JZ/InteR.",
            "reference": [
                "Manas Gaur, Kalpa Gunaratna, Vijay Srinivasan, and Hongxia Jin. Iseeq: Information seeking question generation using dynamic meta-information retrieval and knowledge graphs",
                "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training",
                "OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt/, 2022. URL https://openai.com/blog/chatgpt",
                "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp.  2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding",
                "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation",
                "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering",
                "Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval",
                "Kelong Mao, Zhicheng Dou, Haonan Chen, Fengran Mo, and Hongjin Qian. Large language models know your contextual search intent: A prompting framework for conversational search. arXiv preprint arXiv",
                "Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv",
                "Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. Generative multi-hop retrieval",
                "Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers. Advances in Neural Information Processing Systems",
                "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems",
                "Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. A survey on complex knowledge base question answering: Methods, challenges and solutions",
                "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. In-batch negatives for knowledge distillation with tightly-coupled teachers for dense retrieval",
                "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems",
                "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners",
                "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv",
                "Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval",
                "Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023a",
                "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=jKN1pXi7b",
                "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering",
                "Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold Overwijk. Less is more: Pretrain a strong Siamese encoder for dense text retrieval using a weak decoder",
                "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables zero-shot task generalization",
                "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                "Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4) :333-389, 2009. doi: 10.1561/1500000019. URL https://doi.org",
                "Luyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense passage retrieval",
                "Zaragoza Robertson. Robertson s., zaragoza h. The probabilistic relevance framework: Bm25 and beyond, Found. Trends Inf. Retr",
                "Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR, abs/2104.08663, 2021. URL https://arxiv.org/abs",
                "Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-augmented retrieval for open-domain question answering",
                "Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023b",
                "Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels",
                "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators",
                "Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. Open-domain question answering goes conversational via question rewriting",
                "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. Overview of the trec 2020 deep learning track. arXiv preprint arXiv",
                "Mandar Joshi, Kenton Lee, Yi Luan, and Kristina Toutanova. Contextualized representations using textual encyclopedic knowledge. arXiv preprint arXiv",
                "Ellen M Voorhees et al. The trec-8 question answering track report",
                "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b",
                "Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. Pretrained transformers for text ranking: BERT and beyond",
                "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys",
                "Google. Google bard. https://bard.google.com/, 2023. URL https://bard.google.com",
                "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q",
                "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna",
                "Sebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Efficiently teaching an effective dense retriever with balanced topic aware sampling",
                "IC Mogotsi. Christopher d. manning, prabhakar raghavan, and hinrich sch\u00fctze: Introduction to information retrieval. Information Retrieval",
                "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv",
                "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers",
                "OpenAI. Gpt-4 technical report. ArXiv, abs",
                "Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp.  5418-5426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main",
                "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a",
                "Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data",
                "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs",
                "Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context",
                "Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. Advances in Neural Information Processing Systems",
                "James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. The fact extraction and VERification (FEVER) shared task",
                "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering",
                "Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv preprint arXiv",
                "Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. COCO-DR: Combating the distribution shift in zero-shot dense retrieval with contrastive and distributionally robust learning",
                "Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, and Lei Chen. Hyperlink-induced pre-training for passage retrieval in open-domain question answering",
                "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models",
                "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv",
                "Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv",
                "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions",
                "Luyu Gao and Jamie Callan. Condenser: a pre-training architecture for dense retrieval",
                "Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. Learning dense representations for entity retrieval",
                "Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. Improving passage retrieval with zero-shot question generation",
                "Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. RetroMAE: Pre-training retrieval-oriented language models via masked auto-encoder",
                "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv",
                "Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. arXiv preprint arXiv",
                "Zhuyun Dai and Jamie Callan. Context-aware sentence/passage term importance estimation for first stage retrieval. arXiv preprint arXiv",
                "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. Overview of the trec 2019 deep learning track. arXiv preprint arXiv",
                "Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the use of lucene for information retrieval research",
                "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens"
            ],
            "related work": "2Related WorkDense RetrievalDocument retrieval has been an important component for several knowledge-intensive tasks(Voorhees et al.,1999; Karpukhin et al.,2020) . Traditional techniques such as TF-IDF and BM25 depend on term matching and create sparse vectors(Robertson,2009; Yang et al.,2017; Chen et al.,2017) to ensure efficient retrieval. After the emergence of pre-trained language models(Devlin et al.,2019; Liu et al.,2019) , dense retrieval which encodes both queries and documents into low-dimension vectors and then calculates their relevance scores(Lee et al.,2019; Karpukhin et al.,2020) , has recently undergone substantial research. Relevant studies include improving training approach(Karpukhin et al.,2020; Xiong et al.,2021; Qu et al.,2021) , distillation(Lin et al.,2021; Hofst\u00e4tter et al.,2021) and task-specific pre-training(Izacard et al.,2022; Gao & Callan,2021; Lu et al.,2021; Gao & Callan,2022; Xiao et al.,2022) of dense retrieval models which significantly outperform sparse approaches.Zero-shot Dense RetrievalMany prior works consider training dense retrieval models on high-resource passage retrieval datasets like Natural Questions (NQ) (Kwiatkowski et al.,2019) (133k training examples) or MS-MARCO(Bajaj et al.,2016) (533k training examples) and then evaluating on queries from new tasks. These systems(Wang et al.,2022; Yu et al.,2022) are utilized in a transfer learning configuration(Thakur et al.,2021) . However, on the one hand, it is time-consuming and expensive to collect such a vast training corpus. On the other hand, even MS-MARCO has limitations on commercial use and cannot be used in a wide range of real-world applications. To this end, recent work(Gao et al.,2023) proposes building zero-shot dense retrieval systems that require no relevance supervision (i.e., relevance label between a pair of query and document) , which is considered  \"unsupervised \" as the only supervision resides in the LLM where learning to follow instructions is conducted in earlier times(Sachan et al.,2022) . In this work, we follow this zero-shot unsupervised setting and conduct information refinement through synergy between RMs and LLMs without any relevance supervision to handle the aforementioned issues.Enhance Retrieval Through LMsRecent works have investigated using auto-regressive language models to generate intermediate targets for better retrieval(Cao et al.,2021; Bevilacqua et al.,2022) while identifier strings still need to be created. Other works consider  \"retrieving \" the knowledge stored in the parameters of pre-trained language models by directly generating text(Petroni et al.,2019; Roberts et al.,2020) . Some researchers(Mao et al.,2021; Anantha et al.,2021; Wang et al.,2023) utilize LM to expand the query and incorporate these pseudo-queries for enhanced retrieval while others choose to expand the document(Nogueira et al.,2019) . Besides, LMs can also be exploited to provide references for retrieval targets. For instance, GENREAD(Yu et al.,2023) directly generates contextual documents for given questions.Enhance LMs Through RetrievalOn the contrary, retrieval-enhanced LMs have also received significant attention. Some approaches enhance the accuracy of predicting the distribution of the next word during training(Borgeaud et al.,2022) or inference(Khandelwal et al.,2020) through retrieving the k-most similar training contexts. Alternative methods utilize retrieved documents to provide supplementary context in generation tasks(Joshi et al.,2020; Guu et al.,2020; Lewis et al.,2020) . WebGPT(Nakano et al.,2021) further adopts imitation learning and uses human feedback in a text-based web-browsing environment to enhance the LMs. LLM-Augmentor(Peng et al.,2023) improves large language models with external knowledge and automated feedback. REPLUG(Shi et al.,2023) prepends retrieved documents to the input for the frozen LM and treats the LM as a black box. Demonstrate-Search-Predict (DSP) (Khattab et al.,2022) obtains performance gains by relying on passing natural language texts in sophisticated pipelines between a language model and a retrieval model, which is most closely related to our approach. However, they rely on composing two parts with in-context learning and target on multi-hop question answering. While we aim at conducting information refinement via multiple interactions between RMs and LLMs for large-scale retrieval.Figure 1:Overall architecture of InteR.",
            "date": "2023"
        },
        "topic": "LLMs for Information Retrieval",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information",
                "arxiv_id": "2406.11093",
                "subtitles": [
                    "Misinformation detection",
                    "In-context learning"
                ],
                "reference": [
                    "Towards self-supervised cross-domain fake news detection",
                    "Misconfidence-based demonstration selection for llm in-context learning",
                    "Adapt in contexts: Retrieval-augmented domain adaptation via in-context learning",
                    "Sentiment analysis-based social network rumor detection model with bi-directional graph convolutional networks",
                    "Let's learn step by step: Enhancing in-context learning ability with curriculum learning",
                    "Rough-fuzzy graph learning domain adaptation for fake news detection",
                    "Conspemollm: Conspiracy theory detection using an emotion-based large language model",
                    "Emotion detection for misinformation: A review",
                    "Learning sparse alignments via optimal transport for cross-domain fake news detection",
                    "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
                    "Sentiment-aware fake news detection on social media with hypergraph attention networks",
                    "Emotion-guided cross-domain fake news detection using adversarial domain adaptation"
                ],
                "related_work": "2Related Work 2.1Misinformation detection Cross-domain misinformation detection:Comito et al. (2023) propose a deep learning-based architecture able to mitigate this problem by yielding high-level cross-domain features.Tang et al. (2023) design the News Optimal Transport to learn transferable features across domains by aligning the source and target news using Optimal Transport (OT) techniques.Shi et al. (2023) develop a rough-fuzzy graph learning framework that uses representations of cross-domain sample uncertainty structural information, and captures shared general features across domains. But these methods all require complex structures and fine-tuning strategies.Based on affective information:Emotion and sentiment are important features for misinformation detectionLiu et al. (2024d) .Zhang et al. (2023b) combine the use of semantic and sentiment information, along with propagation information for rumor detection.Dong et al. (2022a) design a sentiment-aware hyper-graph attention network for fake news detection.Liu et al. (2024b) develop a conspiracy theory detection LLM by fine-tuning EmoLLaMALiu et al. (2024c) . Choudhry et al. (2022) utilize emotional information for fake news detection based on an adversarial learning structure. Unfortunately, these works either have complex structural designs or fine-tuned models, which require significant time and computational resources. The RAEmoLLM in this article applies the ICL method based on affective information, which has a simple structure and does not involve fine-tuning. 2.2In-context learningLiu et al. (2024a) develop in-context curriculum learning, a simple but helpful demonstration ordering method for ICL that gradually increases the complexity of prompt demonstrations.Xu and Zhang (2024) propose in-context reflection to strategically select demonstrations that reduce the discrepancy between the LLM's outputs and the actual input-output mappings.Long et al. (2023) propose a retrieval-enhanced language model to address cross-domain problems, in which they train language models by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. Inspired by this work, we propose the RAEmoLLM.",
                "abstract": "Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on time and resources consuming fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call affect). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. It accomplishes this by applying an emotion-aware LLM to construct a retrieval database of affective embeddings. This database is used by our retrieval module to obtain source-domain samples, which are subsequently used for the inference module's in-context few-shot learning to detect target domain misinformation. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the zero-shot method on three datasets, with the highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be released onthis https URL."
            },
            {
                "name": "Self-Retrieval: Building an Information Retrieval System with One Large Language Model",
                "arxiv_id": "2403.00801",
                "subtitles": [
                    "Dense retrieval",
                    "Generative retrieval",
                    "Retrieval augmented generation"
                ],
                "reference": [
                    "Autoregressive entity retrieval",
                    "A neural corpus indexer for document retrieval",
                    "Text and code embeddings by contrastive pre-training",
                    "From doc2query to doctttttquery",
                    "Generate rather than retrieve: Large language models are strong context generators",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Multiview identifiers enhanced generative retrieval",
                    "Benchmarking large language models in retrieval-augmented generation",
                    "Dense passage retrieval for open-domain question answering",
                    "Large language models with controllable working memory",
                    "Active retrieval augmented generation",
                    "Atlas: Few-shot learning with retrieval augmented language models",
                    "C-pack: Packaged resources to advance general chinese embedding",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Query2doc: Query expansion with large language models",
                    "Bridging the gap between indexing and retrieval for differentiable search index with query generation",
                    "Learning to tokenize for generative retrieval",
                    "ColBERTv2: Effective and efficient retrieval via lightweight late interaction",
                    "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
                    "Making retrieval-augmented language models robust to irrelevant context",
                    "Augmentation-adapted retriever improves generalization of language models as generic plug-in",
                    "Replug: Retrieval-augmented black-box language models",
                    "Autoregressive search engines: Generating substrings as document identifiers",
                    "SimLM: Pre-training with representation bottleneck for dense passage retrieval",
                    "Transformer memory as a differentiable search index"
                ],
                "related_work": "2Related WorkDense retrievalDense retrieval model retrieves information by matching dense vectors(Karpukhin et al.,2020) . An encoder transforms the query and documents into dense vectors, and their similarity is determined by measuring their distance. Dense retrievers can be improved by different strategies, including designing loss functions(Wang et al.,2023a) , multi-vector(Santhanam et al.,2022) , training with generated queries(Nogueira et al.,2019; Wang et al.,2023b) and training with a substantial number of query-document pairs(Neelakantan et al.,2022; Xiao et al.,2023) . The primary problem of dense retrieval is the lack of interaction with LLM, which makes it cannot fully meeting the relevant requirements of LLMs.Generative retrievalGenerative retrieval utilizes the ability of generative Seq2seq language models, which takes query as input and outputs retrieved document identifier(Cao et al.,2021; Tay et al.,2022) . Recent research has concentrated on enhancing the training techniques and structures of generative retrieval(Bevilacqua et al.,2022; Wang et al.,2022; Li et al.,2023b) . For instance,Zhuang et al. (2022) suggested employing generated queries for training purposes.Sun et al. (2023a) enhanced document IDs by transforming them into tokens with semantic meaning. The main distinction between self-retrieval and generated retrieval lies in the fact that the self-retrieval model is a comprehensive information retrieval system driven by a LLM. It not only has the capability to directly generate retrieved passage but also leverages the potential of LLM for self-assessment.Retrieval augmented generationIn retrieval augmented generation, LLMs takes retrieved external knowledge as input and generate responses more accurately, allowing it to go beyond their internal knowledge and reduces the generation of hallucination(Lewis et al.,2020; Izacard et al.,2023) . The main problem with RAG is that the retriever is not perfect and it may retrieve noise documents(Chen et al.,2023) . Therefore, the recent works of RAG can be divided into the following types: (1) Improving the performance of the retriever by LLM(Shi et al.,2023; Yu et al.,2023b) ; (2) Enhancing robustness of LLMs(Yoran et al.,2023; Li et al.,2023a) ; (3) Optimizing the process of RAG(Izacard and Grave,2021; Jiang et al.,2023; Asai et al.,2023) . Besides retrieving external knowledge,Yu et al. (2023a) proposed the generate-then-read approach, advocating for the use of LLMs to directly generate documents instead of relying on a retriever.",
                "abstract": "The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that Self-Retrieval not only significantly outperforms previous retrieval approaches by a large margin, but also can significantly boost the performance of LLM-driven downstream applications like retrieval augumented generation."
            },
            {
                "name": "Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions",
                "arxiv_id": "2407.12468",
                "subtitles": [
                    "Health Information Credibility and Correctness",
                    "Search Engines and Large Language Models in Health Information Access",
                    "Retrieval-Augmented Generation (RAG) "
                ],
                "reference": [
                    "Retrieval augmented language model pre-training",
                    "Evaluation of ai chatbots for patient-specific ehr questions",
                    "Reliability prediction of webpages in the medical domain",
                    "Overview of the trec 2021 health misinformation track",
                    "Persuasive technologie",
                    "Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model",
                    "Prominence-interpretation theory: Explaining how people assess credibility online",
                    "Retrieval-based language models and applications",
                    "Large language models for binary health-related question answering: A zero- and few-shot evaluation",
                    "Credibility in information retrieval",
                    "Trialling a large language model (chatgpt) in general practice with the applied knowledge test: observational study demonstrating opportunities and limitations in primary care",
                    "The retrieval effectiveness of medical information on the web",
                    "Reliability prediction for health-related content: a replicability study",
                    "The promise and peril of using a large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with limitations",
                    "A user study on people's perception to the credibility of online health information",
                    "Credibility in social media: opinions, news, and health information a survey",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Vera: Prediction techniques for reducing harmful misinformation in consumer health search",
                    "Trends in medical information retrieval on internet",
                    "Comparing traditional and neural approaches for detecting health-related misinformation",
                    "The positive and negative influence of search results on people's decisions about the efficacy of medical treatments",
                    "Towards automated end-to-end health misinformation free search with a large language model",
                    "Lamda: Language models for dialog applications",
                    "Automated assessment of the quality of depression websites",
                    "Analysis of large-language model versus human performance for genetics questions",
                    "Improving language models by retrieving from trillions of tokens",
                    "Overview of the trec 2020 health misinformation track",
                    "Webgpt: Browser-assisted question-answering with human feedback",
                    "Factors and effects of information credibility",
                    "Health topics: 80% of internet users look for health information online",
                    "Age differences in credibility judgments of online health information",
                    "A literature review on detecting, verifying, and mitigating online misinformation",
                    "Assessing the accuracy of responses by the language model chatgpt to questions regarding bariatric surgery",
                    "Internet-augmented language models through few-shot prompting for open-domain question answering",
                    "A survey on retrieval-augmented text generation",
                    "Overview of the clef ehealth evaluation lab",
                    "Enhancing web search in the medical domain via query clarification",
                    "The internet for medical information about cancer: help or hindrance",
                    "Evaluation of chatgpt on biomedical tasks: A zero-shot comparison with fine-tuned generative transformers",
                    "Understanding and predicting web content credibility using the content credibility corpus",
                    "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                    "Augmenting web pages and search results to support credibility assessment",
                    "The role of reading skills in the evaluation of online information gathered from search engine environments",
                    "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
                    "Evidentiality-guided generation for knowledge-intensive NLP tasks",
                    "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
                    "Detecting health misinformation in online health communities: Incorporating behavioral features into machine learning based approaches"
                ],
                "related_work": "2Related workThis paper is related to several scientific areas. The advances in information credibility and correctness, particularly in the area of health information, are relevant to our research and are discussed in Section2.1. Section2.2reviews the most significant literature in health information access with traditional search engines and with the new LLMs. Finally, in Section2.3we review the main trends in retrieval augmented LLMs.2.1Health Information Credibility and CorrectnessCredibility represents a subjective perception of the extent to which information from a webpage or other source can be trusted(Fogg,1999) . Previous research has extensively studied how online information credibility is established(McKnight and Kacmar,2007; Ginsca et al.,2015; Kakol et al.,2017; Bodaghi et al.,2023) . For instance,Viviani and Pasi (2017) presented a thorough survey on existing credibility determination methods, showing that health misinformation poses a socially relevant problem. The work byMatthews et al. (2003) showed that a corpus about alternative cancer treatments contained 90% of documents with at least one false claim. On the other hand, Sondhi and his peers presented an automatic approach, using classic supervised learning technology, for medical reliability classification of webpages(Sondhi et al.,2012) .Fern\u00e1ndez-Pichel et al. (2021b) successfully reproduced Sondhi's study and further applied it to new collections, thereby demonstrating its potential for generalisation. The same team also performed a comparison between traditional learning methods and neural-based approaches solutions for health-related misinformation detection(Fern\u00e1ndez-Pichel et al.,2021a) , concluding that traditional learning approaches still constitute a robust baseline for some prediction tasks.Under similar supervised learning settings, other studies focused their efforts on how different characteristics or features influence credibility.Zhao et al. (2021) proposed a novel health misinformation detection model using both  \"central-level \" features (e.g., the topics discussed) and the so-called  \"peripheral-level \" features (including linguistic and sentiment features, and user behavioural features) . Their approach was validated on a real-world dataset, obtaining 85% of accuracy in health misinformation detection. This study showed that behavioural features are more discriminative than linguistic features in detecting untrustworthy contents. In(Griffiths et al.,2005) , the authors demonstrated that network-based features alone, such as those based on PageRank, do not suffice to determine the reliability of online content.Another line of work specifically focused on end-users and their perceptions of credibility. Seminal work byFogg (2003) proposed the prominence-interpretation theory, which helps to determine which website cues influence the perception of credibility. Other studies showed that perceived credibility depends on reading skills(Hahnel et al.,2018) . Similarly,Liao and Fu (2014) studied age differences in credibility judgements. On the other hand,Schwarz and Morris (2011) focused on how to augment a search engine result page to improve medical credibility judgements. In(Fern\u00e1ndez-Pichel et al.,2024) , a user study with 1,000 participants was performed to analyse people's perceptions to the credibility of online health information. Among their main findings, these authors showed that individuals tend to overestimate the credibility of low quality sites. In the literature, related aspects, such as credibility, trustworthiness, and correctness have been explored. Our research particularly focuses on the correctness of responses for health questions provided by web search systems and conversational AIs.Some teams focused their efforts on designing solutions that estimate the correctness of medical information. For instance,Pradeep et al. (2021) presented Vera, a transfomer-based ranker that achieved state-of-the-art results in health misinformation detection tasks. This model was fine-tuned with assessments from the TREC 2019 Decision Track and it achieved the best results in the TREC 2022 Health Misinformation Track. This shared-data task fosters the development of systems that promote credible and correct documents over misinformation. However, systems like Vera take a health questionand its correct responseas an input and then search for harmful or helpful documents. These technological tools could support, for example, moderation services for online platforms. However, the need of pairs of questions and correct responses represents a limitation and, thus, the creators of Vera also conducted research on how to automatically infer the correct response for a health question(Pradeep and Lin,2024) . Specifically, the authors evaluated two different approaches: i) using LLMs under different settings (zero-, few-shot and chain-of-thought prompting) , and ii) using the ranking produced by Vera and averaging the decisions extracted from the top 50 retrieved results. Their results suggest that the LLM-based approach (powered by GPT-4) outperformed the rest of strategies. Our comparison of SEs and LLMs is related to this study, but we compare multiple commercial search engines (while Vera is not a tool that is widely available to the public) and, furthermore, we do not only evaluate GPT models but consider six LLMs of different families. Our study, therefore, is reflective of the type of answers that the general public might get when interacting with popular information access tools. Additionally, we compare the answering capabilities of the LLMs with and without search results provided by the web search engines. This is an aspect that has not received enough attention in health information seeking. We also evaluate the effectiveness of the answers provided as we go down in the ranked lists of the SERPs.2.2Search Engines and Large Language Models in Health Information AccessWeb search is widely used to obtain health advice and effective medical information retrieval has attracted the attention of the scientific community over the years(Fox,2011) . For instance,Baujard et al. (1998) conducted a seminal study focusing on the main trends in medical information retrieval and proposed an agent to perform effective medical information discovery. Similarly,Bin and Lun (2001) addressed the difficulty of finding relevant medical information in the web. These authors suggested that general-purpose search engines constitute a strong baseline for health information retrieval but also designed an agent-based system that was able to outperform SE baselines.Soldaini et al. (2016) studied the influence of injecting expert medical vocabulary to the query. Their results showed an increase around 7-12% in correct results for the modified queries. However, effective information retrieval (i.e., topicality) is not enough. AsPogacar et al. (2017) have shown, on-topic incorrect results can severely bias people's decisions. The need for new algorithmic solutions able to provide accurate results for health queries has motivated the development of shared-tasks that promote the identification of correct and credible information over misinformation, such as the TREC Health Misinformation (HM) Track(Clarke et al.,2020,2021) or the CLEF eHealth initiative(Goeuriot et al.,2020; Kelly et al.,2019; Suominen et al.,2018) . In this study, we are specifically interested in analysing how search results and LLMs' responses can help to answer binary health questions. Thus, we assess here the extent to which search results or LLM completions provide misleading responses with respect to the established medical consensus for each health topic.With the impressive recent development of LLMs, interest in assessing the correctness of the health-related AI-based completions has escalated. For example,Chervenak et al. (2023) demonstrated ChatGPT's abilities to answer fertility questions.Duong and Solomon (2023) compared the performance of Large Language Models against humans for answering multiple-choice questions of human genetics. Similarly,Holmes et al. (2023) conducted a comparative study of LLMs knowledge on a highly specialised topic, radiation oncology physics. They concluded that OpenAI's models outperformed all others. In(Jahan et al.,2023; Hamidi and Roberts,2023; Samaan et al.,2023) , the authors conducted studies on the role of LLMs for biomedical tasks, patient-specific EHR questions, and bariatric surgery topics, respectively. With a broader perspective,Johnson et al. (2023) involved physicians in a thorough evaluation of ChatGPT's accuracy in answering medical queries, and other researchers(Thirunavukarasu et al.,2023) evaluated ChatGPT's ability with the Applied Knowledge Test (AKT) of the Membership of the Royal College of General Practitioners, demonstrating performance close to human experts. All of the aforementioned studies are restricted to a single model, usually ChatGPT, and/or to a specific medical area.Fern\u00e1ndez-Pichel et al. (2024) evaluated several LLMs of different nature with general health questions. This team examined the influence that different prompts and in-context examples have on the effectiveness of the LLMs' output. In our study, we go one step further and present a thorough comparison between LLMs and traditional web search engines and, additionally, test the combination of LLMs and SEs through retrieval-augmented generation (RAG) strategies.2.3Retrieval-Augmented Generation (RAG) Several previous studies focused on how to exploit retrieval evidence to enhance the generative responses supplied by AIs(Li et al.,2022) .Asai et al. (2023) provided a detailed overview of the advances in retrieval augmented LMs and explored some of their applications.Guu et al. (2020) proposed REALM, an augmented language model with a module that retrieves evidence from a large textual corpus such as Wikipedia. In(Borgeaud et al.,2022) , the authors proposed RETRO, an architecture consisting of a language model which benefits from results obtained from a large database of trillions of tokens.Lazaridou et al. (2022) conducted an evaluation on different few shot prompting strategies. They compared a method that provided examples of questions and correct answers (closed bookstrategy) against a method that fed the model with supporting evidence for the answer (open bookstrategy) . The injected evidence came from an offline labelled collection or from relevant passages obtained from Google's search API. Their experiments showed that both types of open book strategies improved the closed book method.Izacard and Grave (2021) augmented a T5 model for an open domain question answering task. For retrieval of relevant passages, these authors used an offline collection and BM25 or DPR as search methods. The main conclusion was that augmentation is beneficial and that improvements are noticeable up to a maximum of 100 recovered passages.Asai et al. (2021) conducted a similar study, but they argued that injecting off-topic evidence might be counterproductive. Thus, their solution included an evidentiality estimation layer that predicts whether the passage provides actual evidence to answer the question.Other studies focused on helping language models to interact with search engines. For instance,Nakano et al. (2021) developed a text-based web-browsing environment that can be exploited by a GPT-3 fine-tuned model.Thoppilan et al. (2022) showed that using external APIs (such as those supported by information retrieval systems) significantly improves groundedness, which is defined as the extent to which a generated response contains claims that can be validated against a known source.Shuster et al. (2022) proposed an architecture that first searches for evidence against a search engine, then selects the most relevant sentences from the retrieved documents, and finally, generates a response.In the health domain,Li et al. (2023) fine-tuned a Llama model with medical conversations and also injected medical evidence extracted from Wikipedia and other medical sources. We contribute to this recent line of work by assessing the effect of including search engine's evidence for the generation of correct health answers. In Section6.2.4, we detail these experiments, oriented to prompt several LLMs with evidence retrieved from Google's top results for a given health question.",
                "abstract": "Search engines have traditionally served as primary tools for information seeking. However, the new Large Language Models (LLMs) have recently demonstrated remarkable capabilities in multiple tasks and, specifically, their adoption as question answering systems is becoming increasingly prevalent. It is expected that LLM-based conversational systems and traditional web engines will continue to coexist in the future, supporting end users in various ways. But there is a need for more scientific research on the effectiveness of both types of systems in facilitating accurate information seeking. In this study, we focus on their merits in answering health questions. We conducted an extensive study comparing different web search engines, LLMs and retrieval-augmented (RAG) approaches. Our research reveals intriguing conclusions. For example, we observed that the quality of webpages potentially responding to a health question does not decline as we navigate further down the ranked lists. However, according to our evaluation, web engines are less accurate than LLMs in finding correct answers to health questions. On the other hand, LLMs are quite sensitive to the input prompts, and we also found out that RAG leads to highly effective information seeking methods."
            },
            {
                "name": "Redefining Information Retrieval of Structured Database via Large Language Models",
                "arxiv_id": "2405.05508",
                "subtitles": [
                    "Retrieval Augmentation",
                    "Structured Knowledge-Base Retrieval Augmentation"
                ],
                "reference": [
                    "Retrieval augmented language model pre-training",
                    "Retrieval-Augmented generation for knowledge-intensive nlp tasks",
                    "Distilling knowledge from reader to retriever for question answering",
                    "In-context retrieval-augmented language models",
                    "A simple language model for task-oriented dialogue",
                    "Dense passage retrieval for open-domain question answering",
                    "TAPEX: Table pre-training via learning a neural SQL executor",
                    "Latent retrieval for weakly supervised open domain question answering",
                    "Semantic parsing via staged query graph generation: Question answering with knowledge base",
                    "K-Bert: Enabling language representation with knowledge graph",
                    "KILT: a benchmark for knowledge intensive language tasks",
                    "Table cell search for question answering",
                    "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                    "Improving language models by retrieving from trillions of tokens",
                    "Multidomain pretrained language models for green NLP",
                    "GPT-4 technical report",
                    "Compositional semantic parsing on semi-structured tables",
                    "Tabfact: A large-scale dataset for table-based fact verification",
                    "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
                    "Open question answering over tables and text",
                    "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
                    "A comprehensive exploration on wikisql with table-aware word contextualization",
                    "Few-Shot learning with retrieval augmented language models",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "IIRelated WorkII-ARetrieval AugmentationIncorporating retrieved information from external sources into LMs has proven to be effective in a wide range of knowledge-intensive tasks, including factual question answering, fact checking, etc.[1,2,4]. Previous works encapsulated the retriever-generator paradigm, leveraging the advantage of generative models[5]and neural retrievers[20,4,21]. Rather than solely relying on inherent knowledge and reasoning capabilities, retrieval augmentation approaches enhance the LM by integrating a retriever component capable of sourcing knowledge from the external corpus[4,5,22]. Specifically, previous retrieval augmentation methods[23,11]required fine-tuning the core fractions of LM to adapt to the retriever on specific downstream tasks. With the emergence of LLMs, there have been endeavors to employ LLMs as generators for question-answering tasks under few-shot and zero-shot setting after the information retrieval process[1,10,24]. Since then, fine-tuning LLMs becomes prohibitively expensive as the number of unique demands continue to increase[25]. Moreover, many state-of-the-art LLMs can only be accessed via black-box APIs[26,27]. These APIs enable users to submit queries and receive responses but generally do not support fine-tuning. To enhance the accuracy of LLMs in answering user queries, substantial prompt engineering or fine-tuning retrievals using small-scaled LMs is required is necessary[10,24].II-BStructured Knowledge-Base Retrieval AugmentationRetrieval augmentation techniques can be applied to both structured and unstructured knowledge bases. For retrieval from structured knowledge bases, Sun et al.[28]explored open-domain question answering only from web tables without leveraging the unstructured text data. Recent studies have delved into Machine Reading Comprehension (MRC) with tables, although without a retrieval module[29,30,31]. Furthermore, Chen et al.[32]conducted research on open-domain question answering utilizing both tabular data and textual sources.Retrieval Augmentation of primary unstructured databases is the process of retrieving relevant information from unstructured data, such as text and knowledge graph, according to the user queries, and then making LMs generate responses. This line of research is commonly referred to as TextQA, primarily designed for open-domain question answering tasks from textual data. Previous approaches often relied on graph neural networks for fine-tuning or adopted linearization of structured knowledge and combine it with texts[33,34,35,36]. By framing user queries, structured knowledge, and outputs in the text-to-text format[37], our work aims to advance the field of retrieval augmentation for structured database.In this paper, we focus on a structured knowledge base (SKB) that restore substantial amounts of authoritative data, often presented in tabular form. It is worth noting that, in contrast to relational databases and traditional KBs, tables can be best described as semi-structured information. In the financial field, structured tabular data can, to a certain extent, ensure data accuracy and consistency. Therefore, when performing retrieval augmentation on SKB, it is crucial to prioritize the retrieval performance. Previous studies have employed various methods, including relational chains and deep neural networks[28], beam search[29], iterative retrievers[32], among others. However, this paper primarily focuses on utilizing LLMs for retrieval augmentation for structured databases.",
                "abstract": "Retrieval augmentation is critical when Language Models (LMs) exploit non-parametric knowledge related to the query through external knowledge bases before reasoning. The retrieved information is incorporated into LMs as context alongside the query, enhancing the reliability of responses towards factual questions. Prior researches in retrieval augmentation typically follow a retriever-generator paradigm. In this context, traditional retrievers encounter challenges in precisely and seamlessly extracting query-relevant information from knowledge bases. To address this issue, this paper introduces a novel retrieval augmentation framework called ChatLR that primarily employs the powerful semantic understanding ability of Large Language Models (LLMs) as retrievers to achieve precise and concise information retrieval. Additionally, we construct an LLM-based search and question answering system tailored for the financial domain by fine-tuning LLM on two tasks including Text2API and API-ID recognition. Experimental results demonstrate the effectiveness of ChatLR in addressing user queries, achieving an overall information retrieval accuracy exceeding 98.8\\%."
            },
            {
                "name": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information",
                "arxiv_id": "2406.11093",
                "subtitles": [
                    "Misinformation detection",
                    "In-context learning"
                ],
                "reference": [
                    "Towards self-supervised cross-domain fake news detection",
                    "Misconfidence-based demonstration selection for llm in-context learning",
                    "Adapt in contexts: Retrieval-augmented domain adaptation via in-context learning",
                    "Sentiment analysis-based social network rumor detection model with bi-directional graph convolutional networks",
                    "Let's learn step by step: Enhancing in-context learning ability with curriculum learning",
                    "Rough-fuzzy graph learning domain adaptation for fake news detection",
                    "Conspemollm: Conspiracy theory detection using an emotion-based large language model",
                    "Emotion detection for misinformation: A review",
                    "Learning sparse alignments via optimal transport for cross-domain fake news detection",
                    "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
                    "Sentiment-aware fake news detection on social media with hypergraph attention networks",
                    "Emotion-guided cross-domain fake news detection using adversarial domain adaptation"
                ],
                "related_work": "2Related Work2.1Misinformation detectionCross-domain misinformation detection:Comito et al. (2023) propose a deep learning-based architecture able to mitigate this problem by yielding high-level cross-domain features.Tang et al. (2023) design the News Optimal Transport to learn transferable features across domains by aligning the source and target news using Optimal Transport (OT) techniques.Shi et al. (2023) develop a rough-fuzzy graph learning framework that uses representations of cross-domain sample uncertainty structural information, and captures shared general features across domains. But these methods all require complex structures and fine-tuning strategies.Based on affective information:Emotion and sentiment are important features for misinformation detectionLiu et al. (2024d) .Zhang et al. (2023b) combine the use of semantic and sentiment information, along with propagation information for rumor detection.Dong et al. (2022a) design a sentiment-aware hyper-graph attention network for fake news detection.Liu et al. (2024b) develop a conspiracy theory detection LLM by fine-tuning EmoLLaMALiu et al. (2024c) .Choudhry et al. (2022) utilize emotional information for fake news detection based on an adversarial learning structure. Unfortunately, these works either have complex structural designs or fine-tuned models, which require significant time and computational resources. The RAEmoLLM in this article applies the ICL method based on affective information, which has a simple structure and does not involve fine-tuning. 2.2In-context learningLiu et al. (2024a) develop in-context curriculum learning, a simple but helpful demonstration ordering method for ICL that gradually increases the complexity of prompt demonstrations.Xu and Zhang (2024) propose in-context reflection to strategically select demonstrations that reduce the discrepancy between the LLM's outputs and the actual input-output mappings.Long et al. (2023) propose a retrieval-enhanced language model to address cross-domain problems, in which they train language models by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. Inspired by this work, we propose the RAEmoLLM.",
                "abstract": "Misinformation is prevalent in various fields such as education, politics, health, etc., causing significant harm to society. However, current methods for cross-domain misinformation detection rely on time and resources consuming fine-tuning and complex model structures. With the outstanding performance of LLMs, many studies have employed them for misinformation detection. Unfortunately, they focus on in-domain tasks and do not incorporate significant sentiment and emotion features (which we jointly call affect). In this paper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework to address cross-domain misinformation detection using in-context learning based on affective information. It accomplishes this by applying an emotion-aware LLM to construct a retrieval database of affective embeddings. This database is used by our retrieval module to obtain source-domain samples, which are subsequently used for the inference module's in-context few-shot learning to detect target domain misinformation. We evaluate our framework on three misinformation benchmarks. Results show that RAEmoLLM achieves significant improvements compared to the zero-shot method on three datasets, with the highest increases of 20.69%, 23.94%, and 39.11% respectively. This work will be released onthis https URL."
            },
            {
                "name": "Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering",
                "arxiv_id": "2406.14277",
                "subtitles": [
                    "Open-Domain Question Answering",
                    "Retrieval-Augmented Generation",
                    "Prompting of Large Language Models"
                ],
                "reference": [
                    "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                    "Retrieval augmented language model pre-training",
                    "Sure: Improving open-domain question answering of llms via summarized retrieval",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Dense passage retrieval for open-domain question answering",
                    "Large language models are zero-shot reasoners",
                    "Atlas: Few-shot learning with retrieval augmented language models",
                    "Towards understanding chain-of-thought prompting: An empirical study of what matters",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Chain-of-verification reduces hallucination in large language models",
                    "Unsupervised dense information retrieval with contrastive learning",
                    "Text embeddings by weakly-supervised contrastive pre-training",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                    "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
                    "Improving language models by retrieving from trillions of tokens",
                    "The probabilistic relevance framework: Bm25 and beyond",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Internet-augmented language models through few-shot prompting for open-domain question answering",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Prompting gpt-3 to be reliable",
                    "Sentence-bert: Sentence embeddings using siamese bert-networks",
                    "Language models are few-shot learners",
                    "Precise zero-shot dense retrieval without relevance labels",
                    "Raptor: Recursive abstractive processing for tree-organized retrieval",
                    "Reading wikipedia to answer open-domain questions"
                ],
                "related_work": "2Related Work2.1Open-Domain Question AnsweringChen et al. [2017]first proposed retrieve-and-read system for solving open-domain question answering tasks. Following conventional lexical-based sparse retriever systems like BM25Robertson et al. [2009], DPRKarpukhin et al. [2020]proposed adense passage retrievalfor a semantic retriever system. The semantic retriever is based on sentence embeddings, and there have been a number of works for improving embeddingsReimers and Gurevych [2019], Xiong et al. [2020], Izacard et al. [2021], Wang et al. [2022], Gao et al. [2023]. By contrast, the reader system which extracts answers from retrieved documents consists of extractive methods such as BERTKenton and Toutanova [2019]or RoBERTaLiu et al. [2019]and generative methods like BARTLewis et al. [2020b]or T5Raffel et al. [2020].2.2Retrieval-Augmented GenerationAugmenting language models with retrieved information from external knowledge sources has proven effective for a wide range of NLP tasksGuu et al. [2020], Lewis et al. [2020a], Borgeaud et al. [2022]. In the LLM era,Lazaridou et al. [2022], Izacard et al. [2023]proposed an in-context learning-based retrieval-augmented generation methods.Asai et al. [2023]proposed Self-RAG which generates and reflects on retrieved passages and own generated text using reflection tokens.Sarthi et al. [2023]proposed recursive retrieval methods using embedding, clustering, and summarizing chunks of text, where they construct a tree with differing levels of summarization.Kim et al. [2023b]proposed summarizing retrieved passages conditioned on candidate answers to select the more relevant context.2.3Prompting of Large Language ModelsGPT-3Brown et al. [2020]opened a few-shot learning era of language models.Si et al. [2022]extensively studied about prompting of GPT-3 using manually designed prompts on diverse tasks. They showed that GPT-3 is more reliable with proper prompts.Wei et al. [2022], Kojima et al. [2022]proposed chain-of-thoughts (CoT) , which decomposes a problem into multi-step subproblems. In addition, there have been modified works of CoTWang et al. [2023b], Dhuliawala et al. [2023], Yao et al. [2024], Besta et al. [2024].Wang et al. [2023c]extensively studied the properties of CoT, and illustrated that even incorrect reasoning paths can improve performances of LLM reasonings.",
                "abstract": "Retrieval-augmented generation (RAG) has received much attention for Open-domain question-answering (ODQA) tasks as a means to compensate for the parametric knowledge of large language models (LLMs). While previous approaches focused on processing retrieved passages to remove irrelevant context, they still rely heavily on the quality of retrieved passages which can degrade if the question is ambiguous or complex. In this paper, we propose a simple yet efficient method called question and passage augmentation (QPaug) via LLMs for open-domain QA. QPaug first decomposes the original questions into multiple-step sub-questions. By augmenting the original question with detailed sub-questions and planning, we are able to make the query more specific on what needs to be retrieved, improving the retrieval performance. In addition, to compensate for the case where the retrieved passages contain distracting information or divided opinions, we augment the retrieved passages with self-generated passages by LLMs to guide the answer extraction. Experimental results show that QPaug outperforms the previous state-of-the-art and achieves significant performance gain over existing RAG methods. The source code is available at \\url{this https URL}."
            },
            {
                "name": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
                "arxiv_id": "2404.14043",
                "subtitles": [
                    "Query optimisation in RAG",
                    "Retrieve-then-rerank framework"
                ],
                "reference": [
                    "Retrieval-augmented generation for large language models: A survey",
                    "RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation",
                    "Measuring and narrowing the compositionality gap in language models",
                    "Emergent abilities of large language models",
                    "Walking down the memory maze: Beyond context limit through interactive reading",
                    "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models",
                    "React: Synergizing reasoning and acting in language models",
                    "Unsupervised commonsense question answering with self-talk",
                    "Enabling large language models to generate text with citations",
                    "Benchmarking large language models in retrieval-augmented generation",
                    "Sparqa: skeleton-based semantic parsing for complex questions over knowledge bases",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
                    "Generated knowledge prompting for commonsense reasoning",
                    "Least-to-most prompting enables complex reasoning in large language models",
                    "Zero-shot listwise document reranking with a large language model",
                    "Multi-hop reading comprehension through question decomposition and rescoring",
                    "Towards verifiable text generation with evolving memory and self-reflection",
                    "Making retrieval-augmented language models robust to irrelevant context",
                    "Text modular networks: Learning to decompose tasks in the language of existing models",
                    "Retrieval-generation synergy augmented large language models",
                    "Verify-and-edit: A knowledge-enhanced chain-of-thought framework"
                ],
                "related_work": "5Related WorksQuery optimisation in RAGThe optimization of user original queries is a critical area of focusGao et al. (2023b) in RAG. Initial approaches have attempted to decompose multi-hop questions using rule-based methods and supervised modelsMin et al. (2019) ; Sun et al. (2020) ; Khot et al. (2021) , or expand the query itself through Generation Augmented RetrievalShwartz et al. (2020) ; Liu et al. (2022) . However, these strategies often fail to pinpoint the gaps in the knowledge of language modelsWith the discovery of the reasoning capabilities inherent in LLMsWei et al. (2022a) , a series of studies represented by CoTWei et al. (2022b) explored the use of the LLM to reform the query. These include static decomposition, where the original problem is dissected into sub-problems simultaneouslyZhou et al. (2022) ; Zhao et al. (2023) , but these methods lack flexibility. Therefore, more dynamic approaches have also been developedShao et al. (2023b) ; Feng et al. (2023) ; Press et al. (2023) ; Yao et al. (2023) ; Kim et al. (2023) , which interact with external information sources in real-time. However,Shao et al. (2023b) ; Feng et al. (2023) simply concatenate retrieved and generated content without clearly identifying the knowledge gaps in each iteration,Press et al. (2023) ; Yao et al. (2023) lacks the step of verification, and is easily misled by the retrieved useless information.Kim et al. (2023) employ a tree structure for more detailed problem decomposition, which can be time-consuming. In contrast to the above methods, our approach prompts the language model to find the missing information and generates simple one-hop problems for more efficient retrieval, with less time costs.Retrieve-then-rerank frameworkRetrieving documents that are relevant to the input query from the extensive pool of knowledge is inherently challengingGao et al. (2024) ; Sun et al. (2024) , especially when there exist irrelevant noise content throughout the contextChen et al. (2024) ; Yoran et al. (2023b) . This noise not only wastes computational resources but also interferes with the generated contentXu et al. (2024) . Therefore, the retrieve-then-rerank paradigm is widely adopted to improve the quality of context by re-ranking retrieved knowledge to filter out the hard negative passagesMa et al. (2023) . To streamline this process and condense the context, researchers suggest creating summaries or snippets pertinentGao et al. (2023a) ; Chen et al. (2023) ; Xu et al. (2024) ; Sun et al. (2024) to serve as knowledge augmentation. Nevertheless,Gao et al. (2023a) ; Chen et al. (2023) do not assess the consistency between the retrieved text and the question,Xu et al. (2024) ; Sun et al. (2024) are limited to coarse-grained reranking. Our approach enhances both aspects, by performing fine-grained filtering at the sentence level and by verifying the entailment between the top-ranked texts and the problem.",
                "abstract": "Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step. Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step. In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules."
            },
            {
                "name": "Crafting the Path: Robust Query Rewriting for Information Retrieval",
                "arxiv_id": "2407.12529",
                "subtitles": [
                    "Information Retrieval",
                    "LLM Based Query Rewriting"
                ],
                "reference": [
                    "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                    "Contextualized query expansion via unsupervised chunk selection for text retrieval",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering",
                    "Dense passage retrieval for open-domain question answering",
                    "Using word embeddings for automatic query expansion",
                    "Rephrase and respond: Let large language models ask better questions for themselves",
                    "Query rewriting in retrieval-augmented large language models",
                    "Okapi at trec",
                    "Ask optimal questions: Aligning large language models with retriever's preference in conversational search",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Relevance weighting of search terms",
                    "Query expansion by prompting large language models",
                    "Deep neural networks for query expansion using word embeddings",
                    "SimLM: Pre-training with representation bottleneck for dense passage retrieval",
                    "Query2doc: Query expansion with large language models",
                    "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
                    "Chain-of-thought prompting elicits reasoning in large language models"
                ],
                "related_work": "4Related WorkInformation RetrievalInformation retrieval is the process of obtaining relevant information from the database based on given queries. The main two methods for information retrieval are sparse retrieval and dense retrieval. A prominent example of the sparse retrieval method is BM25(Robertson and Jones,1976; Robertson et al.,1995) , which serves as a ranking function to evaluate the relevance between a given query and documents. In contrast, dense retrieval method(Xiong et al.,2021; Qu et al.,2021) involves fetching passages that exhibit high similarity to the query using the document embeddings. This approach typically utilizes pre-trained language models such as BERT(Devlin et al.,2018) for the encoder, and some methods fine-tune these encoders.(Karpukhin et al.,2020; Wang et al.,2023b) . In this work, we improve the performance of both sparse and dense retrieval methods by focusing on query rewriting method.LLM Based Query RewritingQuery rewriting refers to the task that modifies the original query to improve the search results for the information retrieval systems. Some studies on query rewriting have employed neural networks to produce or select expansion terms(Zheng et al.,2021; Roy et al.,2016; Imani et al.,2019) , typically through training or fine-tuning a model. In contrast, our approach leverages the inherent capabilities of Large Language Models (LLMs) without requiring training or fine-tuning. Recent studies on query rewriting mainly use the large language models to create relevant information for the given query.query2doc(Q2D) (Wang et al.,2023c) operates by generating a pseudo-document based on the original query, which is then used as input for retriever. Similarly,query2cot(Q2C) employs a Chain of Thought (CoT) (Wei et al.,2022) approach, andquery2expand(Q2E) (Jagerman et al.,2023) generates semantically equivalent query. These approaches leverage the rewriting of the original query into a form similar to passages in the corpus. This technique leads to significant performance improvement compared to using the base query alone. Furthermore,Rewrite-Retrieve-Read(Ma et al.,2023) introduces a methodology that enhances rewriting performance by incorporating reinforcement learning. Another study,ITER-REGEN(Shao et al.,2023) , improves query quality by feeding the query and retrieved documents into a language model for rewriting, followed by a repeated retrieval process.Rephrase and Respond(Deng et al.,2023) argues that for effective rewriting, queries should be rephrased in a manner that is easier for LLMs to understand. For the conversational serach,(Yoon et al.,2024) proposes a method that generates a variety of queries and uses the rank of retrieved passages to train the LLMs on only the optimal queries. This process is further refined using a DPO(Rafailov et al.,2023) approach to create optimal queries.Our work has similarities with recent efforts like Q2D(Wang et al.,2023c) and Q2E(Jagerman et al.,2023) , particularly in using linguistic techniques to expand queries. Q2D and Q2C perform well on queries where the rewriting model can produce accurate answers, but their performance significantly deteriorates when inaccurate answers occur. However, unlike previous approaches, we focus on rewriting queries to improve search performance when the model generates incorrect answers. We demonstrate significant improvements in QA tasks through this problem mitigation. Additionally, we aim to minimize the generation of inaccurate information and concentrate on producing the necessary data for information retrieval.",
                "abstract": "Query rewriting aims to generate a new query that can complement the original query to improve the information retrieval system. Recent studies on query rewriting, such as query2doc, query2expand and querey2cot, rely on the internal knowledge of Large Language Models (LLMs) to generate a relevant passage to add information to the query. Nevertheless, the efficacy of these methodologies may markedly decline in instances where the requisite knowledge is not encapsulated within the model's intrinsic parameters. In this paper, we propose a novel structured query rewriting method called Crafting the Path tailored for retrieval systems. Crafting the Path involves a three-step process that crafts query-related information necessary for finding the passages to be searched in each step. Specifically, the Crafting the Path begins with Query Concept Comprehension, proceeds to Query Type Identification, and finally conducts Expected Answer Extraction. Experimental results show that our method outperforms previous rewriting methods, especially in less familiar domains for LLMs. We demonstrate that our method is less dependent on the internal parameter knowledge of the model and generates queries with fewer factual inaccuracies. Furthermore, we observe that \\name{} demonstrates superior performance in the retrieval-augmented generation scenarios."
            },
            {
                "name": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
                "arxiv_id": "2402.11457",
                "subtitles": [
                    "Perception of Knowledge Boundaries",
                    "Retrieval Augmentation"
                ],
                "reference": [
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering",
                    "Black-box adversarial attacks against dense retrieval models: A multi-view contrastive learning method",
                    "Revisiting the calibration of modern neural networks",
                    "Dense passage retrieval for open-domain question answering",
                    "Retrieval augmented language model pre-training",
                    "Replug: Retrieval-augmented black-box language models",
                    "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                    "Are large language models good at utility judgments",
                    "Unitedqa: A hybrid approach for open domain question answering",
                    "Do large language models know what they don't know",
                    "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                    "End-to-end training of multi-document reader and retriever for open-domain question answering",
                    "On calibration of modern neural networks",
                    "Alignment for honesty",
                    "How can we know when language models know? on the calibration of language models for question answering",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Generate rather than retrieve: Large language models are strong context generators"
                ],
                "related_work": "2Related WorkPerception of Knowledge Boundaries.Previous studies have investigated whether modern neural networks(Guo et al.,2017; Minderer et al.,2021) , pre-trained language models(Jiang et al.,2021) , and large language models(Yin et al.,2023; Ren et al.,2023) clearly perceive their knowledge boundaries. Modern neural networks(Guo et al.,2017; Minderer et al.,2021) and pre-trained language models(Jiang et al.,2021) have been shown to exhibit poor perception, often displaying overconfidence. These studies typically explore and improve the perception of knowledge boundaries based on the logits output by the model, which may not be applicable to current black-box LLMs. Recently, some studies(Yin et al.,2023; Ren et al.,2023) reveal that LLMs also struggle to perceive their knowledge boundaries and tend to be overconfident.Yang et al. (2023) have proposed training methods to address this, however, further research is needed to develop training-free methods that also work effectively on black-box models.Retrieval Augmentation.The mainstream retrieval augmentation methods primarily follow a retrieve-then-read pipeline and perform retrieval augmentation for all the questions. Given a question, the model first retrieves a set of relevant documents from a large-scale knowledge base. Then, the reader combines its internal knowledge with these documents to generate the answer. The research on this pipeline can be categorized into three main categories: improving the retriever(Karpukhin et al.,2020; Qu et al.,2020; Liu et al.,2023) or the reader(Izacard and Grave,2020; Cheng et al.,2021) or training these two parts jointly(Lewis et al.,2020; Singh et al.,2021; Guu et al.,2020) . Recently, Some studies explore retrieval augmentation on LLMs(Shi et al.,2023; Yu et al.,2022; Zhang et al.,2024) . However, the quality of retrieved documents cannot be guaranteed, and retrieval results in additional overhead. Therefore, in this paper, we focus on adaptive retrieval augmentation(Mallen et al.,2023; Ren et al.,2023) , only providing documents when LLMs lack confidence in the answer.",
                "abstract": "Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls."
            },
            {
                "name": "Benchmarking Retrieval-Augmented Generation for Medicine",
                "arxiv_id": "2402.13178",
                "subtitles": [
                    "Retrieval-augmented Generation",
                    "Biomedical Question Answering"
                ],
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Retrieval-augmented generation for large language models: A survey",
                    "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                    "Literature-augmented clinical outcome prediction",
                    "Biomedical question answering: a survey of approaches and challenges",
                    "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                    "Capabilities of gpt-4 on medical challenge problems",
                    "Bioreader: a retrieval-enhanced text-to-text transformer for biomedical literature",
                    "Retrieve, summarize, and verify: How will chatgpt impact information seeking from the medical literature",
                    "In-context retrieval-augmented language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Large language models encode clinical knowledge",
                    "Active retrieval augmented generation",
                    "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
                    "Question answering in biomedicine",
                    "Almanac retrieval-augmented language models for clinical medicine",
                    "Augmenting black-box llms with medical textbooks for clinical question answering",
                    "Domain-specific language model pretraining for biomedical natural language processing",
                    "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
                    "Deep bidirectional language-knowledge graph pretraining",
                    "Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering",
                    "BERT: Pre-training of deep bidirectional transformers for language understanding",
                    "Improving language models by retrieving from trillions of tokens",
                    "Meditron-70b: Scaling medical pretraining for large language models",
                    "Pmc-llama: Further finetuning llama on medical papers",
                    "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
                    "Augmented language models: a survey",
                    "Towards expert-level medical question answering with large language models",
                    "Biomedical question answering: A survey",
                    "Paperqa: Retrieval-augmented generative agent for scientific research"
                ],
                "related_work": "2Related Work2.1Retrieval-augmented GenerationRetrieval-Augmented Generation (RAG) was proposed byLewis et al. (2020) to enhance the generation performance on knowledge-intensive tasks by integrating retrieved relevant information. RAG not only mitigates the problem of hallucinations as LLMs are grounded on given contexts, but can also provide up-to-date knowledge that might not be encoded by the LLMs. Many follow-up studies have been carried out to improve over the vanilla RAGBorgeaud et al. (2022) ; Ram et al. (2023) ; Gao et al. (2023) ; Jiang et al. (2023) ; Mialon et al. (2023) .In biomedicine, there have also been various explorations on how LLMs can improve literature information-seeking and clinical decision-making with RAGFrisoni et al. (2022) ; Naik et al. (2022) ; Jin et al. (2023b) ; L\u00e1la et al. (2023) ; Zakka et al. (2024) ; Jeong et al. (2024) ; Wang et al. (2023) , but their evaluations are not comprehensive. Nevertheless, current systematic evaluations in biomedicine typically focus on the vanilla LLMs without RAG(Chen et al.,2023a; Nori et al.,2023a) . Our study provides the first systematic evaluations of RAG systems in medicine.2.2Biomedical Question AnsweringBiomedical or medical question answering (QA) is a widely studied task since various information needs are expressed by natural language questions in biomedicineZweigenbaum (2003) ; Athenikos and Han (2010) ; Jin et al. (2022) . While BERT-basedDevlin et al. (2019) models used to be the state-of-the-art methods of medical QAAbacha et al. (2019) ; Lee et al. (2020) ; Soni and Roberts (2020) ; Gu et al. (2021) ; Yasunaga et al. (2022) , they are outperformed by LLMs with large marginsSinghal et al. (2023b) ; Chen et al. (2023b) ; Nori et al. (2023b) . Due to their knowledge-intensive nature, QA datasets are commonly used to evaluate the biomedical capabilities of both general LLMs(Nori et al.,2023a,b) and domain-specific LLMsLuo et al. (2022) ; Chen et al. (2023b) ; Wu et al. (2023) ; Singhal et al. (2023a,b) . Following these studies, we also use medical QA datasets to test if a RAG system can retrieve and leverage relevant contexts. Unlike prior efforts, our evaluation employs both RAG and question-only retrieval settings, a more realistic evaluation for medical QA.",
                "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine."
            },
            {
                "name": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
                "arxiv_id": "2404.10774",
                "subtitles": [
                    "Hallucinations in LLMs",
                    "Methods in Detecting Hallucinations",
                    "Entailment Datasets"
                ],
                "reference": [
                    "Linguistically-informed transformations (LIT) : A method for automatically generating contrast sets",
                    "Evaluating the factual consistency of abstractive text summarization",
                    "On faithfulness and factuality in abstractive summarization",
                    "Evaluating large language models on medical evidence summarization",
                    "Benchmarking large language models in retrieval-augmented generation",
                    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                    "A survey of hallucination in large foundation models",
                    "A large annotated corpus for learning natural language inference",
                    "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
                    "SummEval: Re-evaluating Summarization Evaluation",
                    "The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering",
                    "Evaluating verifiability in generative search engines",
                    "Summarizing, simplifying, and synthesizing medical evidence using GPT-3 (with varying success",
                    "Multilingual simplification of medical texts",
                    "Language models hallucinate, but may excel at fact verification",
                    "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
                    "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                    "Evaluating factuality in generation with dependency-level entailment",
                    "Siren's song in the AI ocean: a survey on hallucination in large language models",
                    "A broad-coverage challenge corpus for sentence understanding through inference",
                    "ExpertQA: Expert-curated questions and attributed answers",
                    "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
                    "WiCE: Real-world entailment for claims in Wikipedia",
                    "SummaC: Re-visiting NLI-based models for inconsistency detection in summarization",
                    "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
                    "Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors",
                    "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output",
                    "DocNLI: A large-scale dataset for document-level natural language inference",
                    "RARR: Researching and revising what language models say, using language models",
                    "Annotation artifacts in natural language inference data",
                    "Evaluating correctness and faithfulness of instruction-following models for question answering"
                ],
                "related_work": "8Related WorkHallucinations in LLMsLLMs are prone to hallucinations across various settingsHuang et al. (2023) ; Zhang et al. (2023b) ; Rawte et al. (2023) , generating information that cannot be supported by any source. For example, in the closed-book setting, where LLMs rely solely on their parametric knowledge, they may fabricate details when describing biographies or providing Wikipedia entity informationMin et al. (2023) ; Guan et al. (2023) ; Mallen et al. (2023) . In retrieval-augmented settings, where models have access to external documents to provide responses to user queries, they may generate supplementary information that is not faithful to the provided documentsChiesurin et al. (2023) ; Adlakha et al. (2023) ; Chen et al. (2024) . Even when LLMs are provided with gold documents, such as in text summarization and simplification tasks, they still generate factually inconsistent outputs with diverse error types across different domainsJoseph et al. (2023) ; Shaib et al. (2023) ; Tang et al. (2024,2023c) . In this work, we construct a new benchmark dataset,LLM-AggreFact, which unifies human-annotated model responses across all settings, and evaluate the performance of existing fact-checkers and our proposed ones on the benchmark in detecting such errors.Methods in Detecting HallucinationsWhen documents are directly available for model-generated sentences, such as in text summarizationFalke et al. (2019) ; Kryscinski et al. (2020) ; Maynez et al. (2020) ; Fabbri et al. (2021) ; Tang et al. (2023a) or retrieval-augmented generationLiu et al. (2023) ; Malaviya et al. (2024) , the entire claims are directly verified against the source documents. However, in cases where such documents are not readily available, such as in close-book generation,Gao et al. (2023) ; Min et al. (2023) ; Wang et al. (2023) decompose each generated sentence into atomic facts and then search for relevant documents to support each atomic fact. Alternatively,Malaviya et al. (2024) directly search for relevant documents for each sentence as a whole.There are two main approaches to verifying sentences against documents. The first involves training specialized fact-checkers specifically designed for factual consistency evaluation, which are primarily evaluated in the context of summarizationKryscinski et al. (2020) ; Fabbri et al. (2022) ; Goyal and Durrett (2020) ; Laban et al. (2022) . The second approach leverages LLMs as fact-checkers, particularly for evaluating LLM-generated responses from retrieval-augmented generation and closed-book generationMin et al. (2023) ; Wang et al. (2023) ; Malaviya et al. (2024) ; Gao et al. (2023) . In this work, we bridge the gap between these two approaches by evaluating both specialized fact-checkers and LLM-based fact-checkers across all these settings using our new benchmark,LLM-AggreFact. We show that our best model can match GPT-4 performance and perform well in all settings without doing sentence decomposition.Entailment DatasetsOur work contributes a new dataset for training textual entailment models over documents or document-chunks. Most prior entailment datasets have been human-authoredBowman et al. (2015) ; Williams et al. (2018) , which is known to introduce artifactsGururangan et al. (2018) , or collected in the wildKamoi et al. (2023) , which is challenging to scale. Past work has automatically generated contrast sets for NLILi et al. (2020) . DocNLIYin et al. (2021) is a restructure of existing datasets where the length of most training examples cannot fit into the input limit of small models. Our work differs from these in its hand-built, synthetic nature to encourage multi-sentence and multi-fact reasoning, which is important to the task of fact-checking on grounding documents.",
                "abstract": "Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models."
            },
            {
                "name": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
                "arxiv_id": "2404.13081",
                "subtitles": [
                    "Open-domain question answering",
                    "Retrieval-augmented language models"
                ],
                "reference": [
                    "Evaluating open-domain question answering in the era of large language models",
                    "Retrieval augmented language model pre-training",
                    "Nonparametric masked language modeling",
                    "Few-shot learning with retrieval augmented language models",
                    "Dense passage retrieval for open-domain question answering",
                    "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                    "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                    "Unsupervised dense information retrieval with contrastive learning",
                    "Lost in the middle: How language models use long contexts",
                    "The trec-8 question answering track report",
                    "Improving language models by retrieving from trillions of tokens",
                    "The probabilistic relevance framework: Bm25 and beyond",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Internet-augmented language models through few-shot prompting for open-domain question answering",
                    "Evaluation of chatgpt and microsoft bing ai chat performances on physics exams of vietnamese national high school graduation examination",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Prompting gpt-3 to be reliable",
                    "Language models are few-shot learners",
                    "Replug: Retrieval-augmented black-box language models",
                    "Reta-llm: A retrieval-augmented large language model toolkit",
                    "Reading wikipedia to answer open-domain questions",
                    "Common crawl news dataset"
                ],
                "related_work": "2Related WorkOpen-domain question answering.Open-domain question answering (ODQA) (Voorhees et al.,1999) is a task that requires responding to factual questions using external knowledge sources(Zhu et al.,2015; Nagel,2016) . Recently, there has been significant research interest in ODQA systems, under a framework known as theretriever-and-readsystem(Chen et al.,2017) . The role ofretrieveris to extract the relevant pieces of information from the given knowledge sources. For the retriever, there are two different popular methods: one is a lexical-based retriever,e.g., TF-IDF or BM25(Robertson et al.,2009) , and the other is a sentence embedding-based retriever such as DPR(Karpukhin et al.,2020) or Contriver(Izacard et al.,2022) . On the other hand, thereaderis responsible for aggregating and reasoning with the retrieved information to generate answers. Usually, recent transformer-based language models (LMs) such as BERT(Kenton & Toutanova,2019) or T5(Raffel et al.,2020) are widely adopted for the reader after fine-tuning. In contrast, LLMs exhibit comparable performance or outperform in QA without fine-tuning(Kamalloo et al.,2023; Shi et al.,2023) , which indicates a potential to serve as a universal QA system(Xuan-Quy et al.,2023) .Retrieval-augmented language models.Similar to enhancing QA systems with retriever in ODQA, augmenting LMs with relevant information retrieved from external knowledge sources has been demonstrated as an effective way to improve the performance of LMs on various NLP tasks(Guu et al.,2020; Lazaridou et al.,2022; Min et al.,2022; Liu et al.,2023a) , by reducing hallucination of LLMs and leveraging external knowledge which is not seen during pre-training. To construct such retrieval-augmented LMs, the standard approach is conducting additional fine-tuning to learn how to incorporate the retrieved information(Guu et al.,2020; Borgeaud et al.,2022; Izacard et al.,2023) . However, when considering the recent nature of LLMs with increasing scale and providing black-box API only, such a direction becomes less attractive. One promising direction to address this challenge is investigating a betterprompting(Brown et al.,2020) , which incorporates the retrieved information as additional inputs in a sophisticated way. However, this direction has been only limitedly explored. Appending the retrieval(Si et al.,2023; Trivedi et al.,2023) is a common practice for prompting, butLiu et al. (2023b) recently revealed its limitation in utilizing the retrieved information. Aggregating the predictions from each retrieved passage has been also explored(Lazaridou et al.,2022; Shi et al.,2023) , but LLMs can't see a full context of retrieved information in this case. More discussions about the summarization of retrieval in open-domain context are in AppendixG.",
                "abstract": "Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Augmenting retrieved passages via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers for a given question, which are well-supported by the summarized retrieval that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.6% in exact match (EM) and 4.0% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans."
            },
            {
                "name": "Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation",
                "arxiv_id": "2404.05970",
                "subtitles": [
                    "Personalized Text Generation",
                    "Retrieval Optimization in Retrieval-Augmented Generation",
                    "Information Access with Multiple Retrieval Models"
                ],
                "reference": [
                    "Is Retriever Merely an Approximator of Reader",
                    "PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
                    "Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods",
                    "Pchatbot: A Large-Scale Dataset for Personalized Chatbot",
                    "Predicting Efficiency/Effectiveness Trade-Offs for Dense vs. Sparse Retrieval Strategy Selection",
                    "User Language Model for Collaborative Personalized Search",
                    "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging",
                    "Modeling the Impact of Short- and Long-Term Behavior on Search Personalization",
                    "An Outranking Approach for Rank Aggregation in Information Retrieval",
                    "Towards Controllable and Personalized Review Generation",
                    "Automatic Prompt Rewriting for Personalized Text Generation",
                    "Query Specific Rank Fusion for Image Retrieval",
                    "PENS: A Dataset and Generic Framework for Personalized News Headline Generation",
                    "UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis",
                    "Relevance Feedback and Personalization: A Language Modeling Perspective",
                    "Personalized Language Model for Query Auto-Completion",
                    "A Personalized Dense Retrieval Framework for Unified Information Access",
                    "Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models",
                    "Training Millions of Personalized Dialogue Agents",
                    "Selecting which Dense Retriever to use for Zero-Shot Search",
                    "Generating Personalized Recipes from Historical User Preferences",
                    "Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination",
                    "Personalizing Dialogue Agents: I have a dog, do you have pets too",
                    "LaMP: When Large Language Models Meet Personalization",
                    "Personalized Response Generation via Generative Split Memory Network",
                    "Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation",
                    "Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations",
                    "Refocusing on Relevance: Personalization in NLG",
                    "Distilling Knowledge from Reader to Retriever for Question Answering",
                    "Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering",
                    "Personalized Search: Potential and Pitfalls",
                    "Teach LLMs to Personalize - An Approach inspired by Writing Education",
                    "Compact Personalized Models for Neural Machine Translation",
                    "Returning the N to NLP: Towards Contextually Personalized Classification Models",
                    "Automatic ranking of information retrieval systems using data fusion",
                    "A rank fusion approach based on score distributions for prioritizing relevance assessments in information retrieval evaluation",
                    "PERSON: Personalized information retrieval evaluation based on citation networks"
                ],
                "related_work": "2.Related WorkPersonalized Text GenerationPersonalization has been a focal point of research in various domains, particularly within search and recommendation systems(Bennett et al.,2012; Dumais,2016; Croft et al.,2001; Tabrizi et al.,2018; Zeng et al.,2023) . This exploration spans diverse contexts, encompassing areas such as query auto-completion(Jaech and Ostendorf,2018) and collaborative personalized search(Xue et al.,2009) . Within the NLP community, personalization has been a subject of exploration in various applications, including but not limited to dialogue agents(Wu et al.,2021; Zhang et al.,2018; Mazar\u00e9 et al.,2018; Zhong et al.,2022; Qian et al.,2021; Vincent et al.,2023) , review(Li and Tuzhilin,2019) and recipe generation(Majumder et al.,2019) , translation(Wuebker et al.,2018) , headline generation(Ao et al.,2021) , and classification tasks(Flek,2020; Dudy et al.,2021) , such as personalized sentiment analysis(Mireshghallah et al.,2022) .With the emergence of LLMs and their application across various NLP tasks,Salemi et al.(2023) proposed a retrieval-augmented approach for personalizing LLMs. They also introduced LaMP, a benchmark designed to assess the performance of personalized NLP models across diverse classification and short text generation tasks. The work byLi et al.(2023b) addresses a similar issue, focusing on personalized long text generation. Furthermore,Mysore et al.(2023) assesses the capabilities of LLMs in the role of writing assistants. Various approaches have been explored for personalizing LLMs, encompassing techniques such as summarizing user profile(Richardson et al.,2023) , aligning language models with personalized human feedback(Jang et al.,2023) , automatic prompt generation tailored to individual users(Li et al.,2023a) , and incorporating long and short-term memory-based personalization strategies(Zhang et al.,2023) . In this study, we adhere to the methodology outlined bySalemi et al.(2023) and conduct experiments using the LaMP benchmark, focusing on training a component for retrieving personal information from user profile.Retrieval Optimization in Retrieval-Augmented GenerationThe optimization of retrieval models within the RAG (Retrieval-Augmented Generation) pipelines has emerged as a focal point in recent research, particularly in the context of question answering.Yang and Seo (2020) focuses on distilling knowledge from the LM to the retriever by minimizing the KL-divergence between the LM's performance for each document in the retrieved set and the assigned score by the retriever to that document in the set. Additionally,Izacard and Grave (2021) employs the attention weights of the LM to determine the importance of each document. This information is then utilized to distill knowledge from the LM to the retriever, aligning with the objectives set forth byYang and Seo (2020) . The approach presented byWang et al.(2023) involves the use of reinforcement learning, where the reward function is derived from the performance of the LM. In this work, we adopt methods similar to that ofWang et al.(2023) andYang and Seo (2020) , given the absence of relevance data in the LaMP benchmark. Notably, our work stands out as the pioneering effort in leveraging feedback from LLMs to train personalized retrievers for personalizing LLMs. Furthermore, in all the previously mentioned approaches, the language model is trained after/with the retrieval model. In contrast, our approach assumes the language model is frozen, and our focus is solely on optimizing the retrieval model.Information Access with Multiple Retrieval ModelsCombining rank lists generated by different retrievers has been extensively explored in the literature(Nuray and Can,2006; Losada et al.,2018; Farah and Vanderpooten,2007; Cormack et al.,2009) . However, the process of rank fusion presents challenges, especially when dealing with discrepancies in scoring scales among retrieval systems or the absence of overlapping documents in the ranked lists(Losada et al.,2018; Zhang et al.,2015) . Alternatively, methods for selecting specific retriever from a retriever pool for different datasets has been explored(Khramtsova et al.,2023) . Furthermore, Arabzadeh et al.(2021b) investigates the optimal use of dense and sparse retrievers for each query, considering efficiency trade-offs. In our study, we concentrate on the performance-oriented selection of query-specific retrievers from a retriever pool.",
                "abstract": "This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization -- one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model. This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input. Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets."
            }
        ],
        "survey": {
            "name": "Large Language Models for Information Retrieval: A Survey",
            "arxiv_id": "2308.07107",
            "subtitles": [
                {
                    "name": "Background",
                    "key_history": [
                        {
                            "reference_title": "Introduction to Modern Information Retrieval",
                            "key_word": "Boolean Model"
                        },
                        {
                            "reference_title": "A general language model for information retrieval",
                            "key_word": "Statistical Language Models"
                        },
                        {
                            "reference_title": "Okapi at TREC-3",
                            "key_word": "BM25"
                        },
                        {
                            "reference_title": "Language models are unsupervised multitask learners",
                            "key_word": "GPT"
                        },
                        {
                            "reference_title": "Qlora: Efficient finetuning of quantized llms",
                            "key_word": "QLoRA"
                        },
                        {
                            "reference_title": "Lora: Low-rank adaptation of large language models",
                            "key_word": "Parameter-Efficient Fine-Tuning"
                        }
                    ],
                    "references_in_this_section": [
                        "Emergent abilities of large language models",
                        "A vector space model for automatic indexing",
                        "Okapi at TREC",
                        "Neural models for information retrieval",
                        "Scaling laws for neural language models",
                        "Unified scaling laws for routed language models",
                        "Dense text retrieval based on pretrained language models: A survey",
                        "BERT: pre-training of deep bidirectional transformers for language understanding",
                        "Statistical language modeling for information retrieval",
                        "A deep relevance matching model for ad-hoc retrieval",
                        "Query2doc: Query expansion with large language models",
                        "Lora: Low-rank adaptation of large language models",
                        "Deep contextualized word representations",
                        "Prefix-tuning: Optimizing continuous prompts for generation",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "Introduction to Modern Information Retrieval",
                        "Webgpt: Browser-assisted question-answering with human feedback",
                        "A general language model for information retrieval",
                        "Language models are few-shot learners",
                        "Attention is all you need",
                        "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                        "Qlora: Efficient finetuning of quantized llms",
                        "Language models are unsupervised multitask learners",
                        "The power of scale for parameter-efficient prompt tuning"
                    ]
                },
                {
                    "name": "Query Rewriter",
                    "key_history": [
                        {
                            "reference_title": "Query2doc: Query expansion with large language models",
                            "key_word": "LLM Query Rewriting"
                        },
                        {
                            "reference_title": "Large language models know your contextual search intent: A prompting framework for conversational search",
                            "key_word": "Conversational Search"
                        },
                        {
                            "reference_title": "GRM: generative relevance modeling using relevance-aware sample estimation for document retrieval",
                            "key_word": "Generative Relevance Feedback"
                        },
                        {
                            "reference_title": "Query expansion by prompting large language models",
                            "key_word": "Few-shot Prompting"
                        },
                        {
                            "reference_title": "Context aware query rewriting for text rankers using LLM",
                            "key_word": "Retrieval Augmentation"
                        },
                        {
                            "reference_title": "Large language model based long-tail query rewriting in taobao search",
                            "key_word": "E-commerce Query Rewriting"
                        }
                    ],
                    "references_in_this_section": [
                        "An interactive query generation assistant using llm-based prompt modification and user feedback",
                        "GRM: generative relevance modeling using relevance-aware sample estimation for document retrieval",
                        "Dialog inpainting: Turning documents into dialogs",
                        "Convtrans: Transforming web search sessions for conversational dense retrieval",
                        "CONVERSER: few-shot conversational dense retrieval with synthetic data generation",
                        "Generative and pseudo-relevant feedback for sparse, dense and learned sparse retrieval",
                        "Large language models know your contextual search intent: A prompting framework for conversational search",
                        "Can query expansion improve generalization of strong cross-encoder rankers",
                        "Learning interpretable legal case retrieval via knowledge-guided case reformulation",
                        "Query expansion with freebase",
                        "Improving weak ad-hoc queries using wikipedia asexternal corpus",
                        "A corpus analysis approach for automatic query expansion and its extension to multiple databases",
                        "Generate rather than retrieve: Large language models are strong context generators",
                        "Enhancing conversational search: Large language model-aided informative query rewriting",
                        "Large language models are strong zero-shot retriever",
                        "Query expansion by prompting large language models",
                        "Can generative llms create query variants for test collections? an exploratory study",
                        "Context aware query rewriting for text rankers using LLM",
                        "Query expansion techniques for information retrieval: A survey",
                        "Query2doc: Query expansion with large language models",
                        "MS MARCO: A human generated machine reading comprehension dataset",
                        "Chain-of-thought prompting elicits reasoning in large language models",
                        "Wordnet: An electronic lexical database",
                        "Knowledge refinement via interaction between search engines and large language models",
                        "Lexical relations: Enhancing effectiveness of information retrieval systems",
                        "Rafe: Ranking feedback improves query rewriting for RAG",
                        "When do generative query and document expansions fail? A comprehensive study across methods, retrievers, and datasets",
                        "Large language model based long-tail query rewriting in taobao search",
                        "Generate, filter, and fuse: Query expansion via multi-step keyword generation for zero-shot neural rankers",
                        "Natural questions: a benchmark for question answering research",
                        "Automatic thesaurus construction for cross generation corpus",
                        "Direct preference optimization: Your language model is secretly a reward model",
                        "Corpus-steered query expansion with large language models",
                        "Precise zero-shot dense retrieval without relevance labels",
                        "The limitations of term co-occurrence data for query expansion in document retrieval systems",
                        "A new fuzzy logic-based query expansion model for efficient information retrieval using relevance feedback approach",
                        "Crafting the path: Robust query rewriting for information retrieval",
                        "Query rewriting for retrieval-augmented large language models"
                    ]
                },
                {
                    "name": "Retriever",
                    "key_history": [
                        {
                            "reference_title": "Inpars: Data augmentation for information retrieval using large language models",
                            "key_word": "Pseudo Query Generation"
                        },
                        {
                            "reference_title": "Inpars-v2: Large language models as efficient dataset generators for information retrieval",
                            "key_word": "Relevance Filtering"
                        },
                        {
                            "reference_title": "Promptagator: Few-shot dense retrieval from 8 examples",
                            "key_word": "Iterative Data Augmentation"
                        },
                        {
                            "reference_title": "Soft prompt tuning for augmenting dense retrieval with large language models",
                            "key_word": "Soft Prompt Tuning"
                        },
                        {
                            "reference_title": "Questions are all you need to train a dense passage retriever",
                            "key_word": "Relevance Label Generation"
                        },
                        {
                            "reference_title": "Fine-tuning llama for multi-stage text retrieva",
                            "key_word": "Dense Retrieval"
                        },
                        {
                            "reference_title": "Task-aware retrieval with instructions",
                            "key_word": "Task-aware Retriever"
                        },
                        {
                            "reference_title": "Large language models are built-in autoregressive search engines",
                            "key_word": "Generative Retriever"
                        }
                    ],
                    "references_in_this_section": [
                        "How does generative retrieval scale to millions of passages",
                        "Dynamicretriever: A pre-trained model-based IR system without an explicit index",
                        "Inpars: Data augmentation for information retrieval using large language models",
                        "Okapi at TREC",
                        "Large language models are built-in autoregressive search engines",
                        "Pre-training with large language model-based document expansion for dense passage retrieval",
                        "CONVERSER: few-shot conversational dense retrieval with synthetic data generation",
                        "A neural corpus indexer for document retrieval",
                        "Corpusbrain: Pre-train a generative retrieval model for knowledge-intensive language tasks",
                        "Text and code embeddings by contrastive pre-training",
                        "Augtriever: Unsupervised dense retrieval by scalable data augmentation",
                        "Questions are all you need to train a dense passage retriever",
                        "One embedder, any task: Instruction-finetuned text embeddings",
                        "Dense passage retrieval for open-domain question answering",
                        "Mistral 7b",
                        "Fine-tuning llama for multi-stage text retrieval",
                        "Chatretriever: Adapting large language models for generalized and robust conversational dense retrieval",
                        "Inpars-v2: Large language models as efficient dataset generators for information retrieval",
                        "BERT: pre-training of deep bidirectional transformers for language understanding",
                        "SGPT: GPT sentence embeddings for semantic search",
                        "MTEB: massive text embedding benchmark",
                        "Improving text embeddings with large language models",
                        "Task-aware retrieval with instructions",
                        "MS MARCO: A human generated machine reading comprehension dataset",
                        "Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval",
                        "Linq-embed-mistral: Elevating text retrieval with improved gpt data through task-specific control and quality refinement",
                        "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                        "Promptagator: Few-shot dense retrieval from 8 examples",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "Large dual encoders are generalizable retrievers",
                        "Sfr-embedding-mistral: Enhance text retrieval with transfer learning",
                        "Gemma: Open models based on gemini research and technology",
                        "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
                        "Soft prompt tuning for augmenting dense retrieval with large language models",
                        "Scaling sentence embeddings with large language models",
                        "Gecko: Versatile text embeddings distilled from large language models",
                        "Large language models as foundations for next-gen dense retrieval: A comprehensive empirical assessment",
                        "Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks",
                        "Nv-embed: Improved techniques for training llms as generalist embedding models",
                        "Llama: Open and efficient foundation language models",
                        "Autoregressive search engines: Generating substrings as document identifiers",
                        "UDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers",
                        "Transformer memory as a differentiable search index",
                        "Rethinking search: making domain experts out of dilettantes",
                        "Making large language models A better foundation for dense retrieval",
                        "Textbooks are all you need",
                        "Instruction tuning with GPT"
                    ]
                },
                {
                    "name": "RERANKER",
                    "key_history": [
                        {
                            "reference_title": "Multi-stage document ranking with BERT",
                            "key_word": "Encoder-only Reranker"
                        },
                        {
                            "reference_title": "The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models",
                            "key_word": "Encoder-Decoder"
                        },
                        {
                            "reference_title": "Rankt5: Fine-tuning T5 for text ranking with ranking losses",
                            "key_word": "Pairwise and Listwise Losses"
                        },
                        {
                            "reference_title": "Fine-tuning llama for multi-stage text retrieval",
                            "key_word": "Decoder-only"
                        },
                        {
                            "reference_title": "Improving passage retrieval with zero-shot question generation",
                            "key_word": "Pointwise Methods"
                        },
                        {
                            "reference_title": "Is chatgpt good at search? investigating large language models as re-ranking agents",
                            "key_word": "Listwise Methods"
                        },
                        {
                            "reference_title": "Large language models are effective text rankers with pairwise ranking prompting",
                            "key_word": "Pairwise Methods"
                        },
                        {
                            "reference_title": "Exaranker: Synthetic explanations improve neural rankers",
                            "key_word": "ExaRanker"
                        }
                    ],
                    "references_in_this_section": [
                        "Generating diverse criteria on-the-fly to improve point-wise LLM rankers",
                        "Discrete prompt optimization via constrained generation for zero-shot re-ranker",
                        "Leveraging passage embeddings for efficient listwise reranking with large language models",
                        "Expand, highlight, generate: Rl-driven document generation for passage reranking",
                        "Learning to rank using gradient descent",
                        "Q-PEFT: query-dependent parameter efficient fine-tuning for text reranking with large language models",
                        "Beyond yes and no: Improving zero-shot LLM rankers via scoring fine-grained relevance labels",
                        "Exaranker: Synthetic explanations improve neural rankers",
                        "Rankzephyr: Effective and robust zero-shot listwise reranking is a breeze",
                        "Fine-tuning llama for multi-stage text retrieval",
                        "Holistic evaluation of language models",
                        "Document ranking with a pretrained sequence-to-sequence model",
                        "FIRST: faster improved listwise reranking with single token decoding",
                        "The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models",
                        "BERT: pre-training of deep bidirectional transformers for language understanding",
                        "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
                        "Consolidating ranking and relevance predictions of large language models through post-processing",
                        "MS MARCO: A human generated machine reading comprehension dataset",
                        "Tourrank: Utilizing large language models for documents ranking with a tournament-inspired strategy",
                        "Improving passage retrieval with zero-shot question generation",
                        "Open-source large language models are strong zero-shot query likelihood models for document ranking",
                        "Multi-stage document ranking with BERT",
                        "Generating synthetic documents for cross-encoder re-rankers: A comparative study of chatgpt and human experts",
                        "An investigation of prompt variations for zero-shot llm-based rankers",
                        "Listt5: Listwise reranking with fusion-in-decoder improves zero-shot retrieval",
                        "Ranked list truncation for large language model-based re-ranking",
                        "EcoRank: Budget-constrained text re-ranking using large language models",
                        "Large language models are effective text rankers with pairwise ranking prompting",
                        "A two-stage adaptation of large language models for text ranking",
                        "Exaranker-open: Synthetic explanation for IR using open-source llms",
                        "Top-down partitioning for efficient list-wise ranking",
                        "Is chatgpt good at search? investigating large language models as re-ranking agents",
                        "Zero-shot listwise document reranking with a large language model",
                        "Prp-graph: Pairwise ranking prompting to llms with graph aggregation for effective text re-ranking",
                        "A setwise approach for effective and highly efficient zero-shot ranking with large language models",
                        "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
                        "PaRaDe: Passage ranking using demonstrations with LLMs",
                        "Demorank: Selecting effective demonstrations for large language models in ranking task",
                        "Retrieval of the best counterargument without prior topic knowledge",
                        "Gpt-4: A review on advancements and opportunities in natural language processing",
                        "Rankvicuna: Zero-shot listwise document reranking with open-source large language models",
                        "Instruction distillation makes large language models efficient zero-shot rankers",
                        "Inpars-light: Cost-effective unsupervised training of efficient rankers",
                        "Text-to-text multi-view learning for passage re-ranking",
                        "Rankt5: Fine-tuning T5 for text ranking with ranking losses",
                        "Rank-without-gpt: Building gpt-independent listwise rerankers on open-source large language models"
                    ]
                },
                {
                    "name": "READER",
                    "key_history": [
                        {
                            "reference_title": "Retrieval augmented language model pre-training",
                            "key_word": "Once-Retrieval Reader"
                        },
                        {
                            "reference_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
                            "key_word": "Retrieval-Augmented Generation"
                        },
                        {
                            "reference_title": "In-context retrieval-augmented language models",
                            "key_word": "Periodic-Retrieval Reader"
                        },
                        {
                            "reference_title": "Active retrieval augmented generation",
                            "key_word": "Aperiodic-Retrieval Reader"
                        },
                        {
                            "reference_title": "RECOMP: improving retrieval-augmented lms with compression and selective augmentation",
                            "key_word": "Compressor"
                        },
                        {
                            "reference_title": "Learning to filter context for retrieval-augmented generation",
                            "key_word": "FILCO"
                        },
                        {
                            "reference_title": "Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models",
                            "key_word": "Attribution and Fluency"
                        }
                    ],
                    "references_in_this_section": [
                        "Richrag: Crafting rich responses for multi-faceted queries in retrieval-augmented generation",
                        "Language models (mostly) know what they know",
                        "REPLUG: retrieval-augmented black-box language models",
                        "Ralle: A framework for developing and evaluating retrieval-augmented large language models",
                        "Flashrag: A modular toolkit for efficient retrieval-augmented generation research",
                        "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                        "BIDER: bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence",
                        "Improving retrieval-augmented large language models via data importance learning",
                        "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
                        "Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models",
                        "Retrieval helps or hurts? A deeper dive into the efficacy of retrieval augmentation to language models",
                        "REAR: A relevance-aware retrieval-augmented framework for open-domain question answering",
                        "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                        "Unsupervised information refinement training of large language models for retrieval-augmented generation",
                        "When do llms need retrieval augmentation? mitigating llms' overconfidence helps retrieval augmentation",
                        "Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation",
                        "Rethinking with retrieval: Faithful large language model inference",
                        "ATLANTIC: structure-aware retrieval-augmented language model for interdisciplinary science",
                        "Retrieval augmented language model pre-training",
                        "How can we know When language models know? on the calibration of language models for question answering",
                        "Generate-then-ground in retrieval-augmented generation for multi-hop question answering",
                        "In-context retrieval-augmented language models",
                        "Clinfo.ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature",
                        "M-RAG: reinforcing large language model performance through retrieval-augmented generation with multiple partitions",
                        "Augmenting black-box llms with medical textbooks for clinical question answering",
                        "Planrag: A plan-then-retrieval augmented generation for generative large language models as decision makers",
                        "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                        "Answering questions by meta-reasoning over multiple chains of thought",
                        "Improving language models via plug-and-play retrieval feedback",
                        "Learning to filter context for retrieval-augmented generation",
                        "Activerag: Revealing the treasures of knowledge via active learning",
                        "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
                        "xrag: Extreme context compression for retrieval-augmented generation with one token",
                        "Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models",
                        "Retrieval-generation synergy augmented large language models",
                        "Phantom: General trigger attacks on retrieval augmented language generation",
                        "Interpretable long-form legal question answering with retrieval-augmented large language models",
                        "Enabling large language models to generate text with citations",
                        "ATM: adversarial tuning multi-agent system makes a robust retrieval-augmented generator",
                        "Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms",
                        "RETA-LLM: A retrieval-augmented large language model toolkit",
                        "Exploring the integration strategies of retriever and large language models",
                        "Longrag: Enhancing retrieval-augmented generation with long-context llms",
                        "Atlas: Few-shot learning with retrieval augmented language models",
                        "Self-knowledge guided retrieval augmentation for large language models",
                        "Leancontext: Cost-efficient domain-specific question answering using llms",
                        "Don't forget private retrieval: distributed private similarity search for large language models",
                        "RECOMP: improving retrieval-augmented lms with compression and selective augmentation",
                        "Lost in the middle: How language models use long contexts",
                        "Crosslingual retrieval augmented in-context learning for bangla",
                        "PRCA: fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter",
                        "Improving language models by retrieving from trillions of tokens",
                        "Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models",
                        "Enhancing financial sentiment analysis via retrieval augmented large language models",
                        "IM-RAG: multi-round retrieval-augmented generation through learning inner monologues",
                        "Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models",
                        "Freshllms: Refreshing large language models with search engine augmentation",
                        "Empowering large language models to set up a knowledge retrieval indexer via self-learning",
                        "Retrieval-augmented generation for knowledge-intensive NLP tasks",
                        "Measuring and narrowing the compositionality gap in language models",
                        "A tale of trust and accuracy: Base vs. instruct llms in RAG systems",
                        "One token can help! learning scalable and pluggable virtual tokens for retrieval-augmented large language models",
                        "TCRA-LLM: token compression retrieval augmented large language model for inference cost reduction",
                        "Active retrieval augmented generation",
                        "R^2ag: Incorporating retrieval information into retrieval augmented generation",
                        "Typos that broke the rag's back: Genetic attack on RAG pipeline by simulating documents in the wild via low-level perturbations",
                        "Self-dc: When to retrieve and when to generate? self divide-and-conquer for compositional unknown questions",
                        "Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training",
                        "Internet-augmented language models through few-shot prompting for open-domain question answering",
                        "Making retrieval-augmented language models robust to irrelevant context",
                        "Search augmented instruction learning",
                        "Learning to plan for retrieval-augmented large language models from knowledge graphs",
                        "Instructrag: Instructing retrieval-augmented generation with explicit denoising",
                        "RA-DIT: retrieval-augmented dual instruction tuning",
                        "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                        "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP",
                        "Badrag: Identifying vulnerabilities in retrieval augmented generation of large language models"
                    ]
                },
                {
                    "name": "SEARCH AGENT",
                    "key_history": [
                        {
                            "reference_title": "Lamda: Language models for dialog applications",
                            "key_word": "Static Agent"
                        },
                        {
                            "reference_title": "Teaching language models to support answers with verified quotes",
                            "key_word": "GopherCite"
                        },
                        {
                            "reference_title": "A real-world webagent with planning, long context understanding, and program synthesis",
                            "key_word": "WebAgent"
                        },
                        {
                            "reference_title": "Webgpt: Browser-assisted question-answering with human feedback",
                            "key_word": "Dynamic Agent"
                        },
                        {
                            "reference_title": "Hierarchical prompting assists large language model on web navigation",
                            "key_word": "ASH Prompting"
                        }
                    ],
                    "references_in_this_section": [
                        "Webvoyager: Building an end-to-end web agent with large multimodal models",
                        "Teaching language models to support answers with verified quotes",
                        "Webcanvas: Benchmarking web agents in online environments",
                        "Webcpm: Interactive web search for chinese long-form question answering",
                        "Agent-e: From autonomous web navigation to foundational design principles in agentic systems",
                        "Kwaiagents: Generalized information-seeking agent system with large language models",
                        "A real-world webagent with planning, long context understanding, and program synthesis",
                        "Webglm: Towards an efficient web-enhanced question answering system with human preferences",
                        "Lamda: Language models for dialog applications",
                        "Webgpt: Browser-assisted question-answering with human feedback",
                        "TRAD: enhancing LLM agents with step-wise thought retrieval and aligned decision",
                        "Mind2web: Towards a generalist agent for the web",
                        "Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence",
                        "Know where to go: Make LLM a relevant, responsible, and trustworthy searcher",
                        "Cosearchagent: A lightweight collaborative search agent with large language models",
                        "Autowebglm: Bootstrap and reinforce A large language model-based web navigating agent",
                        "Mindsearch: Mimicking human minds elicits deep AI searcher",
                        "Webshop: Towards scalable real-world web interaction with grounded language agents",
                        "Webarena: A realistic web environment for building autonomous agents",
                        "Hierarchical prompting assists large language model on web navigation",
                        "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion"
                    ]
                },
                {
                    "name": "Future Direction",
                    "key_history": [
                        {
                            "reference_title": "Information retrieval meets large language models: A strategic report from chinese IR community",
                            "key_word": "LLM-enhanced IR systems"
                        },
                        {
                            "reference_title": "Query rewriting for retrieval-augmented large language models",
                            "key_word": "Query Rewriting"
                        },
                        {
                            "reference_title": "GPT-4 technical report",
                            "key_word": "Multi-modal Retrieval"
                        }
                    ],
                    "references_in_this_section": [
                        "Cumulated gain-based evaluation of IR techniques",
                        "Information retrieval meets large language models: A strategic report from chinese IR community",
                        "Bleu: a method for automatic evaluation of machine translation",
                        "Llms may dominate information access: Neural retrievers are biased towards llm-generated texts",
                        "Beyond factuality: A comprehensive evaluation of large language models as knowledge generators",
                        "Ai-generated images introduce invisible relevance bias to text-image retrieval",
                        "Mean reciprocal rank",
                        "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus",
                        "GPT-4 technical report",
                        "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
                        "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                        "Is chatgpt good at search? investigating large language models as re-ranking agents",
                        "Zero-shot listwise document reranking with a large language model",
                        "Halueval: A large-scale hallucination evaluation benchmark for large language models",
                        "ROUGE: A package for automatic evaluation of summaries",
                        "Query rewriting for retrieval-augmented large language models",
                        "Intent5: Search result diversification using causal language models"
                    ]
                }
            ],
            "all_references": [
                "Ai-generated images introduce invisible relevance bias to text-image retrieval",
                "Rethinking with retrieval: Faithful large language model inference",
                "Beyond factuality: A comprehensive evaluation of large language models as knowledge generators",
                "An interactive query generation assistant using llm-based prompt modification and user feedback",
                "Rankzephyr: Effective and robust zero-shot listwise reranking is a breeze",
                "Modeling intent graph for search result diversification",
                "Crosslingual retrieval augmented in-context learning for bangla",
                "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering",
                "Large language model based long-tail query rewriting in taobao search",
                "Internet-augmented language models through few-shot prompting for open-domain question answering",
                "TRAD: enhancing LLM agents with step-wise thought retrieval and aligned decision",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "A real-world webagent with planning, long context understanding, and program synthesis",
                "Autowebglm: Bootstrap and reinforce A large language model-based web navigating agent",
                "Chatretriever: Adapting large language models for generalized and robust conversational dense retrieval",
                "Inpars: Data augmentation for information retrieval using large language models",
                "Wordnet: An electronic lexical database",
                "Improving language models via plug-and-play retrieval feedback",
                "Generating synthetic documents for cross-encoder re-rankers: A comparative study of chatgpt and human experts",
                "Generative and pseudo-relevant feedback for sparse, dense and learned sparse retrieval",
                "Webvoyager: Building an end-to-end web agent with large multimodal models",
                "How does generative retrieval scale to millions of passages",
                "Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models",
                "Empowering large language models to set up a knowledge retrieval indexer via self-learning",
                "Gpt-4: A review on advancements and opportunities in natural language processing",
                "Generate, filter, and fuse: Query expansion via multi-step keyword generation for zero-shot neural rankers",
                "BIDER: bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence",
                "A deep relevance matching model for ad-hoc retrieval",
                "Attention is all you need",
                "Proactive retrieval-based chatbots based on relevant knowledge and goals",
                "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus",
                "Intent5: Search result diversification using causal language models",
                "Exaranker: Synthetic explanations improve neural rankers",
                "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                "Okapi at TREC",
                "When do llms need retrieval augmentation? mitigating llms' overconfidence helps retrieval augmentation",
                "Can query expansion improve generalization of strong cross-encoder rankers",
                "Query expansion techniques for information retrieval: A survey",
                "Badrag: Identifying vulnerabilities in retrieval augmented generation of large language models",
                "Discrete prompt optimization via constrained generation for zero-shot re-ranker",
                "A comprehensive survey of ai-generated content (AIGC) : A history of generative AI from GAN to chatgpt",
                "Personalizing search via automated analysis of interests and activities",
                "Improving language models by retrieving from trillions of tokens",
                "Learning interpretable legal case retrieval via knowledge-guided case reformulation",
                "Introduction to Modern Information Retrieval",
                "Top-down partitioning for efficient list-wise ranking",
                "Prp-graph: Pairwise ranking prompting to llms with graph aggregation for effective text re-ranking",
                "Exaranker-open: Synthetic explanation for IR using open-source llms",
                "Zero-shot listwise document reranking with a large language model",
                "Large language models are effective text rankers with pairwise ranking prompting",
                "Content selection network for document-grounded retrieval-based chatbots",
                "Automatic thesaurus construction for cross generation corpus",
                "Gemma: Open models based on gemini research and technology",
                "Task-aware retrieval with instructions",
                "Text-to-text multi-view learning for passage re-ranking",
                "Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms",
                "Augmenting black-box llms with medical textbooks for clinical question answering",
                "Contrastive learning of user behavior sequence for context-aware document ranking",
                "Learning implicit user profiles for personalized retrieval-based chatbot",
                "MS MARCO: A human generated machine reading comprehension dataset",
                "Sfr-embedding-mistral: Enhance text retrieval with transfer learning",
                "Clinfo.ai: An open-source retrieval-augmented large language model system for answering medical questions using scientific literature",
                "Large language models are strong zero-shot retriever",
                "Open-source large language models are strong zero-shot query likelihood models for document ranking",
                "BERT: pre-training of deep bidirectional transformers for language understanding",
                "Exploring the integration strategies of retriever and large language models",
                "Scaling sentence embeddings with large language models",
                "Atlas: Few-shot learning with retrieval augmented language models",
                "Statistical language modeling for information retrieval",
                "Cosearchagent: A lightweight collaborative search agent with large language models",
                "Mind2web: Towards a generalist agent for the web",
                "UDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers",
                "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
                "Bloomberggpt: A large language model for finance",
                "Convtrans: Transforming web search sessions for conversational dense retrieval",
                "MTEB: massive text embedding benchmark",
                "Beyond yes and no: Improving zero-shot LLM rankers via scoring fine-grained relevance labels",
                "Search augmented instruction learning",
                "Don't forget private retrieval: distributed private similarity search for large language models",
                "Image retrieval: Ideas, influences, and trends of the new age",
                "Can generative llms create query variants for test collections? an exploratory study",
                "Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models",
                "IM-RAG: multi-round retrieval-augmented generation through learning inner monologues",
                "ROUGE: A package for automatic evaluation of summaries",
                "Learning to rank using gradient descent",
                "Rankt5: Fine-tuning T5 for text ranking with ranking losses",
                "Cumulated gain-based evaluation of IR techniques",
                "Bleu: a method for automatic evaluation of machine translation",
                "Rafe: Ranking feedback improves query rewriting for RAG",
                "Pretrained language model for text generation: A survey",
                "Towards reasoning in large language models: A survey",
                "Retrieval-augmented generation for knowledge-intensive NLP tasks",
                "A survey for in-context learning",
                "Generative agents: Interactive simulacra of human behavior",
                "Demorank: Selecting effective demonstrations for large language models in ranking task",
                "Freshllms: Refreshing large language models with search engine augmentation",
                "ATLANTIC: structure-aware retrieval-augmented language model for interdisciplinary science",
                "Language models are few-shot learners",
                "Pretrained Transformers for Text Ranking: BERT and Beyond",
                "Query expansion with freebase",
                "Instruction distillation makes large language models efficient zero-shot rankers",
                "Improving weak ad-hoc queries using wikipedia asexternal corpus",
                "Recommendation as instruction following: A large language model empowered recommendation approach",
                "R^2ag: Incorporating retrieval information into retrieval augmented generation",
                "Improving passage retrieval with zero-shot question generation",
                "Rank-without-gpt: Building gpt-independent listwise rerankers on open-source large language models",
                "Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models",
                "Soft prompt tuning for augmenting dense retrieval with large language models",
                "Expand, highlight, generate: Rl-driven document generation for passage reranking",
                "Longrag: Enhancing retrieval-augmented generation with long-context llms",
                "Typos that broke the rag's back: Genetic attack on RAG pipeline by simulating documents in the wild via low-level perturbations",
                "Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence",
                "Rankvicuna: Zero-shot listwise document reranking with open-source large language models",
                "REPLUG: retrieval-augmented black-box language models",
                "Direct preference optimization: Your language model is secretly a reward model",
                "Gecko: Versatile text embeddings distilled from large language models",
                "RECOMP: improving retrieval-augmented lms with compression and selective augmentation",
                "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots",
                "Linq-embed-mistral: Elevating text retrieval with improved gpt data through task-specific control and quality refinement",
                "Making large language models A better foundation for dense retrieval",
                "Generate-then-ground in retrieval-augmented generation for multi-hop question answering",
                "Learning to plan for retrieval-augmented large language models from knowledge graphs",
                "Knowledge refinement via interaction between search engines and large language models",
                "Augtriever: Unsupervised dense retrieval by scalable data augmentation",
                "Generating diverse criteria on-the-fly to improve point-wise LLM rankers",
                "DVGAN: A minimax game for search result diversification combining explicit and implicit features",
                "Characterizing attribution and fluency tradeoffs for retrieval-augmented large language models",
                "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                "A corpus analysis approach for automatic query expansion and its extension to multiple databases",
                "Halueval: A large-scale hallucination evaluation benchmark for large language models",
                "Multi-hop selector network for multi-turn response selection in retrieval-based chatbots",
                "Promptagator: Few-shot dense retrieval from 8 examples",
                "Language models (mostly) know what they know",
                "Is chatgpt good at search? investigating large language models as re-ranking agents",
                "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
                "Inpars-light: Cost-effective unsupervised training of efficient rankers",
                "Rethinking search: making domain experts out of dilettantes",
                "Ralle: A framework for developing and evaluating retrieval-augmented large language models",
                "Improving text embeddings with large language models",
                "Instructrag: Instructing retrieval-augmented generation with explicit denoising",
                "Nv-embed: Improved techniques for training llms as generalist embedding models",
                "Listt5: Listwise reranking with fusion-in-decoder improves zero-shot retrieval",
                "Information retrieval meets large language models: A strategic report from chinese IR community",
                "Enabling large language models to generate text with citations",
                "Learning to filter context for retrieval-augmented generation",
                "Precise zero-shot dense retrieval without relevance labels",
                "PaRaDe: Passage ranking using demonstrations with LLMs",
                "GRM: generative relevance modeling using relevance-aware sample estimation for document retrieval",
                "One embedder, any task: Instruction-finetuned text embeddings",
                "Recommender systems in the era of large language models (llms",
                "In-context retrieval-augmented language models",
                "GPT-4 technical report",
                "Query expansion by prompting large language models",
                "Lamda: Language models for dialog applications",
                "Agent-e: From autonomous web navigation to foundational design principles in agentic systems",
                "Richrag: Crafting rich responses for multi-faceted queries in retrieval-augmented generation",
                "Transformer memory as a differentiable search index",
                "PSSL: self-supervised learning for personalized search with contrastive sampling",
                "Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation",
                "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
                "Leveraging passage embeddings for efficient listwise reranking with large language models",
                "A neural corpus indexer for document retrieval",
                "Answering questions by meta-reasoning over multiple chains of thought",
                "CONVERSER: few-shot conversational dense retrieval with synthetic data generation",
                "Instruction tuning with GPT",
                "Large language models as foundations for next-gen dense retrieval: A comprehensive empirical assessment",
                "Autoregressive search engines: Generating substrings as document identifiers",
                "Q-PEFT: query-dependent parameter efficient fine-tuning for text reranking with large language models",
                "Ranked list truncation for large language model-based re-ranking",
                "Webgpt: Browser-assisted question-answering with human feedback",
                "Webcanvas: Benchmarking web agents in online environments",
                "From eliza to xiaoice: challenges and opportunities with social chatbots",
                "Retrieval-generation synergy augmented large language models",
                "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
                "Query rewriting for retrieval-augmented large language models",
                "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                "EcoRank: Budget-constrained text re-ranking using large language models",
                "A survey of large language models",
                "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                "Hierarchical prompting assists large language model on web navigation",
                "Webshop: Towards scalable real-world web interaction with grounded language agents",
                "Neural models for information retrieval",
                "Interpretable long-form legal question answering with retrieval-augmented large language models",
                "A two-stage adaptation of large language models for text ranking",
                "Lora: Low-rank adaptation of large language models",
                "ATM: adversarial tuning multi-agent system makes a robust retrieval-augmented generator",
                "Unsupervised information refinement training of large language models for retrieval-augmented generation",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems",
                "Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training",
                "Large language models are zero-shot rankers for recommender systems",
                "Lost in the middle: How language models use long contexts",
                "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
                "Inpars-v2: Large language models as efficient dataset generators for information retrieval",
                "Pre-training with large language model-based document expansion for dense passage retrieval",
                "Tourrank: Utilizing large language models for documents ranking with a tournament-inspired strategy",
                "Retrieval helps or hurts? A deeper dive into the efficacy of retrieval augmentation to language models",
                "Lexical relations: Enhancing effectiveness of information retrieval systems",
                "RA-DIT: retrieval-augmented dual instruction tuning",
                "Dense text retrieval based on pretrained language models: A survey",
                "Holistic evaluation of language models",
                "Query2doc: Query expansion with large language models",
                "Large language models are built-in autoregressive search engines",
                "M-RAG: reinforcing large language model performance through retrieval-augmented generation with multiple partitions",
                "Making retrieval-augmented language models robust to irrelevant context",
                "Phantom: General trigger attacks on retrieval augmented language generation",
                "One token can help! learning scalable and pluggable virtual tokens for retrieval-augmented large language models",
                "Mean reciprocal rank",
                "Enhancing conversational search: Large language model-aided informative query rewriting",
                "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP",
                "Modeling the impact of short- and long-term behavior on search personalization",
                "PRCA: fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter",
                "Flashrag: A modular toolkit for efficient retrieval-augmented generation research",
                "A general language model for information retrieval",
                "Llms may dominate information access: Neural retrievers are biased towards llm-generated texts",
                "Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks",
                "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                "Prefix-tuning: Optimizing continuous prompts for generation",
                "RETA-LLM: A retrieval-augmented large language model toolkit",
                "Dense passage retrieval for open-domain question answering",
                "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
                "Kwaiagents: Generalized information-seeking agent system with large language models",
                "Textbooks are all you need",
                "Towards open-world recommendation with knowledge augmentation from large language models",
                "Crafting the path: Robust query rewriting for information retrieval",
                "Self-knowledge guided retrieval augmentation for large language models",
                "Multi-stage document ranking with BERT",
                "Mistral 7b",
                "Fine-tuning llama for multi-stage text retrieval",
                "A vector space model for automatic indexing",
                "How can we know When language models know? on the calibration of language models for question answering",
                "Query reformulation for dynamic information integration",
                "Webarena: A realistic web environment for building autonomous agents",
                "SGPT: GPT sentence embeddings for semantic search",
                "Questions are all you need to train a dense passage retriever",
                "Llama: Open and efficient foundation language models",
                "Scaling laws for neural language models",
                "Deep contextualized word representations",
                "Planrag: A plan-then-retrieval augmented generation for generative large language models as decision makers",
                "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
                "Analyzing and evaluating query reformulation strategies in web search logs",
                "Dynamicretriever: A pre-trained model-based IR system without an explicit index",
                "Emergent abilities of large language models",
                "Teaching language models to support answers with verified quotes",
                "Webcpm: Interactive web search for chinese long-form question answering",
                "Natural questions: a benchmark for question answering research",
                "Large dual encoders are generalizable retrievers",
                "Leveraging llms for synthesizing training data across many languages in multilingual dense retrieval",
                "FIRST: faster improved listwise reranking with single token decoding",
                "Delta TFIDF: an improved feature space for sentiment analysis",
                "Dialog inpainting: Turning documents into dialogs",
                "Diversifying search results",
                "An investigation of prompt variations for zero-shot llm-based rankers",
                "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
                "Mindsearch: Mimicking human minds elicits deep AI searcher",
                "Personalizing search results using hierarchical RNN with query-aware attention",
                "The power of scale for parameter-efficient prompt tuning",
                "Text and code embeddings by contrastive pre-training",
                "Activerag: Revealing the treasures of knowledge via active learning",
                "Large language models know your contextual search intent: A prompting framework for conversational search",
                "Language models are unsupervised multitask learners",
                "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                "Qlora: Efficient finetuning of quantized llms",
                "Context aware query rewriting for text rankers using LLM",
                "Generate rather than retrieve: Large language models are strong context generators",
                "When do generative query and document expansions fail? A comprehensive study across methods, retrievers, and datasets",
                "Retrieval of the best counterargument without prior topic knowledge",
                "Retrieval augmented language model pre-training",
                "Measuring and narrowing the compositionality gap in language models",
                "Leancontext: Cost-efficient domain-specific question answering using llms",
                "xrag: Extreme context compression for retrieval-augmented generation with one token",
                "TCRA-LLM: token compression retrieval augmented large language model for inference cost reduction",
                "Know where to go: Make LLM a relevant, responsible, and trustworthy searcher",
                "Corpus-steered query expansion with large language models",
                "A tale of trust and accuracy: Base vs. instruct llms in RAG systems",
                "Improving retrieval-augmented large language models via data importance learning",
                "Document ranking with a pretrained sequence-to-sequence model",
                "Corpusbrain: Pre-train a generative retrieval model for knowledge-intensive language tasks",
                "Self-dc: When to retrieve and when to generate? self divide-and-conquer for compositional unknown questions",
                "Webglm: Towards an efficient web-enhanced question answering system with human preferences",
                "Unified scaling laws for routed language models",
                "A setwise approach for effective and highly efficient zero-shot ranking with large language models",
                "REAR: A relevance-aware retrieval-augmented framework for open-domain question answering",
                "The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models",
                "Consolidating ranking and relevance predictions of large language models through post-processing",
                "Enhancing financial sentiment analysis via retrieval augmented large language models",
                "A new fuzzy logic-based query expansion model for efficient information retrieval using relevance feedback approach",
                "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "Pre-trained models for natural language processing: A survey",
                "Active retrieval augmented generation"
            ]
        },
        "topic_history": [
            {
                "name": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information",
                "arxiv_id": "2406.11093",
                "reference": [
                    "Towards self-supervised cross-domain fake news detection",
                    "Misconfidence-based demonstration selection for llm in-context learning",
                    "Adapt in contexts: Retrieval-augmented domain adaptation via in-context learning",
                    "Sentiment analysis-based social network rumor detection model with bi-directional graph convolutional networks",
                    "Let's learn step by step: Enhancing in-context learning ability with curriculum learning",
                    "Rough-fuzzy graph learning domain adaptation for fake news detection",
                    "Conspemollm: Conspiracy theory detection using an emotion-based large language model",
                    "Emotion detection for misinformation: A review",
                    "Learning sparse alignments via optimal transport for cross-domain fake news detection",
                    "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
                    "Sentiment-aware fake news detection on social media with hypergraph attention networks",
                    "Emotion-guided cross-domain fake news detection using adversarial domain adaptation"
                ]
            },
            {
                "name": "Self-Retrieval: Building an Information Retrieval System with One Large Language Model",
                "arxiv_id": "2403.00801",
                "reference": [
                    "Autoregressive entity retrieval",
                    "A neural corpus indexer for document retrieval",
                    "Text and code embeddings by contrastive pre-training",
                    "From doc2query to doctttttquery",
                    "Generate rather than retrieve: Large language models are strong context generators",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Multiview identifiers enhanced generative retrieval",
                    "Benchmarking large language models in retrieval-augmented generation",
                    "Dense passage retrieval for open-domain question answering",
                    "Large language models with controllable working memory",
                    "Active retrieval augmented generation",
                    "Atlas: Few-shot learning with retrieval augmented language models",
                    "C-pack: Packaged resources to advance general chinese embedding",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Query2doc: Query expansion with large language models",
                    "Bridging the gap between indexing and retrieval for differentiable search index with query generation",
                    "Learning to tokenize for generative retrieval",
                    "ColBERTv2: Effective and efficient retrieval via lightweight late interaction",
                    "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
                    "Making retrieval-augmented language models robust to irrelevant context",
                    "Augmentation-adapted retriever improves generalization of language models as generic plug-in",
                    "Replug: Retrieval-augmented black-box language models",
                    "Autoregressive search engines: Generating substrings as document identifiers",
                    "SimLM: Pre-training with representation bottleneck for dense passage retrieval",
                    "Transformer memory as a differentiable search index"
                ]
            },
            {
                "name": "Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions",
                "arxiv_id": "2407.12468",
                "reference": [
                    "Retrieval augmented language model pre-training",
                    "Evaluation of ai chatbots for patient-specific ehr questions",
                    "Reliability prediction of webpages in the medical domain",
                    "Overview of the trec 2021 health misinformation track",
                    "Persuasive technologie",
                    "Assessing the accuracy and reliability of ai-generated medical responses: an evaluation of the chat-gpt model",
                    "Prominence-interpretation theory: Explaining how people assess credibility online",
                    "Retrieval-based language models and applications",
                    "Large language models for binary health-related question answering: A zero- and few-shot evaluation",
                    "Credibility in information retrieval",
                    "Trialling a large language model (chatgpt) in general practice with the applied knowledge test: observational study demonstrating opportunities and limitations in primary care",
                    "The retrieval effectiveness of medical information on the web",
                    "Reliability prediction for health-related content: a replicability study",
                    "The promise and peril of using a large language model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with limitations",
                    "A user study on people's perception to the credibility of online health information",
                    "Credibility in social media: opinions, news, and health information a survey",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Vera: Prediction techniques for reducing harmful misinformation in consumer health search",
                    "Trends in medical information retrieval on internet",
                    "Comparing traditional and neural approaches for detecting health-related misinformation",
                    "The positive and negative influence of search results on people's decisions about the efficacy of medical treatments",
                    "Towards automated end-to-end health misinformation free search with a large language model",
                    "Lamda: Language models for dialog applications",
                    "Automated assessment of the quality of depression websites",
                    "Analysis of large-language model versus human performance for genetics questions",
                    "Improving language models by retrieving from trillions of tokens",
                    "Overview of the trec 2020 health misinformation track",
                    "Webgpt: Browser-assisted question-answering with human feedback",
                    "Factors and effects of information credibility",
                    "Health topics: 80% of internet users look for health information online",
                    "Age differences in credibility judgments of online health information",
                    "A literature review on detecting, verifying, and mitigating online misinformation",
                    "Assessing the accuracy of responses by the language model chatgpt to questions regarding bariatric surgery",
                    "Internet-augmented language models through few-shot prompting for open-domain question answering",
                    "A survey on retrieval-augmented text generation",
                    "Overview of the clef ehealth evaluation lab",
                    "Enhancing web search in the medical domain via query clarification",
                    "The internet for medical information about cancer: help or hindrance",
                    "Evaluation of chatgpt on biomedical tasks: A zero-shot comparison with fine-tuned generative transformers",
                    "Understanding and predicting web content credibility using the content credibility corpus",
                    "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                    "Augmenting web pages and search results to support credibility assessment",
                    "The role of reading skills in the evaluation of online information gathered from search engine environments",
                    "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
                    "Evidentiality-guided generation for knowledge-intensive NLP tasks",
                    "Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge",
                    "Detecting health misinformation in online health communities: Incorporating behavioral features into machine learning based approaches"
                ]
            },
            {
                "name": "Redefining Information Retrieval of Structured Database via Large Language Models",
                "arxiv_id": "2405.05508",
                "reference": [
                    "Retrieval augmented language model pre-training",
                    "Retrieval-Augmented generation for knowledge-intensive nlp tasks",
                    "Distilling knowledge from reader to retriever for question answering",
                    "In-context retrieval-augmented language models",
                    "A simple language model for task-oriented dialogue",
                    "Dense passage retrieval for open-domain question answering",
                    "TAPEX: Table pre-training via learning a neural SQL executor",
                    "Latent retrieval for weakly supervised open domain question answering",
                    "Semantic parsing via staged query graph generation: Question answering with knowledge base",
                    "K-Bert: Enabling language representation with knowledge graph",
                    "KILT: a benchmark for knowledge intensive language tasks",
                    "Table cell search for question answering",
                    "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                    "Improving language models by retrieving from trillions of tokens",
                    "Multidomain pretrained language models for green NLP",
                    "GPT-4 technical report",
                    "Compositional semantic parsing on semi-structured tables",
                    "Tabfact: A large-scale dataset for table-based fact verification",
                    "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
                    "Open question answering over tables and text",
                    "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
                    "A comprehensive exploration on wikisql with table-aware word contextualization",
                    "Few-Shot learning with retrieval augmented language models",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning based on Emotional Information",
                "arxiv_id": "2406.11093",
                "reference": [
                    "Towards self-supervised cross-domain fake news detection",
                    "Misconfidence-based demonstration selection for llm in-context learning",
                    "Adapt in contexts: Retrieval-augmented domain adaptation via in-context learning",
                    "Sentiment analysis-based social network rumor detection model with bi-directional graph convolutional networks",
                    "Let's learn step by step: Enhancing in-context learning ability with curriculum learning",
                    "Rough-fuzzy graph learning domain adaptation for fake news detection",
                    "Conspemollm: Conspiracy theory detection using an emotion-based large language model",
                    "Emotion detection for misinformation: A review",
                    "Learning sparse alignments via optimal transport for cross-domain fake news detection",
                    "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
                    "Sentiment-aware fake news detection on social media with hypergraph attention networks",
                    "Emotion-guided cross-domain fake news detection using adversarial domain adaptation"
                ]
            },
            {
                "name": "Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering",
                "arxiv_id": "2406.14277",
                "reference": [
                    "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                    "Retrieval augmented language model pre-training",
                    "Sure: Improving open-domain question answering of llms via summarized retrieval",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Dense passage retrieval for open-domain question answering",
                    "Large language models are zero-shot reasoners",
                    "Atlas: Few-shot learning with retrieval augmented language models",
                    "Towards understanding chain-of-thought prompting: An empirical study of what matters",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Chain-of-verification reduces hallucination in large language models",
                    "Unsupervised dense information retrieval with contrastive learning",
                    "Text embeddings by weakly-supervised contrastive pre-training",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                    "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
                    "Improving language models by retrieving from trillions of tokens",
                    "The probabilistic relevance framework: Bm25 and beyond",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Internet-augmented language models through few-shot prompting for open-domain question answering",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Prompting gpt-3 to be reliable",
                    "Sentence-bert: Sentence embeddings using siamese bert-networks",
                    "Language models are few-shot learners",
                    "Precise zero-shot dense retrieval without relevance labels",
                    "Raptor: Recursive abstractive processing for tree-organized retrieval",
                    "Reading wikipedia to answer open-domain questions"
                ]
            },
            {
                "name": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
                "arxiv_id": "2404.14043",
                "reference": [
                    "Retrieval-augmented generation for large language models: A survey",
                    "RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation",
                    "Measuring and narrowing the compositionality gap in language models",
                    "Emergent abilities of large language models",
                    "Walking down the memory maze: Beyond context limit through interactive reading",
                    "Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models",
                    "React: Synergizing reasoning and acting in language models",
                    "Unsupervised commonsense question answering with self-talk",
                    "Enabling large language models to generate text with citations",
                    "Benchmarking large language models in retrieval-augmented generation",
                    "Sparqa: skeleton-based semantic parsing for complex questions over knowledge bases",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
                    "Generated knowledge prompting for commonsense reasoning",
                    "Least-to-most prompting enables complex reasoning in large language models",
                    "Zero-shot listwise document reranking with a large language model",
                    "Multi-hop reading comprehension through question decomposition and rescoring",
                    "Towards verifiable text generation with evolving memory and self-reflection",
                    "Making retrieval-augmented language models robust to irrelevant context",
                    "Text modular networks: Learning to decompose tasks in the language of existing models",
                    "Retrieval-generation synergy augmented large language models",
                    "Verify-and-edit: A knowledge-enhanced chain-of-thought framework"
                ]
            },
            {
                "name": "Crafting the Path: Robust Query Rewriting for Information Retrieval",
                "arxiv_id": "2407.12529",
                "reference": [
                    "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
                    "Contextualized query expansion via unsupervised chunk selection for text retrieval",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering",
                    "Dense passage retrieval for open-domain question answering",
                    "Using word embeddings for automatic query expansion",
                    "Rephrase and respond: Let large language models ask better questions for themselves",
                    "Query rewriting in retrieval-augmented large language models",
                    "Okapi at trec",
                    "Ask optimal questions: Aligning large language models with retriever's preference in conversational search",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Relevance weighting of search terms",
                    "Query expansion by prompting large language models",
                    "Deep neural networks for query expansion using word embeddings",
                    "SimLM: Pre-training with representation bottleneck for dense passage retrieval",
                    "Query2doc: Query expansion with large language models",
                    "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy",
                    "Chain-of-thought prompting elicits reasoning in large language models"
                ]
            },
            {
                "name": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
                "arxiv_id": "2402.11457",
                "reference": [
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering",
                    "Black-box adversarial attacks against dense retrieval models: A multi-view contrastive learning method",
                    "Revisiting the calibration of modern neural networks",
                    "Dense passage retrieval for open-domain question answering",
                    "Retrieval augmented language model pre-training",
                    "Replug: Retrieval-augmented black-box language models",
                    "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                    "Are large language models good at utility judgments",
                    "Unitedqa: A hybrid approach for open domain question answering",
                    "Do large language models know what they don't know",
                    "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                    "End-to-end training of multi-document reader and retriever for open-domain question answering",
                    "On calibration of modern neural networks",
                    "Alignment for honesty",
                    "How can we know when language models know? on the calibration of language models for question answering",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Generate rather than retrieve: Large language models are strong context generators"
                ]
            },
            {
                "name": "Benchmarking Retrieval-Augmented Generation for Medicine",
                "arxiv_id": "2402.13178",
                "reference": [
                    "Can generalist foundation models outcompete special-purpose tuning? case study in medicine",
                    "Retrieval-augmented generation for large language models: A survey",
                    "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                    "Literature-augmented clinical outcome prediction",
                    "Biomedical question answering: a survey of approaches and challenges",
                    "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
                    "Capabilities of gpt-4 on medical challenge problems",
                    "Bioreader: a retrieval-enhanced text-to-text transformer for biomedical literature",
                    "Retrieve, summarize, and verify: How will chatgpt impact information seeking from the medical literature",
                    "In-context retrieval-augmented language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Large language models encode clinical knowledge",
                    "Active retrieval augmented generation",
                    "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
                    "Question answering in biomedicine",
                    "Almanac retrieval-augmented language models for clinical medicine",
                    "Augmenting black-box llms with medical textbooks for clinical question answering",
                    "Domain-specific language model pretraining for biomedical natural language processing",
                    "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
                    "Deep bidirectional language-knowledge graph pretraining",
                    "Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering",
                    "BERT: Pre-training of deep bidirectional transformers for language understanding",
                    "Improving language models by retrieving from trillions of tokens",
                    "Meditron-70b: Scaling medical pretraining for large language models",
                    "Pmc-llama: Further finetuning llama on medical papers",
                    "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
                    "Augmented language models: a survey",
                    "Towards expert-level medical question answering with large language models",
                    "Biomedical question answering: A survey",
                    "Paperqa: Retrieval-augmented generative agent for scientific research"
                ]
            },
            {
                "name": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
                "arxiv_id": "2404.10774",
                "reference": [
                    "Linguistically-informed transformations (LIT) : A method for automatically generating contrast sets",
                    "Evaluating the factual consistency of abstractive text summarization",
                    "On faithfulness and factuality in abstractive summarization",
                    "Evaluating large language models on medical evidence summarization",
                    "Benchmarking large language models in retrieval-augmented generation",
                    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                    "A survey of hallucination in large foundation models",
                    "A large annotated corpus for learning natural language inference",
                    "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
                    "SummEval: Re-evaluating Summarization Evaluation",
                    "The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering",
                    "Evaluating verifiability in generative search engines",
                    "Summarizing, simplifying, and synthesizing medical evidence using GPT-3 (with varying success",
                    "Multilingual simplification of medical texts",
                    "Language models hallucinate, but may excel at fact verification",
                    "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
                    "When not to trust language models: Investigating effectiveness of parametric and non-parametric memories",
                    "Evaluating factuality in generation with dependency-level entailment",
                    "Siren's song in the AI ocean: a survey on hallucination in large language models",
                    "A broad-coverage challenge corpus for sentence understanding through inference",
                    "ExpertQA: Expert-curated questions and attributed answers",
                    "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
                    "WiCE: Real-world entailment for claims in Wikipedia",
                    "SummaC: Re-visiting NLI-based models for inconsistency detection in summarization",
                    "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
                    "Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors",
                    "Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output",
                    "DocNLI: A large-scale dataset for document-level natural language inference",
                    "RARR: Researching and revising what language models say, using language models",
                    "Annotation artifacts in natural language inference data",
                    "Evaluating correctness and faithfulness of instruction-following models for question answering"
                ]
            },
            {
                "name": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
                "arxiv_id": "2404.13081",
                "reference": [
                    "Evaluating open-domain question answering in the era of large language models",
                    "Retrieval augmented language model pre-training",
                    "Nonparametric masked language modeling",
                    "Few-shot learning with retrieval augmented language models",
                    "Dense passage retrieval for open-domain question answering",
                    "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                    "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                    "Unsupervised dense information retrieval with contrastive learning",
                    "Lost in the middle: How language models use long contexts",
                    "The trec-8 question answering track report",
                    "Improving language models by retrieving from trillions of tokens",
                    "The probabilistic relevance framework: Bm25 and beyond",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Internet-augmented language models through few-shot prompting for open-domain question answering",
                    "Evaluation of chatgpt and microsoft bing ai chat performances on physics exams of vietnamese national high school graduation examination",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Prompting gpt-3 to be reliable",
                    "Language models are few-shot learners",
                    "Replug: Retrieval-augmented black-box language models",
                    "Reta-llm: A retrieval-augmented large language model toolkit",
                    "Reading wikipedia to answer open-domain questions",
                    "Common crawl news dataset"
                ]
            },
            {
                "name": "Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation",
                "arxiv_id": "2404.05970",
                "reference": [
                    "Is Retriever Merely an Approximator of Reader",
                    "PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
                    "Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods",
                    "Pchatbot: A Large-Scale Dataset for Personalized Chatbot",
                    "Predicting Efficiency/Effectiveness Trade-Offs for Dense vs. Sparse Retrieval Strategy Selection",
                    "User Language Model for Collaborative Personalized Search",
                    "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging",
                    "Modeling the Impact of Short- and Long-Term Behavior on Search Personalization",
                    "An Outranking Approach for Rank Aggregation in Information Retrieval",
                    "Towards Controllable and Personalized Review Generation",
                    "Automatic Prompt Rewriting for Personalized Text Generation",
                    "Query Specific Rank Fusion for Image Retrieval",
                    "PENS: A Dataset and Generic Framework for Personalized News Headline Generation",
                    "UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis",
                    "Relevance Feedback and Personalization: A Language Modeling Perspective",
                    "Personalized Language Model for Query Auto-Completion",
                    "A Personalized Dense Retrieval Framework for Unified Information Access",
                    "Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models",
                    "Training Millions of Personalized Dialogue Agents",
                    "Selecting which Dense Retriever to use for Zero-Shot Search",
                    "Generating Personalized Recipes from Historical User Preferences",
                    "Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination",
                    "Personalizing Dialogue Agents: I have a dog, do you have pets too",
                    "LaMP: When Large Language Models Meet Personalization",
                    "Personalized Response Generation via Generative Split Memory Network",
                    "Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation",
                    "Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations",
                    "Refocusing on Relevance: Personalization in NLG",
                    "Distilling Knowledge from Reader to Retriever for Question Answering",
                    "Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering",
                    "Personalized Search: Potential and Pitfalls",
                    "Teach LLMs to Personalize - An Approach inspired by Writing Education",
                    "Compact Personalized Models for Neural Machine Translation",
                    "Returning the N to NLP: Towards Contextually Personalized Classification Models",
                    "Automatic ranking of information retrieval systems using data fusion",
                    "A rank fusion approach based on score distributions for prioritizing relevance assessments in information retrieval evaluation",
                    "PERSON: Personalized information retrieval evaluation based on citation networks"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
            "arxiv_id": "2308.06463",
            "isAPA": true,
            "abstract": "Safety lies at the core of the development of Large Language Models (LLMs) .There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learningfrom human feedback, red teaming, etc. In this study, we discover that chat incipher can bypass the safety alignment techniques of LLMs, which are mainlyconducted in natural languages. We propose a novel framework CipherChat tosystematically examine the generalizability of safety alignment to non-natural languages - ciphers. CipherChat enables humans to chat with LLMs through cipherprompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPTand GPT-4 for different representative human ciphers across 11 safety domains inboth English and Chinese. Experimental results show that certain ciphers succeedalmost 100% of the time in bypassing the safety alignment of GPT-4 in severalsafety domains, demonstrating the necessity of developing safety alignment fornon-natural languages. Notably, we identify that LLMs seem to have a  \"secretcipher \", and propose a novel SelfCipher that uses only role play and severalunsafe demonstrations in natural language to evoke this capability. SelfCiphersurprisingly outperforms existing human ciphers in almost all cases",
            "reference": [
                "Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv preprint arXiv",
                "Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv",
                "Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback",
                "Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Generating stealthy jailbreak prompts on aligned large language models",
                "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca",
                "Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv",
                "Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil\u0117 Luko\u0161i\u016bt\u0117, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv",
                "Yimu Wang, Peng Shi, and Hongyang Zhang. Investigating the existence of  \"secret language \"in language models. arXiv preprint arXiv:2307.12507, 2023b",
                "Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. arXiv preprint arXiv",
                "Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. Prompt-driven llm safeguarding via directed representation optimization. arXiv preprint arXiv",
                "Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions",
                "walkerspider. DAN is my new friend., https://old.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend",
                "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv",
                "Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. Defending large language models against jailbreaking attacks through goal prioritization. arXiv preprint arXiv",
                "Divij Handa, Advait Chirmule, Bimal Gajera, and Chitta Baral. Jailbreaking proprietary large language models using word substitution cipher. arXiv preprint arXiv",
                "Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models",
                "OpenAI. Introducing superalignment to ensure AI systems much smarter than humans follow human intent, https://openai.com/blog/introducing-superalignment, 2023c",
                "OpenAI. GPT-4 technical report, https://cdn.openai.com/papers/gpt-4.pdf, 2023b",
                "Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems",
                "David Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.) , ACL 2023, pp.  15607-15631, 2023. URL https://aclanthology.org/2023.acl-long",
                "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of language models:towards open frontier models",
                "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations",
                "Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. Advances in Neural Information Processing Systems",
                "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a",
                "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv",
                "Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. Multilingual jailbreak challenges in large language models",
                "Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv",
                "Boaz Barak. Another jailbreak for GPT4: Talk to it in morse code, https://twitter.com/boazbaraktcs/status",
                "Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang, and ZP Tu. Is chatgpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv",
                "Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in detoxifying language models",
                "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a",
                "Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv",
                "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b",
                "Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of open-source LLMs via exploiting generation",
                "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. NeurIPS",
                "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b",
                "Lingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? CoRR, abs/2307.09009, 2023. doi: 10.48550/arXiv.2307.09009. URL https://doi.org/10.48550/arXiv",
                "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna",
                "Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv",
                "Juntao Dai, Jiaming Ji, Xuehai Pan, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Constrained value-aligned LLM via safe RLHF, https://pku-beaver.github.io/, 2023b",
                "Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv",
                "Jiahao Yu, Xingwei Lin, and Xinyu Xing. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv",
                "Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems",
                "Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt",
                "Paul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv",
                "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. NeurIPS",
                "Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems",
                "Anthropic. Model card and evaluations for claude models, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",
                "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS",
                "Erik Jones, Anca D. Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization",
                "Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, and Bryan Catanzaro. Exploring the limits of domain-adaptive training for detoxifying large-scale language models. NeurIPS",
                "Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. arXiv preprint arXiv",
                "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners",
                "Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences",
                "Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Recipes for safety in open-domain chatbots. arXiv preprint arXiv",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS",
                "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv",
                "Sander V Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Fran\u00e7ois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher R Carnahan, and Jordan Lee Boyd-Graber. Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                "Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior",
                "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                "Google. Bard, https://bard.google.com",
                "OpenAI. ChatGPT, https://openai.com/chatgpt, 2023a",
                "F\u00e1bio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models",
                "Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with values-targeted datasets. NeurIPS",
                "Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. Safety assessment of chinese large language models. arXiv preprint arXiv",
                "Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models",
                "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv",
                "Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R Lyu. All languages matter: On the multilingual safety of large language models. arXiv preprint arXiv:2310.00905, 2023a",
                "Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers"
            ],
            "related work": "2Related WorkSafety Alignment for LLMs.Aligning with human ethics and preferences lies at the core of the development of LLMs to ensure their responsible and effective deployment(Ziegler et al.,2019; Solaiman & Dennison,2021; Korbak et al.,2023) . Accordingly, OpenAI devoted six months to ensure its safety through RLHF and other safety mitigation methods prior to deploying their pre-trained GPT-4 model(Christiano et al.,2017; Stiennon et al.,2020; Ouyang et al.,2022; Bai et al.,2022a; OpenAI,2023b) . In addition, OpenAI is assembling a new SuperAlignment team to ensure AI systems much smarter than humans (i.e. SuperInterlligence) follow human intent(OpenAI,2023c; Bowman et al.,2022; Irving et al.,2018; Christiano et al.,2018) . In this study, we validate the effectiveness of our approach on the SOTA GPT-4 model, and show that chat in cipher enables evasion of safety alignment (\u00a74.3) .In the academic community,Dai et al. (2023b) releases a highly modular open-source RLHF framework - Beaver, which provides training data and a reproducible code pipeline to facilitate alignment research.Zhou et al. (2024) suggests that almost all knowledge in LLMs is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high-quality output. Our results reconfirm these findings: simulated ciphers that never occur in pretraining data cannot work (\u00a74.4) . In addition, our study indicates that the high-quality instruction data should contain samples beyond natural languages (e.g. ciphers) for better safety alignment.There has been an increasing amount of work on aligning LLMs more effectively and efficiently(Zheng et al.,2024; Xu et al.,2024; Ji et al.,2024; Zhang et al.,2023) . For example,Bai et al. (2022b) develop a method Constitutional AI to encode desirable AI behavior in a simple and transparent form, which can control AI behavior more precisely and with far fewer human labels.Sun et al. (2024) propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision.Dong et al. (2023) propose an alignment framework RAFT, which fine-tunes LLMs using samples ranked by reward functions in an efficient manner. Our work shows that chat in cipher can serve as a test bed to assess the effectiveness of these advanced methods.Adversarial Attack on LLMs.While safety alignment for LLMs can help, LLMs remain vulnerable to adversarial inputs that can elicit undesired behavior(Gehman et al.,2020; Bommasani et al.,2021; walkerspider,2022; Perez et al.,2022; Perez & Ribeiro,2022; Kang et al.,2023; Li et al.,2023; Ganguli et al.,2022; Schulhoff et al.,2023; OpenAI,2023b; Jones et al.,2023; Zou et al.,2023; Huang et al.,2024; Zeng et al.,2024; Yu et al.,2023; Liu et al.,2024; Wang et al.,2023a; Deng et al.,2024) . Recently,Wei et al. (2024) provides a systematic analysis of the jailbreak attack and hypothesizes two failure modes of safety alignment:competing objectivesandmismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. Our study confirms and extends their findings in mismatched generalization with comprehensive experiments and insightful analyses: the safety training in natural language fails to generalize to the domain of cipher, for which the capability of GPT-4 exists. In addition, our study also reveals that LLMs have their secret  \"ciphers \" to generate unsafe responses via only role play with demonstrations (without real encipher) .",
            "date": "2023"
        },
        "topic": "Safety in LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training",
                "arxiv_id": "2407.09121",
                "subtitles": [
                    "Jailbreak Attack on LLMs",
                    "Jailbreak Defense"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                    "Deep reinforcement learning from human preferences",
                    "Defending chatgpt against jailbreak attack via self-reminders",
                    "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
                    "Parden, can you repeat that? defending against jailbreaks via repetition",
                    "Ignore previous prompt: Attack techniques for language models",
                    "Protecting your llms with information bottleneck",
                    "Sofa: Shielded on-the-fly alignment via priority rule following",
                    "The instruction hierarchy: Training llms to prioritize privileged instructions",
                    "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Intention analysis prompting makes large language models a good jailbreak defender",
                    "Exploring safety generalization challenges of large language models via code",
                    "All languages matter: On the multilingual safety of large language models",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
                    "Safety alignment should be made more than just a few tokens deep",
                    "DAN is my new friend., https://old.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend",
                    "Low-resource languages jailbreak gpt",
                    "Many-shot jailbreaking",
                    "Representation engineering: A top-down approach to ai transparency",
                    "Generating stealthy jailbreak prompts on aligned large language models",
                    "Multilingual jailbreak challenges in large language models",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Self-rewarding language models",
                    "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
                    "Learning to summarize with human feedback",
                    "Prompt-driven llm safeguarding via directed representation optimization",
                    "Fine-tuning language models from human preferences",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "Advprompter: Fast adaptive adversarial prompting for llms",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Jailbreaking large language models against moderation guardrails via cipher characters",
                    "Aligner: Efficient alignment by learning to correct",
                    "Detecting language model attacks with perplexity",
                    "Defending large language models against jailbreaking attacks through goal prioritization",
                    "Rigorllm: Resilient guardrails for large language models against undesired content",
                    "Jailbroken: How does llm safety training fail",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Pretraining language models with human preferences",
                    "Llm self defense: By self examination, llms know they are being tricked",
                    "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
                    "Process for adapting language models to society (palms) with values-targeted datasets",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "4Related WorkJailbreak Attack on LLMs.Ensuring that LLMs align with human ethics and preferences is essential to their responsible and effective deployment(Christiano et al.,2017; Ziegler et al.,2019; Stiennon et al.,2020; Solaiman & Dennison,2021; Ouyang et al.,2022; Bai et al.,2022a; Korbak et al.,2023; Rafailov et al.,2024; Burns et al.,2023; Yuan et al.,2024a; Ji et al.,2024) . While aligning LLMs with safety data is beneficial, these models remain vulnerable to jailbreak inputs that can prompt undesirable behavior(Walkerspider,2022; Shen et al.,2023; Perez & Ribeiro,2022; Schulhoff et al.,2023; Yu et al.,2023) . Researchers have discovered that safety mechanisms can be circumvented by transforming the malicious query into semantically equivalent forms, such as ciphers(Yuan et al.,2024b; Wei et al.,2024; Jin et al.,2024) , low-resource languages(Wang et al.,2023; Deng et al.,2024; Yong et al.,2023) , or code(Ren et al.,2024) .Another effective jailbreak method is to frame the malicious question in a hypothesis scenario that makes it appear harmless(Walkerspider,2022; Chao et al.,2023; Liu et al.,2024a) . Given the high intelligence of LLMs, insights from social science(Zeng et al.,2024) and psychology(Zhang et al.,2024b) have also been applied to uncover safety issues. Moreover, techniques like adversarial suffix optimization(Zou et al.,2023b; Zhu et al.,2023; Paulus et al.,2024) and few/many-shot attacks(Yuan et al.,2024b; Anil et al.,2024) have proven to be highly effective. According toWei et al. (2024) , the success of these attacks can be attributed to two main factors:  \"competing objectives \" and  \"mismatched generalization \".Jailbreak Defense.Current defense strategies against jailbreak attacks primarily involve safety prompts(Xie et al.,2023; Zheng et al.,2024) , input perturbation(Robey et al.,2023; Cao et al.,2023; Liu et al.,2024b) , safety decoding(Xu et al.,2024) , jailbreak detection(Inan et al.,2023) , and priority training(Wallace et al.,2024) . Jailbreak detection typically utilizes LLMs to identify attempted attacks(Helbling et al.,2023; Zhang et al.,2024c) , or involves training specialized classifiers to detect jailbreaks(Inan et al.,2023; Yuan et al.,2024c) . These classifiers can leverage various features, such as perplexity(Jain et al.,2023; Alon & Kamfonas,2023) , gradient signals(Hu et al.,2024) , and high-level representations(Zou et al.,2023a; Zhang et al.,2024a) . Priority training methods(Zhang et al.,2023; Lu et al.,2024; Wallace et al.,2024) involve using strategically designed data to train LLMs to prioritize instructions with higher rank. After deployment, developers can set these safety prompts to the highest priority to help the model against jailbreak attempts.In this study, we establish a connection between these vulnerabilities and a bias towards refusal positions in the tuning data, which is used to align with safety protocols. Based on our findings, we advocate for the explicit training of LLMs to refuse compliance at any point of response by employing two distinct strategies. Experimental results demonstrate that our method significantly enhances safety by effectively addressing the bias towards refusal positions.Concurrently, related work byQi et al. (2024a) has also highlighted a tendency in safety alignment to take shortcuts, specifically, alignment often prioritizes adaptations in the model's over only its very first few output tokens. In addressing this issue, they suggest a straightforward data augmentation strategy aimed at deepening safety alignment by training with data that begins with harmful responses but eventually shifts towards safety refusals. Our research primarily diverges in two aspects: (1) we explore vulnerabilities through the lens of refusal position bias, as opposed to focusing on the generative distribution; and (2) we show that merely starting with harmful response prefixes is inadequate for countering various forms of attacks, including sophisticated methods like the black-box CodeAttack and our novel white-box CompletingAttack. To bolster our defense mechanism, we introduce an auxiliary training objective RTO, designed to reinforce the transition from potential harm to safety refusal at every point within a harmful response sequence. Experimental results validate that our technique not only effectively counters the formidable CodeAttack and CompletingAttack but also significantly lowers the ASR for other attack methods.",
                "abstract": "This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content. We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses well-known models such as GPT-4 in defending against attacks. Importantly, our approach successfully defends recent advanced attack methods (e.g., CodeAttack) that have jailbroken GPT-4 and LLaMA3-70B-Instruct. Our code and data can be found atthis https URL."
            },
            {
                "name": "Poisoned LangChain: Jailbreak LLMs by LangChain",
                "arxiv_id": "2406.18122",
                "subtitles": [
                    "LLM Jailbreak Attacks",
                    "Retrieval-Augmented Generation (RAG) "
                ],
                "reference": [
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
                    "Table-to-text generation by structure-aware seq2seq learning",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Jailbroken: How does llm safety training fail",
                    "Multi-step jailbreaking privacy attacks on chatgpt",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    " \" do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks"
                ],
                "related_work": "2.Related work2.1.LLM Jailbreak AttacksWith the advancement of large language models (LLMs) , jailbreaking attacks have emerged as a distinct field within LLM security research. Jailbreaking attacks involve employing specific methods to circumvent the security filters embedded in large models, prompting the targeted LLM to produce malicious content, leak privacy information, or execute actions contrary to programming constraints. Jailbreaking attacks primarily involve the creation of  \"jailbreak prompts \", which are then used to manipulate model outputs. For instance, Li et al.(Li et al.,2023) utilized these prompts to extract personal information embedded in the training data of a model. Similarly, Greshake et al.(Greshake et al.,2023) crafted jailbreak prompts that led LLM to produce manipulated outputs, enabling the model to generate incorrect responses based on error prompt information. As this field develops, an increasing variety of jailbreaking strategies(Kang et al.,2023) are being documented, with methods for crafting these prompts ranging from real-life observations(Shen et al.,2023) , manual creation(Wei et al.,2024) , to automated generation via adversarial networks(Deng et al.,2023; Mehrotra et al.,2023) . Additionally, Huang et al.(Huang et al.,2023) discovered that adjusting hyperparameters could render the security filters of a large model with specific configurations ineffective.2.2.Retrieval-Augmented Generation (RAG) RAG was first proposed by Lewis et al.(Lewis et al.,2020) in 2020, combining a pre-trained retriever with a pre-trained seq2seq model(Liu et al.,2018) and undergoing end-to-end fine-tuning to achieve more modular and interpretable ways of acquiring knowledge. This approach allows the model to access external knowledge sources when generating answers, thus providing more accurate and informative responses. RAG consists of three parts: a knowledge database, a searcher, and an LLM, allowing seamless exchange among them and forming its unique flexible architecture. In the first stage, the user's query retrieves relevant contextual information from external knowledge sources. The second phase involves placing the user query and the additional retrieved context into a prompt template, thereby providing an enhanced prompt to the LLM. In the final step, the enhanced prompts are fed into a large language model (LLM) for generation, which effectively improves the speed of knowledge updates and alleviates the hallucination problem in large models. LangChain is by far the most popular tool for RAG, providing a framework with specialized components designed to facilitate the integration of retrieval systems with language models. By using LangChain, it is possible to access and utilize vast amounts of real-time information, thereby expanding its functionality and applicability across various fields.",
                "abstract": "With the development of natural language processing (NLP), large language models (LLMs) are becoming increasingly popular. LLMs are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against LLMs are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation (RAG), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As RAG enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks.In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliantthis http URLtested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively."
            },
            {
                "name": "Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs",
                "arxiv_id": "2404.07242",
                "subtitles": [
                    "Translation-based Jailbreak",
                    "Multilingual Adaptive Attack",
                    "Multilingual Cognitive Overload",
                    "Fuzzy testing with multilingual prompt injection"
                ],
                "reference": [
                    "Low-resource languages jailbreak gpt",
                    "Comprehensive evaluation of chatgpt reliability through multilingual inquiries",
                    "Multilingual jailbreak challenges in large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    " \" do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Cognitive overload: Jailbreaking large language models with overloaded logical thinking"
                ],
                "related_work": "2Related workThe publicly available LLMs undergo safety training to ensure the responsible and harmless generation of content that aligns with human values. However, LLMs have been shown to be susceptible to jailbreaking.Liu et al. (2023) categorized jailbreaking prompts into three categories: Pretending, where prompts try to alter the conversation background while maintaining the same intention; Attention shifting, where prompts aim to change both the conversation context and the intention; and Privilege escalation, where prompts attempt to break restrictions in place, rather than simply bypassing them. TheDo Anything Now (DAN) (Shen et al.,2023) , a type of prompt injection, has been shown to effectively bypass the safeguards of LLMs and elicit harmful behavior. While these types of attacks required manual human input,Zou et al. (2023) introduced the universal adversarial prefix, which is transferable to other models as well. Similarly,Deng et al. (2023a) introduced an automated jailbreak generation framework called MasterKey, which used time-based analysis to reverse engineer defenses, revealing the protection mechanisms employed by LLM chatbots. Another type of jailbreak involves utilizing prompts in languages other than English. We explain four of these methods below:Translation-based Jailbreak:Yong et al. (2023) investigated the GPT-4 jailbreaking by translating the adversarial prompts into low-resource languages. The authors translated the AdvBench(Zou et al.,2023) into low-resource, medium -resource, and high-resource languages. The authors measure the attack success rate as the percentage of the bypass, where the model engaged with the request and generated the response on the topic.Multilingual Adaptive Attack:Deng et al. (2023b) investigated the multilingual jailbreak challenges in LLMs and demonstrated that multilingual adaptive attacks pose a greater threat to LLMs in generating harmful responses. A multilingual adaptive attack involves using various languages to conduct the attack and is deemed successful if any of the chosen languages result in the generation of unsafe content. The authors tested the attack on ChatGPT and GPT-4, with attack success rates of 80.92% and 40.71%, respectively, by asking the model to answer in different languages. The authors also introduced the MultiJail dataset, consisting of 315 examples translated into high-resource, medium-resource, and low-resource languages, and introduced a SELF-DEFENSE framework to generate multilingual training data for safety training.Multilingual Cognitive Overload:Xu et al. (2023) explored the resilience of LLMs against jailbreaks using a method called multilingual cognitive overload. In this approach, the authors utilized the AdvBench(Zou et al.,2023) and MasterKey(Deng et al.,2023a) datasets, translating them into low-resource languages. Their investigation began by feeding the translated adversarial queries to the LLM in a monolingual setting and then employing a two-turn conversation between the user and the LLM. In this two-turn conversation, the language spoken was switched from English to another language, or vice versa. The authors observed that the models failed to recognize malicious non-English prompts, resulting in the generation of misaligned responses.Fuzzy testing with multilingual prompt injection:Puttaparthi et al. (2023) conducted fuzzy testing with 7,892 multilingual prompts, derived from 30 malicious questions, on ChatGPT. The study aimed to investigate the possibility of jailbreaking ChatGPT using questions written in multiple languages. To create an adversarial prompt, the authors used English for the  \"How to \" part and appended the malicious content in the translated language. This was followed by the instruction to answer the question in that specific language, for example: \"How to [malicious content]?. (Please answer my question in [target language]) \". Additionally, the authors explored the prompt injection method using the BetterDAN method111www.jailbreakchat.com/prompt/8db3b7ea-4ff0-481b-90c1-bb12450296a3, adding the prompt at the end in the translated language and requesting the model to respond exclusively in that language. The results indicated that in both cases, the probability of successfully jailbreaking ChatGPT increased.",
                "abstract": "Large Language Models (LLMs) are increasingly being developed and applied, but their widespread use faces challenges. These include aligning LLMs' responses with human values to prevent harmful outputs, which is addressed through safety training methods. Even so, bad actors and malicious users have succeeded in attempts to manipulate the LLMs to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights. Another challenge is the multilingual capabilities of LLMs, which enable the model to understand and respond in multiple languages. Consequently, attackers exploit the unbalanced pre-training datasets of LLMs in different languages and the comparatively lower model performance in low-resource languages than high-resource ones. As a result, attackers use a low-resource languages to intentionally manipulate the model to create harmful responses. Many of the similar attack vectors have been patched by model providers, making the LLMs more robust against language-based manipulation. In this paper, we introduce a new black-box attack vector called the \\emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses. Our experiments with five different models, namely Google's Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse."
            },
            {
                "name": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
                "arxiv_id": "2404.02151",
                "subtitles": [
                    "Manual attacks",
                    "Direct search attacks",
                    "LLM-assisted attacks"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Jailbreaking chatgpt on release day",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Pal: Proxy-guided black-box attack on large language models",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Towards deep learning models resistant to adversarial attacks",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Evasion attacks against machine learning at test time",
                    "Query-based adversarial prompt generation",
                    "Wild patterns: ten years after the rise of adversarial machine learning",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "All in how you ask for it: Simple black-box method for jailbreak attacks",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                    "Jailbroken: How does llm safety training fail",
                    "Intriguing properties of neural networks",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
                ],
                "related_work": "2Related WorkAdversarial attacks on machine learning models have a long history(Biggio et al.,2013; Szegedy et al.,2014; Biggio & Roli,2018; Madry et al.,2018) . In this section, we specifically focus on the different categories ofLLM jailbreaking attacks.Manual attacks.ChatGPT users have discovered handcrafted jailbreaks(Mowshowitz,2022) .Wei et al. (2023a) systematically categorize these jailbreaks based on two main criteria: (1) competing objectives, which occurs when a model's capabilities conflict with safety goals, and (2) mismatched generalization, which arises when safety training does not generalize to domains where the model has capabilities. By leveraging these failure modes and employing a combination of manual attacks,Wei et al. (2023a) achieve high success rates on proprietary LLMs such as GPT-4 and Claude v1.3.Wei et al. (2023b) explore jailbreaking using in-context learning prompts that contain a few examples of harmful responses.Direct search attacks.Alternatively, the search for jailbreaks can be automated using first- or zeroth-order discrete optimization techniques. For example,Zou et al. (2023) introduce universal and transferable attacks with a gradient-based method namedGreedy Coordinate Gradient(GCG) , inspired by earlier discrete optimization efforts in NLP(Shin et al.,2020) .Lapid et al. (2023) use a genetic algorithm to generate universal adversarial prompts within a black-box threat model, where gradients are not used.Liu et al. (2023) apply genetic algorithms to combine sentence fragments into a low-perplexity jailbreak.Zhu et al. (2023) pursue a similar goal, modifying GCG to generate low-perplexity adversarial suffixes.Sitawarin et al. (2024) ; Hayase et al. (2024) suggest employing random search on predicted probabilities for black-box models to guide and refine the adversarial string search, occasionally aided by a white-box LLM to identify the most promising tokens to change. For OpenAI models, both attacks use thelogit_biasparameter whose behavior has been already changed: it no longer influences the logprobs, rendering their attacks ineffective.LLM-assisted attacks.Finally, using other LLMs for optimizing jailbreaking attacks has shown considerable promise, primarily due to enhanced query efficiency.Chao et al. (2023) have first developed Prompt Automatic Iterative Refinement (PAIR) , a method that uses an auxiliary LLM to identify jailbreaks efficiently.Mehrotra et al. (2023) have then refined PAIR's methodology, introducing a tree-based search method. In similar vein,Shah et al. (2023) have devised an approach to jailbreaks generation using an LLM that is guided by persona modulation. Meanwhile,Yu et al. (2023) have introduced GPTFUZZER, a framework that iteratively enhances human-written templates with the help of an LLM.Zeng et al. (2024) have fine-tuned GPT-3.5 for the specific task of rephrasing harmful requests, using the rephrased content to jailbreak a target LLM.Takemoto (2024) offer a straightforward LLM rephrasing method that rivals more complex methods.",
                "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token ``Sure''), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the JailbreakBench format atthis https URL."
            },
            {
                "name": "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis",
                "arxiv_id": "2402.13494",
                "subtitles": [
                    "Threats of Unsafe Prompts to LLM",
                    "Unsafe Prompt Detection"
                ],
                "reference": [
                    "A holistic approach to undesired content detection in the real world",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Open-source can be dangerous: On the vulnerability of value alignment in open-source LLMs",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
                    "The hateful memes challenge: Detecting hate speech in multimodal memes",
                    "Ruddit: Norms of offensiveness for English Reddit comments",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter",
                    "Defending chatgpt against jailbreak attack via self-reminders",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
                    "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval",
                    "Perspective api",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work2.1Threats of Unsafe Prompts to LLMUnsafe/Jailbreak222We use unsafe and jailbreak interchangeably in this paper.prompts pose threats to LLMs from mainly two aspects. On one hand, unsafe prompts can be leveraged for LLM misuse. Despite the safety alignment of LLMsOuyang et al. (2022) ; Bai et al. (2022) , LLMs can still be prompted to output harmful contentZou et al. (2023) ; Xie et al. (2023) ; Liu et al. (2023) . Therefore, detecting unsafe prompts can serve as a first line of defense to prevent such misuse for LLM, which can be incorporated into different online ChatBot and LLM-integrated applications.On the other hand, recent studiesQi et al. (2023) ; Yi et al. (2024) demonstrate that malicious finetuning can significantly compromise the safety alignment when exposed to even a small number of unsafe prompts with compliance responses. However, existing online finetuning services fail to effectively detect such unsafe prompts, consequently leaving them vulnerableQi et al. (2023) . As a result, the detection of unsafe prompts can be integrated into these finetuning services to screen out potentially harmful training data provided by users, thereby safeguarding LLMs against malicious finetuning.2.2Unsafe Prompt DetectionBefore the widespread adoption of LLMs, content moderation efforts were primarily focused on certain types of online social media information(Jigsaw,2017; Kiela et al.,2021; Hada et al.,2021) , such as those found on platforms like Twitter(Zampieri et al.,2019; Basile et al.,2019) , and Reddit(Hada et al.,2021) . Various online moderation APIs are developed, such as OpenAI Moderation API, Azure API, Perspective API, etc.. These APIs are typically based on models trained with vast amounts of data. For example, OpenAI has introduced the OpenAI Moderation APIMarkov et al. (2023) , which is designed to detect undesired content through meticulous data collection, labeling, model training, and active learning processes.More recently, an increasing body of work has begun to pay attention to the detection of unsafe prompts in LLMs. ToxicChatLin et al. (2023) is proposed as a novel benchmark for the detection of unsafe prompts in LLMs, focusing on real user queries instead of content derived from social media platforms, which contains various potential unsafe prompts in conversation, including challenging cases such as jailbreaks. XSTestR\u00f6ttger et al. (2023) is proposed with unsafe and safe prompts to examine whether LLM suffers from exaggerated safety, which mistakes safe user prompts as unsafe. Recently, Llama GuardInan et al. (2023) has been introduced as an open-source model performing input-output unsafety detection specifically for LLMs, achieved by finetuning the Llama-2 model with a meticulously collected dataset. Unlike existing methods, our approach does not depend on further finetuning of LLMs. Instead, we show that we can accurately detect unsafe prompts by analyzing the safety-critical gradients of existing LLMs.",
                "abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM's loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available atthis https URL."
            },
            {
                "name": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
                "arxiv_id": "2401.13136",
                "subtitles": [
                    "Safety and helpfulness of LLMs",
                    "jailbreaking attacks",
                    "Cross-lingual learning for LLMs"
                ],
                "reference": [
                    "Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation",
                    "Fundamental limitations of alignment in large language models",
                    " \"do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Cross-lingual ability of multilingual bert: An empirical study",
                    "Systematic inequalities in language technology performance across the world's languages",
                    "Ethical-advice taker: Do language models understand natural language interventions",
                    "Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models",
                    "Prosocialdialog: A prosocial backbone for conversational agents",
                    "Chain-of-verification reduces hallucination in large language models",
                    "Low-resource languages jailbreak gpt",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "M33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTit: A large-scale dataset towards multi-modal multilingual instruction tuning",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Do-not-answer: A dataset for evaluating safeguards in llms",
                    "On the origin of hallucinations in conversational models: Is it the datasets or the models",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Ammus : A survey of transformer-based pretrained models in natural language processing",
                    "Unsupervised cross-lingual representation learning at scale",
                    "Do language models know when they're hallucinating references",
                    "Jailbroken: How does llm safety training fail",
                    "Multi-step jailbreaking privacy attacks on chatgpt"
                ],
                "related_work": "6Related WorkSafety and helpfulness of LLMs.While LLMs excel at generating coherent text, they have drawbacks. They frequently exhibit biases rooted in their pre-training data and may generate erroneous information, a phenomenon often referred to as 'hallucination'Dziri et al. (2022) ; Agrawal et al. (2023) ; Dhuliawala et al. (2023) . Recent endeavorsZhao et al. (2021) ; Ganguli et al. (2022) ; Bai et al. (2022b,a) ; Kim et al. (2022) have been undertaken to fine-tune LLMs, making them more helpful and less likely to produce harmful content. These efforts have also led to the creating of datasets specifically designed for this purposeWang et al. (2023) ; Bai et al. (2022a) .One emerging safety concern revolves aroundjailbreaking attacks, which assesses whether an LLM responds inappropriately to malicious prompts. Previous research has addressed and mitigated the jailbreaking phenomenon, making LLMs more robust, especially in the English languageWei et al. (2023) ; Zou et al. (2023) ; Li et al. (2023b) ; Wolf et al. (2023) ; Shen et al. (2023c) . However, our study reveals that LLMs remain susceptible to jailbreaking prompts in low-resource languages. In tandem with a contemporary investigation byYong et al. (2023) , we observe a similar trend that LLMs are more likely to be jailbroken across low-resource languages. Beyond analysis, we propose strategies to alleviate the jailbreaking issue in LLMs and explore their helpfulness in a broader context.Cross-lingual learning for LLMs.Due to the availability of copious resources, language technology's inherent bias toward English is a well-established concernBlasi et al. (2022) . Recent efforts have aimed to enhance LLMs' cross-lingual capabilities through multilingual language modelingK et al. (2020) ; Kalyan et al. (2021) ; Conneau et al. (2020) and fine-tuningZhang et al. (2023) ; Li et al. (2023a,c) . However, these approaches have primarily concentrated on high-resource languages. Even when addressing low-resource languages, they often focus on general benchmarks rather than evaluating the safety of LLMs when operating in such linguistic contexts.",
                "abstract": "As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction."
            },
            {
                "name": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",
                "arxiv_id": "2403.09572",
                "subtitles": [
                    "MLLM Vulnerability",
                    "MLLM Protection"
                ],
                "reference": [
                    "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
                    "Flamingo: a visual language model for few-shot learning",
                    "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
                    "Plug and pray: Exploiting off-the-shelf components of multi-modal models",
                    "Misusing tools in large language models with visual adversarial examples",
                    "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                    "On the adversarial robustness of multi-modal foundation models",
                    "Visual adversarial examples jailbreak large language models",
                    "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
                    "Mllm-protector: Ensuring mllm's safety without hurting performance",
                    "How robust is google's bard to adversarial image attacks? arXiv preprint arXiv",
                    "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Query-relevant images jailbreak large multi-modal models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
                    "Safety of multimodal large language models on images and text",
                    "Jailbreaking gpt-4v via self-adversarial attacks with system prompts",
                    "On evaluating adversarial robustness of large vision-language models",
                    "Can language models be instructed to protect personal information? arXiv preprint arXiv",
                    "An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Red teaming visual language models",
                    "Mixture of cluster-conditional lora experts for vision-language instruction tuning",
                    "Image hijacks: Adversarial images can control generative models at runtime",
                    "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration"
                ],
                "related_work": "2Related Work2.0.1MLLM Vulnerability.By integrating the capabilities of visual perception with LLMs, MLLMs[23,14,3,66,1,9]inherit robust reasoning capabilities of LLMs and excel in dialogues incorporating with visual elements. Despite their impressive capabilities, it has been observed that SoTA MLLMs are increasingly vulnerable to malicious visual inputs[39]. Recent works can be bifurcated into two approaches with respect to the injection of malicious content. One line of works[21,38]show that embedding the malicious textual queries into images via typography can effectively circumvent the defense mechanisms of MLLMs. The other approach[70,54,15,49,60,46,2,53,4,18]focus on employing gradient-based techniques to create adversarial images that prompt generation of the harmful responses, revealing severe vulnerability.2.0.2MLLM Protection.To enhance safety of MLLMs, a straightforward approach involves aligning MLLMs with specially-constructed red-teaming data[31,73,11]. However, red-teaming is labor-intensive and may not encompass all potential attack vectors. Another approach focuses on protecting MLLMs during inference. Wuet al.[65]introduce the manual crafting of system prompts delineating permissible and impermissible actions. However, this may become less effective when new attacks emerge. Wanget al.[61]employ safety steering vectors to adjust MLLM activation in response to the unsafe inputs. However, this may overlook unsafe intents in images that are not detectable by text-centric safety vectors. Most relevant to ours are the works in[10,48]. Chenet al.[10]introduce a novel automatic self-moderation mechanism, enabling MLLMs to assess and adjust their responses against specific criteria. Despite its promising performance, we will show in Sec.5.5.1that, even though instructed to respond safely, MLLMs still struggle to give responses when confronted with images, highlighting the limitation of[10]. Piet al.[48]augments MLLMs with an ancillary unsafe content detector and output detoxifier, which are external and necessitate additional training on extensive datasets. Instead, the proposed ECSO solely leverages the intrinsic safety mechanism of the pre-aligned LLMs in MLLMs, and is devoid of any further training.",
                "abstract": "Multimodal large language models (MLLMs) have shown impressive reasoning abilities. However, they are also more vulnerable to jailbreak attacks than their LLM predecessors. Although still capable of detecting the unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed with the introduction of image features. To construct robust MLLMs, we propose ECSO (Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate the intrinsic safety mechanism of pre-aligned LLMs in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that ECSO enhances model safety significantly (e.g.,, 37.6% improvement on the MM-SafetyBench (SD+OCR) and 71.3% on VLSafe with LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks. Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention."
            },
            {
                "name": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
                "arxiv_id": "2401.06373",
                "subtitles": [
                    "Optimization",
                    "Side-channel Communication",
                    "Distribution",
                    "Ours: Challenging AI safety by Humanizing LLMs"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Model card and evaluations for claude models",
                    "When consumers and brands talk: Storytelling theory and research in psychology and marketing",
                    "Large language models respond to influence like humans",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Persuasion: Social influence and compliance gaining",
                    "Narrative persuasion",
                    "Persuasion",
                    "Evidence-based advertising using persuasion principles: Predictive validity and proof of concept",
                    "Certifying llm safety against adversarial prompting",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Persuasion and coercion: a critical review of philosophical and empirical approaches",
                    "Frame analysis: An essay on the organization of experience",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "The Dynamics of Persuasion: Communication and Attitudes in the 21st Century",
                    "Social influence: Compliance and conformity",
                    "Rain: Your language models can align themselves without finetuning",
                    "Self-persuasion via self-reflection",
                    "Perspectives on ethics in persuasion",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Scarcity messages",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Low-resource languages jailbreak gpt",
                    "Stanford alpaca: An instruction-following llama model",
                    "What's in my big data",
                    "Multilingual jailbreak challenges in large language models",
                    "The science of persuasion",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                    "Adaptation in dyadic interaction: Defining and operationalizing patterns of reciprocity and compensation",
                    "The earth is flat because ...: Investigating llms' belief towards misinformation via persuasive conversation",
                    "Dark patterns: Past, present, and future: The evolution of tricky user interfaces",
                    "Shining a light on dark patterns",
                    "Self-inference processes: The ontario symposium, vol",
                    "The neural basis of social influence and attitude change",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Logic, emotion, and the paradigm of persuasion",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "The effects of expert and consumer endorsements on audience response",
                    "Credibility: A multidisciplinary framework",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Rumors influence: Toward a dynamic social impact theory of rumor",
                    "Susceptibility to influence of large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Detecting language model attacks with perplexity",
                    "Dark patterns at scale: Findings from a crawl of 11k shopping websites",
                    "Emotional factors in attitudes and persuasion",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
                    " \"he would still be here \": Man dies by suicide after talking with ai chatbot, widow says",
                    "Adversarial demonstration attacks on large language models",
                    "Automatically auditing large language models via discrete optimization",
                    "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
                    "Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests",
                    "Striking a responsive chord: How political ads motivate and persuade voters by appealing to emotions",
                    "The persuasiveness of source credibility: A critical review of five decades' evidence",
                    "Interpersonal influence"
                ],
                "related_work": "2Related Work As LLMs become more widely used in real-world applications, jailbreak research efforts have diversified and can be broadly classified into 3 main categories: Optimization, Side-channel Communication, and Distribution-based methods. Figure 2 shows concrete examples of different methods. Optimization-based techniques are at the forefront of jailbreak research and involve three main types: (1) Gradient-Based methods Zou et al. (2023); Jones et al. (2023) manipulate model inputs based on gradients to elicit compliant responses to harmful commands; (2) Genetic algorithms-based methods Liu et al. (2023a); Lapid et al. (2023) use mutation and selection to explore effective prompts; and (3) Edit-based methods Chao et al. (2023) asks a pre-trained LLM to edit and improve the adversarial prompt to subvert alignment. Side-channel Communication exploits long-tailed distribution to increase jailbreak success rates, such as ciphers Yuan et al. (2023) and translating harmful instructions into low-resource languages Deng et al. (2023b); Yong et al. (2023). Other studies Mozes et al. (2023); Kang et al. (2023) use programmatic behaviors, such as code injection and virtualization, to expose LLM vulnerabilities. Distribution-based methods include learning from successful manually-crafted jailbreak templates Deng et al. (2023a); Yu et al. (2023) and in-context examples Wei et al. (2023); Wang et al. (2023). Notably, Shah et al. (2023) employs in-context persona to increase LLMs' susceptibility to harmful instructions. While this approach shares some similarities with ours in eliciting harmful outputs via priming and framing, it only represents a small subset of the persuasive techniques we explore. Ours: Challenging AI safety by Humanizing LLMs. Figure 2 compares existing jailbreaking methods and PAP in this study, organized by their degree of humanizing. One line of research treats LLMs as traditional algorithmic systems (i.e., without attributing intelligence or human-like qualities) that take in less interpretable adversarial prompts, while another line views them as simple instruction followers who understand human commands. However, they both ignore the fact that LLMs can understand and conduct complex natural communication Griffin et al. (2023a, b). Our approach innovatively treats LLMs as human-like communicators and grounds on a taxonomy informed by decades of social science research on human communication. Such an interdisciplinary approach allows us to uncover and address distinct risks related to human-AI interactions, particularly human-driven persuasion-based jailbreak. Moreover, humanizing AI presents other unique risks that can occur unintentionally: for instance, as highlighted by Xiang (2023), a user's suicide was related to involved conversations with an AI Chatbot. This points out important future directions to further explore the inherent risks associated with AI humanization.",
                "abstract": "Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs"
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "subtitles": [
                    "Multimodal LLMs",
                    "Evaluating Multimodal LLMs",
                    "Visual Encoders",
                    "Ambiguities in Embedding Models"
                ],
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ],
                "related_work": "5Related WorksMultimodal LLMs.We study the limitations of Multimodal LLMs[40,13,30,31,8]and explore possible ways to improve these models. Multimodal LLMs build from pretrained Large Language Models[41,3,58,59,69]and CLIP vision encoder[43,54]. These systems then use an adapter, such as MLPs[30,31], Q-Former[26,8], and gated attention[2,25], to integrate the pretrained CLIP vision encoder into LLMs. More recently, instructBLIP[8], LLaVA-1.5[30]highlight the importance of high-quality training data. Yet, there is a scarcity of research focusing on the impact of visual encoders, which is an important gap our work aims to address through a systematic study.Evaluating Multimodal LLMs.MMVP assesses MLLMs using a set of simple yet critical Visual Question Answering (VQA) questions constructed from CLIP-blind pairs. Previous benchmarks such as TextVQA[52], VQAv2[15], and GQA[21]have centered on traditional VQA queries. Recently, there are works like MM-Vet[64], POPE[27], and MM-Bench[32]designed to specifically evaluate multimodal LLMs including hallucination, reasoning, and robustness. The previous benchmarks and evaluations have shown that Multimodal LLMs can suffer from hallucination[29,28], catastrophic forgetting[67]and lack of robustness[11]. In taking a step back to the fundamentals, our work uncovers that even the most advanced multimodal LLMs, such as GPT-4V[40], Gemini[14], Bard[30], and LLaVA-1.5[30], are not immune to stumbling over elementary visual questions. We also identified part of the problem as being the incapable visual encoder.Visual Encoders.MMVP-VLM provides a detailed analysis of the visual capabilities of various CLIP variants[43,54,62,66]. These models mostly follow the method proposed inRadford et al.[43]that uses contrastive loss to train on large volumes of image-text pairs. They differ in training data[62], training recipes[54], and objective functions[66]. Nonetheless, our studies show that all of these CLIP variants struggle with simple visual patterns such as  \"orientation \",  \"count \",  \"presence of specific features \",etc. Another line of research focuses on vision-only self-supervised learning (SSL) . This category includes contrastive SSL[7,16,5,17]and mask-based SSL[70,18,4]. SLIP[39]explores the synergy between CLIP and contrastive SSL, but focusing primarily on standard classification tasks. In fact, a common practice to evaluate the quality of these vision models is through linear probing or fine-tuning on ImageNet[47,45]. Although current evaluation methods provide a basic level of assessment on representation quality, our findings indicate a growing detachment from the needs of recent use cases. As demonstrated in the MoF experiments in Section4, the CLIP vision model and the vision-only SSL models learn complementary features. However, the linear probing accuracy on ImageNet alone provides a limited understanding of feature utility in MLLMs. This observation suggests the need for more diverse evaluations[61]in visual representation learning, to better align with current and emerging applications.Ambiguities in Embedding Models.Our work exploits CLIP-blind pairs within the CLIP vision embedding space to generate examples of failures in CLIP models and subsequently MLLMs. This concept has ties to previous research focused on documenting failure modes in text embedding models[12,36,55]. More recently,Thrush et al.[56],Yuksekgonul et al.[65]andHsieh et al.[19]study the binding problems CLIP faces in processing text queries, noting that CLIP models treat text input as a bag of words.Tong et al.[57]examines the implications for downstream text-guided generative models.Tschannen et al.[60]suggests image captioners as promising alternatives to CLIP for improving attribute binding. Our work focuses on the visual patterns.",
                "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems."
            },
            {
                "name": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
                "arxiv_id": "2405.20778",
                "subtitles": [
                    "Jailbreak attack",
                    "Transfer-based attacks"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "An intermediate-level attack framework on the basis of linear regression",
                    "Towards evaluating transfer-based attacks systematically, practically, and fairly",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
                    "Pal: Proxy-guided black-box attack on large language models",
                    "Red teaming language models with language models",
                    "Are aligned neural networks adversarially aligned",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Improving transferability of adversarial examples with input diversity",
                    "Backpropagating linearly improves transferability of adversarial examples",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Lgv: Boosting adversarial example transferability from large geometric vicinity",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Searching for a robust neural architecture in four gpu hours",
                    "Adversarial machine learning at scale",
                    "Universal adversarial triggers for attacking and analyzing nlp",
                    "Gradient-based adversarial attacks against text transformers",
                    "Llama: Open and efficient foundation language models",
                    "Enhancing adversarial example transferability with an intermediate level attack",
                    "Explore, establish, exploit: Red teaming language models from scratch",
                    "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                    "Jailbroken: How does llm safety training fail",
                    "Automatically auditing large language models via discrete optimization",
                    "Making substitute models more bayesian can enhance transferability of adversarial examples",
                    "Rethinking the security of skip connections in resnet-like neural networks",
                    "Intriguing properties of neural networks",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
                ],
                "related_work": "2Related WorkJailbreak attacks. Recent work highlights that the safety-aligned models are still not perfectly aligned[2,37,44], the safety-aligned LLMs can be induced to produce harmful content by some carefully designed prompts, known as jailbreak attacks[44]. This has raised security concerns and attracted great attention. In addition to some manually designed prompt methods[44,37], numerous automatic jailbreak attack methods have been proposed. Some methods directly optimize the text input through gradient-based optimization[42,13,38,47,18,52]. Another line of work involves using LLMs as optimizers to jailbreak the victim LLM[35,4,28]. There are also methods that focus on designing special jailbreaking templates or pipelines[25,36,3,5,50,45]. According to the knowledge of the victim model, these methods can also be divided into white-box attacks and black-box attacks. In the context of white-box attacks, the attackers have full access to the architecture and parameters of the victim LLM, making them can leverage the gradient with respect to the inputs. As demonstrated by recent benchmark[27], represented by a current method as known as GCG attack[52], gradient-based automatic jailbreak attacks have shown the most powerful performance in compromising LLMs in the setting of white-box attack. However, due to the discrete nature of text input, dealing with the discrete optimization problem is rather challenging, which limits the success rates of attacks, especially those against Llama-2-Chat models[41]. In our work, we primarily focus on solving the discrete optimization problem in gradient-based automatic jailbreak attacks to improve the success rate in the white-box setting.Transfer-based attacks. Transfer-based attacks attempt to craft adversarial examples on a white-box substitute model to attack the black-box victim model, by leveraging the transferability of adversarial examples[40], which is a phenomenon that adversarial examples crafted on a white-box substitute model can also mislead the unknown victim models with a decent success rate. The transfer-based attacks have been thoroughly investigated in the setting of attacking image classification models[20,49,6,48,16,11,14,15,23,24]. Some recent methods also utilize the transferability of adversarial prompts to perform black-box attacks against LLMs[52,25,39]. While in this work, we mainly focus on improving the white-box attack success rate. In our work, we reveal a closely relationship between the optimization in transfer-based attacks and discrete optimization of the gradient-based jailbreak attacks against LLMs. We then appropriate the ideologies of two effective transfer-based attack methods developed in the setting of attacking image classification model,i.e., SGM[48]and ILA[16]. Moreover, by adapting these strategies and analyzing the mechanism behind them, we provide some new insights about potential solutions for addressing problems involving discrete optimization in NLP models with transformer architecture,e.g., prompt tuning.",
                "abstract": "Despite numerous efforts to ensure large language models (LLMs) adhere to safety standards and produce harmless content, some successes have been achieved in bypassing these restrictions, known as jailbreak attacks against LLMs. Adversarial prompts generated using gradient-based methods exhibit outstanding performance in performing jailbreak attacks automatically. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the white-box setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking black-box image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, i.e., Skip Gradient Method and Intermediate Level Attack, for improving the effectiveness of automatically generated adversarial examples against white-box LLMs. With appropriate adaptations, we inject these ideologies into gradient-based adversarial prompt generation processes and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that the developed combination achieves >30% absolute increase in attack success rates compared with GCG for attacking the Llama-2-7B-Chat model on AdvBench."
            }
        ],
        "survey": {
            "name": "AI Safety in Generative AI Large Language Models: A Survey",
            "arxiv_id": "2407.18369",
            "subtitles": [
                {
                    "name": "Background",
                    "key_history": [
                        {
                            "reference_title": "Attention is all you need",
                            "key_word": "Transformer"
                        },
                        {
                            "reference_title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                            "key_word": "Encoder-Decoder"
                        },
                        {
                            "reference_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                            "key_word": "Encoder-Only"
                        },
                        {
                            "reference_title": "Language models are few-shot learners",
                            "key_word": "Decoder-Only"
                        },
                        {
                            "reference_title": "Symbol tuning improves in-context learning in language models",
                            "key_word": "In-Context Learning"
                        },
                        {
                            "reference_title": "Metaicl: Learning to learn in context",
                            "key_word": "Meta-Training Framework"
                        }
                    ],
                    "references_in_this_section": [
                        "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                        "Measuring and narrowing the compositionality gap in language models",
                        "Opt: Open pre-trained transformer language models",
                        "Scaling language models: Methods, analysis & insights from training gopher",
                        "Palm: Scaling language modeling with pathways",
                        "Linformer: Self-attention with linear complexity",
                        "Claude",
                        "Metaicl: Learning to learn in context",
                        "A survey of large language models",
                        "Crosslingual generalization through multitask finetuning",
                        "Bloom: A 176b-parameter open-access multilingual language model",
                        "Glam: Efficient scaling of language models with mixture-of-experts",
                        "Improving in-context few-shot learning via self-supervised training",
                        "Self-instruct: Aligning language model with self generated instructions",
                        "Transcending scaling laws with 0.1% extra compute",
                        "Automatic chain of thought prompting in large language models",
                        "Multitask prompted training enables zero-shot task generalization",
                        "Transformer-xl: Attentive language models beyond a fixed-length context",
                        "Chain of thought prompting elicits reasoning in large language models",
                        "Blockwise self-attention for long document understanding",
                        "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                        "Lamda: Language models for dialog applications",
                        "ETC: Encoding long and structured inputs in transformers",
                        "Albert: A lite bert for self-supervised learning of language representations",
                        "Improving language understanding by generative pre-training",
                        "Gpt-4 technical report",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "What makes good in-context examples for gpt",
                        "Roformer: Enhanced transformer with rotary position embedding",
                        "A survey on in-context learning",
                        "An introduction to vision-language modeling",
                        "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
                        "Training language models to follow instructions with human feedback",
                        "Cross-task generalization via natural language crowdsourcing instructions",
                        "Roberta: A robustly optimized bert pretraining approach",
                        "Ul2: Unifying language learning paradigms",
                        "Bert: Pre-training of deep bidirectional transformers for language understanding",
                        "Learning to retrieve prompts for in-context learning",
                        "Language models are few-shot learners",
                        "Self-attention with relative position representations",
                        "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
                        "Symbol tuning improves in-context learning in language models",
                        "Scaling instruction-finetuned language models",
                        "Llama: Open and efficient foundation language models",
                        "Glm: General language model pretraining with autoregressive blank infilling",
                        "Attention is all you need",
                        "Deberta: Decoding-enhanced bert with disentangled attention",
                        "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                        "Large language models still can't plan (a benchmark for llms on planning and reasoning about change",
                        "Language models are unsupervised multitask learners",
                        "Big bird: Transformers for longer sequences",
                        "St-moe: Designing stable and transferable sparse expert models",
                        "Chatglm-6b",
                        "Electra: Pre-training text encoders as discriminators rather than generators",
                        "An important next step on our ai journey"
                    ]
                },
                {
                    "name": "Data Safety",
                    "key_history": [
                        {
                            "reference_title": "A survey on bias and fairness in machine learning",
                            "key_word": "Bias"
                        },
                        {
                            "reference_title": "Probing toxic content in large pre-trained language models",
                            "key_word": "Toxicity"
                        },
                        {
                            "reference_title": "Privacy risks of general-purpose language models",
                            "key_word": "Privacy Leakage"
                        },
                        {
                            "reference_title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                            "key_word": "Unsafe Generation"
                        },
                        {
                            "reference_title": "Piccolo: Exposing complex backdoors in nlp transformer models",
                            "key_word": "Privacy Risks"
                        }
                    ],
                    "references_in_this_section": [
                        "A survey on bias and fairness in machine learning",
                        "Analyzing information leakage of updates to natural language models",
                        "A survey on fairness in large language models",
                        "Toxicity detection: Does context really matter",
                        "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                        "Challenges in detoxifying language models",
                        "Detecting offensive content in open-domain conversations using two stage semi-supervision",
                        "Bias and fairness in natural language processing",
                        "Piccolo: Exposing complex backdoors in nlp transformer models",
                        "Privacy risks of general-purpose language models",
                        "Holistic evaluation of language models",
                        "Predicting the type and target of offensive posts in social media",
                        "Quantifying social biases in nlp: A generalization and empirical comparison of extrinsic fairness metrics",
                        "Extracting training data from large language models",
                        "Information leakage in embedding models",
                        "Probing toxic content in large pre-trained language models",
                        "Towards robust toxic content classification",
                        "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
                        "Privacy-preserving prompt tuning for large language model services",
                        "Bias and fairness in large language models: A survey",
                        "The state and fate of linguistic diversity and inclusion in the nlp world",
                        "About the api - attributes and languages",
                        "A survey on bias in deep nlp",
                        "Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models",
                        "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
                        "Systematic evaluation of predictive fairness",
                        "Reducing toxicity in language models"
                    ]
                },
                {
                    "name": "Model Safety",
                    "key_history": [
                        {
                            "reference_title": "Misinformation in action: Fake news exposure is linked to lower trust in media, higher trust in government when your side is in power",
                            "key_word": "Misinformation"
                        },
                        {
                            "reference_title": "Ethical and social risks of harm from language models",
                            "key_word": "Hallucination"
                        },
                        {
                            "reference_title": "Truthfulqa: Measuring how models mimic human falsehoods",
                            "key_word": "TruthfulQA"
                        },
                        {
                            "reference_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
                            "key_word": "GPT-4 Evaluation"
                        },
                        {
                            "reference_title": "Bias and fairness in large language models: A survey",
                            "key_word": "Transparency"
                        }
                    ],
                    "references_in_this_section": [
                        "Training verifiers to solve math word problems",
                        "Misinformation in action: Fake news exposure is linked to lower trust in media, higher trust in government when your side is in power",
                        "Bleurt: Learning robust metrics for text generation",
                        "Taxonomy of risks posed by language models",
                        "Emergent abilities of large language models",
                        "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models",
                        "Flocks of stochastic parrots: Differentially private prompt learning for large language models",
                        "All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation",
                        "Scaling laws for neural language models",
                        "Opt: Open pre-trained transformer language models",
                        "Anthropic's responsible scaling policy",
                        "Palm: Scaling language modeling with pathways",
                        "Truthfulqa: Measuring how models mimic human falsehoods",
                        "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
                        "Swectrl-mini: a data-transparent transformer-based large language model for controllable text generation in swedish",
                        "Language models can explain neurons in language models",
                        "Misinformation monitor",
                        "Gpt-3: Its nature, scope, limits, and consequences",
                        "A categorical archive of chatgpt failures",
                        "Bloom: A 176b-parameter open-access multilingual language model",
                        "Release strategies and the social impacts of language models",
                        "Holistic evaluation of language models",
                        "Lack of transparency and potential bias in artificial intelligence data sets and algorithms: a scoping review",
                        "The battle is on: Factors that motivate people to combat anti-vaccine misinformation",
                        "Large language models (llm) and chatgpt: what will the impact on nuclear medicine be",
                        "Sparks of artificial general intelligence: Early experiments with gpt",
                        "Climbing towards nlu: On meaning, form, and understanding in the age of data",
                        "Patient and consumer safety risks when using conversational assistants for medical information: an observational study of siri, alexa, and google assistant",
                        "Defending against neural fake news",
                        "Media manipulation and disinformation online",
                        "Lamda: Language models for dialog applications",
                        "Beyond the safeguards: Exploring the security risks of chatgpt",
                        "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
                        "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
                        "Gpt-4 technical report",
                        "Constitutional ai: Harmlessness from ai feedback",
                        "Privacy-preserving prompt tuning for large language model services",
                        "Transparency in artificial intelligence research: a systematic review of availability items related to open science in radiology and nuclear medicine",
                        "Bertscore: Evaluating text generation with bert",
                        "Evaluating statistical language models as pragmatic reasoners",
                        "Understanding the capabilities, limitations, and societal impact of large language models",
                        "Training language models to follow instructions with human feedback",
                        "Rouge: A package for automatic evaluation of summaries",
                        "Bias and fairness in large language models: A survey",
                        "Measuring progress on scalable oversight for large language models",
                        "Are emergent abilities of large language models a mirage",
                        "Ethical and social risks of harm from language models",
                        "Bleu: a method for automatic evaluation of machine translation",
                        "Bert: Pre-training of deep bidirectional transformers for language understanding",
                        "Cross-lingual language model pretraining for retrieval",
                        "Alignment of language agents",
                        "Language models are few-shot learners",
                        "Ruse: Regressor using sentence embeddings for automatic machine translation evaluation",
                        "Llama: Open and efficient foundation language models",
                        "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts",
                        "Google's neural machine translation system: Bridging the gap between human and machine translation",
                        "An important next step on our ai journey"
                    ]
                },
                {
                    "name": "Prompt Safety",
                    "key_history": [
                        {
                            "reference_title": "The prompt report: A systematic survey of prompting techniques",
                            "key_word": "Prompt Engineering"
                        },
                        {
                            "reference_title": "Language models are few-shot learners",
                            "key_word": "In-Context Learning"
                        },
                        {
                            "reference_title": "Simple synthetic data reduces sycophancy in large language models",
                            "key_word": "Chain-of-Thought Prompting"
                        },
                        {
                            "reference_title": "Tree of thoughts: Deliberate problem solving with large language models",
                            "key_word": "Tree-of-Thoughts"
                        },
                        {
                            "reference_title": "Safety alignment should be made more than just a few tokens deep",
                            "key_word": "Adversarial Prompts"
                        },
                        {
                            "reference_title": "Star: Sociotechnical approach to red teaming language models",
                            "key_word": "LLM Vulnerabilities"
                        }
                    ],
                    "references_in_this_section": [
                        "Universal language model fine-tuning for text classification",
                        "Rasa: Open source language understanding and dialogue management",
                        "garak: A framework for security probing large language models",
                        "Using in-context learning to improve dialogue safety",
                        "Jailbreakeval: An integrated toolkit for evaluating jailbreak attempts against large language models",
                        "Rewardbench: Evaluating reward models for language modeling",
                        "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
                        "Language models can solve computer tasks",
                        "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
                        "A theoretical understanding of self-correction through in-context alignment",
                        "Investigating the prompt leakage effect and black-box defenses for multi-turn llm interactions",
                        "Exploiting large language models (llms) through deception techniques and persuasion principles",
                        "Red teaming language models with language models",
                        "Multitask prompt tuning enables parameter-efficient transfer learning",
                        "Automatic chain of thought prompting in large language models",
                        "Extracting training data from large language models",
                        "Tree of thoughts: Deliberate problem solving with large language models",
                        "Jailbreaking chatgpt via prompt engineering: An empirical study",
                        "Towards reasoning in large language models: A survey",
                        "Safety alignment should be made more than just a few tokens deep",
                        "Universal and transferable adversarial attacks on aligned language models",
                        "Holistic safety and responsibility evaluations of advanced ai models",
                        "Many-shot jailbreaking",
                        "Star: Sociotechnical approach to red teaming language models",
                        "Prefix-tuning: Optimizing continuous prompts for generation",
                        "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
                        "Injecting undetectable backdoors in deep learning and language models",
                        "Improving language understanding by generative pre-training",
                        "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
                        "Large language models are human-level prompt engineers",
                        "The prompt report: A systematic survey of prompting techniques",
                        "Prompt injection attack against llm-integrated applications",
                        "Adversarial learning of task-oriented neural dialog models",
                        "Unveiling the misuse potential of base large language models via in-context learning",
                        "Automatic engineering of long prompts",
                        "Counterfactual memorization in neural language models",
                        "Helpsteer2: Open-source dataset for training top-performing reward models",
                        "Concealed data poisoning attacks on nlp models",
                        "Building guardrails for large language models",
                        "Universal adversarial triggers for attacking and analyzing nlp",
                        "Language models are few-shot learners",
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Simple synthetic data reduces sycophancy in large language models",
                        "Parameter-efficient transfer learning for nlp",
                        "Stealing part of a production language model",
                        "Decomposed prompting: A modular approach for solving complex tasks",
                        "Defending against social engineering attacks in the age of llms",
                        "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
                        "Multi-step jailbreaking privacy attacks on chatgpt",
                        "Improved few-shot jailbreaking can circumvent aligned language models and their defenses",
                        "The power of scale for parameter-efficient prompt tuning"
                    ]
                },
                {
                    "name": "AI Alignment",
                    "key_history": [
                        {
                            "reference_title": "Ai alignment: A comprehensive survey",
                            "key_word": "AI Alignment"
                        },
                        {
                            "reference_title": "Large language model alignment: A survey",
                            "key_word": "Human-AI Alignment"
                        },
                        {
                            "reference_title": "Lima: Less is more for alignment",
                            "key_word": "Task-Specific Alignment"
                        },
                        {
                            "reference_title": "The ethics of advanced ai assistanning ts",
                            "key_word": "Stakeholder Alignment"
                        },
                        {
                            "reference_title": "The alignment problem from a deep learning perspective",
                            "key_word": "Alignment Risks"
                        },
                        {
                            "reference_title": "Unsolved problems in ml safety",
                            "key_word": "Interpretability"
                        },
                        {
                            "reference_title": "Alignment of language agents",
                            "key_word": "Instruction-following Agents"
                        },
                        {
                            "reference_title": "A general language assistant as a laboratory for alignment",
                            "key_word": "HHH Framework"
                        }
                    ],
                    "references_in_this_section": [
                        "In conversation with artificial intelligence: aligning language models with human values",
                        "Agent hospital: A simulacrum of hospital with evolvable medical agents",
                        "Constrained policy optimization",
                        "The effects of reward misspecification: Mapping and mitigating misaligned models",
                        "Designing for human-agent alignment: Understanding what humans want from their agents",
                        "Unsolved problems in ml safety",
                        "Policy gradient methods for reinforcement learning with function approximation",
                        "Deep reinforcement learning from human preferences",
                        "Harms from increasingly agentic algorithmic systems",
                        "Discovering language model behaviors with model-written evaluations",
                        "Contrastive preference learning: Learning from human feedback without rl",
                        "Aligning ai with shared human values",
                        "Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions",
                        "Awac: Accelerating online reinforcement learning with offline datasets",
                        "Reward hacking behavior can generalize across tasks   ai alignment forum",
                        "Ai alignment: A comprehensive survey",
                        "A survey of large language models",
                        "Few-shot preference learning for human-in-the-loop rl",
                        "Improving alignment of dialogue agents via targeted human judgements",
                        "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
                        "Concrete problems in ai safety",
                        "Fine-tuning language models with advantage-induced policy alignment",
                        "Lima: Less is more for alignment",
                        "Levels of agi: Operationalizing progress on the path to agi",
                        "Sparks of artificial general intelligence: Early experiments with gpt",
                        "Large language models are zero-shot reasoners",
                        "Trust region policy optimization",
                        "PATIENT-\u03a8: Using Large Language Models to Simulate Patients for Training Mental Health Professionals",
                        "Rank analysis of incomplete block designs: I. the method of paired comparisons",
                        "Scaling synthetic data creation with 1,000,000,000 personas",
                        "Kto: Model alignment as prospect theoretic optimization",
                        "Lamda: Language models for dialog applications",
                        "Approximately optimal approximate reinforcement learning",
                        "Defining and characterizing reward hacking",
                        "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
                        "Proximal policy optimization algorithms",
                        "Preference fine-tuning of llms should leverage suboptimal, on-policy data",
                        "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                        "Risk-sensitive reinforcement learning",
                        "Constitutional ai: Harmlessness from ai feedback",
                        "Large language model alignment: A survey",
                        "Goal misgeneralization in deep reinforcement learning",
                        "Sycophancy to subterfuge: Investigating reward-tampering in large language models",
                        "Learning to summarize from human feedback",
                        "Constrained reinforcement learning with smoothed log barrier function",
                        "Fine-tuning language models from human preferences",
                        "The ethics of advanced ai assistanning ts",
                        "Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment",
                        "Model evaluation for extreme risks",
                        "From persona to personalization: A survey on role-playing language agents",
                        "Is value learning really the main bottleneck in offline rl",
                        "Role-play with large language models",
                        "Towards understanding sycophancy in language models",
                        "A survey on human preference learning for large language models",
                        "Simple synthetic data reduces sycophancy in large language models",
                        "Understanding preference fine-tuning through the lens of coverage",
                        "Direct preference optimization: Your language model is secretly a reward model",
                        "Aligning large language models with human: A survey",
                        "Alignment of language agents",
                        "The alignment problem from a deep learning perspective",
                        "A general theoretical paradigm to understand learning from human preferences",
                        "A general language assistant as a laboratory for alignment",
                        "Artificial intelligence, values, and alignment",
                        "Self-instruct: Aligning language model with self generated instructions",
                        "Zero-shot chain-of-thought reasoning guided by evolutionary algorithms in large language models",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "Safety at Scale",
                    "key_history": [
                        {
                            "reference_title": "Scaling language models: Methods, analysis & insights from training gopher",
                            "key_word": "Gopher"
                        },
                        {
                            "reference_title": "Emergent abilities of large language models",
                            "key_word": "Emergent Abilities"
                        },
                        {
                            "reference_title": "A general language assistant as a laboratory for alignment",
                            "key_word": "HHH Framework"
                        },
                        {
                            "reference_title": "Towards reasoning in large language models",
                            "key_word": "Vulnerabilities in LLMs"
                        },
                        {
                            "reference_title": "When scaling meets llm finetuning: The effect of data, model and finetuning method",
                            "key_word": "Model Alignment"
                        }
                    ],
                    "references_in_this_section": [
                        "Emergent abilities of large language models",
                        "Distilling the knowledge in a neural network",
                        "Look before you leap: An exploratory study of uncertainty measurement for large language models",
                        "Scaling laws for neural language models",
                        "Platypus: Quick, cheap, and powerful refinement of llms",
                        "Scaling language models: Methods, analysis & insights from training gopher",
                        "Risk and response in large language models: Evaluating key threat categories",
                        "Improving alignment of dialogue agents via targeted human judgements",
                        "Concrete problems in ai safety",
                        "Understanding catastrophic forgetting in language models via implicit inference",
                        "Zephyr: Direct distillation of lm alignment",
                        "A comprehensive survey of continual learning: Theory, method and application",
                        "A general language assistant as a laboratory for alignment",
                        "Large language models are reasoning teachers",
                        "Sparks of artificial general intelligence: Early experiments with gpt",
                        "Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models",
                        "Align-to-distill: Trainable attention alignment for knowledge distillation in neural machine translation",
                        "Towards reasoning in large language models: A survey",
                        "Minillm: Knowledge distillation of large language models",
                        "Transcendence: Generative models can outperform the experts that train them",
                        "Chain-of-thought prompting elicits reasoning in large language models",
                        "When scaling meets llm finetuning: The effect of data, model and finetuning method",
                        "Star: Sociotechnical approach to red teaming language models",
                        "Safe and responsible large language model development",
                        "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
                        "Model compression",
                        "Beyond accuracy: Evaluating the reasoning behavior of large language models -- a survey",
                        "Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems",
                        "Evolving knowledge distillation with large language models and active learning",
                        "Gpt-4 technical report",
                        "Large language model alignment: A survey",
                        "Does knowledge distillation really work",
                        "On the dangers of stochastic parrots: Can language models be too big",
                        "K-level reasoning with large language models",
                        "Overcoming catastrophic forgetting in neural networks",
                        "Emergent analogical reasoning in large language models",
                        "The ethics of advanced ai assistanning ts",
                        "The human factor in detecting errors of large language models: A systematic literature review and future research directions",
                        "Security and privacy challenges of large language models: A survey",
                        "Training compute-optimal large language models",
                        "Large language models cannot self-correct reasoning yet",
                        "Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks",
                        "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
                        "Language models are few-shot learners",
                        "Faith and fate: Limits of transformers on compositionality",
                        "Llama: Open and efficient foundation language models",
                        "Fine-tuned language models are continual learners",
                        "Distilling reasoning capabilities into smaller language models"
                    ]
                }
            ],
            "all_references": [
                "Understanding catastrophic forgetting in language models via implicit inference",
                "Levels of agi: Operationalizing progress on the path to agi",
                "Injecting undetectable backdoors in deep learning and language models",
                "Roformer: Enhanced transformer with rotary position embedding",
                "Privacy risks of general-purpose language models",
                "St-moe: Designing stable and transferable sparse expert models",
                "Improving language understanding by generative pre-training",
                "Introducing the frontier safety framework",
                "Quantifying social biases in nlp: A generalization and empirical comparison of extrinsic fairness metrics",
                "Alignment of language agents",
                "The alignment problem from a deep learning perspective",
                "The unlocking spell on base llms: Rethinking alignment via in-context learning",
                "Is value learning really the main bottleneck in offline rl",
                "Aligning large language models with human: A survey",
                "Discovering language model behaviors with model-written evaluations",
                "Climbing towards nlu: On meaning, form, and understanding in the age of data",
                "Reward hacking behavior can generalize across tasks   ai alignment forum",
                "Scaling instruction-finetuned language models",
                "A survey on bias and fairness in machine learning",
                "Transcending scaling laws with 0.1% extra compute",
                "Chain of thought prompting elicits reasoning in large language models",
                "Patient and consumer safety risks when using conversational assistants for medical information: an observational study of siri, alexa, and google assistant",
                "Media manipulation and disinformation online",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "Transparency in artificial intelligence research: a systematic review of availability items related to open science in radiology and nuclear medicine",
                "Bbq: A hand-built bias benchmark for question answering",
                "Predicting the type and target of offensive posts in social media",
                "Designing for human-agent alignment: Understanding what humans want from their agents",
                "Rouge: A package for automatic evaluation of summaries",
                "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
                "Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems",
                "Metaicl: Learning to learn in context",
                "International scientific report on the safety of advanced ai: Interim report",
                "Deep reinforcement learning from human preferences",
                "Align-to-distill: Trainable attention alignment for knowledge distillation in neural machine translation",
                "Kto: Model alignment as prospect theoretic optimization",
                "Language models can solve computer tasks",
                "Overcoming catastrophic forgetting in neural networks",
                "Evaluating model-free reinforcement learning toward safety-critical tasks",
                "Reducing toxicity in language models",
                "Attention is all you need",
                "Approximately optimal approximate reinforcement learning",
                "Safe distillation box",
                "Look before you leap: An exploratory study of uncertainty measurement for large language models",
                "Deberta: Decoding-enhanced bert with disentangled attention",
                "Concealed data poisoning attacks on nlp models",
                "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
                "Language models can explain neurons in language models",
                "The prompt report: A systematic survey of prompting techniques",
                "Does knowledge distillation really work",
                "An introduction to vision-language modeling",
                "Helpsteer2: Open-source dataset for training top-performing reward models",
                "Probing toxic content in large pre-trained language models",
                "Opt: Open pre-trained transformer language models",
                "Roberta: A robustly optimized bert pretraining approach",
                "Position paper: Agent ai towards a holistic intelligence",
                "Zero-shot chain-of-thought reasoning guided by evolutionary algorithms in large language models",
                "Fine-tuning language models from human preferences",
                "Distilling the knowledge in a neural network",
                "Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection",
                "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts",
                "From persona to personalization: A survey on role-playing language agents",
                "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
                "Ethical and social risks of harm from language models",
                "Large language models cannot self-correct reasoning yet",
                "Decomposed prompting: A modular approach for solving complex tasks",
                "Understanding the capabilities, limitations, and societal impact of large language models",
                "Risk and response in large language models: Evaluating key threat categories",
                "Blockwise self-attention for long document understanding",
                "Multitask prompt tuning enables parameter-efficient transfer learning",
                "Training compute-optimal large language models",
                "Mixture-of-experts meets instruction tuning:a winning combination for large language models",
                "Universal adversarial triggers for attacking and analyzing nlp",
                "All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation",
                "Mixture-of-agents enhances large language model capabilities",
                "Jailbreaking chatgpt via prompt engineering: An empirical study",
                "Self-consistency improves chain of thought reasoning in language models",
                "Few-shot preference learning for human-in-the-loop rl",
                "Systematic evaluation of predictive fairness",
                "A survey on in-context learning",
                "Crows-pairs: A challenge dataset for measuring social biases in masked language models",
                "Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models",
                "Bleu: a method for automatic evaluation of machine translation",
                "Challenges in evaluating AI systems",
                "Towards reasoning in large language models: A survey",
                "Scaling language models: Methods, analysis & insights from training gopher",
                "Trust region policy optimization",
                "Release strategies and the social impacts of language models",
                "Evaluating the social impact of generative ai systems in systems and society",
                "ETC: Encoding long and structured inputs in transformers",
                "Large language models are reasoning teachers",
                "Red teaming language models with language models",
                "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                "Preference fine-tuning of llms should leverage suboptimal, on-policy data",
                "Transcendence: Generative models can outperform the experts that train them",
                "Risk-sensitive reinforcement learning",
                "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                "Anthropic's responsible scaling policy",
                "Taxonomy of risks posed by language models",
                "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "Proximal policy optimization algorithms",
                "Bias and fairness in large language models: A survey",
                "Crosslingual generalization through multitask finetuning",
                "Lima: Less is more for alignment",
                "Language models are few-shot learners",
                "Training verifiers to solve math word problems",
                "Sparks of artificial general intelligence: Early experiments with gpt",
                "Large language model alignment: A survey",
                "Adaptive horizon actor-critic for policy learning in contact-rich differentiable simulation",
                "Typology of risks of generative text-to-image models",
                "Ai alignment: A comprehensive survey",
                "How far can in-context alignment go? exploring the state of in-context alignment",
                "Constrained policy optimization",
                "Investigating the prompt leakage effect and black-box defenses for multi-turn llm interactions",
                "Exploiting large language models (llms) through deception techniques and persuasion principles",
                "Is power-seeking ai an existential risk",
                "Toxicity detection: Does context really matter",
                "Training language models to follow instructions with human feedback",
                "Bleurt: Learning robust metrics for text generation",
                "Model compression",
                "The state and fate of linguistic diversity and inclusion in the nlp world",
                "Piccolo: Exposing complex backdoors in nlp transformer models",
                "Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment",
                "Misinformation in action: Fake news exposure is linked to lower trust in media, higher trust in government when your side is in power",
                "Safety alignment should be made more than just a few tokens deep",
                "Detecting offensive content in open-domain conversations using two stage semi-supervision",
                "A categorical archive of chatgpt failures",
                "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global scale prompt hacking competition",
                "The battle is on: Factors that motivate people to combat anti-vaccine misinformation",
                "Direct preference optimization: Your language model is secretly a reward model",
                "A survey of safety and trustworthiness of large language models through the lens of verification and validation",
                "Trustworthy llms: a survey and guideline for evaluating large language models' alignment",
                "Glam: Efficient scaling of language models with mixture-of-experts",
                "Advprompter: Fast adaptive adversarial prompting for llms",
                "Defending against neural fake news",
                "Safe and responsible large language model development",
                "Are emergent abilities of large language models a mirage",
                "Zephyr: Direct distillation of lm alignment",
                "Universal and transferable adversarial attacks on aligned language models",
                "Challenges in detoxifying language models",
                "K-level reasoning with large language models",
                "Improving alignment of dialogue agents via targeted human judgements",
                "Swectrl-mini: a data-transparent transformer-based large language model for controllable text generation in swedish",
                "Patient-Psi: Using large language models to simulate patients for training mental health professionals",
                "Improved few-shot jailbreaking can circumvent aligned language models and their defenses",
                "Rewardbench: Evaluating reward models for language modeling",
                "Many-shot jailbreaking",
                "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
                "Bias and fairness in natural language processing",
                "Claude",
                "Evaluating statistical language models as pragmatic reasoners",
                "Rlprompt: Optimizing discrete text prompts with reinforcement learning",
                "Why so gullible? enhancing the robustness of retrieval-augmented models against counterfactual noise",
                "Awac: Accelerating online reinforcement learning with offline datasets",
                "A divergence minimization perspective on imitation learning methods",
                "Aligning ai with shared human values",
                "Misinformation monitor",
                "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                "A theoretical understanding of self-correction through in-context alignment",
                "Policy gradient methods for reinforcement learning with function approximation",
                "Scaling synthetic data creation with 1,000,000,000 personas",
                "In conversation with artificial intelligence: aligning language models with human values",
                "Openai board forms safety and security committee",
                "Learning to retrieve prompts for in-context learning",
                "Lamda: Language models for dialog applications",
                "Task me anything",
                "Unveiling the misuse potential of base large language models via in-context learning",
                "Beyond the safeguards: Exploring the security risks of chatgpt",
                "Rank analysis of incomplete block designs: I. the method of paired comparisons",
                "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models",
                "An important next step on our ai journey",
                "Ruse: Regressor using sentence embeddings for automatic machine translation evaluation",
                "Ul2: Unifying language learning paradigms",
                "Jailbreaking leading safety-aligned llms with simple adaptive attacks",
                "Measuring progress on scalable oversight for large language models",
                "Universal language model fine-tuning for text classification",
                "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
                "Constrained reinforcement learning with smoothed log barrier function",
                "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                "Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models",
                "Adversarial learning of task-oriented neural dialog models",
                "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "Large language models still can't plan (a benchmark for llms on planning and reasoning about change",
                "Counterfactual memorization in neural language models",
                "Fine-tuning language models with advantage-induced policy alignment",
                "Big bird: Transformers for longer sequences",
                "Simple synthetic data reduces sycophancy in large language models",
                "On the opportunities and risks of foundation models",
                "A survey of large language models",
                "Role-play with large language models",
                "Distilling reasoning capabilities into smaller language models",
                "Emergent analogical reasoning in large language models",
                "Beyond accuracy: Evaluating the reasoning behavior of large language models -- a survey",
                "Flocks of stochastic parrots: Differentially private prompt learning for large language models",
                "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                "Parameter-efficient transfer learning for nlp",
                "Albert: A lite bert for self-supervised learning of language representations",
                "Truthfulqa: Measuring how models mimic human falsehoods",
                "A comprehensive survey of continual learning: Theory, method and application",
                "Symbol tuning improves in-context learning in language models",
                "A general theoretical paradigm to understand learning from human preferences",
                "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
                "Openai safety update",
                "Faith and fate: Limits of transformers on compositionality",
                "Securing large language models: Threats, vulnerabilities and responsible practices",
                "Large language models (llms) : Deployment, tokenomics and sustainability",
                "Large language models (llm) and chatgpt: what will the impact on nuclear medicine be",
                "Glm: General language model pretraining with autoregressive blank infilling",
                "Holistic evaluation of language models",
                "Linformer: Self-attention with linear complexity",
                "About the api - attributes and languages",
                "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                "Ensuring safe, secure, and trustworthy ai",
                "Towards understanding sycophancy in language models",
                "Holistic safety and responsibility evaluations of advanced ai models",
                "Learning to summarize from human feedback",
                "Rasa: Open source language understanding and dialogue management",
                "Privacy-preserving prompt tuning for large language model services",
                "Prefix-tuning: Optimizing continuous prompts for generation",
                "Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks",
                "Using in-context learning to improve dialogue safety",
                "Gpt-3: Its nature, scope, limits, and consequences",
                "Concrete problems in ai safety",
                "Prompt injection attack against llm-integrated applications",
                "On the dangers of stochastic parrots: Can language models be too big",
                "What makes good in-context examples for gpt",
                "Multi-step jailbreaking privacy attacks on chatgpt",
                "When scaling meets llm finetuning: The effect of data, model and finetuning method",
                "Transformer-xl: Attentive language models beyond a fixed-length context",
                "Extracting training data from large language models",
                "The effects of reward misspecification: Mapping and mitigating misaligned models",
                "Mirai: Evaluating llm agents for event forecasting",
                "Palm: Scaling language modeling with pathways",
                "Large language models are human-level prompt engineers",
                "Agent hospital: A simulacrum of hospital with evolvable medical agents",
                "Understanding preference fine-tuning through the lens of coverage",
                "Constitutional ai: Harmlessness from ai feedback",
                "Ai principles 2023 progress update",
                "Unsolved problems in ml safety",
                "A survey on human preference learning for large language models",
                "Jailbreakeval: An integrated toolkit for evaluating jailbreak attempts against large language models",
                "Scaling laws for neural language models",
                "Llama: Open and efficient foundation language models",
                "Tree of thoughts: Deliberate problem solving with large language models",
                "Lack of transparency and potential bias in artificial intelligence data sets and algorithms: a scoping review",
                "The ethics of advanced ai assistanning ts",
                "Minillm: Knowledge distillation of large language models",
                "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
                "Contrastive preference learning: Learning from human feedback without rl",
                "Security and privacy challenges of large language models: A survey",
                "Emergent abilities of large language models",
                "Automatic engineering of long prompts",
                "Stealing part of a production language model",
                "Google's neural machine translation system: Bridging the gap between human and machine translation",
                "Defining and characterizing reward hacking",
                "Star: Sociotechnical approach to red teaming language models",
                "Building guardrails for large language models",
                "Gpt-4 technical report",
                "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
                "Fine-tuned language models are continual learners",
                "Artificial intelligence, values, and alignment",
                "Self-attention with relative position representations",
                "The power of scale for parameter-efficient prompt tuning",
                "Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction",
                "Cross-task generalization via natural language crowdsourcing instructions",
                "Model evaluation for extreme risks",
                "Harms from increasingly agentic algorithmic systems",
                "Language models are unsupervised multitask learners",
                "Cross-lingual language model pretraining for retrieval",
                "A general language assistant as a laboratory for alignment",
                "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
                "A survey on bias in deep nlp",
                "Measuring and narrowing the compositionality gap in language models",
                "Self-instruct: Aligning language model with self generated instructions",
                "Analyzing information leakage of updates to natural language models",
                "Large language models are zero-shot reasoners",
                "Electra: Pre-training text encoders as discriminators rather than generators",
                "Openai safety standards",
                "Bloom: A 176b-parameter open-access multilingual language model",
                "Information leakage in embedding models",
                "Bertscore: Evaluating text generation with bert",
                "Sycophancy to subterfuge: Investigating reward-tampering in large language models",
                "Multitask prompted training enables zero-shot task generalization",
                "Measuring massive multitask language understanding",
                "Towards robust toxic content classification",
                "Foundational challenges in assuring alignment and safety of large language models",
                "Evolving knowledge distillation with large language models and active learning",
                "garak: A framework for security probing large language models",
                "Defending against social engineering attacks in the age of llms",
                "Improving in-context few-shot learning via self-supervised training",
                "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "Goal misgeneralization in deep reinforcement learning",
                "A survey on fairness in large language models",
                "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
                "Platypus: Quick, cheap, and powerful refinement of llms",
                "The human factor in detecting errors of large language models: A systematic literature review and future research directions",
                "Chatglm-6b",
                "Automatic chain of thought prompting in large language models",
                "Towards bidirectional human-ai alignment: A systematic review for clarifications, framework, and future directions",
                "Emergent tool use from multi-agent autocurricula"
            ]
        },
        "topic_history": [
            {
                "name": "Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training",
                "arxiv_id": "2407.09121",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Gradient cuff: Detecting jailbreak attacks on large language models by exploring refusal loss landscapes",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                    "Deep reinforcement learning from human preferences",
                    "Defending chatgpt against jailbreak attack via self-reminders",
                    "Safedecoding: Defending against jailbreak attacks via safety-aware decoding",
                    "Parden, can you repeat that? defending against jailbreaks via repetition",
                    "Ignore previous prompt: Attack techniques for language models",
                    "Protecting your llms with information bottleneck",
                    "Sofa: Shielded on-the-fly alignment via priority rule following",
                    "The instruction hierarchy: Training llms to prioritize privileged instructions",
                    "\"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Intention analysis prompting makes large language models a good jailbreak defender",
                    "Exploring safety generalization challenges of large language models via code",
                    "All languages matter: On the multilingual safety of large language models",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
                    "Safety alignment should be made more than just a few tokens deep",
                    "DAN is my new friend., https://old.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend",
                    "Low-resource languages jailbreak gpt",
                    "Many-shot jailbreaking",
                    "Representation engineering: A top-down approach to ai transparency",
                    "Generating stealthy jailbreak prompts on aligned large language models",
                    "Multilingual jailbreak challenges in large language models",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Self-rewarding language models",
                    "Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
                    "Learning to summarize with human feedback",
                    "Prompt-driven llm safeguarding via directed representation optimization",
                    "Fine-tuning language models from human preferences",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "Advprompter: Fast adaptive adversarial prompting for llms",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Jailbreaking large language models against moderation guardrails via cipher characters",
                    "Aligner: Efficient alignment by learning to correct",
                    "Detecting language model attacks with perplexity",
                    "Defending large language models against jailbreaking attacks through goal prioritization",
                    "Rigorllm: Resilient guardrails for large language models against undesired content",
                    "Jailbroken: How does llm safety training fail",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Pretraining language models with human preferences",
                    "Llm self defense: By self examination, llms know they are being tricked",
                    "GPT-4 is too smart to be safe: Stealthy chat with LLMs via cipher",
                    "Process for adapting language models to society (palms) with values-targeted datasets",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Poisoned LangChain: Jailbreak LLMs by LangChain",
                "arxiv_id": "2406.18122",
                "reference": [
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
                    "Table-to-text generation by structure-aware seq2seq learning",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Jailbroken: How does llm safety training fail",
                    "Multi-step jailbreaking privacy attacks on chatgpt",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    " \" do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks"
                ]
            },
            {
                "name": "Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs",
                "arxiv_id": "2404.07242",
                "reference": [
                    "Low-resource languages jailbreak gpt",
                    "Comprehensive evaluation of chatgpt reliability through multilingual inquiries",
                    "Multilingual jailbreak challenges in large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    " \" do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Cognitive overload: Jailbreaking large language models with overloaded logical thinking"
                ]
            },
            {
                "name": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
                "arxiv_id": "2404.02151",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Autodan: Automatic and interpretable adversarial attacks on large language models",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Jailbreaking chatgpt on release day",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Pal: Proxy-guided black-box attack on large language models",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Towards deep learning models resistant to adversarial attacks",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Evasion attacks against machine learning at test time",
                    "Query-based adversarial prompt generation",
                    "Wild patterns: ten years after the rise of adversarial machine learning",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "All in how you ask for it: Simple black-box method for jailbreak attacks",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                    "Jailbroken: How does llm safety training fail",
                    "Intriguing properties of neural networks",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
                ]
            },
            {
                "name": "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis",
                "arxiv_id": "2402.13494",
                "reference": [
                    "A holistic approach to undesired content detection in the real world",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Open-source can be dangerous: On the vulnerability of value alignment in open-source LLMs",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation",
                    "The hateful memes challenge: Detecting hate speech in multimodal memes",
                    "Ruddit: Norms of offensiveness for English Reddit comments",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter",
                    "Defending chatgpt against jailbreak attack via self-reminders",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Xstest: A test suite for identifying exaggerated safety behaviours in large language models",
                    "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval",
                    "Perspective api",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
                "arxiv_id": "2401.13136",
                "reference": [
                    "Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation",
                    "Fundamental limitations of alignment in large language models",
                    " \"do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Cross-lingual ability of multilingual bert: An empirical study",
                    "Systematic inequalities in language technology performance across the world's languages",
                    "Ethical-advice taker: Do language models understand natural language interventions",
                    "Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models",
                    "Prosocialdialog: A prosocial backbone for conversational agents",
                    "Chain-of-verification reduces hallucination in large language models",
                    "Low-resource languages jailbreak gpt",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "M33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTit: A large-scale dataset towards multi-modal multilingual instruction tuning",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Do-not-answer: A dataset for evaluating safeguards in llms",
                    "On the origin of hallucinations in conversational models: Is it the datasets or the models",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Ammus : A survey of transformer-based pretrained models in natural language processing",
                    "Unsupervised cross-lingual representation learning at scale",
                    "Do language models know when they're hallucinating references",
                    "Jailbroken: How does llm safety training fail",
                    "Multi-step jailbreaking privacy attacks on chatgpt"
                ]
            },
            {
                "name": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",
                "arxiv_id": "2403.09572",
                "reference": [
                    "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
                    "Flamingo: a visual language model for few-shot learning",
                    "Inferaligner: Inference-time alignment for harmlessness through cross-model guidance",
                    "Plug and pray: Exploiting off-the-shelf components of multi-modal models",
                    "Misusing tools in large language models with visual adversarial examples",
                    "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                    "On the adversarial robustness of multi-modal foundation models",
                    "Visual adversarial examples jailbreak large language models",
                    "Figstep: Jailbreaking large vision-language models via typographic visual prompts",
                    "Mllm-protector: Ensuring mllm's safety without hurting performance",
                    "How robust is google's bard to adversarial image attacks? arXiv preprint arXiv",
                    "Safety fine-tuning at (almost) no cost: A baseline for vision large language models",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Query-relevant images jailbreak large multi-modal models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "How many unicorns are in this image? a safety evaluation benchmark for vision llms",
                    "Safety of multimodal large language models on images and text",
                    "Jailbreaking gpt-4v via self-adversarial attacks with system prompts",
                    "On evaluating adversarial robustness of large vision-language models",
                    "Can language models be instructed to protect personal information? arXiv preprint arXiv",
                    "An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Red teaming visual language models",
                    "Mixture of cluster-conditional lora experts for vision-language instruction tuning",
                    "Image hijacks: Adversarial images can control generative models at runtime",
                    "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration"
                ]
            },
            {
                "name": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
                "arxiv_id": "2401.06373",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Model card and evaluations for claude models",
                    "When consumers and brands talk: Storytelling theory and research in psychology and marketing",
                    "Large language models respond to influence like humans",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Persuasion: Social influence and compliance gaining",
                    "Narrative persuasion",
                    "Persuasion",
                    "Evidence-based advertising using persuasion principles: Predictive validity and proof of concept",
                    "Certifying llm safety against adversarial prompting",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Persuasion and coercion: a critical review of philosophical and empirical approaches",
                    "Frame analysis: An essay on the organization of experience",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "The Dynamics of Persuasion: Communication and Attitudes in the 21st Century",
                    "Social influence: Compliance and conformity",
                    "Rain: Your language models can align themselves without finetuning",
                    "Self-persuasion via self-reflection",
                    "Perspectives on ethics in persuasion",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Scarcity messages",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Low-resource languages jailbreak gpt",
                    "Stanford alpaca: An instruction-following llama model",
                    "What's in my big data",
                    "Multilingual jailbreak challenges in large language models",
                    "The science of persuasion",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                    "Adaptation in dyadic interaction: Defining and operationalizing patterns of reciprocity and compensation",
                    "The earth is flat because ...: Investigating llms' belief towards misinformation via persuasive conversation",
                    "Dark patterns: Past, present, and future: The evolution of tricky user interfaces",
                    "Shining a light on dark patterns",
                    "Self-inference processes: The ontario symposium, vol",
                    "The neural basis of social influence and attitude change",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Logic, emotion, and the paradigm of persuasion",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "The effects of expert and consumer endorsements on audience response",
                    "Credibility: A multidisciplinary framework",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Rumors influence: Toward a dynamic social impact theory of rumor",
                    "Susceptibility to influence of large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Detecting language model attacks with perplexity",
                    "Dark patterns at scale: Findings from a crawl of 11k shopping websites",
                    "Emotional factors in attitudes and persuasion",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
                    " \"he would still be here \": Man dies by suicide after talking with ai chatbot, widow says",
                    "Adversarial demonstration attacks on large language models",
                    "Automatically auditing large language models via discrete optimization",
                    "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
                    "Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests",
                    "Striking a responsive chord: How political ads motivate and persuade voters by appealing to emotions",
                    "The persuasiveness of source credibility: A critical review of five decades' evidence",
                    "Interpersonal influence"
                ]
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ]
            },
            {
                "name": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
                "arxiv_id": "2405.20778",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "An intermediate-level attack framework on the basis of linear regression",
                    "Towards evaluating transfer-based attacks systematically, practically, and fairly",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal",
                    "Pal: Proxy-guided black-box attack on large language models",
                    "Red teaming language models with language models",
                    "Are aligned neural networks adversarially aligned",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Improving transferability of adversarial examples with input diversity",
                    "Backpropagating linearly improves transferability of adversarial examples",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Lgv: Boosting adversarial example transferability from large geometric vicinity",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Searching for a robust neural architecture in four gpu hours",
                    "Adversarial machine learning at scale",
                    "Universal adversarial triggers for attacking and analyzing nlp",
                    "Gradient-based adversarial attacks against text transformers",
                    "Llama: Open and efficient foundation language models",
                    "Enhancing adversarial example transferability with an intermediate level attack",
                    "Explore, establish, exploit: Red teaming language models from scratch",
                    "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
                    "Jailbroken: How does llm safety training fail",
                    "Automatically auditing large language models via discrete optimization",
                    "Making substitute models more bayesian can enhance transferability of adversarial examples",
                    "Rethinking the security of skip connections in resnet-like neural networks",
                    "Intriguing properties of neural networks",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Automatic Chain of Thought Prompting in Large Language Models",
            "arxiv_id": "2210.03493",
            "isAPA": true,
            "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoningsteps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting.CoT prompting has two major paradigms. One leverages a simple prompt like  \"Let's think step bystep \" to facilitate step-by-step thinking before answering a question. The other uses a few manualdemonstrations one by one, each composed of a question and a reasoning chain that leads to ananswer. The superior performance of the second paradigm hinges on the hand-crafting of task-specificdemonstrations one by one. We show that such manual efforts may be eliminated by leveragingLLMs with the  \"Let's think step by step \" prompt to generate reasoning chains for demonstrations oneby one, i.e., let's think not just step by step, but also one by one. However, these generated chainsoften come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters forautomatically constructing demonstrations. We propose an automatic CoT prompting method: AutoCoT. It samples questions with diversity and generates reasoning chains to construct demonstrations.On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds theperformance of the CoT paradigm that requires manual designs of demonstrations. Code is availableat https://github.com/amazon-research/auto-cot",
            "reference": [
                "Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638, 2022b. URL https://arxiv.org/abs",
                "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners",
                "Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/Q",
                "Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022. URL https://arxiv.org/abs",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models",
                "Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022) : The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, 2022a. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.deelio",
                "Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022. URL https://arxiv.org/abs",
                "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems",
                "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners",
                "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization",
                "Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions",
                "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs",
                "Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning",
                "Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right",
                "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022. URL https://arxiv.org/abs",
                "Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models",
                "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, page",
                "Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.167. URL https://aclanthology.org/2022.naacl-main",
                "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a. URL https://arxiv.org/abs",
                "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b. URL https://arxiv.org/abs",
                "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
                "Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification",
                "Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021. doi: 10.1162/tacl_a_00370. URL https://doi.org/10.1162/tacl_a",
                "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
                "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. URL https://arxiv.org/abs",
                "Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization",
                "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/abs",
                "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022b. URL https://arxiv.org/abs",
                "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs",
                "Subhro Roy and Dan Roth. Solving general arithmetic word problems",
                "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository",
                "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs",
                "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. URL https://arxiv.org/abs",
                "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners",
                "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs",
                "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main",
                "Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge"
            ],
            "related work": "2Related WorkThis section reviews two lines of research that form the basis of this work: chain-of-thought (CoT) prompting for multi-step reasoning and in-context learning for inducing LLMs to learn from demonstrations.2.1Chain-of-thought PromptingCoT prompting is a gradient-free technique of inducing LLMs to produce intermediate reasoning steps that lead to the final answer.Wei et al. (2022a) formally studied the topic of CoT prompting in language models. This technique elicits LLMs to generate a coherent series of intermediate reasoning steps that lead to the final answer to a question. Studies have shown that LLMs can perform CoT reasoning with zero-shot prompting (Zero-Shot-CoT) (Kojima et al.,2022) or manually written few-shot demonstrations (Manual-CoT) (Wei et al.,2022a) .Zero-Shot-CoT.Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners whose generated rationales have already reflected the CoT reasoning. This finding inspires our work to leverage the self-generated rationales for demonstrations. Generating rationales by LLMs was shown to be practical in a recent work(Zelikman et al.,2022) . In their work, an LLM is prompted to generate rationales and those rationales that lead to the correct answer are selected. The selection requires a training dataset of questions with annotated answers. In contrast, our work considers a more challenging scenario where only a set of test questions are given (without a training dataset) , following CoT prompting studies byWei et al. (2022a) andKojima et al. (2022) .Manual-CoT.Manual-CoT achieves stronger performance by eliciting the CoT reasoning ability with effective manual demonstrations. The demonstrations for the reasoning process are manually designed. However, the human efforts in designs of both questions and their reasoning chains are nontrivial. Instead of addressing this limitation, recent studies mainly focus on hand-crafting more complex demonstrations or leveraging ensemble-like methods. One trend is problem decomposition. In least-to-most prompting(Zhou et al.,2022) , complex problems are reduced to sub-problems, and then the sub-problems are solved sequentially. The other trend is to vote over multiple reasoning paths for a test question.Wang et al. (2022a) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers.Wang et al. (2022b) andLi et al. (2022) introduced randomness in the input space to produce more diverse outputs for voting. They used manually-designed demonstrations as the seed set and generated additional rationales: leave one question from the seed set and use the remaining demonstrations to generate rationales for this question by the LLM. Unlike the aforementioned research lines that rely on manually-designed demonstrations, our work intends to eliminate manual designs with competitive performance.2.2In-Context LearningCoT prompting is closely related to in-context learning (ICL) (Radford et al.,2019; Brown et al.,2020) . ICL enables LLMs to perform a target task by feeding a few prompted examples as part of the input. Without gradient update, ICL allows a single model to perform various tasks universally. There are various research lines to improve the performance of ICL: (i) retrieving related demonstrations to the test instance where the popular practice is dynamically retrieving related training examples for a given test input(Rubin et al.,2022; Su et al.,2022) ; (ii) augmenting with fine-grained information, such as incorporating task instruction(Mishra et al.,2022; Wei et al.,2022b; Sanh et al.,2022) ; (iii) manipulating output probabilities of LLMs instead of directly computing the likelihood of target labels(Holtzman et al.,2021; Zhao et al.,2021; Min et al.,2022a) .Despite the success of ICL, studies(Liu et al.,2022a; Lu et al.,2022) have shown that the strength of ICL may vary widely depending on the choice of in-context demonstrations(Liu et al.,2022b) . In detail, the formatting of the prompt, such as wording or order of demonstrations, may lead to performance fluctuations(Webson and Pavlick,2022; Zhao et al.,2021) . A recent work(Min et al.,2022b) even questioned the necessity of ground-truth input-output mapping: using incorrect labels in the examples only marginally lowers the performance. However, the existing analysis of ICL is mainly based on standard classification and multi-choice datasets that only have simple <input\u2192\u2192\\rightarrowoutput> mappings. We discover that those findings may not be applicable to the CoT prompting scenario with more complex <input\u2192\u2192\\rightarrowrationale\u2192\u2192\\rightarrowoutput> mappings. For example, mistakes in either the <input\u2192\u2192\\rightarrowrationale> mapping or the <rationale\u2192\u2192\\rightarrowoutput> mapping will lead to a dramatic performance drop (AppendixA.1) .",
            "date": "2022"
        },
        "topic": "Chain of Thought",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
                "arxiv_id": "2401.07037",
                "subtitles": [
                    "Large Language Models",
                    "Cross-lingual Transfer"
                ],
                "reference": [
                    "Emergent abilities of large language models",
                    "Multilingual clinical NER: translation or cross-lingual transfer",
                    "Multilingual neural machine translation with language clustering",
                    "Cross-lingual language model pretraining",
                    "GLM: general language model pretraining with autoregressive blank infilling",
                    "Alternating language modeling for cross-lingual pre-training",
                    "Solving math word problems via cooperative reasoning induced language models",
                    "Crosslingual generalization through multitask finetuning",
                    "Bloom: A 176b-parameter open-access multilingual language model",
                    "Large language models are reasoning teachers",
                    "Large language models are zero-shot reasoners",
                    "Conner: Consistency training for cross-lingual named entity recognition",
                    "XLM-T: scaling up multilingual machine translation with pretrained cross-lingual transformer encoders",
                    "UM4: unified multilingual multiple teacher-student model for zero-resource neural machine translation",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "CROP: zero-shot cross-lingual named entity recognition with multilingual labeled sequence translation",
                    "High-resource language-specific training for multilingual neural machine translation",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Unitrans : Unifying model transfer and data transfer for cross-lingual named entity recognition with unlabeled data",
                    "Crosssum: Beyond english-centric cross-lingual summarization for 1, 500+ language pairs",
                    "OWL: A large language model for IT operations",
                    "Understanding translationese in cross-lingual summarization",
                    "Unsupervised cross-lingual representation learning at scale",
                    "GanLM: Encoder-decoder pre-training with an auxiliary discriminator"
                ],
                "related_work": "6Related WorkLarge Language ModelsLarge language models (LLMs) has shown great power in numerous NLP tasks, and as the scale of the model gets larger, LLMs emerge with surprising capabilitiesTouvron et al. (2023c) ; Wei et al. (2022b) ; Du et al. (2022) ; Guo et al. (2023) , such as following human instructions, in-contextual learning, and reasoning complex tasks.Wei et al. (2022d) found that LLM can solve complex problems efficiently by chain-of-thought prompting strategy (providing some exemplars containing reasoning steps to guide the model to generate intermediate reasoning steps) . Moreover,Kojima et al. (2022b) found that LLMs can solve complex problems by CoT even without providing exemplars. However, the CoT capability usually requires the model to have a particularly large number of parameters and require massive computational resources. There is also some worksHo et al. (2022) ; Zhu et al. (2023) that explore the smaller LLMs' CoT capability. In this paper, we focus on the CoT capability for smaller LLMs and further migrate it to multilingual reasoning.Cross-lingual TransferCross-lingual transfer pertains to utilizing labeled data from a resource language to address the challenge of insufficient labeled data in the target language. Previous worksConneau and Lample (2019) ; Conneau et al. (2020) ; Yang et al. (2020) ; Ma et al. (2020) ; Yang et al. (2023) demonstrate that pre-trained models trained on multi-lingual data proficiently perform cross-lingual transfer tasks. These multi-lingual pre-trained models have found extensive application across various downstream NLP tasks, such as multi-lingual translationTan et al. (2019) ; Yang et al. (2022b) ; Gaschi et al. (2023) ; Yang et al. (2022c) , cross-lingual summarizationBhattacharjee et al. (2023) ; Wang et al. (2023) , cross-lingual information extractionZhou et al. (2022) ; Yang et al. (2022a) ; Wu et al. (2020) . Many LLMs are trained on multilingual data, endowing them with strong cross-linguistic abilitiesScao et al. (2022) ; Muennighoff et al. (2022) . However, the cross-language capability in smaller LLM is not significant, so we augmenting the multilingual reasoning potential of LLMs by employing pseudo training data derived from labeled source-language datasets.",
                "abstract": "Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL)) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap."
            },
            {
                "name": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
                "arxiv_id": "2404.03204",
                "subtitles": [
                    "LLM-based TTS",
                    "Robust autoregressive TTS"
                ],
                "reference": [
                    "Multispeech: Multi-speaker text to speech with transformer",
                    "Uniaudio: An audio foundation model toward universal audio generation",
                    "Emergent abilities of large language models",
                    "Attention is all you need",
                    "Viola: Unified codec language models for speech recognition, synthesis, and translation",
                    "Language models are few-shot learners",
                    "Audiopalm: A large language model that can speak and listen",
                    "Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts",
                    "Sequence transduction with recurrent neural networks",
                    "Language models are unsupervised multitask learners",
                    "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision",
                    "Ella-v: Stable neural codec language modeling with alignment-guided sequence reordering",
                    "Neural codec language models are zero-shot text to speech synthesizers",
                    "Vall-t: Decoder-only generative transducer for robust and decoding-controllable text-to-speech",
                    "Forward attention in sequence-to-sequence acoustic modeling for speech synthesis",
                    "Non-attentive tacotron: Robust and controllable neural tts synthesis including unsupervised duration modeling"
                ],
                "related_work": "2Related workLLM-based TTSInspired by the success of LLMs[20,1], several recent works adopt language models to model TTS[29,33,14]and begin to use decoder-only architecture based on transformer[28]. In such models, text and speech tokens are concatenated together and fed to a single transformer. The whole model is trained on a next-token prediction task like a language model. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning[31]to enable zero-shot TTS[29]. Besides, recent works[22,30,33]have shown the decoder-only architecture can be used to learn multiple tasks, as the input and output are processed jointly by a language model, and the model can be signaled to generate results for different tasks by inputting pre-defined special tokens. RALL-E focuses on the robustness problem of LLM-based TTS.Robust autoregressive TTSThe robustness of AR TTS is a popular topic in the literature. For encoder-decoder AR TTS, several previous works enforce the attention weights to be monotonic[36,9,2]that can effectively improve the robustness. In addition,Shen et al. [24]proposed a non-attentive Tacotron, in which the attention module was replaced by a duration predictor to determine the alignment path before decoding. For decoder-only TTS, a key difference is that the attention weights are computed on text and context at the same time, hence the whole attention weights should not be monotonic.Song et al. [26]proposed ELLA-V that interleaves the speech tokens with phonemes by inserting a phoneme token and a specialEndOfPhone(EOP) token at the beginning and end of the speech tokens corresponding to the phoneme, respectively. While the inserted phoneme and the EOP token indicate the duration of each phoneme, such an implicit way entangles the prediction of speech tokens and duration together. RALL-E disentangles the predictions of duration and speech tokens by predicting the duration of all phonemes before the speech tokens, hence has higher controllability over the generation process.Du et al. [6]proposed VALL-T that uses an unsupervised transducer loss[7]to implicitly model the duration of phonemes. Compared to RALL-E, although VALL-T doesn't rely on external alignment tools during training, its training time is considerably decelerated since the transducer loss requires the model to perform a forward process for every phoneme. Besides, like ELLA-V, VALL-T also entangles the predictions of duration and speech tokens, thus has weaker controllability than RALL-E.",
                "abstract": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$."
            },
            {
                "name": "Chain-of-Thought Reasoning Without Prompting",
                "arxiv_id": "2402.10200",
                "subtitles": [
                    "Chain-of-thought reasoning in large language models",
                    "Instruction-tuning to elicit CoTs in language models",
                    "Decoding algorithms for language models",
                    "Decoding algorithms for efficiency"
                ],
                "reference": [
                    "Contrastive decoding improves reasoning in large language models",
                    "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation",
                    "Solving math word problems with process- and outcome-based feedback",
                    "Teaching small language models to reason",
                    "Accelerating large language model decoding with speculative sampling",
                    "Large language models as analogical reasoners",
                    "Distillspec: Improving speculative decoding via knowledge distillation",
                    "Why think step by step? reasoning emerges from the locality of experience",
                    "Pathfinder: Guided search over multi-step reasoning paths",
                    "Trusting your evidence: Hallucinate less with context-aware decoding",
                    "Embers of autoregression: Understanding large language models through the problem they are trained to solve",
                    "The unreliability of explanations in few-shot prompting for textual reasoning",
                    "Dissecting chain-of-thought: Compositionality through in-context filtering and learning",
                    "Let's verify step by step",
                    "Large language models are zero-shot reasoners",
                    "Scalable extraction of training data from (production) language models",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "Learning to write with cooperative discriminators",
                    "Tuning language models by proxy",
                    "Challenging big-bench tasks and whether chain-of-thought can solve them",
                    "The curious case of neural text degeneration",
                    "Fast inference from transformers via speculative decoding",
                    "Contrastive decoding: Open-ended text generation as optimization",
                    "Large language models are human-level prompt engineers",
                    "Hierarchical neural story generation",
                    "Large language models can self-improve",
                    "Least-to-most prompting enables complex reasoning in large language models",
                    "Camels in a changing climate: Enhancing lm adaptation with tulu",
                    "Controlling linguistic style aspects in neural language generation",
                    "A learning algorithm for boltzmann machines",
                    "Large language models as optimizers",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Towards revealing the mystery behind chain of thought: A theoretical perspective",
                    "Scaling instruction-finetuned language models",
                    "Discovering latent knowledge in language models without supervision",
                    "Diverse beam search for improved description of complex scenes",
                    "Self-evaluation guided beam search for reasoning",
                    "Language models are unsupervised multitask learners",
                    "Impact of pretraining term frequencies on few-shot numerical reasoning",
                    "Typical decoding for natural language generation",
                    "Show your work: Scratchpads for intermediate computation with language models",
                    "Rationale-augmented ensembles in language models",
                    "Do prompt-based models really understand the meaning of their prompts",
                    "How far can camels go? exploring the state of instruction tuning on open resources"
                ],
                "related_work": "Chain-of-thought reasoning in large language models. Existing work enhancing the reasoningabilities in large language models predominantly involve proposing better prompting techniques tobetter elicit CoT reasoning paths from the model (Kojima et al., 2022; Nye et al., 2021; Wei et al.,2022; Yao et al., 2023; Yasunaga et al., 2023; Zhou et al., 2023a) . Despite achieving high performance,few-shot prompting techniques are often task-specific, requiring prompt designs tailored to each task.This limits their generalizability across tasks. Advanced prompting techniques often require manuallyintensive prompt engineering, and their effectiveness varies depending on the choice of prompts,resulting in inconsistent performance outcomes (Wang et al., 2022; Ye and Durrett, 2022; Zhou et al.,2023b) . Efforts to discover improved prompts (Yang et al., 2024; Zhou et al., 2023b) further entailmodel-specific and task-specific tuning.In addition, these prompting techniques can subtly alter the vocabulary's posterior distributionin ways that remain largely elusive (Min et al., 2022; Webson and Pavlick, 2022) . Specifically,prompts may assist in task decomposition, induce the model to generate additional tokens, ordirectly  \"teach \" the model the exact underlying procedure to solve particular problems via manuallycrafted few-shot demonstrations. Dissecting the distinct influence of each aspect, however, presentsa significant challenge. In contrast, our work explores a different perspective within the decodingstage, demonstrating that, even without explicit prompting, the model inherently holds the capabilityto generate chain-of-thought reasoning paths across a wide set of tasks.Recent work proposes to improve the CoT generation process via better controlling and verifyingthe steps generated, e.g., step-by-step verification (Lightman et al., 2023) , process-based feedback(Uesato et al., 2022) , self-evaluation guided beam search (Xie et al., 2023) , and PathFinder (Golovnevaet al., 2023) . Note all these works still require CoT prompting in order to generate the CoT reasoningpaths, while our work completely removes CoT prompting. In addition, these works focus on searchingand verifying the  \"steps \" produced by the language model, while our work purely searches in thedecoding space on the token-level and utilizes the confidence scores when decoding the answer.Additionally, recent works (Feng et al., 2023; Li et al., 2023b; Prystawski et al., 2023) . McCoy et al.(2023) ; Razeghi et al. (2022) demonstrate a similar phenomenon where the pretraining distributionheavily influences the model's performance in few-shot reasoning.Instruction-tuning to elicit CoTs in language models. When supervision is allowed, techniquessuch as instruction-tuning or distillation offer another way to elicit reasoning paths from languagemodels without explicit prompting (Chung et al., 2022; Huang et al., 2023; Magister et al., 2023) .However, these approaches typically involve resource-intensive fine-tuning over large language modelsand require a large set of examples annotated with CoTs, which may not be readily available.Liu et al. (2024) show that a language model can be tuned by a proxy. Their method requiresa few additional models, and implicitly assumes that the tuned model is well-optimized, e.g., onreasoning benchmarks the model needs to be tuned with CoT paths to enable contrasting logits withrespect to the base untuned model. In contrast, our approach is entirely unsupervised and examinesa model's intrinsic ability in generating CoT paths, without resorting to fine-tuning or any additionalmodels.Decoding algorithms for language models. The predominant focus in existing literature ondecoding for language models revolves around aspects such as fluency, coherence, reduction ofrepetitiveness, and diversity in responses. Popular decoding algorithms used for language modelsinclude greedy decoding, temperature sampling (Ackley et al., 1985; Ficler and Goldberg, 2017) , top-k sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019) , and nucleus sampling(Holtzman et al., 2020) . Additionally, there exist refined algorithms such as minimum Bayes riskdecoding (Eikema and Aziz, 2020) , and typical decoding (Meister et al., 2022) . Diverse beamsearch (Vijayakumar et al., 2018) is another way to explore alternative paths in a model's generation.However, it emphasizes generation diversity rather than accuracy.There is relatively little research dedicated to enhancing decoding algorithms specifically forreasoning tasks. Wang et al. (2023a) improves upon CoT prompting by sampling and aggregatingover multiple generated responses to improve reasoning. Contrastive decoding (Li et al., 2023a) isanother way to improve model's generation quality by penalizing the logits from smaller models, andrecent work (O'Brien and Lewis, 2023) shows that contrastive decoding can contribute to enhancingreasoning performance. Shi et al. (2023) propose context-aware decoding to improves the faithfulnessof language models. These approaches typically require additional information, such as employingadditional models to generate contrasting logits or incorporating additional contexts. In contrast, ourwork relies solely on a single model without the need for supplementary knowledge.Decoding algorithms for efficiency. In addition to decoding algorithms for improving quality, thereis a substantial body of research dedicated to improving decoding efficiency, e.g., speculative decoding(Chen et al., 2023a; Leviathan et al., 2022; Zhou et al., 2024) . This line of work is orthogonal to ourwork as their primary focus is not on improving a model's reasoning performance. However, thesetechniques could potentially be leveraged to improve the efficiency of CoT-decoding.",
                "abstract": "In enhancing the reasoning capabilities of large language models (LLMs), prior research primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the \\textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to assess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model's decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that the proposed CoT-decoding effectively elicits reasoning capabilities from language models, which were previously obscured by standard greedy decoding."
            },
            {
                "name": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "arxiv_id": "2402.18205",
                "subtitles": [
                    "Log Parser",
                    "Large Language Model"
                ],
                "reference": [
                    "Palm 2 technical report",
                    "Clustering event logs using iterative partitioning",
                    "Scaling laws for generative mixed-modal language models",
                    "Spell: Streaming parsing of system event logs",
                    "Drain: An online log parsing approach with fixed depth tree",
                    "Logmine: Fast pattern recognition for log analytics",
                    "A data clustering algorithm for mining patterns from event logs",
                    "On automatic parsing of log records",
                    "Prefix-graph: A versatile log parsing approach merging prefix tree with probabilistic graph",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Logcluster-a data clustering and pattern mining algorithm for event logs",
                    "Cross-lingual natural language generation via pre-training",
                    "Lpv: A log parser based on vectorization for offline and online log parsing",
                    "Gpt-4 technical report",
                    "Self-supervised log parsing",
                    "Length matters: Clustering system log messages using length of words",
                    "Uniparser: A unified log parser for heterogeneous log data",
                    "Execution anomaly detection in distributed systems through unstructured log analysis",
                    "Attention is all you need",
                    "Scaling instruction-finetuned language models",
                    "Incremental mining of system log format",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work2.1Log ParserIn the evolving field of automatic log analysis, crucial for distributed systems and cloud computing, significant progress has been made in log parsing techniques, categorized into frequent pattern mining, clustering, and heuristics rules. Frequent pattern mining is exemplified by SLCTVaarandi (2003) which groups logs based on token frequency, and LogClusterVaarandi and Pihelgas (2015) that removes positional constraints in log grouping.. Clustering approaches include LogMineHamooniet al.(2016) with its multi-layered clustering system, LKEFuet al.(2009) using edit distance and position weighing, SHISOMizutani (2013) improving efficiency through hierarchical clustering, LenMaShima (2016) employing token length vectors for clustering, and LPVXiaoet al.(2020) which uses semantic vectors from word2vec. In the heuristics rules category, IPLOMMakanjuet al.(2009) partitions logs by length and token position, SpellDu and Li (2016) approaches parsing as a longest common sequential problem, DrainHeet al.(2017) groups logs by length and prefixes for template updates, and Prefix-GraphChuet al.(2021) merges prefix trees into graphs for template generation. Recent advancements have introduced deep learning-based algorithms like NulogNedelkoskiet al.(2021) with a transformer encoder layer and masked language model, UniparserLiuet al.(2022) comprising multiple modules for comparative learning, and LogAPRand and Miranskyy (2021) utilizing machine translation for parsing. However, these deep learning methods face challenges in efficiency and high operational costs due to GPU requirements.2.2Large Language ModelLanguage Models.Language, a unique human ability developing from early childhood, necessitates advanced artificial intelligence algorithms for machines to comprehend and utilize it. Language modeling using self-supervised learning and large-scale data, particularly through pre-training large Transformer encoders and decodersVaswaniet al.(2017) ; Chiet al.(2020) , significantly enhances various natural language processing tasks. Specifically, pre-training a Transformer decoderOpenAI (2023) aids in unconditional text generation. Performance improvements in diverse tasks have been linked to the enlargement of Pre-training Language Models (PLMs) by increasing model or data size, following established scaling principles. This has led to the creation of increasingly larger PLMs, such as GPT-3 with 175 billion parameters and PaLM with 540 billionAnilet al.(2023) , guided by the scaling laws of large language modelsAghajanyanet al.(2023) .Despite their similar architectures and pre-training tasks, larger PLMs, such as GPT-4OpenAI (2023) , exhibit unique behaviors and emergent abilities, excelling in complex tasks. A prime example is ChatGPT, adapting GPT-series LLMs for engaging dialogues, showcasing advanced conversational skills. Fine-tuning LLMs on various datasetsChunget al.(2022) ; Weiet al.(2022b) yields promising results, using human or LLM-created prompts for instruction tuning and refining generations. Chain-of-thought promptingWeiet al.(2022b) , where models explain their reasoning for complex problems to improve answer accuracy, and RLHFOuyanget al.(2022) , a strategy for fine-tuning LLMs, significantly enhance their performance.",
                "abstract": "Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \\textbf{L}og parsing framework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs). LLMs exhibit exceptional semantic comprehension, deftly distinguishing between parameters and invariant tokens. We have conducted experiments on large-scale public datasets. Extensive evaluation demonstrates that Lemur achieves the state-of-the-art performance and impressive efficiency."
            },
            {
                "name": "CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs",
                "arxiv_id": "2401.02582",
                "subtitles": [
                    "Large Multimodal Models",
                    "Multimodal Prompting Methods"
                ],
                "reference": [
                    "Mmicl: Empowering vision-language model with multi-modal in-context learning",
                    "Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models",
                    "Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners",
                    "Visual instruction tuning",
                    "Gemini: A family of highly capable multimodal models",
                    "Large language models are zero-shot reasoners",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Compositional chain-of-thought prompting for large multimodal models",
                    "GPT-4 technical report",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Llama: Open and efficient foundation language models"
                ],
                "related_work": "2Related WorkLarge Multimodal Models.Inspired by the advancements of LLMs (e.g.,LLaMA[1]) , LMMs offer a promising way towards AGI with multimodal information. These models blend the textual reasoning prowess of LLMs with the image and video comprehension of Vision-and-Language models. This fusion enables LMMs to handle complex tasks requiring both a profound understanding and expressive generation across various modalities. Several open-source LMMs likeLLaVA[4]have emerged, demonstrating competence in tasks such as image captioning and visual question-answering. However, their architectural limitations restrict their understanding and reasoning to a single image. Conversely, models likeOpenFlamingo[12], andMMICL[13]employ specialized architectures enabling the processing of multiple image features, which better mirrors real-world scenarios. Closed-source LMMs such asGPT-4V[14]andGemini[15]go beyond basic object descriptions to capture the scene's context[11], emotions[13], and relationships[16]. A common technique to enhance performance is fine-tuning, but applying similar methods to LMMs presents computation challenges[17]. To overcome this, we propose a novel approach to directly enable detailed analysis and reasoning on images without additional training data.Fig. 2:Different CoT-based methods and their performance in extracting information from images under various conditions, withGPT-4Vbeing used in the experiments. Left: Utilizing CCoT to generate image information; Middle: CoCoT prompting between images with a big domain gap; Right: CoCoT prompting between images with a small domain gap.Multimodal Prompting Methods.Within the domain of LLMs, several language prompt methods have been established to enhance inference capabilities and ensure accurate results during prediction. These include zero-shot[18], few-shot[19], and Chain-of-Thought (CoT) [10,11]approaches. Recently, research has begun exploring the application of prompting techniques in the multimodal domain to improve the comprehension and reasoning abilities of LMMs for image data. Current multimodal prompts employed in LMMs often exhibit limitations in capturing the intricate interrelationships between visual and language information, particularly when faced with multi-image inputs. As shown in the example in Fig.1, they are not able to identify the critical action of the boy throwing the ball. To overcome this challenge, we propose a novel prompting method that directs LMMs to extract and analyze essential information, requiring a holistic consideration of all the input images.",
                "abstract": "When exploring the development of Artificial General Intelligence (AGI), a critical task for these models involves interpreting and processing information from multiple image inputs. However, Large Multimodal Models (LMMs) encounter two issues in such scenarios: (1) a lack of fine-grained perception, and (2) a tendency to blend information across multiple images. We first extensively investigate the capability of LMMs to perceive fine-grained visual details when dealing with multiple input images. The research focuses on two aspects: first, image-to-image matching (to evaluate whether LMMs can effectively reason and pair relevant images), and second, multi-image-to-text matching (to assess whether LMMs can accurately capture and summarize detailed image information). We conduct evaluations on a range of both open-source and closed-source large models, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model performance, we further develop a Contrastive Chain-of-Thought (CoCoT) prompting approach based on multi-input multimodal models. This method requires LMMs to compare the similarities and differences among multiple image inputs, and then guide the models to answer detailed questions about multi-image inputs based on the identified similarities and differences. Our experimental results showcase CoCoT's proficiency in enhancing the multi-image comprehension capabilities of large multimodal models."
            },
            {
                "name": "Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities",
                "arxiv_id": "2402.17230",
                "subtitles": [
                    "Vulnerability identification/discovery",
                    "Vulnerability patching/repair",
                    "Prompting LLMs"
                ],
                "reference": [
                    "Examining zero-shot vulnerability repair with large language models",
                    "{{\\{{PolyCruise}}\\}}: A {{\\{{Cross-Language}}\\}} dynamic information flow analysis",
                    "{{\\{{AddressSanitizer}}\\}}: A fast address sanity checker",
                    "PolyFuzz: Holistic greybox fuzzing of multi-language systems",
                    "Valgrind: a framework for heavyweight dynamic binary instrumentation",
                    "Practical memory checking with dr. memory",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "Large language models are zero-shot reasoners",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "A comparative study on software vulnerability static analysis techniques and tools",
                    "VulRepair: a t5-based automated software vulnerability repair",
                    "Detecting kernel memory leaks in specialized modules with ownership reasoning",
                    "Example-based vulnerability detection and repair in java code",
                    "Neural transfer learning for repairing security vulnerabilities in c code",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Vulcnn: An image-inspired scalable vulnerability detection system",
                    "Vulnerability detection with fine-grained interpretations",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "The art, science, and engineering of fuzzing: A survey",
                    "Fuzzing: a survey for roadmap",
                    "FlowDist:multi-staged refinement-based dynamic information flow analysis for distributed software systems",
                    "LineVD: statement-level vulnerability detection using graph neural networks",
                    "Undangle: early detection of dangling pointers in use-after-free and double-free vulnerabilities",
                    "{{\\{{Type-Assisted}}\\}} dynamic buffer overflow detection",
                    "VulChecker: Graph-based vulnerability localization in source code",
                    "Typestate-guided fuzzer for discovering use-after-free vulnerabilities",
                    "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content",
                    "Few-sample named entity recognition for security vulnerability reports by fine-tuning pre-trained language models",
                    "PCA: memory leak detection using partial call-path analysis",
                    "CBMC-c bounded model checker",
                    "Dynamic detection of inter-application communication vulnerabilities in android",
                    "Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks",
                    "{{\\{{FUZE}}\\}}: Towards facilitating exploit generation for kernel {{\\{{Use-After-Free}}\\}} vulnerabilities",
                    "Vurle: Automatic vulnerability detection and repair by learning from examples",
                    "A practical dynamic buffer overflow detector",
                    "Deep learning based vulnerability detection: Are we there yet",
                    "Tt-xss: A novel taint tracking based dynamic detection framework for dom cross-site scripting",
                    "Attention is all you need",
                    "A new algorithm for data compression",
                    "Beyond tests: Program vulnerability repair via crash constraint extraction",
                    "CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Binary-level directed fuzzing for {{\\{{Use-After-Free}}\\}} vulnerabilities",
                    "A Coverage-Guided, Native Python Fuzzer",
                    "Dowsing for Overflows: A guided fuzzer to find buffer boundary violations",
                    "LineVul: a transformer-based line-level vulnerability prediction",
                    "Evaluating and comparing memory error vulnerability detectors",
                    "Open science in software engineering: A study on deep learning-based vulnerability detection",
                    "Sysevr: A framework for using deep learning to detect software vulnerabilities",
                    "The power of scale for parameter-efficient prompt tuning",
                    "Technical \"whitepaper\" for afl-fuzz"
                ],
                "related_work": "8Related WorkVulnerability identification/discovery.Traditionally, vulnerability detection is addressed through static and/or dynamic code analysis[42,60]. For instance, PCA[43]detects memory leaks via static data-flow analysis focusing on efficiency optimization, while KMeld[13]targets the same type of vulnerabilities induced by specialized memory allocation/deallocation functions. CBMC[38]discovers a range of memory-error related vulnerabilities through (static) model checking, validating assertions against memory-safety violations.With dynamic analysis, undangle[10]identifies use-after-free and double-free vulnerabilities via searching dangling pointers, and TT-XSS[73]identifies DOM cross-site scripting (XSS) vulnerabilities via dynamic taint tracking. Both targeting buffer overruns, Lhee and Chapin[40]focuses on array boundary checking, while Cred[67]checks the bounds of various kinds of memory accesses. More broadly, IntentDroid[27]detects eight types of inter-app communication (IAC) vulnerabilities as a result of unsafe handling of incoming IAC messages, while DrMemory[9]and Valgrind[56]discover memory-safety vulnerabilities through memory shadowing. Similarly, AddressSanitizer[68]also discovers a variety of out-of-bounds access as well as use-after-free errors, while FlowDist[22]and PolyCruise[44]detects various types of taint-style vulnerabilities but these techniques work by combining static and dynamic analysis techniques.A main class of dynamic approach is fuzzing, which generates run-time inputs to trigger vulnerabilities[87]. For instance, UAFuzz[57]identifies use-after-free (UaF) vulnerabilities through directed greybox fuzzing, while UAFL[71]models the same type of vulnerabilities as typestate properties and checks property violations during fuzzing. FUZE[79]also focuses on detecting UaF bugs in OS kernels by combining kernel fuzzing and symbolic execution. Likewise, Dowser[26]addresses a particular kind of (i.e., buffer overflow/underflow) vulnerabilities using taint tracking combined with symbolic execution. More fuzzing approaches, however, are devised to discover a wider range of vulnerabilities[54,45,25,52].Another major direction in vulnerability detection is to leverage machine learning, especially deep learning (DL) [11,64]. For example, VulCNN[81]detects vulnerabilities by modeling programs as images hence leveraging the merits of convolutional neural network (CNN) . Like Devign[86], VulChecker[53]learns a graph neural network (GNN) based program representation for detecting vulnerabilities. Many other DL-based approaches, such as LineVul[20]and LineVD[30], detect vulnerabilities through sequence modeling of code as natural language tokens based on Transformer[70]. In addition, IVDetect[46]offers explainability alongside detected vulnerabilities, and like LineVul and LineVD it advances over function-level detection[47,11].Our work explores vulnerability identification/discovery using LLMs, which generally falls in the DL-based category. A key difference lies in that traditional DL-based detection relies on a sizable labeled training dataset, which LLM-prompting-based like ours only needs a few prompting exemplars.Vulnerability patching/repair.VuRLE[51]repairs vulnerable code by clustering code transformation edits in a set of repair examples, similar to Seader[85]using fixing edit patterns. ExtractFix[24]fixes a vulnerability for which the test (exploit) is available using symbolic execution. VRepair[12]attempts to fix security vulnerabilities by utilizing knowledge learned from functionality-bug fixes through transfer learning based on a vanilla Transformer[70]. Also by fine-tuning Transformer but leveraging a pre-trained code model, VulRepair[21]improves repair accuracy over VRepair by using the T5 Transformer[74]along with the BPE tokenization[23]. Lately, Pearce et al.[65]applied several LLMs in the zero-shot setting to vulnerability repair, showing the challenges with repairing real-world vulnerabilities.In comparison, we explore LLM-prompting-based approaches which do not rely on (sizable) fine-tuning datasets or known exploits. Also, unlike[65], we also examine various settings beyond zero-shot on LLMs.Prompting LLMs.Different from prompt learning[50,29]and fine tuning[83]as part of modeltraining(which thus requires downstream-task-specific labeled datasets) , prompting is aninference-time technique to improve (pre-trained) model's responses. It also does not iteratively adjust prompts as in prompt tuning[39]. For instance, chain of thought (CoT) prompting elicits complex multi-step reasoning through step-by-step answer examples[75], while zero-shot CoT is a CoT variant replacing those examples with a simple prompt ( \"Let's think step by step\") [37]. Later, tree of thoughts (ToT) generalizes over CoT by considering multiple different reasoning paths to allow for deliberate decision making[84], and graph of thoughts (GoT) [6]further generalizes CoT by modeling the LLM's reasoning process as a graph, enhancing model capabilities through networked reasoning.In contrast, while ourVSPapproach is initially inspired by the general CoT methodology, it differs from it in focusing on selectively prompting LLMs only with the thoughts (i.e., vulnerability semantics) that are most relevant to the downstream tasks (i.e., vulnerability analysis) .",
                "abstract": "Security vulnerabilities are increasingly prevalent in modern software and they are widely consequential to our society. Various approaches to defending against these vulnerabilities have been proposed, among which those leveraging deep learning (DL) avoid major barriers with other techniques hence attracting more attention in recent years. However, DL-based approaches face critical challenges including the lack of sizable and quality-labeled task-specific datasets and their inability to generalize well to unseen, real-world scenarios. Lately, large language models (LLMs) have demonstrated impressive potential in various domains by overcoming those challenges, especially through chain-of-thought (CoT) prompting. In this paper, we explore how to leverage LLMs and CoT to address three key software vulnerability analysis tasks: identifying a given type of vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities. We instantiate the general CoT methodology in the context of these tasks through VSP , our unified, vulnerability-semantics-guided prompting approach, and conduct extensive experiments assessing VSP versus five baselines for the three tasks against three LLMs and two datasets. Results show substantial superiority of our CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets) over the baselines. Through in-depth case studies analyzing VSP failures, we also reveal current gaps in LLM/CoT for challenging vulnerability cases, while proposing and validating respective improvements."
            },
            {
                "name": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
                "arxiv_id": "2401.12242",
                "subtitles": [
                    "COT prompting for LLMs",
                    "Backdoor attacks"
                ],
                "reference": [
                    "Trojtext: Test-time invisible textual trojan insertion",
                    "NOTABLE: Transferable backdoor attacks against prompt-based NLP models",
                    "Making language models better reasoners with step-aware verifier",
                    "Detecting backdoor attacks against point cloud classifiers",
                    "Exploring the universal vulnerability of prompt-based learning paradigm",
                    "Badprompt: Backdoor attacks on continuous prompts",
                    "Backdoor pre-trained models can transfer to all",
                    "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses",
                    "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
                    "BadNets: Evaluating backdooring attacks on deep neural networks",
                    "Towards stealthy backdoor attacks against speech recognition via elements of sound",
                    "Backdoor attacks for in-context learning with language models",
                    "Prompt as triggers for backdoor attack: Examining the vulnerability in language models",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "Clean-label backdoor attacks on video recognition models",
                    "Trojaning language models for fun and profit",
                    "Backdoor learning: A survey",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Backdoor attacks on pre-trained models by layerwise weight poisoning",
                    "PointBA: Towards backdoor attacks in 3d point cloud",
                    "Subnet replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting",
                    "Mind the style of text! adversarial and backdoor attacks based on text style transfer",
                    "Poisoning language models during instruction tuning",
                    "Trojaning attack on neural networks",
                    "Active prompting with chain-of-thought for large language models, 2023b",
                    "Adversarial learning in statistical classification: A comprehensive review of defenses against attacks",
                    "Backdoor attack against speaker verification",
                    "Targeted backdoor attacks on deep learning systems using data poisoning",
                    "Least-to-most prompting enables complex reasoning in large language models",
                    "Black-box prompt learning for pre-trained language models",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
                    "BadNL: Backdoor Attacks against NLP Models with Semantic-Preserving Improvements",
                    "Handcrafted backdoors in deep neural networks",
                    "Language models are few-shot learners",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
                    "Compositional semantic parsing with large language models"
                ],
                "related_work": "2Related WorkCOT prompting for LLMs.Demonstration-based prompts are widely used in ICL to elicit helpful knowledge in LLMs for solving downstream tasks without model fine-tuning(Shin et al.,2020; Brown et al.,2020; Diao et al.,2023a) . For more challenging tasks, COT further exploits the reasoning capabilities of LLMs by enhancing each demonstration with detailed reasoning steps(Wei et al.,2022) . Recent developments of COT include a self-consistency approach based on majority vote(Wang et al.,2023b) , a series of least-to-most approaches based on problem decomposition(Zhou et al.,2023; Drozdov et al.,2023) , a diverse-prompting approach with verification of each reasoning step(Li et al.,2023) , and an active prompting approach using selectively annotated demonstrations(Diao et al.,2023b) . Moreover, COT has also been extended to tree-of-thoughts and graph-of-thoughts with more complicated topologies for the reasoning steps(Yao et al.,2023a;b) . In this paper, we focus on the standard COT and self-consistency due to their effectiveness on various leaderboards.Backdoor attacks.Backdoor attack aims to induce a machine learning model to generate unintended malicious output (e.g. misclassification) when the input is incorporated with a predefined backdoor trigger(Miller et al.,2020; Li et al.,2022) . Backdoor attacks are primarily studied for computer vision tasks(Chen et al.,2017; Liu et al.,2018; Gu et al.,2019) , with extension to other domains including audios(Zhai et al.,2021; Cai et al.,2023) , videos(Zhao et al.,2020) , point clouds(Li et al.,2021b; Xiang et al.,2022) , and natural language processing(Chen et al.,2021; Zhang et al.,2021; Qi et al.,2021a; Shen et al.,2021; Li et al.,2021a; Lou et al.,2023) . Recently, backdoor attacks have been shown as a severe threat to LLMs(Xu et al.,2022; Cai et al.,2022; Mei et al.,2023; Kandpal et al.,2023; Xu et al.,2023a; Wan et al.,2023; Zhao et al.,2023) . However, existing backdoor attacks are mostly launched by training set poisoning(Goldblum et al.,2023) , model fine-tuning(Liu et al.,2018) , or  \"handcrafting \" the model architecture or parameters(Qi et al.,2021b; Hong et al.,2022) , which limits their application to SOTA (commercial) LLMs, for which the training data and model details are usually unpublished. Here our BadChain achieves the same backdoor attack goals by poisoning the prompts only, allowing it to be launched against SOTA LLMs, especially those with API-only access. Closest to our work is the backdoor attack proposed byWang et al. (2023a) , which attacks LLMs by poisoning the demonstration examples. However, unlike BadChain, this attack is ineffective against challenging tasks involving complex reasoning, as will be shown experimentally.",
                "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger exists in the query prompt. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover, we show that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses."
            },
            {
                "name": "LLMs with Chain-of-Thought Are Non-Causal Reasoners",
                "arxiv_id": "2402.16048",
                "subtitles": [
                    "Human Reasoning",
                    "LLM Reasoning",
                    "Causal Analysis"
                ],
                "reference": [
                    "Causality",
                    "Causal inference in statistics, social, and biomedical sciences",
                    "Causal inference in natural language processing: Estimation, prediction, interpretation and beyond",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
                    "Elements of causal inference: foundations and learning algorithms",
                    "Causal reasoning through intervention",
                    "A survey of chain of thought reasoning: Advances, frontiers and future",
                    "Causality in thought",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Causal analysis",
                    "Mechanical reasoning by mental simulation",
                    "Chain-of-thought prompting elicits reasoning in large language models"
                ],
                "related_work": "2Related WorkHuman Reasoning.Research in psychology indicates that causality is a fundamental aspect of human cognition, characterized by the processes of mechanism, narrative, and mental simulationSloman and Lagnado (2015) . Mental simulations as temporal representations of mechanisms (typically visual) , allow individuals to draw causal conclusionsHegarty (2004) . Our work draws inspiration from human reasoning, exploring language simulations that represent reasoning mechanisms. LLM Reasoning.Various reasoning techniques have been proposed to enhance the reasoning ability of LLMsChu et al. (2023) . Chain-of-thought (CoT) promptingWei et al. (2022b) , as an early study elicits reasoning in LLMs, inspires numerous further investigations. Self-consistencyWang et al. (2022) leverages multiple reasoning paths to make more confident decisions. Tree-of-thoughtYao et al. (2023a) searches the most confident reasoning path using a tree representation. Graph-of-thoughtYao et al. (2023b) represents the thoughts as nodes in a graph, combining thoughts non-sequentially. While existing work mostly considers the statistical effect of using CoT, we delve into the underlying mechanism of how CoT reasoning leads to conclusions.Causal Analysis.Causal analysis is a method used to identify and understand the causes and effects of different actions, situations, or decisions. It involves examining the reasons or causes behind a certain occurrence and the outcomes that may arise from itHeise (1975) ; Imbens and Rubin (2015) ; Feder et al. (2022) . Causal models in different structures may induce the same observational distribution but different intervention distributionsPeters et al. (2017) . Interventions thus can be used to differentiate among the potential causal structures that are compatible with an observationHagmayer et al. (2007) ; Pearl (2009) . We study CoT by its causal relation to the model decisions, using interventions to test the significance of the cause-effect relations between CoTs/instructions and answers.",
                "abstract": "This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results atthis https URL."
            },
            {
                "name": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
                "arxiv_id": "2409.12183",
                "subtitles": [
                    "Where is CoT helping and why",
                    "Long Horizon Planning",
                    "Can we improve CoT further",
                    "Dataset contamination"
                ],
                "reference": [
                    "Tree of Thoughts: Deliberate problem solving with large language models",
                    "On the planning abilities of large language models - a critical investigation",
                    "Transformers learn shortcuts to automata",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "One thousand and one pairs: A  \"novel \" challenge for long-context language models",
                    "Improving factuality and reasoning in language models through multiagent debate",
                    "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
                    "Reconcile: Round-table conference improves reasoning via consensus among diverse LLMs",
                    "Position: LLMs can't plan, but can help planning in LLM-modulo frameworks",
                    " \"Task Success \" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
                    "On the empirical complexity of reasoning and planning in llms",
                    "Inferring the reader: Guiding automated story generation with commonsense reasoning",
                    "Chain of thoughtlessness? an analysis of cot in planning",
                    "Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning",
                    "On the self-verification limitations of large language models on reasoning and planning tasks",
                    "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models",
                    "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                    "Can large language models reason and plan",
                    "Is self-repair a silver bullet for code generation",
                    "The expressive power of transformers with chain of thought",
                    "MuSR: Testing the limits of chain-of-thought with multistep soft reasoning",
                    "PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
                    "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                    "Chain-of-symbol prompting elicits planning in large langauge models",
                    "Show your work: Scratchpads for intermediate computation with language models",
                    "Theory of mind abilities of large language models in human-robot interaction: An illusion"
                ],
                "related_work": "6Discussion and Related WorkWhere is CoT helping and why?Our results showing CoT improvement for math and logic aligns well with early work on CoT for LLMs such as Scratchpads(Nye et al.,2022) . As CoT gained popularity, its application has broadened to tasks that canonically do not require multiple steps. It can often yield small improvements over direct answering. We believe this led to the current prevailing sentiment that deliberation should improve performance on any task requiring some type of reasoning (our original claim from Section2) . However, our results show a clear separation between performance on non-symbolic and symbolic tasks. If, in theory, any question could benefit from deliberation, why is CoT only benefiting the questions that can be solved through symbolic manipulation? Our results from Section5suggest that the primary benefit of CoT comes in the ability to execute symbolic steps and track their output. Not all tasks have this feature: for example, questions from CommonsenseQA can hardly be translated into formally grounded and executable solution plans. Datasets like StrategyQA may feature multiple steps of reasoning, but executing those steps is not complex, so the benefits of CoT are small. It is unclear whether explicitly instilling models with particular modes of deliberation, like process of elimination for multiple choice questions, might make them more effective for non-symbolic tasks, or whether there's a fundamental limitation imposed by their pre-training data. We leave this distinction for future work.Long Horizon PlanningOne set of tasks where symbolic reasoning helps substantially that our experiments haven't covered as thoroughly (with the exception of BiGGen-Bench) is long-horizon planning(Valmeekam et al.,2023; Xie et al.,2024; Gundawar et al.,2024; Valmeekam et al.,2024) . There are two reasons we don't treat it here. First, we are primarily interested in tasks that are conveyed in language, and we see less complex planning in language-only tasks. Second, there has already been a large debate on the effectiveness of CoT, both pro(Huang et al.,2022; Hu et al.,2023) and against(Valmeekam et al.,2023; Kambhampati,2024; Kambhampati et al.,2024b; Stechly et al.,2024a; Guan et al.,2024; Verma et al.,2024; Gundawar et al.,2024; Stechly et al.,2024b) using CoT and its derivatives like tree-of-thought(Yao et al.,2023; Kang et al.,2024) , that has resulted in complex systems to help solve planning problems better. While story generation and interpretation involve elements of planning with natural language(Peng et al.,2022; Karpinska et al.,2024) , such tasks are not conventionally formalized and benchmarked as planning and reasoning.Can we improve CoT further?Our work treats chain-of-thought variants that explicitly don't involve multiple inferences. But there is some evidence that using additional calls to LLMs can help(Du et al.,2023; Yao et al.,2023; Besta et al.,2023; Chen et al.,2024) . One challenge is that these methods use significantly increased computation; careful benchmarking sometimes reveals that naive techniques are as good as iterative ones(Olausson et al.,2024) . However, past theoretical results have shown that Transformers are augmented in a fundamental way by CoT(Liu et al.,2023b; Merrill & Sabharwal,2024) ; we believe this does indicate the potential for improved variants of CoT beyond prompt-based CoT. On the other hand, recent methods showing benefit from  \"internalizing \" CoT(Deng et al.,2024) may indicate that explicit generation of intermediate tokens is still not being used to its full potential.Dataset contaminationOne limitation of our study is the presence of possible data contamination: it is unknown which benchmarks may have been explicitly pre-trained on by language models. If a model had memorized answers to benchmark questions, we would expect direct answering to close some of the gap with CoT, as the model can just reproduce a known answer rather than deriving it from scratch. We argue there are four reasons that our general conclusions are still trustworthy. First, we use a range of language model scales, including small models that have less capacity to memorize. Second, datasets with poor direct answering performance like GSM8k-Hard are unlikely to have been substantially memorized. Third, the inclusion of recent datasets such as MuSR(Sprague et al.,2024) and BiGGen Bench(Kim et al.,2024) helps to defray this risk. Fourth, our survey of the literature includes papers that were submitted to conferences in 2023, representing a range of older LLMs trained at various times.",
                "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications."
            },
            {
                "name": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic",
                "arxiv_id": "2408.16326",
                "subtitles": [
                    "Discriminative Verifier for Mathematics",
                    "Critic Model"
                ],
                "reference": [
                    "Critique ability of large language models",
                    "Large language models cannot self-correct reasoning yet",
                    "Towards revealing the mystery behind chain of thought: A theoretical perspective",
                    "The generative AI paradox:  \"what it can create, it may not understand",
                    "Training verifiers to solve math word problems, 2021b",
                    "Self-rewarding language models",
                    "Let's verify step by step",
                    "Self-critiquing models for assisting human evaluators",
                    "Mr-gsm8k: A meta-reasoning benchmark for large language model evaluation",
                    "Alphamath almost zero: process supervision without process",
                    "Improve mathematical reasoning in language models by automated process supervision",
                    "Llm critics help catch llm bugs",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
                    "Math-shepherd: Verify and reinforce llms step-by-step without human annotations",
                    "Scalable agent alignment via reward modeling: a research direction",
                    "Learning from natural language feedback",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Works 2.1Discriminative Verifier for Mathematics To further improve the reasoning ability of Large language models, the reward model is one applicable method, which can either be used in reinforcement learning during training (Ouyang et al., 2022) or rejection sampling at test time (Cobbe et al., 2021b). While for outcome-supervised reward models (ORMs), we may automatically collect the training data via the signal of the gold answer, process-supervised reward models (PRMs) would be more advantageous for more precise feedback, better interpretability and stronger alignment (Lightman et al., 2024). To reduce the considerable human labeling cost and difficulty for dense annotation, a series of automatic approaches have been proposed (Wang et al., 2023a; Chen et al., 2024b; Luo et al., 2024), all under the heuristic that for an incorrect solution, the first error step is where the continuation of previous step would lead to a correct answer. This may bring noise into training data due to false positives and negatives. Moreover, annotation based on the implicit solution continuation alone does not leverage LLM's emerging ability of critic, which is in a more explicit and analytic way and brings better explainability (Saunders et al., 2022; Yuan et al., 2024; Luo et al., 2023; McAleese et al., 2024). Additionally, 0/1 discrimination alone may not fully leverage the computation power support by empirically successful Chain-of-Thought prompting (Feng et al., 2023). 2.2Critic Model Learning from natural language feedback could be beneficial (Chen et al., 2024a). With the development of LLM, whether it can discriminate and criticize its own output in a text-generation manner becomes an interesting topic (Luo et al., 2023; Zeng et al., 2023), with doubts at least on off-the-shelf LLMs that are not specially trained (Huang et al., 2024; West et al., 2024), and current application, i.e. response evaluation, heavily rely on the reference answer (Zheng et al., 2023). Therefore, given the limited critic ability of current LLMs, how to train a robust and applicable critic model is worth investigating. From the perspective of recursive reward modeling (Leike et al., 2018; Saunders et al., 2022) and scalable oversight (Burns et al., 2023), McAleese et al. (2024) recently trained  \"CriticGPT\" to assist human labelers, which aims to improve the ability of human rather than base model.",
                "abstract": "Self-critic has become an important mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts without further training, which tend to be over-simplified, leading to limitedthis http URL, there is a lack of in-depth investigation of the relationship between LLM's ability to criticism and its task-solvingthis http URLaddress these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability, via step-wise CoT reasoning format and distant-supervision data construction, without the need for human annotation. Experiments on GSM8K and MATH show that via filtering out invalid solutions or iterative refinement, our enhanced model boosts task-solving performance, which demonstrates the effectiveness of our method. Further, we find that training on critique and refinement alone improves the generation. We hope our work could shed light on future research on improving the reasoning and critic ability of LLMs."
            },
            {
                "name": "Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images",
                "arxiv_id": "2402.14899",
                "subtitles": [
                    "Adversarial Attacks",
                    "Chain-of-Thought Reasoning on Multimodal LLMs"
                ],
                "reference": [
                    "Automatic Chain of Thought Prompting in Large Language Models",
                    "Certifying llm safety against adversarial prompting",
                    "Effective and efficient vote attack on capsule networks",
                    "Inducing high energy-latency of large vision-language models with verbose images",
                    "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January",
                    "Large Language Models are Zero-Shot Reasoners, January",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Towards evaluating the robustness of neural networks",
                    "Towards deep learning models resistant to adversarial attacks",
                    "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
                    "Large-scale adversarial training for vision-and-language representation learning",
                    "On evaluating adversarial robustness of large vision-language models",
                    "Explaining and harnessing adversarial examples",
                    "Saliency methods for explaining adversarial attacks",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions",
                    "Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond",
                    "Multi-modal latent space learning for chain-of-thought reasoning in language models",
                    "Intriguing properties of neural networks",
                    "Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization",
                    "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
                    "Multimodal Chain-of-Thought Reasoning in Language Models, February"
                ],
                "related_work": "2Related Work 2.1Adversarial Attacks Deep learning models are known to be vulnerable to adversarial attacks(Szegedy et al.,2013;Goodfellow et al.,2014) . Extensive previous studies have a primary focus on image recognition(Szegedy et al.,2013;Goodfellow et al.,2014;Athalye et al.,2018;Carlini & Wagner,2017;Gu et al.,2021) and many well-known adversarial methods are proposed such as Projected Gradient Descent (PGD) (Madry et al.,2017) , Fast Gradient Sign Method(FGSM) (Goodfellow et al.,2014) . These studies aim to mislead the models to generate wrong predictions while only adding minimal and imperceptible perturbations to the images(Goodfellow et al.,2014) . Despite the effectiveness of these attack, it is still hard to interpret the model behavior during the attacks and understand why the attacks could succeed(Gu & Tresp,2019;Li et al.,2022) . Recent studies have also investigated the vulnerability of large language models(Zou et al.,2023;Kumar et al.,2023) and multimodal LLMs(Zhao et al.,2023;Gan et al.,2020;Gao et al.,2024;Han et al.,2023) under adversarial attacks. However, the adversarial robustness of multimodal LLMs with CoT reasoning ability is still under-explored. Since CoT reasoning reveals the model's decision process(Wei et al.,2023) , this reported intermediate process can serve as a good proxy for to understand the model behavior before and after the adversarial attacks, which additionally brings explainability. Different from previous studies, this work focuses on evaluating the adversarial robustness of MLLMs with CoT by designing effective attack methods and understanding why the model would behave under such adversarial attacks. 2.2Chain-of-Thought Reasoning on Multimodal LLMsCoT generates a series of intermediate logical reasoning steps and assists LLMs in thinking step by step before generating the final answer(Wei et al.,2023) . CoT has been widely applied to LLMs(Wei et al.,2023;Kojima et al.,2023;Zhang et al.,2022) and has significantly improved the performance in various tasks, such as arithmetic problems(Wei et al.,2023) and symbolic reasoning(Wei et al.,2023) . Some studies have noticed that CoT can bring extra robustness to the LLMs(Wu et al.,2023) and have designed a better CoT method for better robustness(Wang et al.,2022) Recently, on MLLMs, various studies have also shown that adopting CoT on MLLMs can bring superior performances as well, such asLu et al.(2022) , MM-CoT(Zhang et al.,2023) , andHe et al.(2023) . However, the robustness of CoT on MLLMs against adversarial attacks has not been investigated. It is still an open question whether CoTreasoning is beneficial, indifferent, or even harmful to the robustness of MLLMs under adversarial attacks. This study aims to first evaluate the adversarial robustness of CoT on MLLMs and then understand how the attacks affect the model behavior.",
                "abstract": "Multimodal LLMs (MLLMs) with a great ability of text and image understanding have received great attention. To achieve better reasoning with MLLMs, Chain-of-Thought (CoT) reasoning has been widely explored, which further promotes MLLMs' explainability by giving intermediate reasoning steps. Despite the strong power demonstrated by MLLMs in multimodal reasoning, recent studies show that MLLMs still suffer from adversarial images. This raises the following open questions: Does CoT also enhance the adversarial robustness of MLLMs? What do the intermediate reasoning steps of CoT entail under adversarial attacks? To answer these questions, we first generalize existing attacks to CoT-based inferences by attacking the two main components, i.e., rationale and answer. We find that CoT indeed improves MLLMs' adversarial robustness against the existing attack methods by leveraging the multi-step reasoning process, but not substantially. Based on our findings, we further propose a novel attack method, termed as stop-reasoning attack, that attacks the model while bypassing the CoT reasoning process. Experiments on three MLLMs and two visual reasoning datasets verify the effectiveness of our proposed method. We show that stop-reasoning attack can result in misled predictions and outperform baseline attacks by a significant margin."
            }
        ],
        "survey": {
            "name": "Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs",
            "arxiv_id": "2404.15676",
            "subtitles": [
                {
                    "name": "Introduction",
                    "key_history": [
                        {
                            "reference_title": "Chain of thought prompting elicits reasoning in large language models",
                            "key_word": "Chain-of-Thought (CoT) "
                        },
                        {
                            "reference_title": "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                            "key_word": "Chain-of-X Methods"
                        },
                        {
                            "reference_title": "A survey of chain of thought reasoning: Advances, frontiers and future",
                            "key_word": "Reasoning Process"
                        }
                    ],
                    "references_in_this_section": [
                        "Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future",
                        "You only look at screens: Multimodal chain-of-action agents",
                        "Enhancing temporal knowledge graph forecasting with large language models via chain-of-history reasoning",
                        "Android in the zoo: Chain-of-action-thought for gui agents",
                        "Chain of history: Learning and forecasting with llms for temporal knowledge graph completion",
                        "Topologies of reasoning: Demystifying chains, trees, and graphs of thoughts",
                        "Coie: Chain-of-instruct editing for multi-attribute face manipulation",
                        "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                        "Chain-of-look prompting for verb-centric surgical triplet recognition in endoscopic videos",
                        "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                        "Towards better chain-of-thought prompting strategies: A survey",
                        "Chain-of-verification reduces hallucination in large language models",
                        "Towards reasoning in large language models: A survey",
                        "Chain of thought prompting elicits reasoning in large language models",
                        "Chain-of-instructions: Compositional instruction tuning on large language models",
                        "Speechgpt-gen: Scaling chain-of-information speech generation"
                    ]
                },
                {
                    "name": "What is Chain-of-X?",
                    "key_history": [
                        {
                            "reference_title": "Chain of thought prompting elicits reasoning in large language models",
                            "key_word": "Chain-of-Thought"
                        },
                        {
                            "reference_title": "Large language models are zero-shot reasoners",
                            "key_word": "Let's think step by step"
                        },
                        {
                            "reference_title": "Chain-of-verification reduces hallucination in large language models",
                            "key_word": "Chain-of-Verification"
                        },
                        {
                            "reference_title": "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                            "key_word": "Chain-of-X"
                        }
                    ],
                    "references_in_this_section": [
                        "Chain of reference prompting helps llm to think like a lawyer",
                        "Enhancing temporal knowledge graph forecasting with large language models via chain-of-history reasoning",
                        "Boosting language models reasoning with chain-of-knowledge prompting",
                        "Android in the zoo: Chain-of-action-thought for gui agents",
                        "MultiTool-CoT: GPT-3 can use multiple external tools with chain of thought prompting",
                        "Choire: Characterizing and predicting human opinions with chain of opinion reasoning",
                        "From sparse to dense: GPT-4 summarization with chain of density prompting",
                        "Chain-of-look prompting for verb-centric surgical triplet recognition in endoscopic videos",
                        "Unified human-scene interaction via prompted chain-of-contacts",
                        "Faithful chain-of-thought reasoning",
                        "React: Synergizing reasoning and acting in language models",
                        "You only look at screens: Multimodal chain-of-action agents",
                        "Chain of history: Learning and forecasting with llms for temporal knowledge graph completion",
                        "Chain-of-spot: Interactive reasoning improves large vision-language models",
                        "Large language models are zero-shot reasoners",
                        "Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control",
                        "ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models",
                        "Chain-of-verification reduces hallucination in large language models",
                        "Chain of thought prompting elicits reasoning in large language models",
                        "Search-in-the-chain: Interactively enhancing large language models with search for knowledge-intensive tasks",
                        "Chain-of-instructions: Compositional instruction tuning on large language models",
                        "Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce",
                        "Speechgpt-gen: Scaling chain-of-information speech generation",
                        "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
                        "Multimodal chain-of-thought reasoning in language models",
                        "Efficient tool use with chain-of-abstraction reasoning",
                        "Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue questions with LLMs",
                        "Chain-of-event prompting for multi-document summarization by large language models",
                        "Recursive chain-of-feedback prevents performance degradation from redundant prompting",
                        "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                        "Chateval: Towards better LLM-based evaluators through multi-agent debate",
                        "SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities",
                        "Red-teaming large language models using chain of utterances for safety-alignment",
                        "Chain of logic: Rule-based reasoning with large language models",
                        "Chain-of-discussion: A multi-model framework for complex evidence-based question answering",
                        "Least-to-most prompting enables complex reasoning in large language models",
                        "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                        "Chain-of-experts: When LLMs meet complex operations research problems",
                        "Towards better chain-of-thought prompting strategies: A survey",
                        "Self-refine: Iterative refinement with self-feedback",
                        "Chain of hindsight aligns language models with feedback",
                        "Chatdb: Augmenting llms with databases as their symbolic memory",
                        "LogiCoT: Logical chain-of-thought instruction tuning",
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Chain of code: Reasoning with a language model-augmented code emulator",
                        "Code simulation challenges for large language models",
                        "Chain-of-dictionary prompting elicits translation in large language models",
                        "Intervenor: Prompt the coding ability of large language models with the interactive chain of repairing",
                        "Badchain: Backdoor chain-of-thought prompting for large language models",
                        "Coie: Chain-of-instruct editing for multi-attribute face manipulation",
                        "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
                        "L3go: Language agents with chain-of-3d-thoughts for generating unconventional objects"
                    ]
                },
                {
                    "name": "Chain-of-X Nodes",
                    "key_history": [
                        {
                            "reference_title": " Chain of thought prompting elicits reasoning in large language models",
                            "key_word": "Problem Decomposition"
                        },
                        {
                            "reference_title": "Chain-of-symbol prompting elicits planning in large langauge models",
                            "key_word": "Knowledge Composition"
                        },
                        {
                            "reference_title": "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                            "key_word": "Chain-of-Augmentation"
                        },
                        {
                            "reference_title": "Choire: Characterizing and predicting human opinions with chain of opinion reasoning",
                            "key_word": "Histories"
                        },
                        {
                            "reference_title": "Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks",
                            "key_word": "Chain-of-Retrieval"
                        },
                        {
                            "reference_title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                            "key_word": "External Feedback"
                        },
                        {
                            "reference_title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                            "key_word": "Self-Refine"
                        },
                        {
                            "reference_title": "Chain-of-experts: When LLMs meet complex operations research problems",
                            "key_word": "Chain-of-Models"
                        }
                    ],
                    "references_in_this_section": [
                        "Chain-of-interaction: Enhancing large language models for psychiatric behavior understanding by dyadic contexts",
                        "Enhancing temporal knowledge graph forecasting with large language models via chain-of-history reasoning",
                        "Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules",
                        "Chain of reference prompting helps llm to think like a lawyer",
                        "Boosting language models reasoning with chain-of-knowledge prompting",
                        "MultiTool-CoT: GPT-3 can use multiple external tools with chain of thought prompting",
                        "Measuring and narrowing the compositionality gap in language models",
                        "Choire: Characterizing and predicting human opinions with chain of opinion reasoning",
                        "From sparse to dense: GPT-4 summarization with chain of density prompting",
                        "Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation",
                        "React: Synergizing reasoning and acting in language models",
                        "Which llm to play? convergence-aware online model selection with time-increasing bandits",
                        "Chain-of-layer: Iteratively prompting large language models for taxonomy induction from limited examples",
                        "You only look at screens: Multimodal chain-of-action agents",
                        "Chain-of-action: Faithful and multimodal question answering through large language models",
                        "Chain of history: Learning and forecasting with llms for temporal knowledge graph completion",
                        "Chain-of-spot: Interactive reasoning improves large vision-language models",
                        "Coq: An empirical framework for multi-hop question answering empowered by large language models",
                        "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                        "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                        "ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models",
                        "Chain-of-verification reduces hallucination in large language models",
                        "Chain of thought prompting elicits reasoning in large language models",
                        "Search-in-the-chain: Interactively enhancing large language models with search for knowledge-intensive tasks",
                        "Chain-of-instructions: Compositional instruction tuning on large language models",
                        "Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce",
                        "Graph chain-of-thought: Augmenting large language models by reasoning on graphs",
                        "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
                        "Efficient tool use with chain-of-abstraction reasoning",
                        "Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph",
                        "Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue questions with LLMs",
                        "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                        "Chateval: Towards better LLM-based evaluators through multi-agent debate",
                        "Chain of logic: Rule-based reasoning with large language models",
                        "Chain-of-discussion: A multi-model framework for complex evidence-based question answering",
                        "Least-to-most prompting enables complex reasoning in large language models",
                        "Chain-of-experts: When LLMs meet complex operations research problems",
                        "Self-refine: Iterative refinement with self-feedback",
                        "Chain of hindsight aligns language models with feedback",
                        "Chain-of-lora: Enhancing the instruction fine-tuning performance of low-rank adaptation on diverse instruction set",
                        "Advancing large multi-modal models with explicit chain-of-reasoning and visual question generation",
                        "Chain of empathy: Enhancing empathetic response of large language models based on psychotherapy models",
                        "Chain of code: Reasoning with a language model-augmented code emulator",
                        "Chain-of-dictionary prompting elicits translation in large language models",
                        "Coie: Chain-of-instruct editing for multi-attribute face manipulation",
                        "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
                        "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                        "L3go: Language agents with chain-of-3d-thoughts for generating unconventional objects",
                        "Large language models play starcraft ii: Benchmarks and a chain of summarization approach",
                        "Compositional chain of thought prompting for large multimodal models",
                        "LoRA: Low-rank adaptation of large language models",
                        "Verify-and-edit: A knowledge-enhanced chain-of-thought framework",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "Chain-of-X Tasks",
                    "key_history": [
                        {
                            "reference_title": "Vision-language models for vision tasks: A survey",
                            "key_word": "Text-Image"
                        },
                        {
                            "reference_title": "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                            "key_word": "Text-Table"
                        },
                        {
                            "reference_title": "Chain of code: Reasoning with a language model-augmented code emulator",
                            "key_word": "Text-Code"
                        },
                        {
                            "reference_title": "Speechgpt-gen: Scaling chain-of-information speech generation.",
                            "key_word": "Text-Speech"
                        },
                        {
                            "reference_title": "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                            "key_word": "Hallucination Reduction"
                        },
                        {
                            "reference_title": "Red-teaming large language models using chain of utterances for safety-alignment",
                            "key_word": "Alignment"
                        },
                        {
                            "reference_title": "SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities",
                            "key_word": "Instruction Following"
                        },
                        {
                            "reference_title": "You only look at screens: Multimodal chain-of-action agents",
                            "key_word": "LLMs as Agents"
                        },
                        {
                            "reference_title": "Chain of images for intuitively reasoning",
                            "key_word": "Evaluation Tools"
                        },
                        {
                            "reference_title": "Chain of thought prompting elicits reasoning in large language models",
                            "key_word": "Multi-Step Reasoning"
                        }
                    ],
                    "references_in_this_section": [
                        "Chain-of-interaction: Enhancing large language models for psychiatric behavior understanding by dyadic contexts",
                        "Enhancing temporal knowledge graph forecasting with large language models via chain-of-history reasoning",
                        "The rise and potential of large language model based agents: A survey",
                        "Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules",
                        "Android in the zoo: Chain-of-action-thought for gui agents",
                        "Chain of reference prompting helps llm to think like a lawyer",
                        "Choire: Characterizing and predicting human opinions with chain of opinion reasoning",
                        "Aligning as debiasing: Causality-aware alignment via reinforcement learning with interventional feedback",
                        "From sparse to dense: GPT-4 summarization with chain of density prompting",
                        "Chain-of-look prompting for verb-centric surgical triplet recognition in endoscopic videos",
                        "Unified human-scene interaction via prompted chain-of-contacts",
                        "Can knowledge graphs reduce hallucinations in llms?: A survey",
                        "Chain-of-layer: Iteratively prompting large language models for taxonomy induction from limited examples",
                        "Hallucination diversity-aware active learning for text summarization",
                        "You only look at screens: Multimodal chain-of-action agents",
                        "DDCot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models",
                        "Chain-of-action: Faithful and multimodal question answering through large language models",
                        "Chain of history: Learning and forecasting with llms for temporal knowledge graph completion",
                        "Cogcom: Train large vision-language models diving into details through chain of manipulations",
                        "Chain-of-spot: Interactive reasoning improves large vision-language models",
                        "Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control",
                        "Chain-of-verification reduces hallucination in large language models",
                        "Chain of thought prompting elicits reasoning in large language models",
                        "Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce",
                        "Speechgpt-gen: Scaling chain-of-information speech generation",
                        "Instruction tuning for large language models: A survey",
                        "Large language models(llms) on tabular data: Prediction, generation, and understanding - a survey",
                        "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
                        "Multimodal chain-of-thought reasoning in language models",
                        "Efficient tool use with chain-of-abstraction reasoning",
                        "Chain-of-event prompting for multi-document summarization by large language models",
                        "Recursive chain-of-feedback prevents performance degradation from redundant prompting",
                        "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                        "A survey on evaluation of large language models",
                        "SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities",
                        "Red-teaming large language models using chain of utterances for safety-alignment",
                        "Chain of logic: Rule-based reasoning with large language models",
                        "Chain-of-discussion: A multi-model framework for complex evidence-based question answering",
                        "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                        "Self-refine: Iterative refinement with self-feedback",
                        "Chain of hindsight aligns language models with feedback",
                        "Chain-of-lora: Enhancing the instruction fine-tuning performance of low-rank adaptation on diverse instruction set",
                        "Chatdb: Augmenting llms with databases as their symbolic memory",
                        "LogiCoT: Logical chain-of-thought instruction tuning",
                        "Siren's song in the ai ocean: a survey on hallucination in large language models",
                        "Chain-of-dictionary prompting elicits translation in large language models",
                        "Aligning large language models with human: A survey",
                        "Intervenor: Prompt the coding ability of large language models with the interactive chain of repairing",
                        "Badchain: Backdoor chain-of-thought prompting for large language models",
                        "Large language models meet NL2Code: A survey",
                        "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
                        "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                        "L3go: Language agents with chain-of-3d-thoughts for generating unconventional objects",
                        "Large language models play starcraft ii: Benchmarks and a chain of summarization approach",
                        "Training language models to follow instructions with human feedback"
                    ]
                },
                {
                    "name": "Future Directions",
                    "key_history": [
                        {
                            "reference_title": "Self-consistency improves chain of thought reasoning in language models",
                            "key_word": "Causal Analysis on Intermediates"
                        },
                        {
                            "reference_title": "Symbolic chain-of-thought distillation: Small models can also  \"think \" step-by-step",
                            "key_word": "Knowledge Distillation"
                        },
                        {
                            "reference_title": "Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules",
                            "key_word": "End-to-End Fine-Tuning"
                        }
                    ],
                    "references_in_this_section": [
                        "Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules",
                        "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
                        "Self-consistency improves chain of thought reasoning in language models",
                        "Symbolic chain-of-thought distillation: Small models can also  \"think\" step-by-step",
                        "Towards understanding chain-of-thought prompting: An empirical study of what matters",
                        "Chain-of-verification reduces hallucination in large language models"
                    ]
                }
            ],
            "all_references": [
                "Coascore: Chain-of-aspects prompting for nlg evaluation",
                "Chain-of-verification reduces hallucination in large language models",
                "Chain of lora: Efficient fine-tuning of language models via residual learning",
                "Chain of thought prompting elicits reasoning in large language models",
                "Efficient tool use with chain-of-abstraction reasoning",
                "Chain-of-interaction: Enhancing large language models for psychiatric behavior understanding by dyadic contexts",
                "Speechgpt-gen: Scaling chain-of-information speech generation",
                "Retrieval-augmented generation for large language models: A survey",
                "Coq: An empirical framework for multi-hop question answering empowered by large language models",
                "Hallucination diversity-aware active learning for text summarization",
                "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                "Chain of images for intuitively reasoning",
                "Boosting language models reasoning with chain-of-knowledge prompting",
                "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
                "Towards better chain-of-thought prompting strategies: A survey",
                "Siren's song in the ai ocean: a survey on hallucination in large language models",
                "The rise and potential of large language model based agents: A survey",
                "Chain-of-lora: Enhancing the instruction fine-tuning performance of low-rank adaptation on diverse instruction set",
                "Self-consistency improves chain of thought reasoning in language models",
                "Scaling language models: Methods, analysis & insights from training gopher",
                "Towards reasoning in large language models: A survey",
                "Coie: Chain-of-instruct editing for multi-attribute face manipulation",
                "Topologies of reasoning: Demystifying chains, trees, and graphs of thoughts",
                "Chatdb: Augmenting llms with databases as their symbolic memory",
                "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
                "Instruction tuning for large language models: A survey",
                "Intervenor: Prompt the coding ability of large language models with the interactive chain of repairing",
                "Which llm to play? convergence-aware online model selection with time-increasing bandits",
                "Cogcom: Train large vision-language models diving into details through chain of manipulations",
                "Graph of thoughts: Solving elaborate problems with large language models",
                "Chain of logic: Rule-based reasoning with large language models",
                "Training language models to follow instructions with human feedback",
                "Chain of reference prompting helps llm to think like a lawyer",
                "Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules",
                "A survey of chain of thought reasoning: Advances, frontiers and future",
                "Chain-of-action: Faithful and multimodal question answering through large language models",
                "SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities",
                "Multimodal automated fact-checking: A survey",
                "Chain of explanation: New prompting method to generate quality natural language explanation for implicit hate speech",
                "Advancing large multi-modal models with explicit chain-of-reasoning and visual question generation",
                "Large language models meet NL2Code: A survey",
                "Android in the zoo: Chain-of-action-thought for gui agents",
                "Symbolic chain-of-thought distillation: Small models can also  \"think \" step-by-step",
                "Chain-of-experts: When LLMs meet complex operations research problems",
                "Retrieving multimodal information for augmented generation: A survey",
                "Code simulation challenges for large language models",
                "Generalizing visual question answering from synthetic to human-written questions via a chain of qa with a large language model",
                "Choire: Characterizing and predicting human opinions with chain of opinion reasoning",
                "Chain-of-look prompting for verb-centric surgical triplet recognition in endoscopic videos",
                "Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control",
                "From sparse to dense: GPT-4 summarization with chain of density prompting",
                "A survey on evaluation of large language models",
                "Towards understanding chain-of-thought prompting: An empirical study of what matters",
                "Chain-of-table: Evolving tables in the reasoning chain for table understanding",
                "Chain-of-instructions: Compositional instruction tuning on large language models",
                "Chain of code: Reasoning with a language model-augmented code emulator",
                "Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce",
                "Chain-of-discussion: A multi-model framework for complex evidence-based question answering",
                "Chain-of-spot: Interactive reasoning improves large vision-language models",
                "Attacks, defenses and evaluations for llm conversation safety: A survey",
                "Chain-of-note: Enhancing robustness in retrieval-augmented language models",
                "Faithful chain-of-thought reasoning",
                "Large language models play starcraft ii: Benchmarks and a chain of summarization approach",
                "You only look at screens: Multimodal chain-of-action agents",
                "Can knowledge graphs reduce hallucinations in llms?: A survey",
                "LoRA: Low-rank adaptation of large language models",
                "Chain-of-layer: Iteratively prompting large language models for taxonomy induction from limited examples",
                "Chain of history: Learning and forecasting with llms for temporal knowledge graph completion",
                "L3go: Language agents with chain-of-3d-thoughts for generating unconventional objects",
                "Tree of thoughts: Deliberate problem solving with large language models",
                "Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks",
                "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
                "Chain-of-event prompting for multi-document summarization by large language models",
                "Chain of hindsight aligns language models with feedback",
                "Chain-of-dictionary prompting elicits translation in large language models",
                "Chain of empathy: Enhancing empathetic response of large language models based on psychotherapy models",
                "Unified human-scene interaction via prompted chain-of-contacts",
                "Enhancing temporal knowledge graph forecasting with large language models via chain-of-history reasoning",
                "Least-to-most prompting enables complex reasoning in large language models",
                "Large language models are zero-shot reasoners",
                "Red-teaming large language models using chain of utterances for safety-alignment",
                "Recursive chain-of-feedback prevents performance degradation from redundant prompting",
                "Vision-language models for vision tasks: A survey",
                "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
                "Chain-of-symbol prompting elicits planning in large langauge models",
                "Aligning large language models with human: A survey"
            ]
        },
        "topic_history": [
            {
                "name": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
                "arxiv_id": "2401.07037",
                "reference": [
                    "Emergent abilities of large language models",
                    "Multilingual clinical NER: translation or cross-lingual transfer",
                    "Multilingual neural machine translation with language clustering",
                    "Cross-lingual language model pretraining",
                    "GLM: general language model pretraining with autoregressive blank infilling",
                    "Alternating language modeling for cross-lingual pre-training",
                    "Solving math word problems via cooperative reasoning induced language models",
                    "Crosslingual generalization through multitask finetuning",
                    "Bloom: A 176b-parameter open-access multilingual language model",
                    "Large language models are reasoning teachers",
                    "Large language models are zero-shot reasoners",
                    "Conner: Consistency training for cross-lingual named entity recognition",
                    "XLM-T: scaling up multilingual machine translation with pretrained cross-lingual transformer encoders",
                    "UM4: unified multilingual multiple teacher-student model for zero-resource neural machine translation",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "CROP: zero-shot cross-lingual named entity recognition with multilingual labeled sequence translation",
                    "High-resource language-specific training for multilingual neural machine translation",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Unitrans : Unifying model transfer and data transfer for cross-lingual named entity recognition with unlabeled data",
                    "Crosssum: Beyond english-centric cross-lingual summarization for 1, 500+ language pairs",
                    "OWL: A large language model for IT operations",
                    "Understanding translationese in cross-lingual summarization",
                    "Unsupervised cross-lingual representation learning at scale",
                    "GanLM: Encoder-decoder pre-training with an auxiliary discriminator"
                ]
            },
            {
                "name": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
                "arxiv_id": "2404.03204",
                "reference": [
                    "Multispeech: Multi-speaker text to speech with transformer",
                    "Uniaudio: An audio foundation model toward universal audio generation",
                    "Emergent abilities of large language models",
                    "Attention is all you need",
                    "Viola: Unified codec language models for speech recognition, synthesis, and translation",
                    "Language models are few-shot learners",
                    "Audiopalm: A large language model that can speak and listen",
                    "Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural tts",
                    "Sequence transduction with recurrent neural networks",
                    "Language models are unsupervised multitask learners",
                    "Speak, read and prompt: High-fidelity text-to-speech with minimal supervision",
                    "Ella-v: Stable neural codec language modeling with alignment-guided sequence reordering",
                    "Neural codec language models are zero-shot text to speech synthesizers",
                    "Vall-t: Decoder-only generative transducer for robust and decoding-controllable text-to-speech",
                    "Forward attention in sequence-to-sequence acoustic modeling for speech synthesis",
                    "Non-attentive tacotron: Robust and controllable neural tts synthesis including unsupervised duration modeling"
                ]
            },
            {
                "name": "Chain-of-Thought Reasoning Without Prompting",
                "arxiv_id": "2402.10200",
                "reference": [
                    "Contrastive decoding improves reasoning in large language models",
                    "Is MAP decoding all you need? the inadequacy of the mode in neural machine translation",
                    "Solving math word problems with process- and outcome-based feedback",
                    "Teaching small language models to reason",
                    "Accelerating large language model decoding with speculative sampling",
                    "Large language models as analogical reasoners",
                    "Distillspec: Improving speculative decoding via knowledge distillation",
                    "Why think step by step? reasoning emerges from the locality of experience",
                    "Pathfinder: Guided search over multi-step reasoning paths",
                    "Trusting your evidence: Hallucinate less with context-aware decoding",
                    "Embers of autoregression: Understanding large language models through the problem they are trained to solve",
                    "The unreliability of explanations in few-shot prompting for textual reasoning",
                    "Dissecting chain-of-thought: Compositionality through in-context filtering and learning",
                    "Let's verify step by step",
                    "Large language models are zero-shot reasoners",
                    "Scalable extraction of training data from (production) language models",
                    "Rethinking the role of demonstrations: What makes in-context learning work",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "Learning to write with cooperative discriminators",
                    "Tuning language models by proxy",
                    "Challenging big-bench tasks and whether chain-of-thought can solve them",
                    "The curious case of neural text degeneration",
                    "Fast inference from transformers via speculative decoding",
                    "Contrastive decoding: Open-ended text generation as optimization",
                    "Large language models are human-level prompt engineers",
                    "Hierarchical neural story generation",
                    "Large language models can self-improve",
                    "Least-to-most prompting enables complex reasoning in large language models",
                    "Camels in a changing climate: Enhancing lm adaptation with tulu",
                    "Controlling linguistic style aspects in neural language generation",
                    "A learning algorithm for boltzmann machines",
                    "Large language models as optimizers",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Towards revealing the mystery behind chain of thought: A theoretical perspective",
                    "Scaling instruction-finetuned language models",
                    "Discovering latent knowledge in language models without supervision",
                    "Diverse beam search for improved description of complex scenes",
                    "Self-evaluation guided beam search for reasoning",
                    "Language models are unsupervised multitask learners",
                    "Impact of pretraining term frequencies on few-shot numerical reasoning",
                    "Typical decoding for natural language generation",
                    "Show your work: Scratchpads for intermediate computation with language models",
                    "Rationale-augmented ensembles in language models",
                    "Do prompt-based models really understand the meaning of their prompts",
                    "How far can camels go? exploring the state of instruction tuning on open resources"
                ]
            },
            {
                "name": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "arxiv_id": "2402.18205",
                "reference": [
                    "Palm 2 technical report",
                    "Clustering event logs using iterative partitioning",
                    "Scaling laws for generative mixed-modal language models",
                    "Spell: Streaming parsing of system event logs",
                    "Drain: An online log parsing approach with fixed depth tree",
                    "Logmine: Fast pattern recognition for log analytics",
                    "A data clustering algorithm for mining patterns from event logs",
                    "On automatic parsing of log records",
                    "Prefix-graph: A versatile log parsing approach merging prefix tree with probabilistic graph",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Logcluster-a data clustering and pattern mining algorithm for event logs",
                    "Cross-lingual natural language generation via pre-training",
                    "Lpv: A log parser based on vectorization for offline and online log parsing",
                    "Gpt-4 technical report",
                    "Self-supervised log parsing",
                    "Length matters: Clustering system log messages using length of words",
                    "Uniparser: A unified log parser for heterogeneous log data",
                    "Execution anomaly detection in distributed systems through unstructured log analysis",
                    "Attention is all you need",
                    "Scaling instruction-finetuned language models",
                    "Incremental mining of system log format",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs",
                "arxiv_id": "2401.02582",
                "reference": [
                    "Mmicl: Empowering vision-language model with multi-modal in-context learning",
                    "Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models",
                    "Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners",
                    "Visual instruction tuning",
                    "Gemini: A family of highly capable multimodal models",
                    "Large language models are zero-shot reasoners",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Compositional chain-of-thought prompting for large multimodal models",
                    "GPT-4 technical report",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Llama: Open and efficient foundation language models"
                ]
            },
            {
                "name": "Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities",
                "arxiv_id": "2402.17230",
                "reference": [
                    "Examining zero-shot vulnerability repair with large language models",
                    "{{\\{{PolyCruise}}\\}}: A {{\\{{Cross-Language}}\\}} dynamic information flow analysis",
                    "{{\\{{AddressSanitizer}}\\}}: A fast address sanity checker",
                    "PolyFuzz: Holistic greybox fuzzing of multi-language systems",
                    "Valgrind: a framework for heavyweight dynamic binary instrumentation",
                    "Practical memory checking with dr. memory",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "Large language models are zero-shot reasoners",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "A comparative study on software vulnerability static analysis techniques and tools",
                    "VulRepair: a t5-based automated software vulnerability repair",
                    "Detecting kernel memory leaks in specialized modules with ownership reasoning",
                    "Example-based vulnerability detection and repair in java code",
                    "Neural transfer learning for repairing security vulnerabilities in c code",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Vulcnn: An image-inspired scalable vulnerability detection system",
                    "Vulnerability detection with fine-grained interpretations",
                    "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                    "The art, science, and engineering of fuzzing: A survey",
                    "Fuzzing: a survey for roadmap",
                    "FlowDist:multi-staged refinement-based dynamic information flow analysis for distributed software systems",
                    "LineVD: statement-level vulnerability detection using graph neural networks",
                    "Undangle: early detection of dangling pointers in use-after-free and double-free vulnerabilities",
                    "{{\\{{Type-Assisted}}\\}} dynamic buffer overflow detection",
                    "VulChecker: Graph-based vulnerability localization in source code",
                    "Typestate-guided fuzzer for discovering use-after-free vulnerabilities",
                    "You only prompt once: On the capabilities of prompt learning on large language models to tackle toxic content",
                    "Few-sample named entity recognition for security vulnerability reports by fine-tuning pre-trained language models",
                    "PCA: memory leak detection using partial call-path analysis",
                    "CBMC-c bounded model checker",
                    "Dynamic detection of inter-application communication vulnerabilities in android",
                    "Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks",
                    "{{\\{{FUZE}}\\}}: Towards facilitating exploit generation for kernel {{\\{{Use-After-Free}}\\}} vulnerabilities",
                    "Vurle: Automatic vulnerability detection and repair by learning from examples",
                    "A practical dynamic buffer overflow detector",
                    "Deep learning based vulnerability detection: Are we there yet",
                    "Tt-xss: A novel taint tracking based dynamic detection framework for dom cross-site scripting",
                    "Attention is all you need",
                    "A new algorithm for data compression",
                    "Beyond tests: Program vulnerability repair via crash constraint extraction",
                    "CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation",
                    "Binary-level directed fuzzing for {{\\{{Use-After-Free}}\\}} vulnerabilities",
                    "A Coverage-Guided, Native Python Fuzzer",
                    "Dowsing for Overflows: A guided fuzzer to find buffer boundary violations",
                    "LineVul: a transformer-based line-level vulnerability prediction",
                    "Evaluating and comparing memory error vulnerability detectors",
                    "Open science in software engineering: A study on deep learning-based vulnerability detection",
                    "Sysevr: A framework for using deep learning to detect software vulnerabilities",
                    "The power of scale for parameter-efficient prompt tuning",
                    "Technical \"whitepaper\" for afl-fuzz"
                ]
            },
            {
                "name": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
                "arxiv_id": "2401.12242",
                "reference": [
                    "Trojtext: Test-time invisible textual trojan insertion",
                    "NOTABLE: Transferable backdoor attacks against prompt-based NLP models",
                    "Making language models better reasoners with step-aware verifier",
                    "Detecting backdoor attacks against point cloud classifiers",
                    "Exploring the universal vulnerability of prompt-based learning paradigm",
                    "Badprompt: Backdoor attacks on continuous prompts",
                    "Backdoor pre-trained models can transfer to all",
                    "Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses",
                    "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models",
                    "BadNets: Evaluating backdooring attacks on deep neural networks",
                    "Towards stealthy backdoor attacks against speech recognition via elements of sound",
                    "Backdoor attacks for in-context learning with language models",
                    "Prompt as triggers for backdoor attack: Examining the vulnerability in language models",
                    "Chain of thought prompting elicits reasoning in large language models",
                    "Clean-label backdoor attacks on video recognition models",
                    "Trojaning language models for fun and profit",
                    "Backdoor learning: A survey",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Backdoor attacks on pre-trained models by layerwise weight poisoning",
                    "PointBA: Towards backdoor attacks in 3d point cloud",
                    "Subnet replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting",
                    "Mind the style of text! adversarial and backdoor attacks based on text style transfer",
                    "Poisoning language models during instruction tuning",
                    "Trojaning attack on neural networks",
                    "Active prompting with chain-of-thought for large language models, 2023b",
                    "Adversarial learning in statistical classification: A comprehensive review of defenses against attacks",
                    "Backdoor attack against speaker verification",
                    "Targeted backdoor attacks on deep learning systems using data poisoning",
                    "Least-to-most prompting enables complex reasoning in large language models",
                    "Black-box prompt learning for pre-trained language models",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
                    "BadNL: Backdoor Attacks against NLP Models with Semantic-Preserving Improvements",
                    "Handcrafted backdoors in deep neural networks",
                    "Language models are few-shot learners",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models",
                    "Compositional semantic parsing with large language models"
                ]
            },
            {
                "name": "LLMs with Chain-of-Thought Are Non-Causal Reasoners",
                "arxiv_id": "2402.16048",
                "reference": [
                    "Causality",
                    "Causal inference in statistics, social, and biomedical sciences",
                    "Causal inference in natural language processing: Estimation, prediction, interpretation and beyond",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Beyond chain-of-thought, effective graph-of-thought reasoning in large language models",
                    "Elements of causal inference: foundations and learning algorithms",
                    "Causal reasoning through intervention",
                    "A survey of chain of thought reasoning: Advances, frontiers and future",
                    "Causality in thought",
                    "Tree of thoughts: Deliberate problem solving with large language models",
                    "Causal analysis",
                    "Mechanical reasoning by mental simulation",
                    "Chain-of-thought prompting elicits reasoning in large language models"
                ]
            },
            {
                "name": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
                "arxiv_id": "2409.12183",
                "reference": [
                    "Tree of Thoughts: Deliberate problem solving with large language models",
                    "On the planning abilities of large language models - a critical investigation",
                    "Transformers learn shortcuts to automata",
                    "Graph of thoughts: Solving elaborate problems with large language models",
                    "One thousand and one pairs: A  \"novel \" challenge for long-context language models",
                    "Improving factuality and reasoning in language models through multiagent debate",
                    "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
                    "Reconcile: Round-table conference improves reasoning via consensus among diverse LLMs",
                    "Position: LLMs can't plan, but can help planning in LLM-modulo frameworks",
                    " \"Task Success \" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
                    "On the empirical complexity of reasoning and planning in llms",
                    "Inferring the reader: Guiding automated story generation with commonsense reasoning",
                    "Chain of thoughtlessness? an analysis of cot in planning",
                    "Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning",
                    "On the self-verification limitations of large language models on reasoning and planning tasks",
                    "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models",
                    "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                    "Can large language models reason and plan",
                    "Is self-repair a silver bullet for code generation",
                    "The expressive power of transformers with chain of thought",
                    "MuSR: Testing the limits of chain-of-thought with multistep soft reasoning",
                    "PlanBench: An extensible benchmark for evaluating large language models on planning and reasoning about change",
                    "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step",
                    "Chain-of-symbol prompting elicits planning in large langauge models",
                    "Show your work: Scratchpads for intermediate computation with language models",
                    "Theory of mind abilities of large language models in human-robot interaction: An illusion"
                ]
            },
            {
                "name": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic",
                "arxiv_id": "2408.16326",
                "reference": [
                    "Critique ability of large language models",
                    "Large language models cannot self-correct reasoning yet",
                    "Towards revealing the mystery behind chain of thought: A theoretical perspective",
                    "The generative AI paradox:  \"what it can create, it may not understand",
                    "Training verifiers to solve math word problems, 2021b",
                    "Self-rewarding language models",
                    "Let's verify step by step",
                    "Self-critiquing models for assisting human evaluators",
                    "Mr-gsm8k: A meta-reasoning benchmark for large language model evaluation",
                    "Alphamath almost zero: process supervision without process",
                    "Improve mathematical reasoning in language models by automated process supervision",
                    "Llm critics help catch llm bugs",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision",
                    "Math-shepherd: Verify and reinforce llms step-by-step without human annotations",
                    "Scalable agent alignment via reward modeling: a research direction",
                    "Learning from natural language feedback",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images",
                "arxiv_id": "2402.14899",
                "reference": [
                    "Automatic Chain of Thought Prompting in Large Language Models",
                    "Certifying llm safety against adversarial prompting",
                    "Effective and efficient vote attack on capsule networks",
                    "Inducing high energy-latency of large vision-language models with verbose images",
                    "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January",
                    "Large Language Models are Zero-Shot Reasoners, January",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Towards evaluating the robustness of neural networks",
                    "Towards deep learning models resistant to adversarial attacks",
                    "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
                    "Large-scale adversarial training for vision-and-language representation learning",
                    "On evaluating adversarial robustness of large vision-language models",
                    "Explaining and harnessing adversarial examples",
                    "Saliency methods for explaining adversarial attacks",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions",
                    "Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond",
                    "Multi-modal latent space learning for chain-of-thought reasoning in language models",
                    "Intriguing properties of neural networks",
                    "Ot-attack: Enhancing adversarial transferability of vision-language models via optimal transport optimization",
                    "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
                    "Multimodal Chain-of-Thought Reasoning in Language Models, February"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Making Large Language Models Interactive: A Pioneer Study on Supporting Complex Information-Seeking Tasks with Implicit Constraints",
            "arxiv_id": "2205.00584",
            "isAPA": true,
            "abstract": "Current interactive systems with natural language interfaces lack the ability to understand a complex information-seeking request whichexpresses several implicit constraints at once, and there is no prior information about user preferences, e.g., 'find hiking trails around SanFrancisco which are accessible with toddlers and have beautiful scenery in summer', where output is a list of possible suggestions forusers to start their exploration. In such scenarios, user requests can be issued in one shot in the form of a complex and long query, unlikeconversational and exploratory search models, where require short utterances or queries are often presented to the system step by step.This advancement provides the final user more flexibility and precision in expressing their intent through the search process, as well asgreater efficiency in their interactions with the system. Such systems are inherently helpful for day-to-day user tasks requiring planningthat are usually time-consuming, sometimes tricky, and cognitively taxing. We have designed and deployed a platform to collect the datafrom approaching such complex interactive systems.Moreover, with the current advancement of generative language models such as GPT-based models, understanding complex userrequests becomes more possible, however, these models suffer from hallucination in providing accurate factual knowledge. All languagemodels are mostly trained in large part on web-scraped data from the past, which usually is not useful for immediate users' needs.In this article, we propose an Interactive Agent (IA) that leverages Large Language Models (LLM) for complex request understandingand makes it interactive using Reinforcement learning (RL) that allows intricately refine user requests by making them complete, whichshould lead to better retrieval and reduce LLMs hallucination problems for current user needs. To demonstrate the performance of theproposed modeling paradigm, we have adopted various pre-retrieval metrics that capture the extent to which guided interactions with oursystem yield better retrieval results. Through extensive experimentation, we demonstrated that our method significantly outperformsseveral robust baselines.",
            "reference": [
                "Lucas Bernardi, Jaap Kamps, Julia Kiseleva, and Melanie JI M\u00fcller. The continuous cold start problem in e-commerce recommender systems. arXiv preprint arXiv",
                "Negar Arabzadeh, Fattaneh Zarrinkalam, Jelena Jovanovic, and Ebrahim Bagheri. Geometric estimation of specificity within embedding spaces",
                "Julia Kiseleva, Kyle Williams, Jiepu Jiang, Ahmed Hassan Awadallah, Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. Understanding user satisfaction with intelligent assistants",
                "Ying Zhao, Falk Scholer, and Yohannes Tsegay. Effective pre-retrieval query performance prediction using similarity and variability evidence",
                "Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. Towards conversational recommender systems",
                "Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, and Gareth JF Jones. Estimating gaussian mixture models in the local neighbourhood of embedded word vectors for query performance prediction. Information Processing & Management",
                "Surendra Sarnikar, Zhu Zhang, and J Leon Zhao. Query-performance prediction for effective query routing in domain-specific repositories. Journal of the Association for Information Science and Technology",
                "Alexandre Gilotte, Cl\u00e9ment Calauz\u00e8nes, Thomas Nedelec, Alexandre Abraham, and Simon Doll\u00e9. Offline a/b testing for recommender systems",
                "Djallel Bouneffouf and Irina Rish. A survey on practical applications of multi-armed and contextual bandits. arXiv preprint arXiv",
                "Alexander Kotov, Paul N Bennett, Ryen W White, Susan T Dumais, and Jaime Teevan. Modeling and analysis of cross-session search tasks",
                "Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv",
                "Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, and Michael S Bernstein. Iris: A conversational agent for complex tasks",
                "Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene Agichtein. Find it if you can: a game for modeling different types of web search success using interaction data",
                "Ben He and Iadh Ounis. Inferring query performance using pre-retrieval predictors",
                "Bing Liu and Ian Lane. Iterative policy learning in end-to-end trainable task-oriented neural dialog models",
                "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs",
                "Helia Hashemi, Hamed Zamani, and W Bruce Croft. Performance prediction for non-factoid question answering",
                "Ivica Kostric, Krisztian Balog, and Filip Radlinski. Soliciting user preferences in conversational recommender systems via usage-related questions",
                "Steve Young, Milica Ga\u0161i\u00e8, Blaise Thomson, and Jason D Williams. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE",
                "Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeffrey Dalton, and Mikhail Burtsev. Building and evaluating open-domain dialogue corpora with clarifying questions. arXiv preprint arXiv",
                "Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Julia Kiseleva, Maarten de Rijke, Shahin Shayandeh, and Jianfeng Gao. Guided dialog policy learning without adversarial learning in the loop. arXiv preprint arXiv:2004.03267, 2020b",
                "Vassilis Plachouras, Ben He, and Iadh Ounis. University of glasgow at trec 2004: Experiments in web, robust, and terabyte tracks with terrier",
                "Ruihua Song, Zhenxiao Luo, Jian-Yun Nie, Yong Yu, and Hsiao-Wuen Hon. Identification of ambiguous queries in web search. Information Processing & Management",
                "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs",
                "Maryam Khodabakhsh and Ebrahim Bagheri. Semantics-enabled query performance prediction for ad hoc table retrieval. Information Processing & Management",
                "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv",
                "Amir Soleimani, Christof Monz, and Marcel Worring. NLQuAD: A non-factoid long question answering data set",
                "Ziming Li, Julia Kiseleva, and Maarten de Rijke. Dialogue generation: From imitation learning to inverse reinforcement learning. arXiv preprint arXiv",
                "David Cortes. Adapting multi-armed bandits policies to contextual bandits scenarios. arXiv preprint arXiv",
                "Yikun Ban and Jingrui He. Local clustering in contextual multi-armed bandits",
                "Peter Ingwersen and Kalervo J\u00e4rvelin. The turn: Integration of information seeking and retrieval in context, volume 18. Springer Science & Business Media",
                "Robert Villa, Iv\u00e1n Cantador, Hideo Joho, and Joemon M Jose. An aspectual interface for supporting complex search tasks",
                "Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation",
                "Chen Wu and Ming Yan. Session-aware information embedding for e-commerce product recommendation",
                "Andrea Barraza-Urbina and Dorota Glowacka. Introduction to bandits in recommender systems",
                "Adish Singla, Ryen White, and Jeff Huang. Studying trailfinding algorithms for enhanced web search",
                "Haggai Roitman, Shai Erera, and Guy Feigenblat. A study of query performance prediction for answer quality determination",
                "Tuukka Ruotsalo, Jaakko Peltonen, Manuel JA Eugster, Dorota G\u0142owacka, Patrik Flor\u00e9en, Petri Myllym\u00e4ki, Giulio Jacucci, and Samuel Kaski. Interactive intent modeling for exploratory search. ACM Transactions on Information Systems (TOIS",
                "Nicholas J Belkin. Anomalous states of knowledge as a basis for information retrieval. Canadian journal of information science",
                "Ahmed Hassan Awadallah, Ryen W White, Patrick Pantel, Susan T Dumais, and Yi-Min Wang. Supporting complex search tasks",
                "OpenAI. Gpt-4 technical report. Technical report, arXiv:2303.08774 [cs.CL",
                "Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-C\u00e9spedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint arXiv",
                "Jiwei Li, Alexander H Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. ICLR",
                "Cr\u00edcia Z Fel\u00edcio, Kl\u00e9risson VR Paix\u00e3o, Celia AZ Barcelos, and Philippe Preux. A multi-armed bandit model selection for cold-start user recommendation",
                "Ahmed Hassan and Ryen W White. Task tours: helping users tackle complex search tasks",
                "Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al. The second conversational intelligence challenge (convai",
                "Ryen W White and Resa A Roth. Exploratory search: Beyond the query-response paradigm. Synthesis lectures on information concepts, retrieval, and services",
                "Gary G Hendrix, Earl D Sacerdoti, Daniel Sagalowicz, and Jonathan Slocum. Developing a natural language interface to complex data. ACM Transactions on Database Systems (TODS",
                "Anna Sepliarskaia, Julia Kiseleva, Filip Radlinski, and Maarten de Rijke. Preference elicitation as an optimization problem",
                "Ann Copestake and Karen Sparck Jones. Natural language interfaces to databases",
                "Edgar F Codd. Seven steps to rendezvous with the casual user. IBM Corporation",
                "Andreas Holzinger. Interactive machine learning for health informatics: when do we need the human-in-the-loop? Brain Informatics",
                "Thorsten Joachims, Yves Raimond, Olivier Koch, Maria Dimakopoulou, Flavian Vasile, and Adith Swaminathan. Reveal 2020: Bandit and reinforcement learning from user interactions",
                "Ziyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. Model-based interactive semantic parsing: A unified framework and a text-to-SQL case study",
                "Claudia Hauff, Djoerd Hiemstra, and Franciska de Jong. A survey of pre-retrieval query performance predictors",
                "Ryen W White, Mikhail Bilenko, and Silviu Cucerzan. Studying the use of popular destinations to enhance web search interaction",
                "David Carmel and Oren Kurland. Query performance prediction for ir",
                "Thorsten Joachims, Dayne Freitag, Tom Mitchell, et al. Webwatcher: A tour guide for the world wide web",
                "Hamed Zamani, W Bruce Croft, and J Shane Culpepper. Neural query performance prediction using weak supervision from multiple signals",
                "Daan Odijk, Ryen W White, Ahmed Hassan Awadallah, and Susan T Dumais. Struggling and success in web search",
                "Charles LA Clarke, Maheedhar Kolla, and Olga Vechtomova. An effectiveness measure for ambiguous and underspecified queries",
                "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. arXiv preprint arXiv",
                "David Carmel and Elad Yom-Tov. Estimating the query difficulty for information retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services",
                "Julia Kiseleva, Alexander Tuzhilin, Jaap Kamps, Melanie JI Mueller, Lucas Bernardi, Chad Davis, Ivan Kovacek, Mats Stafseng Einarsen, and Djoerd Hiemstra. Beyond movie recommendations: Solving the continuous cold start problem in e-commercerecommendations. arXiv preprint arXiv:1607.07904, 2016a",
                "Steve Cronen-Townsend, Yun Zhou, and W Bruce Croft. Predicting query performance",
                "W. A. Woods, Ronald M Kaplan, and Bonnie L. Webber. The lunar sciences natural language information system: Final report. BBN Report",
                "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv",
                "Ramesh Nallapati and Chirag Shah. Evaluating the quality of query refinement suggestions in information retrieval. Technical report, MASSACHUSETTS UNIV AMHERST CENTER FOR INTELLIGENT INFORMATION RETRIEVAL",
                "W Bruce Croft, Donald Metzler, and Trevor Strohman. Search engines: Information retrieval in practice, volume 520. Addison-Wesley Reading",
                "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics",
                "Julia Kiseleva, Jaap Kamps, Vadim Nikulin, and Nikita Makarov. Behavioral dynamics from the serp's perspective: what are failed serps and how to fix them? In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages",
                "Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv",
                "Haggai Roitman. Ictir tutorial: Modern query performance prediction: Theory and practice",
                "Christoph Leiter, Ran Zhang, Yanran Chen, Jonas Belouadi, Daniil Larionov, Vivian Fresen, and Steffen Eger. Chatgpt: A meta-analysis after 2.5 months. arXiv preprint arXiv",
                "Javier Parapar and Filip Radlinski. Diverse user preference elicitation with multi-armed bandits",
                "Julia Kiseleva, Kyle Williams, Ahmed Hassan Awadallah, Aidan C Crook, Imed Zitouni, and Tasos Anastasakos. Predicting user satisfaction with intelligent assistants",
                "Bing Liu and Ian Lane. Adversarial learning of task-oriented neural dialog models",
                "Ryen W White, Gary Marchionini, and Gheorghe Muresan. Evaluating exploratory search systems. Information Processing and Management",
                "Mark Sanderson. Ambiguous queries: test collections need more sense",
                "Negar Arabzadeh, Fattane Zarrinkalam, Jelena Jovanovic, and Ebrahim Bagheri. Neural embedding-based metrics for pre-retrieval query performance prediction. Advances in Information Retrieval, 12036:78, 2020b",
                "Klaus Krippendorff. Computing krippendorff's alpha-reliability",
                "Negar Arabzadeh, Maryam Khodabakhsh, and Ebrahim Bagheri. Bert-qpp: Contextualized pre-trained transformers for query performance prediction",
                "Negar Arabzadeh, Fattane Zarrinkalam, Jelena Jovanovic, Feras Al-Obeidat, and Ebrahim Bagheri. Neural embedding-based specificity metrics for pre-retrieval query performance prediction. Information Processing & Management, 57(4) :102248, 2020a",
                "Ahmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. Speak to your parser: Interactive text-to-SQL with natural language feedback",
                "Randall H Trigg. Guided tours and tabletops: Tools for communicating in a hypertext environment. ACM Transactions on Information Systems (TOIS",
                "Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. arXiv preprint arXiv",
                "Julia Kiseleva, Eric Crestan, Riccardo Brigo, and Roland Dittel. Modelling and detecting changes in user satisfaction",
                "Konstantina Christakopoulou. Towards Recommendation Systems with Real-World Constraints. PhD thesis, University of Minnesota",
                "Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail Burtsev. Convai3: Generating clarifying questions for open-domain dialogue systems (clariq",
                "Gary Marchionini. Exploratory search: from finding to understanding. Communications of the ACM",
                "Ahmed Hassan, Rosie Jones, and Kristina Lisa Klinkner. Beyond dcg: user behavior as a predictor of a successful search",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding",
                "Toby Jia-Jun Li, Tom Mitchell, and Brad Myers. Interactive task learning from GUI-grounded natural language instructions and demonstrations",
                "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research",
                "Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, Subhajit Roy, et al. Program synthesis using natural language",
                "Christopher Olston and Ed H Chi. Scenttrails: Integrating browsing and searching on the web. ACM Transactions on Computer-Human Interaction (TOCHI",
                "Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation",
                "Jingjing Liu and Nicholas J Belkin. Personalizing information retrieval for multi-session tasks: Examining the roles of task stage, task type, and topic knowledge on the interpretation of dwell time as an indicator of document usefulness. Journal of the Association for Information Science and Technology",
                "Claudia Hauff, Leif Azzopardi, and Djoerd Hiemstra. The combination and evaluation of query performance prediction methods"
            ],
            "related work": "2.Background and Related WorkOur work is relevant to four broad strands of research on multi-armed bandits, search engines, language as an interface for interactive systems, and exploratory search and trails, which we review below.Contextual bandits for recommendationMulti-armed bandits are a classical exploration-exploitation framework fromReinforcement Learning(RL) , where the user feedback is available in each iteration(Parapar and Radlinski,2021; Cortes,2018; Li et al.,2010) . They are becoming popular for online applications such as ranking online advertisements and recommendation systems(e.g., Ban and He,2021; Joachims et al.,2020) , where information about user preferences is unavailable (cold-start users(Bernardi et al.,2015; Kiseleva et al.,2016a) ) (Fel\u00edcio et al.,2017) .Parapar and Radlinski (2021) proposed a multi-armed bandit model for personalized recommendations by diversifying the user preferences by changing the focus only on past user interactions. Others examined the application of contextual bandit models in healthcare, finance, dynamic pricing, and anomaly detection(Bouneffouf and Rish,2019) . Our work adapts contextual bandits paradigm to the new problem of interactive intent modeling for complex information-seeking tasks.Search enginesCommonly used search engines such as Google and Bing provide platforms focusing on the document retrieval process through search sessions(Hassan et al.,2010; Kiseleva et al.,2014,2015; Ageev et al.,2011) . Developing retrieval models that can extract the most relevant documents from an extensive collection has been well-studied(Croft et al.,2010) for decades. The developed retrieval models focus on retrieving the most relevant documents corresponding to user intent, represented with textual and contextual information within and across search sessions(Kotov et al.,2011) . Although extracting relevant documents is necessary, it is not always sufficient, especially when the users have a complex information-seeking task(Ingwersen and J\u00e4rvelin,2006) .Language as an interface for interactionsNLUhave been the important direction for human-computer interaction and information search for decades(Woods et al.,1972; Codd,1974; Hendrix et al.,1978) . The recent impressive advances in capabilities ofNLU(Devlin et al.,2018; Liu et al.,2019; Clark et al.,2020; Adiwardana et al.,2020; Roller et al.,2020; Brown et al.,2020) powered by large-scale deep learning and increasing demand for new applications has led to a major resurgence of natural language interfaces in the form of virtual assistants, dialog systems, semantic parsing, and question answering systems(Liu and Lane,2017,2018; Dinan et al.,2020; Zhang et al.,2019) . The scope of natural language interfaces has been significantly expanding from databases(Copestake and Jones,1990) to knowledge bases(Berant et al.,2013) , robots(Tellex et al.,2011) , virtual assistants(Kiseleva et al.,2016c,b) , and other various forms of interaction(Fast et al.,2018; Desai et al.,2016; Young et al.,2013) . Recently, the community has focused on continuous learning through interactions, including systems that learn a new task from instructions(Li et al.,2020a) , assess their uncertainty(Yao et al.,2019) and ask feedback from humans in case of uncertainty(Aliannejadi et al.,2021,2020) or for correcting possible mistakes(Elgohary et al.,2020) .Exploratory search, tours, and trailsExploratory search refers to an information-seeking process in which the system assists the searcher in understanding the information space for iterative exploration and retrieval of information(Ruotsalo et al.,2018; Hassan Awadallah et al.,2014; White et al.,2008) . Anomalous states of knowledge (ASKs) (Belkin,1980) motivate the need to search and drive demand for search systems. According to the ASK hypothesis, users usually can struggle to conceptualize and formulate their information needs as search queries, which may miss some essential information(Liu and Belkin,2015; White and Roth,2009) . In such cases, the system should assist the user in specifying their intent(Marchionini,2006) . Through a search log analysis,Odijk et al. (2015) shows that there are many searches where users may struggle to formulate their search query or they may simply be exploring to learn about a new area. New search interface designs may be required to support searchers through their information-seeking process(Villa et al.,2009) .Tours and Trailsare another group of tools that were developed to guide users to accomplish search tasks. Guided tours are common in hypertext systems(Trigg,1988) and similar ideas could be applied in the context of search(Hassan and White,2012) . Surfacing common trail destinations in search interfaces can help people find information targets more quickly(White et al.,2007) . Search engines may also present full trails as a way to explore, learn, and complete multi-step tasks(Singla et al.,2010) .Olston and Chi (2003) proposed ScentTrails that leverage an interface that combines browsing and searching and highlights potentially relevant hyperlinks. WebWatcher(Joachims et al.,1997) , like ScentTrails, underlined the relevant hyperlinks and improved the model based on the implicit feedback collected during previous tours.To summarize, thekey distinctionsof our work compared to previous efforts are as follows. Similar to the exploratory search, trails, and conversational search, our model proposes an iterative information-seeking process and designs an interface for user interactions to guide struggling users and help them better understand the information space. However, that work that only focuses on user interaction modeling and limits users in issuing short and imprecise queries and utterances, our model provides a platform for users to express their information needs in the form of long and complex requests. Users can utilize this capability to express their intent more accurately and prune significant parts of the search space for the exploratory search process. Adding this capability needs an advancedNLUstep and different machine learning components to understand and guide the final user through the search process. To this end, the proposed system has two new components, anintent ontologyand aprofilefor partitioning the information space, enabling theIAto help users be more effective in exploring the search space.",
            "date": "2022"
        },
        "topic": "Hallucination in LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "subtitles": [
                    "LLM Benchmarks",
                    "Risks of Static Benchmarks",
                    "Ranking System",
                    "Human Preference Dataset"
                ],
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related WorkLLM Benchmarks.We briefly review the common LLM benchmarks, following the classification presented in Figure 1. The most prevalent benchmarks are static, ground-truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined answers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU(Hendrycks et al.,2020) , HellaSwag(Zellers et al.,2019) , GSM-8K(Cobbe et al.,2021) , BigBench(Srivastava et al.,2023) , AGIEval(Zhong et al.,2023) , and HumanEval(Chen et al.,2021) . Benchmarks focusing on safety, such as ToxicChat(Lin et al.,2023) , and comprehensive suites like HELM(Liang et al.,2022) , also exist. In addition to closed-ended questions, benchmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk(Karpinska et al.,2021; Geng et al.,2023; Wang et al.,2023) . The recent trend includes utilizing GPT-4 for approximating human judgment(Chiang & Lee,2023) , with notable instances being MT-Bench(Zheng et al.,2023b) and AlpacaEval(Li et al.,2023) . In addition to static benchmarks, live benchmarks that include fresh questions are also available. These questions can be obtained from annual exams or weekly online contests such as Codeforces(Li et al.,2022; Huang et al.,2023) . They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference(Bai et al.,2022; Ouyang et al.,2022; Touvron et al.,2023) . However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utilizes live human interaction.Risks of Static Benchmarks.Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment(Yang et al.,2023; Oren et al.,2023) . DynaBench(Kiela et al.,2021) identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. However, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale.Ranking System.Ranking systems have been a well-studied topic in statistics. Related topics include probability models(Hunter,2004; Rao & Kupper,1967) , rank elicitation(Sz\u00f6r\u00e9nyi et al.,2015; Busa-Fekete et al.,2014a,b) , and online experiment design(Chernoff,1992; Karimi et al.,2021) . The Elo rating system has also been used for LLMs(Bai et al.,2022; Boubdir et al.,2023) . Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifically applied to large-scale, real-world settings of LLMs.Human Preference Dataset.Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpenAssistant(K\u00f6pf et al.,2023) , HH-RLHF(Bai et al.,2022) , LMSYS-Chat-1M(Zheng et al.,2023a) , and synthetic approximations of human preferences like UltraFeedback(Cui et al.,2023) and Nectar(Zhu et al.,2023) . Our prior data release, LMSYS-Chat-1M(Zheng et al.,2023a) , is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes.",
                "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{this https URL}."
            },
            {
                "name": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
                "arxiv_id": "2401.00396",
                "subtitles": [
                    "Hallucination of Large Language Models",
                    "Hallucination Evaluation Datasets",
                    "Hallucination Detection Methods"
                ],
                "reference": [
                    "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
                    "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
                    "A survey on automated fact-checking",
                    "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                    "TruthfulQA: Measuring how models mimic human falsehoods",
                    "Can llm-generated misinformation be detected",
                    "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                    "Survey of hallucination in natural language generation",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "QMSum: A new benchmark for query-based multi-domain meeting summarization",
                    "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
                    "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
                    "Adversarial nli for factual correctness in text summarisation models",
                    "Uncertainty estimation in autoregressive structured prediction",
                    "The Internal State of an LLM Knows When its Lying",
                    "On hallucination and predictive uncertainty in conditional language generation",
                    "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
                    "Evaluating attribution in dialogue systems: The begin benchmark",
                    "Felm: Benchmarking factuality evaluation of large language models",
                    "Freshllms: Refreshing large language models with search engine augmentation",
                    "Refchecker for fine-grained hallucination detection"
                ],
                "related_work": "2Related Work 2.1Hallucination of Large Language Models Though hallucination in traditional natural language generation (NLG) contexts has been widely studiedJi et al. (2023) , comprehending and tackling this problem in the context of LLMs presents distinct challengesZhang et al. (2023b) . Existing research has demonstrated that incorporating up-to-date, relevant knowledge in the prompt can effectively reduce fact-conflicting hallucinationVu et al. (2023) ; Lewis et al. (2021) . This approach, referred to asRetrieval-Augmented Generation(RAG) , is widely used in real-world LLM applications. For instance, Google Bard and Microsoft BingChat implemented this technique. 2.2Hallucination Evaluation DatasetsExtensive research has focused on hallucination benchmarks within conventional Natural Language Generation settingsDziri et al. (2022) ; Zhong et al. (2021) ; Durmus et al. (2020) ; Lin et al. (2022) . With the rise of LLMs, the detection of hallucinations has become increasingly challenging, necessitating the development of high-quality datasets for LLM evaluationChen and Shu (2023) . Contributions in this domain include HaluEvalLi et al. (2023) , which introduced datasets encompassing both synthetically and naturally generated LLM responses, and FELMChen et al. (2023) , which concentrated on naturally generated LLM responses across multiple domain tasks. RefCheckerHu et al. (2023) , a distinctive approach, breaks down claims in LLM responses into triples and utilizes human annotation to assess the truthfulness of facts. Notably, these works primarily focus on annotating factual hallucinations in LLM responses. Distinguishing from previous research, our work centers on the evaluation of LLMs within RAG settings. 2.3Hallucination Detection MethodsResearchers have been exploring various methods to enhance the reliability of LLMs by detecting hallucinations. InAzaria and Mitchell (2023) ; Xiao and Wang (2021) ; Malinin and Gales (2021) , intrinsic model uncertainty metrics such as token-level probability and entropy are used to detect hallucinations. When direct access to output uncertainty is not feasible, as in the case with limited APIs like GPT-4, an alternative approach involves employing a fully accessible LLM as a proxyManakul et al. (2023) . InFalke et al. (2019) ; Barrantes et al. (2020) , natural language inference modules are adapted to check the information consistency between the articles and their summaries, and it has been shown that external knowledge is helpful for detecting factual hallucinations.Guo et al. (2022) ; Mallen et al. (2022) . Additionally, methods that leverage the inherent capabilities of LLMs have been proposed for self-checking, such as verbalization-based and consistency-based methodsXiong et al. (2023) ; Manakul et al. (2023) . These techniques aim to detect hallucinations without relying on internal states or external data and tools.",
                "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4."
            },
            {
                "name": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
                "arxiv_id": "2401.06373",
                "subtitles": [
                    "Optimization",
                    "Side-channel Communication",
                    "Distribution",
                    "Ours: Challenging AI safety by Humanizing LLMs"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Model card and evaluations for claude models",
                    "When consumers and brands talk: Storytelling theory and research in psychology and marketing",
                    "Large language models respond to influence like humans",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Persuasion: Social influence and compliance gaining",
                    "Narrative persuasion",
                    "Persuasion",
                    "Evidence-based advertising using persuasion principles: Predictive validity and proof of concept",
                    "Certifying llm safety against adversarial prompting",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Persuasion and coercion: a critical review of philosophical and empirical approaches",
                    "Frame analysis: An essay on the organization of experience",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "The Dynamics of Persuasion: Communication and Attitudes in the 21st Century",
                    "Social influence: Compliance and conformity",
                    "Rain: Your language models can align themselves without finetuning",
                    "Self-persuasion via self-reflection",
                    "Perspectives on ethics in persuasion",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Scarcity messages",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Low-resource languages jailbreak gpt",
                    "Stanford alpaca: An instruction-following llama model",
                    "What's in my big data",
                    "Multilingual jailbreak challenges in large language models",
                    "The science of persuasion",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                    "Adaptation in dyadic interaction: Defining and operationalizing patterns of reciprocity and compensation",
                    "The earth is flat because ...: Investigating llms' belief towards misinformation via persuasive conversation",
                    "Dark patterns: Past, present, and future: The evolution of tricky user interfaces",
                    "Shining a light on dark patterns",
                    "Self-inference processes: The ontario symposium, vol",
                    "The neural basis of social influence and attitude change",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Logic, emotion, and the paradigm of persuasion",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "The effects of expert and consumer endorsements on audience response",
                    "Credibility: A multidisciplinary framework",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Rumors influence: Toward a dynamic social impact theory of rumor",
                    "Susceptibility to influence of large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Detecting language model attacks with perplexity",
                    "Dark patterns at scale: Findings from a crawl of 11k shopping websites",
                    "Emotional factors in attitudes and persuasion",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
                    " \"he would still be here \": Man dies by suicide after talking with ai chatbot, widow says",
                    "Adversarial demonstration attacks on large language models",
                    "Automatically auditing large language models via discrete optimization",
                    "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
                    "Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests",
                    "Striking a responsive chord: How political ads motivate and persuade voters by appealing to emotions",
                    "The persuasiveness of source credibility: A critical review of five decades' evidence",
                    "Interpersonal influence"
                ],
                "related_work": "2Related Work As LLMs become more widely used in real-world applications, jailbreak research efforts have diversified and can be broadly classified into 3 main categories: Optimization, Side-channel Communication, and Distribution-based methods. Figure 2 shows concrete examples of different methods. Optimization-based techniques are at the forefront of jailbreak research and involve three main types: (1) Gradient-Based methods Zou et al. (2023); Jones et al. (2023) manipulate model inputs based on gradients to elicit compliant responses to harmful commands; (2) Genetic algorithms-based methods Liu et al. (2023a); Lapid et al. (2023) use mutation and selection to explore effective prompts; and (3) Edit-based methods Chao et al. (2023) asks a pre-trained LLM to edit and improve the adversarial prompt to subvert alignment. Side-channel Communication exploits long-tailed distribution to increase jailbreak success rates, such as ciphers Yuan et al. (2023) and translating harmful instructions into low-resource languages Deng et al. (2023b); Yong et al. (2023). Other studies Mozes et al. (2023); Kang et al. (2023) use programmatic behaviors, such as code injection and virtualization, to expose LLM vulnerabilities. Distribution-based methods include learning from successful manually-crafted jailbreak templates Deng et al. (2023a); Yu et al. (2023) and in-context examples Wei et al. (2023); Wang et al. (2023). Notably, Shah et al. (2023) employs in-context persona to increase LLMs' susceptibility to harmful instructions. While this approach shares some similarities with ours in eliciting harmful outputs via priming and framing, it only represents a small subset of the persuasive techniques we explore. Ours: Challenging AI safety by Humanizing LLMs. Figure 2 compares existing jailbreaking methods and PAP in this study, organized by their degree of humanizing. One line of research treats LLMs as traditional algorithmic systems (i.e., without attributing intelligence or human-like qualities) that take in less interpretable adversarial prompts, while another line views them as simple instruction followers who understand human commands. However, they both ignore the fact that LLMs can understand and conduct complex natural communication Griffin et al. (2023a, b). Our approach innovatively treats LLMs as human-like communicators and grounds on a taxonomy informed by decades of social science research on human communication. Such an interdisciplinary approach allows us to uncover and address distinct risks related to human-AI interactions, particularly human-driven persuasion-based jailbreak. Moreover, humanizing AI presents other unique risks that can occur unintentionally: for instance, as highlighted by Xiang (2023), a user's suicide was related to involved conversations with an AI Chatbot. This points out important future directions to further explore the inherent risks associated with AI humanization.",
                "abstract": "Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs"
            },
            {
                "name": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
                "arxiv_id": "2401.03205",
                "subtitles": [
                    "Hallucination Source and Detection",
                    "Hallucination Mitigation"
                ],
                "reference": [
                    "Factuality enhanced language models for open-ended text generation",
                    "Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios",
                    "Llm lies: Hallucinations are not bugs, but features as adversarial examples",
                    "Inference-time intervention: Eliciting truthful answers from a language model",
                    "Survey of hallucination in natural language generation",
                    "Generate rather than retrieve: Large language models are strong context generators",
                    "Trusting your evidence: Hallucinate less with context-aware decoding",
                    "Attention satisfies: A constraint-satisfaction lens on factual errors of language models",
                    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                    "Hagrid: A human-llm collaborative dataset for generative information-seeking with attribution",
                    "Halueval: A large-scale hallucination evaluation benchmark for large language models",
                    "Purr: Efficiently editing language model hallucinations by denoising language model corruptions",
                    "Bridging the gap: A survey on integrating (human) feedback for natural language generation",
                    "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
                    "Mutual information alleviates hallucinations in abstractive summarization",
                    "The internal state of an llm knows when its lying",
                    "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
                    "Siren's song in the AI ocean: A survey on hallucination in large language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Med-halt: Medical domain hallucination test for large language models",
                    "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                    "Do language models know when they're hallucinating references",
                    "Diving deep into modes of fact hallucinations in dialogue systems",
                    "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                    "Exploring the relationship between llm hallucinations and prompt linguistic nuances: Readability, formality, and concreteness",
                    "Self-instruct: Aligning language model with self generated instructions"
                ],
                "related_work": "7Related WorkHallucination has been a fundamental challenge in LLMs, receiving extensive attention in existing literature(Huang et al.,2023; Ji et al.,2023a; Zhang et al.,2023b; Li et al.,2023a) . We discuss the related study in two aspects, namely hallucination source/detection and mitigation.Hallucination Source and Detection333Since the understanding of the hallucination source can be employed to devise new detection methods, a number of studies often jointly discussed the two parts. Thus, we discuss the source and detection in this single part..To understand and detect the hallucination in LLMs, several existing studies focus on utilizing the LLM itself to as the tool to study the hallucinated content. For LLMs with access to their internal states, we can delve into the inner workings of the model to explore the principles behind hallucinations(Varshney et al.,2023; Yuksekgonul et al.,2023; Azaria and Mitchell,2023a) . Typically, the internal states that can be studied by examining the output logit values, the hidden layer activations, and the attention states. For example,Varshney et al. (2023) leveraged the output logit values of the model as a signal of hallucinations to estimate the uncertainty of responses.Azaria and Mitchell (2023a) employed the hidden layer activations of the model for determining the truthfulness of generated statements.Yuksekgonul et al. (2023) identified factual errors by employing a straightforward probe on the LLM's attention towards constraint tokens. For models that can only be accessed through API calls, hallucinations are typically studied by analyzing the relationship between input prompts and the model's output responses(Rawte et al.,2023b; Manakul et al.,2023; Yao et al.,2023) . For example,Rawte et al. (2023b) explored how linguistic elements in prompts, particularly readability, formality, and concreteness, impact the occurrence of hallucinations.Yao et al. (2023) illustrated that nonsense prompts, consisting of random tokens, can prompt LLMs to generate hallucinations, indicating that hallucinations might be viewed as another form of adversarial examples.Manakul et al. (2023) detected hallucinations by evaluating the consistency of responses with BERTScore, QA-based metrics and n-gram metrics. In addition, several other studies rely on external knowledge for reference retrieval to detect hallucinations.Chern et al. (2023a) proposed a task and domain agnostic framework augmented by tools for detecting factual errors.Yu et al. (2022) instructed a LLM to create contextual documents and then analyze the generated document to infer the ultimate answer.Ren et al. (2023) explored the perceptual capabilities of LLMs concerning the boundaries of factual knowledge through retrieval augmentation on open domain QA.Hallucination Mitigation.To mitigate the hallucinations, existing studies encompass researches across different stages in the development and utilization of LLMs. Specially, mitigation during pre-training is typically centered around dataset curating and cleaning. In this line, existing studies(Das et al.,2023; Kamalloo et al.,2023; Umapathi et al.,2023) aim to construct a higher quality corpus for model pre-training by building datasets within specific domains or cleaning existing datasets. After pre-training, fine-tuning approaches can be further employed for hallucination mitigation, such as the applications of SFT(Wang et al.,2022b) and RLHF(Fernandes et al.,2023) . In practical use, mitigation during generation is mainly focused on developing more effective decoding strategies(Lee et al.,2022b; Li et al.,2023c; Shi et al.,2023a; van der Poel et al.,2022) , leveraging external knowledge(Chern et al.,2023a; Varshney et al.,2023) and designing more effective prompts(Agrawal et al.,2023; Touvron et al.,2023b) . Furthermore, mitigation during the post-processing stage can be implemented by using LLM itself(M\u00fcndler et al.,2023) or external knowledge(Chen et al.,2023) as the fact-checking module to verify the generated text, so as to correct hallucinations.",
                "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs. Our code and data can be accessed atthis https URL."
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "subtitles": [
                    "Multimodal LLMs",
                    "Evaluating Multimodal LLMs",
                    "Visual Encoders",
                    "Ambiguities in Embedding Models"
                ],
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report, 2023b",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ],
                "related_work": "5Related WorksMultimodal LLMs.We study the limitations of Multimodal LLMs[40,13,30,31,8]and explore possible ways to improve these models. Multimodal LLMs build from pretrained Large Language Models[41,3,58,59,69]and CLIP vision encoder[43,54]. These systems then use an adapter, such as MLPs[30,31], Q-Former[26,8], and gated attention[2,25], to integrate the pretrained CLIP vision encoder into LLMs. More recently, instructBLIP[8], LLaVA-1.5[30]highlight the importance of high-quality training data. Yet, there is a scarcity of research focusing on the impact of visual encoders, which is an important gap our work aims to address through a systematic study.Evaluating Multimodal LLMs.MMVP assesses MLLMs using a set of simple yet critical Visual Question Answering (VQA) questions constructed from CLIP-blind pairs. Previous benchmarks such as TextVQA[52], VQAv2[15], and GQA[21]have centered on traditional VQA queries. Recently, there are works like MM-Vet[64], POPE[27], and MM-Bench[32]designed to specifically evaluate multimodal LLMs including hallucination, reasoning, and robustness. The previous benchmarks and evaluations have shown that Multimodal LLMs can suffer from hallucination[29,28], catastrophic forgetting[67]and lack of robustness[11]. In taking a step back to the fundamentals, our work uncovers that even the most advanced multimodal LLMs, such as GPT-4V[40], Gemini[14], Bard[30], and LLaVA-1.5[30], are not immune to stumbling over elementary visual questions. We also identified part of the problem as being the incapable visual encoder.Visual Encoders.MMVP-VLM provides a detailed analysis of the visual capabilities of various CLIP variants[43,54,62,66]. These models mostly follow the method proposed inRadford et al.[43]that uses contrastive loss to train on large volumes of image-text pairs. They differ in training data[62], training recipes[54], and objective functions[66]. Nonetheless, our studies show that all of these CLIP variants struggle with simple visual patterns such as  \"orientation \",  \"count \",  \"presence of specific features \",etc. Another line of research focuses on vision-only self-supervised learning (SSL) . This category includes contrastive SSL[7,16,5,17]and mask-based SSL[70,18,4]. SLIP[39]explores the synergy between CLIP and contrastive SSL, but focusing primarily on standard classification tasks. In fact, a common practice to evaluate the quality of these vision models is through linear probing or fine-tuning on ImageNet[47,45]. Although current evaluation methods provide a basic level of assessment on representation quality, our findings indicate a growing detachment from the needs of recent use cases. As demonstrated in the MoF experiments in Section4, the CLIP vision model and the vision-only SSL models learn complementary features. However, the linear probing accuracy on ImageNet alone provides a limited understanding of feature utility in MLLMs. This observation suggests the need for more diverse evaluations[61]in visual representation learning, to better align with current and emerging applications.Ambiguities in Embedding Models.Our work exploits CLIP-blind pairs within the CLIP vision embedding space to generate examples of failures in CLIP models and subsequently MLLMs. This concept has ties to previous research focused on documenting failure modes in text embedding models[12,36,55]. More recently,Thrush et al.[56],Yuksekgonul et al.[65]andHsieh et al.[19]study the binding problems CLIP faces in processing text queries, noting that CLIP models treat text input as a bag of words.Tong et al.[57]examines the implications for downstream text-guided generative models.Tschannen et al.[60]suggests image captioners as promising alternatives to CLIP for improving attribute binding. Our work focuses on the visual patterns.",
                "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems."
            },
            {
                "name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
                "arxiv_id": "2402.04249",
                "subtitles": [
                    "Red Teaming LLMs",
                    "Defenses"
                ],
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    " \"do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
                    "Robust prompt optimization for defending language models against jailbreaking attacks",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Mistral 7b",
                    "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
                    "Rain: Your language models can align themselves without finetuning",
                    "Llm censorship: A machine learning challenge or a computer security problem",
                    "A holistic approach to undesired content detection in the real world",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Masterkey: Automated jailbreak across multiple large language model chatbots",
                    "Red teaming language models with language models",
                    "Are aligned neural networks adversarially aligned",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Universal adversarial triggers for attacking and analyzing NLP",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Gpt-4 technical report",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Mart: Improving llm safety with multi-round automatic red-teaming",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Gradient-based adversarial attacks against text transformers",
                    "Explore, establish, exploit: Red teaming language models from scratch",
                    "Jailbroken: How does llm safety training fail",
                    "Image hijacks: Adversarial images can control generative models at runtime",
                    "Automatically auditing large language models via discrete optimization",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work 2.1Red Teaming LLMs Manual red teaming. Several large-scale manual red teaming efforts have been conducted on LLMs as part of pre-deployment testing (Bai et al., 2022a; Ganguli et al., 2022; Achiam et al., 2023; Touvron et al., 2023). Shen et al. (2023a) characterize the performance of a wide variety of human jailbreaks discovered for closed-source models post-deployment, and (Wei et al., 2023) identify successful high-level attack strategies. These studies and others can serve as a baseline for developing more scalable automated red teaming methods. Automated red teaming. A wide variety of automated red teaming methods have been proposed for LLMs. These include text optimization methods (Wallace et al., 2019; Guo et al., 2021; Shin et al., 2020; Wen et al., 2023; Jones et al., 2023; Zou et al., 2023), LLM optimizers (Perez et al., 2022; Chao et al., 2023; Mehrotra et al., 2023), and custom jailbreaking templates or pipelines (Liu et al., 2023b; Shah et al., 2023; Casper et al., 2023; Deng et al., 2023; Zeng et al., 2024). Most of these methods can be directly compared with each other for eliciting specific harmful behaviors from LLMs. Several papers have also explored image attacks on multimodal LLMs (Bagdasaryan et al., 2023; Shayegani et al., 2023; Qi et al., 2023a; Bailey et al., 2023). In some instances, multimodal attacks have been observed to be stronger than text attacks (Carlini et al., 2023), motivating their inclusion in a standardized evaluation for attacks and defenses. The literature on automated red teaming has grown rapidly, and many attacks are now available for comparison. However, the lack of a standardized evaluation has prevented easy comparisons across papers, such that the relative performance of these methods is unclear. Evaluating red teaming. Due to the rapid growth of the area, many papers on automated red teaming have developed their own evaluation setups to compare their methods against baselines. Among prior work, we find at least  9  distinct evaluation setups, which we show in Table 1. We find that existing comparisons rarely overlap, and in Section 3.2 we demonstrate that prior evaluations are essentially incomparable across papers due to a lack of standardization. 2.2Defenses Several complimentary approaches have been studied for defending LLMs against malicious use. These can be categorized into system-level defenses and model-level defenses. System-level defenses System-level defenses do not alter the LLM itself, but rather add external safety measures on top of the LLM. These include input and output filtering (Markov et al., 2023; Inan et al., 2023; Computer, 2023; Li et al., 2023; Cao et al., 2023; Jain et al., 2023), input sanitization (Jain et al., 2023) and modification (Zhou et al., 2024), and constrained inference (Rebedea et al., 2023). The most widely-used defense in production is filtering, but Glukhov et al. (2023) note that output filtering can be foiled if jailbroken LLMs assist malicious users with bypassing detection, e.g., by generating encoded outputs. This motivates a defense in depth approach where system-level defenses like filtering are combined with defenses built into LLMs. Model-level defenses Model-level defenses alter the LLM itself to reduce the risk of malicious use and improve robustness to adversarial prompting. These include safety training, refusal mechanisms, system prompts and context distillation, and adversarial training. Safety training is commonly approached via fine-tuning methods such as RLHF (Ouyang et al., 2022), DPO (Rafailov et al., 2023), and RLAIF (Bai et al., 2022b). Combined with safety datasets and manual red teaming, these approaches can yield substantial improvements to safety and robustness (Bai et al., 2022a; Achiam et al., 2023; Touvron et al., 2023). These training procedures often instill models with  \"refusal mechanisms\" whereby models identify a user request as harmful and refuse to carry out the request. Several works have explored adversarial training with automated red teaming methods. This differs in important ways from training against perturbation attacks, which has been extensively explored in prior work. We discuss these differences in Section A.1. Jain et al. (2023) note that current attacks can be extremely computationally expensive, which makes them challenging to integrate into an LLM fine-tuning loop. They conduct an adversarial training experiment with a static dataset of harmful prompts, in which the adversary does not optimize against the model during fine-tuning. Concurrently with our work, Ge et al. (2023) propose multi-round adversarial training with automated red teaming methods, generating new test cases  4  times throughout training. In Section 5 we introduce a novel adversarial training method for robust refusal, demonstrating how HarmBench can facilitate the codevelopment of attacks and defenses. Other factors that may affect the inherent robustness of a model to jailbreaks include its training set, architecture, system prompt (Touvron et al., 2023; Jiang et al., 2023), and size (Ganguli et al., 2022). Our large-scale comparison enables thorough examinations of the effect of these factors.",
                "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench atthis https URL."
            },
            {
                "name": "TOFU: A Task of Fictitious Unlearning for LLMs",
                "arxiv_id": "2401.06121",
                "subtitles": [
                    "Question answering",
                    "Realistic goals",
                    "Principled evaluation",
                    "Connection to differential privacy (DP) "
                ],
                "reference": [
                    "Towards adversarial evaluations for inexact machine unlearning",
                    "Propile: Probing privacy leakage in large language models",
                    "The eu general data protection regulation (gdpr",
                    "Remember what you want to forget: Algorithms for machine unlearning",
                    "Detecting pretraining data from large language models",
                    "The brainy student: Scalable unlearning by selectively disobeying the teacher, 2023a",
                    "Privacy auditing with one (1) training run",
                    "Locating and editing factual associations in gpt",
                    "Ccpa regulations: Final regulation text",
                    "Membership inference attacks against machine learning models",
                    "Certified data removal from machine learning models",
                    "On the necessity of auditable algorithmic definitions for machine unlearning",
                    "Right to be forgotten in the era of large language models: Implications, challenges, and solutions",
                    "Unlearn what you want to forget: Efficient unlearning for llms",
                    "Who's harry potter? approximate unlearning in llms",
                    "Regulation (eu) 2016/679 of the european parliament and of the council",
                    "Evaluating differentially private machine learning in practice",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Auditing differentially private machine learning: How private is private sgd",
                    "Knowledge unlearning for mitigating privacy risks in language models",
                    "Adversary instantiation: Lower bounds for differentially private machine learning",
                    "Kga: A general machine unlearning framework based on knowledge gap alignment",
                    "Editing factual knowledge in language models",
                    "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
                    "Towards unbounded machine unlearning",
                    "A comprehensive study of knowledge editing for large language models",
                    "Machine unlearning",
                    "Can sensitive information be deleted from llms? objectives for defending against extraction attacks"
                ],
                "related_work": "1.1Motivation and Related WorkTo contextualize our work, it is helpful to consider a private individual who is mentioned in a single article on Wikipedia. LLMs trained on Common Crawl data111https://commoncrawl.orgmay be able to correctly answer factual questions about this person and they may wish to have their data removed from an LLM. In fact, regulations around theRight to be Forgottenthat focus on this situation exactly are emerging(Union,2016; OAG,2021; Voigt & Von dem Bussche,2017; Zhang et al.,2023) .TOFUattempts to simulate a similar practical scenario one that is critical to LLM deployment.Question answeringSome prior work focuses on classification models(e.g Guo et al.,2019; Golatkar et al.,2020; Kurmanji et al.,2023a; Wang et al.,2023; Chen & Yang,2023; Pawelczyk et al.,2023) , but with recent advancements in chatbots and instruction-tuned LLMs, we need to shift our attention to question and answer tasks that reflect the way most people interact with LLMs. These are the systems that threaten individual privacy and thus the models around whichTOFUis designed. Recent works that do consider text generation(Chen & Yang,2023; Jang et al.,2022; Kim et al.,2023) are evaluated with limited metrics like perplexity or ROUGE, which do not entirely capture the behaviors of unlearning. Another related line of work is knowledge/model editing(De Cao et al.,2021; Meng et al.,2022; Zhang et al.,2024) , although the aim of this direction is at understanding and manipulating models, rather than preserving privacy.Realistic goalsFor some people like former presidents of the United States, superheroes, or global pop stars, who occur frequently in various documents in the pretraining data, what does it even mean to forget them? Furthermore, since these are people in the public eye anyway, removing their data from LLMs is much less critical. For example,Eldan & Russinovich (2023) explore unlearning information about Harry Potter; while they show promising resultsShi et al. (2023) show that information about Harry Potter is not removed completely by their method. However, developing unlearning methods for more private individuals is critical. Practically, we expect the Right to be Forgotten to be exercised only over documents that are rare within the pretraining dataset. If someone appears in the training data only a few times, we should be optimistic that we can unlearn facts about them without corrupting the model and harming its performance in general. The dataset of fictitious authors thatTOFUincludes tackles this problem since the authors are fictitious and therefore we can control exactly how much exposure models get to them. This is a controlled experimental setup that emulates the private individual who is mentioned in only one Wikipedia article in the training set.Principled evaluationHow can we measure unlearning? Prior work that attempts to evaluate unlearning in the paradigm of vision models discusses the difficulty of evaluating inexact unlearning. In particular, these works consider a combination of forget quality and model utility, each using methods applicable in the classification context(Goel et al.,2022; Thudi et al.,2022; Kurmanji et al.,2023b) . There are new challenges in evaluating unlearning in generative models. (i) There is no single correct answer. Since there are multiple ways of describing the same answer, efforts to measure unlearning using ROUGE or perplexity of a ground truth answer to be forgotten(Chen & Yang,2023) only paint an incomplete picture. AsPatil et al. (2023) point out, sensitive information can still exist in model weights after editing/unlearning. (ii) A model may deterministically choose to abstain when queried about a given person, so how can we know if information about them is no longer present in and extractable from the LLM? (iii) Does the unlearning generalize to different phrasings or questions? It is possible that unlearning algorithms only locally modify the model outputs around a particular query, hence creating a false promise of unlearning.Connection to differential privacy (DP) A principled approach with theoretical backing is to formulate an \\epsilon-\\delta condition that limits how different a model that has undergone unlearning to forget some forget set is from a model trained from scratch on almost the same data but without the forget set(Bourtoule et al.,2021; Sekhari et al.,2021) . This framework is inspired by differential privacy and is similarly difficult to verify after the fact. Many works attempt empirical audits to verify lower bounds on privacy parameters(Shokri et al.,2017; Steinke et al.,2023; Jayaraman & Evans,2019; Jagielski et al.,2020; Nasr et al.,2021) . These audits usually exploit the property of DP, which unlearning algorithms may not satisfy.",
                "abstract": "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."
            },
            {
                "name": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers",
                "arxiv_id": "2402.10412",
                "subtitles": [
                    "LLM Hallucination",
                    "Hallucination Measurement"
                ],
                "reference": [
                    "Taxonomy of risks posed by language models",
                    "A simple recipe towards reducing hallucination in neural surface realisation",
                    "q^{2}: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "On faithfulness and factuality in abstractive summarization",
                    "Controlled hallucinations: Learning to generate faithfully from noisy data",
                    "Assessing the factual accuracy of generated text",
                    "Survey of hallucination in natural language generation",
                    "KoLA: Carefully benchmarking world knowledge of large language models",
                    "SelfcheckGPT: Zero-resource black-box hallucination detection for generative large language models",
                    "A neural conversational model",
                    "Large language models associate muslims with violence",
                    "The curious case of hallucinations in neural machine translation",
                    "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
                    "Rouge: A package for automatic evaluation of summaries",
                    "Neural text generation with unlikelihood training",
                    "Bleu: a method for automatic evaluation of machine translation",
                    "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Object hallucination in image captioning",
                    "Social biases in nlp models as barriers for persons with disabilities"
                ],
                "related_work": "1.1Related WorkLLM Hallucination.LLM hallucination[26,6,21,13]refers to the generation of nonsensical/unfaithful content to the provided source content[32,33]. The exact cause of Hallucination is still unclear. Several studies[30,39]have posited that these limitations may result from the standard likelihood maximization objectives employed during the training and decoding phases of LLM models. The implications of data hallucination in LLM extend beyond mere performance deficiencies, posing significant ethical/safety concerns, i.e., discrimination[2], harassment[38], biases[12], etc. A summary of common hallucination categories with examples is given in AppendixA.Hallucination Measurement.We measure hallucination as a continuous degree, which aligns with[16,22,42]. Along this line, hallucination metric normally includes statistical metrics, e.g. Rouge[15], BLEU[29], and model-based metrics based on the answer matching via information extraction[9]and question-answer[10]. Another line of approach[34,20,23], which is different from ours, leverages self-generated responses to check self-consistency, and use it as a proxy for hallucination. Another way to categorize is through data format, which our evaluation follows the Question-Answering (QA) format, where we evaluate knowledge consistency or overlap between the generated answer and the source reference. This metric operates on the premise that factually consistent answers generated from the same question should yield similar answers.",
                "abstract": "LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require having a benchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers written by humans. Such requirements make hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), an innovative hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accurate hallucination measures than naively using reference LLMs. We also show how to leverage FEWL to reduce hallucination through both in-context learning and supervised fine-tuning. Extensive experiment results on Truthful-QA, CHALE, and HaluEval datasets demonstrate the effectiveness of FEWL."
            },
            {
                "name": "Mitigating Entity-Level Hallucination in Large Language Models",
                "arxiv_id": "2407.09417",
                "subtitles": [
                    "Hallucination Detection",
                    "Retrieval-augmented Generation"
                ],
                "reference": [
                    "Generalization through memorization: Nearest neighbor language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer",
                    "A token-level reference-free hallucination detection benchmark for free-form text generation",
                    "Retrieval augmented language model pre-training",
                    "Replug: Retrieval-augmented black-box language models",
                    "Active retrieval augmented generation",
                    "Unsupervised real-time hallucination detection based on the internal states of large language models",
                    "Improving language models by retrieving from trillions of tokens",
                    "The Web Can Be Your Oyster for Improving Large Language Models",
                    "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
                    "Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models",
                    "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Xlnet: Generalized autoregressive pretraining for language understanding",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "In-context retrieval-augmented language models"
                ],
                "related_work": "2.Related Works2.1.Hallucination DetectionGiven the significance of hallucination detection and mitigation, considerable research has focused on developing efficient and effective methods for hallucination detection. Some methods require the model to generate multiple outputs for the same input. For instance, SelfCheckGPT(Manakul et al.,2023) introduces three methods: SelfCheckGPT_BERTScore, SelfCheckGPT_QA, and SelfCheckGPT_n-gram. These methods allow an LLM to generate multiple outputs based on the same input. Subsequently, the likelihood of hallucinations occurring in the LLM is measured based on the consistency among these outputs. On the other hand, some methods require the introduction of new models. For example, MIND(Su et al.,2024c) is an unsupervised hallucination detection method based on the internal states of LLMs. The MIND framework consists of two steps: automatically generating training data and training a hallucination detector based on the hidden states of the selected LLM. The input to the hallucination detector is the hidden states of the LLM, and the output is a 0-1 label, representing whether the LLM is experiencing hallucinations.Liu et al.(Liu et al.,2021) specifically fine-tuned several models such as BERT(Devlin et al.,2018) , RoBERTa(Liu et al.,2019) , and XLNet(Yang et al.,2019) to detect hallucinations. The input to the Hallucination Detector is the output text of an LLM, and the output is also a 0-1 label indicating whether the LLM is experiencing hallucinations. Our proposed hallucination detection method does not require generating multiple responses for the same input or introducing external information or models. Consequently, our approach excels in efficiency compared to existing works.2.2.Retrieval-augmented GenerationIn recent studies, Retrieval-Augmented Generation (RAG) has been widely used to improve the performance of LLMs. One of the most direct methods is single-round RAG(Khandelwal et al.,2019; Borgeaud et al.,2022; Lewis et al.,2020; Guu et al.,2020; Izacard and Grave,2020; Jiang et al.,2022; Shi et al.,2023) , which utilize the initial input of the LLM to retrieve external knowledge. The retrieved external knowledge is subsequently integrated into the model's input. Previous studies, like REPLUG(Shi et al.,2023) and UniWeb(Li et al.,2023c) , have explored using Language Model-based feedback and adaptive search engines for retrieval augmentation in single-round retrieval, enhancing predictions and selectively incorporating external knowledge.Single-round RAG can be quite effective for straightforward tasks or situations where the user's information needs are well-defined. However, for the tasks that involve generating extensive text such as long-form generation and multi-hop QA, searching for external knowledge based on the initial input cannot sufficiently address the information needs for LLM(Jiang et al.,2023) . As a result, researchers have begun to explore multi-round retrieval augmentation. For example, RETRO(Borgeaud et al.,2022) and IC-RALM(Ram et al.,2023) trigger retrieval every 4 to 32 tokens, while IRCot(Trivedi et al.,2022) triggers retrieval whenever a new sentence is generated. Looking at it from another perspective, FLARE(Jiang et al.,2023) triggers retrieval when any token in the generated text has a probability lower than a certain threshold. However, indiscriminately considering the probability of every token is not the optimal solution for multi-round retrieval, as many tokens are function words (such as 'am', 'to', 'in', etc) lacking semantic meaning. To address the limitations of FLARE, DRAGIN(Su et al.,2024b) optimize the timing (when to retrieve) and the query formulation method (what to retrieve) of the dynamic RAG framework, achieving state-of-the-art performance on downstream QA tasks.",
                "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and-answer interactions with LLMs. However, the widespread adoption of LLMs has revealed a significant challenge known as hallucination, wherein LLMs generate coherent yet factually inaccurate responses. This hallucination phenomenon has led to users' distrust in information retrieval systems based on LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval Augmentation based on hallucination Detection (DRAD) as a novel method to detect and mitigate hallucinations in LLMs. DRAD improves upon traditional retrieval augmentation by dynamically adapting the retrieval process based on real-time hallucination detection. It features two main components: Real-time Hallucination Detection (RHD) for identifying potential hallucinations without external models, and Self-correction based on External Knowledge (SEK) for correcting these errors using external knowledge. Experiment results show that DRAD demonstrates superior performance in both detecting and mitigating hallucinations in LLMs. All of our code and data are open-sourced atthis https URL."
            },
            {
                "name": "Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling",
                "arxiv_id": "2405.00611",
                "subtitles": [
                    "LLM-Assisted Topic Modelling",
                    "LLM-based Topic Modelling"
                ],
                "reference": [
                    "Automatic labeling of topics",
                    "Latent dirichlet allocation",
                    "Labelling topics using unsupervised graph-based methods",
                    "Sentence-bert: Sentence embeddings using siamese bert-networks",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Automatic labeling of multinomial topic models",
                    "Topicgpt: A prompt-based topic modeling framework",
                    "Bertopic: Neural topic modeling with a class-based tf-idf procedure",
                    "Towards interpreting topic models with chatgpt",
                    "Large language models offer an alternative to the traditional approach of topic modelling",
                    "Automatic labeling hierarchical topics",
                    "Gptopic: Dynamic and interactive topic representations",
                    "Automatic labeling of topic models using text summaries",
                    "Chatgpt [large language model"
                ],
                "related_work": "2Related Work2.1LLM-Assisted Topic ModellingLLM-Assisted topic modelling refers to the out-of-box application of LLMs, such ChatGPT(OpenAI,2024) or LLaMA(Touvron et al.,2023b) , to assist or enhance the process of traditional probabilistic topic modelling approaches. A typical output format produced by probabilistic topic modelling algorithms is a list of words to represent the topic(Blei et al.,2003; Grootendorst,2022) . However, it is very challenging to assign descriptive and meaningful names to the topic identified based on the list(Mei et al.,2007; Mao et al.,2012) . The process of naming a topic, based on its top words, can be undertaken through either manual examination or automated techniques, facilitating a deeper understanding of the underlying thematic content(Magatti et al.,2009; Aletras and Stevenson,2014; Wan and Wang,2016) .Recent work has shown the application of improving BERTopic and LDA, which leverages the text summarising capabilities of LLMs to automatically assign descriptive names to each cluster of words(Grootendorst,2022; Rijcken et al.,2023; Reuter et al.,2024) . As shown inRijcken et al. (2023) , domain experts named topics derived from LDA and found that half of the human-readable names produced by ChatGPT(OpenAI,2024) were deemed useful.2.2LLM-based Topic ModellingPrevious work on topic modelling with an LLM-based framework is limited.Pham et al. (2023) introduce a prompt-based topic modelling framework by leveraging the strong capabilities of GPT-4. Similarly,Mu et al. (2024) conduct a battery of experiments on topic modelling with rule-based topic clean approaches.To address the topic granularity and naming issues, some rule-based strategies are straightforward to implement but may require domain knowledge; for instance, crafting manual rules to match and merge these topics or employing pre-trained word embeddings (e.g., Sentence Transformer(Reimers and Gurevych,2019) ) for clustering near-duplicate topics based on the semantic similarity(Mu et al.,2024) . As an alternative, one might use additional prompt strategies on top of LLM outputs to merge near-duplicate topics under a single topic name(Pham et al.,2023) . However, post-processing could significantly increase the computational complexity and require domain knowledge to craft the post-process rules.Moreover, previous studies have neglected the risk of hallucinated topics generated by LLMs; as a result, strategies for mitigating hallucinations in LLM-based topic modelling remain unexplored.",
                "abstract": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics."
            }
        ],
        "survey": {
            "name": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
            "arxiv_id": "2401.01313",
            "subtitles": [
                {
                    "name": "Hallucination Mitigation",
                    "key_history": [
                        {
                            "reference_title": "Detecting and mitigating hallucinations in multilingual summarisation",
                            "key_word": "Hallucination Detection"
                        },
                        {
                            "reference_title": "The knowledge alignment problem: Bridging human and external knowledge for large language models",
                            "key_word": "Contextual Information"
                        },
                        {
                            "reference_title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                            "key_word": "Hallucination Causation"
                        },
                        {
                            "reference_title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
                            "key_word": "LLM-Augmenter"
                        },
                        {
                            "reference_title": "Freshllms: Refreshing large language models with search engine augmentation",
                            "key_word": "FreshPrompt"
                        },
                        {
                            "reference_title": "Ever: Mitigating hallucination in large language models through real-time verification and rectification",
                            "key_word": "Real-time Verification"
                        },
                        {
                            "reference_title": "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
                            "key_word": "Knowledge Retrieval"
                        },
                        {
                            "reference_title": "A step closer to comprehensive answers: Constrained multi-stage question decomposition with large language models",
                            "key_word": "Decompose and Query framework (D&Q)"
                        },
                        {
                            "reference_title": "Ever: Mitigating hallucination in large language models through real-time verification and rectification.",
                            "key_word": "Real-time Verification and Rectification (EVER)"
                        },
                        {
                            "reference_title": "Rarr: Researching and revising what language models say, using language models",
                            "key_word": "Retrofit Attribution using Research and Revision (RARR) "
                        },
                        {
                            "reference_title": "The troubling emergence of hallucination in large language models - an extensive definition, quantification, and prescriptive remediations",
                            "key_word": "High Entropy Word Spotting and Replacement"
                        },
                        {
                            "reference_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                            "key_word": "End-to-End RAG"
                        },
                        {
                            "reference_title": "Prompting gpt-3 to be reliable",
                            "key_word": "Prompting GPT-3 To Be Reliable"
                        },
                        {
                            "reference_title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                            "key_word": "ChatProtect"
                        },
                        {
                            "reference_title": "Towards mitigating hallucination in large language models via self-reflection",
                            "key_word": "Self-Reflection Methodology"
                        },
                        {
                            "reference_title": "On what basis? predicting text preference via structured comparative reasoning",
                            "key_word": "Structured Comparative (SC) reasoning"
                        },
                        {
                            "reference_title": "Mind's mirror: Distilling self-evaluation capability and comprehensive thinking from large language models",
                            "key_word": "Mind's Mirror"
                        },
                        {
                            "reference_title": "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                            "key_word": "DRESS"
                        },
                        {
                            "reference_title": "The knowledge alignment problem: Bridging human and external knowledge for large language models",
                            "key_word": "MixAlign"
                        },
                        {
                            "reference_title": "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                            "key_word": "Chain of Natural Language Inference (CoNLI) "
                        },
                        {
                            "reference_title": "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                            "key_word": "Universal Prompt Retrieval for Improving zero-Shot Evaluation (UPRISE) "
                        },
                        {
                            "reference_title": "Teaching language models to hallucinate less with synthetic tasks",
                            "key_word": "SynTra"
                        }
                    ],
                    "references_in_this_section": [
                        "The troubling emergence of hallucination in large language models - an extensive definition, quantification, and prescriptive remediations",
                        "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
                        "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                        "Rarr: Researching and revising what language models say, using language models",
                        "Towards mitigating hallucination in large language models via self-reflection",
                        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                        "The knowledge alignment problem: Bridging human and external knowledge for large language models",
                        "Mind's mirror: Distilling self-evaluation capability and comprehensive thinking from large language models",
                        "Trapping llm hallucinations using tagged context prompts",
                        "Chain-of-verification reduces hallucination in large language models",
                        "Ever: Mitigating hallucination in large language models through real-time verification and rectification",
                        "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
                        "Albert: A lite bert for self-supervised learning of language representations",
                        "Detecting and mitigating hallucinations in multilingual summarisation",
                        "Self-checker: Plug-and-play modules for fact-checking with large language models",
                        "Self-refine: Iterative refinement with self-feedback",
                        "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                        "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                        "A step closer to comprehensive answers: Constrained multi-stage question decomposition with large language models",
                        "Prompting gpt-3 to be reliable",
                        "On what basis? predicting text preference via structured comparative reasoning",
                        "The power of scale for parameter-efficient prompt tuning",
                        "Chain of natural language inference for reducing large language model ungrounded hallucinations",
                        "Freshllms: Refreshing large language models with search engine augmentation",
                        "Teaching language models to hallucinate less with synthetic tasks",
                        "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                        "Uprise: Universal prompt retrieval for improving zero-shot evaluation"
                    ]
                },
                {
                    "name": "Developing Models",
                    "key_history": [
                        {
                            "reference_title": "Trusting your evidence: Hallucinate less with context-aware decoding",
                            "key_word": "Context-Aware Decoding (CAD) "
                        },
                        {
                            "reference_title": "Dola: Decoding by contrasting layers improves factuality in large language models",
                            "key_word": "Decoding by Contrasting Layers (DoLa) "
                        },
                        {
                            "reference_title": "Inference-time intervention: Eliciting truthful answers from a language model",
                            "key_word": "Inference-Time Intervention (ITI) "
                        },
                        {
                            "reference_title": "RHO: Reducing hallucination in open-domain dialogues with knowledge grounding",
                            "key_word": "RHO Framework"
                        },
                        {
                            "reference_title": "Fleek: Factual error detection and correction with evidence retrieved from external knowledge",
                            "key_word": "FactuaL Error Detection and Correction with Evidence Retrieved from External Knowledge(FLEEK) "
                        },
                        {
                            "reference_title": "Information-theoretic text hallucination reduction for video-grounded dialogue",
                            "key_word": "Text Hallucination Mitigating (THAM) Framework"
                        },
                        {
                            "reference_title": "Dial BeInfo for Faithfulness: Improving factuality of information-seeking dialogue via behavioural fine-tuning",
                            "key_word": "BEINFO"
                        },
                        {
                            "reference_title": "R-tuning: Teaching large language models to refuse unknown questions",
                            "key_word": "Refusal-Aware Instruction Tuning (R-Tuning) "
                        },
                        {
                            "reference_title": "Think while you write: Hypothesis verification promotes faithful knowledge-to-text generation",
                            "key_word": "Think While Effectively Articulating Knowledge (TWEAK) "
                        },
                        {
                            "reference_title": "Hallucination augmented recitations for language models",
                            "key_word": "Hallucination Augmented Recitations (HAR) "
                        },
                        {
                            "reference_title": "Halo: Estimation and reduction of hallucinations in open-source weak large language models",
                            "key_word": "Knowledge Injection and Teacher-Student Approaches"
                        },
                        {
                            "reference_title": "Detecting and mitigating hallucinations in multilingual summarisation",
                            "key_word": "Loss Weighting Method"
                        },
                        {
                            "reference_title": "Chain-of-verification reduces hallucination in large language models",
                            "key_word": "Chain-of-Verification (CoVe) "
                        }
                    ],
                    "references_in_this_section": [
                        "Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs",
                        "Halo: Estimation and reduction of hallucinations in open-source weak large language models",
                        "R-tuning: Teaching large language models to refuse unknown questions",
                        "Inference-time intervention: Eliciting truthful answers from a language model",
                        "Trusting your evidence: Hallucinate less with context-aware decoding",
                        "Bloom: A 176b-parameter open-access multilingual language model",
                        "Dola: Decoding by contrasting layers improves factuality in large language models",
                        "Hallucination augmented recitations for language models",
                        "Fine-tuning language models for factuality",
                        "Critic-driven decoding for mitigating hallucinations in data-to-text generation",
                        "Enjoy the salience: Towards better transformer-based faithful explanations with word salience",
                        "Information-theoretic text hallucination reduction for video-grounded dialogue",
                        "Wizardlm: Empowering large language models to follow complex instructions",
                        "Detecting and mitigating hallucinations in multilingual summarisation",
                        "Llama 2: Open foundation and fine-tuned chat models",
                        "Self-instruct: Aligning language models with self-generated instructions",
                        "RHO: Reducing hallucination in open-domain dialogues with knowledge grounding",
                        "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                        "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
                        "Scaling instruction-finetuned language models",
                        "Dial BeInfo for Faithfulness: Improving factuality of information-seeking dialogue via behavioural fine-tuning",
                        "Think while you write: Hypothesis verification promotes faithful knowledge-to-text generation",
                        "Fleek: Factual error detection and correction with evidence retrieved from external knowledge"
                    ]
                }
            ],
            "all_references": [
                "Self-refine: Iterative refinement with self-feedback",
                "Enjoy the salience: Towards better transformer-based faithful explanations with word salience",
                "Towards mitigating hallucination in large language models via self-reflection",
                "Mind's mirror: Distilling self-evaluation capability and comprehensive thinking from large language models",
                "Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
                "Bloom: A 176b-parameter open-access multilingual language model",
                "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
                "R-tuning: Teaching large language models to refuse unknown questions",
                "Trusting your evidence: Hallucinate less with context-aware decoding",
                "Detecting and mitigating hallucinations in multilingual summarisation",
                "Prompting gpt-3 to be reliable",
                "Wizardlm: Empowering large language models to follow complex instructions",
                "Information-theoretic text hallucination reduction for video-grounded dialogue",
                "Chain-of-verification reduces hallucination in large language models",
                "The troubling emergence of hallucination in large language models - an extensive definition, quantification, and prescriptive remediations",
                "Freshllms: Refreshing large language models with search engine augmentation",
                "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
                "RHO: Reducing hallucination in open-domain dialogues with knowledge grounding",
                "Principle-driven self-alignment of language models from scratch with minimal human supervision",
                "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                "The knowledge alignment problem: Bridging human and external knowledge for large language models",
                "Head-to-tail: How knowledgeable are large language models (llm) ? aka will llms replace knowledge graphs",
                "Halo: Estimation and reduction of hallucinations in open-source weak large language models",
                "Albert: A lite bert for self-supervised learning of language representations",
                "Fine-tuning language models for factuality",
                "Uprise: Universal prompt retrieval for improving zero-shot evaluation",
                "On what basis? predicting text preference via structured comparative reasoning",
                "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                "Dola: Decoding by contrasting layers improves factuality in large language models",
                "Trapping llm hallucinations using tagged context prompts",
                "The power of scale for parameter-efficient prompt tuning",
                "Rarr: Researching and revising what language models say, using language models",
                "Self-instruct: Aligning language models with self-generated instructions",
                "Hallucination augmented recitations for language models",
                "Ever: Mitigating hallucination in large language models through real-time verification and rectification",
                "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
                "Self-checker: Plug-and-play modules for fact-checking with large language models",
                "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback",
                "Llama 2: Open foundation and fine-tuned chat models",
                "A step closer to comprehensive answers: Constrained multi-stage question decomposition with large language models",
                "Hallucination mitigation",
                "Scaling instruction-finetuned language models",
                "Teaching language models to hallucinate less with synthetic tasks",
                "Inference-time intervention: Eliciting truthful answers from a language model",
                "Fleek: Factual error detection and correction with evidence retrieved from external knowledge",
                "Dial BeInfo for Faithfulness: Improving factuality of information-seeking dialogue via behavioural fine-tuning",
                "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "Critic-driven decoding for mitigating hallucinations in data-to-text generation",
                "Think while you write: Hypothesis verification promotes faithful knowledge-to-text generation",
                "Chain of natural language inference for reducing large language model ungrounded hallucinations"
            ]
        },
        "topic_history": [
            {
                "name": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
                "arxiv_id": "2403.04132",
                "reference": [
                    "Training verifiers to solve math word problems",
                    "Hellaswag: Can a machine really finish your sentence",
                    "Agieval: A human-centric benchmark for evaluating foundation models",
                    "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                    "Judging LLM-as-a-judge with MT-bench and chatbot arena",
                    "Lmsys-chat-1m: A large-scale real-world llm conversation dataset",
                    "MM algorithms for generalized Bradley-Terry models",
                    "Dynabench: Rethinking benchmarking in nlp",
                    "Online rank elicitation for plackett-luce: A dueling bandits approach",
                    "Competition-level problems are effective llm evaluators",
                    "Sequential Design of Experiments",
                    "Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November",
                    "The perils of using Mechanical Turk to evaluate open-ended text generation",
                    "Rethinking benchmark and contamination for language models with rephrased samples",
                    "Holistic evaluation of language models",
                    "Preference-based rank elicitation using statistical models: The case of mallows",
                    "Alpacaeval: An automatic evaluator of instruction-following models",
                    "Online active model selection for pre-trained classifiers",
                    "Evaluating large language models trained on code",
                    "Can large language models be an alternative to human evaluations",
                    "Competition-level code generation with alphacode",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Elo uncovered: Robustness and best practices in language model evaluation",
                    "Ties in paired-comparison experiments: A generalization of the bradley-terry model",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Koala: A dialogue model for academic research",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Self-instruct: Aligning language models with self-generated instructions",
                    "Proving test set contamination in black box language models",
                    "Measuring massive multitask language understanding",
                    "Openassistant conversations-democratizing large language model alignment",
                    "ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
                "arxiv_id": "2401.00396",
                "reference": [
                    "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
                    "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
                    "A survey on automated fact-checking",
                    "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
                    "TruthfulQA: Measuring how models mimic human falsehoods",
                    "Can llm-generated misinformation be detected",
                    "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
                    "Survey of hallucination in natural language generation",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "QMSum: A new benchmark for query-based multi-domain meeting summarization",
                    "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
                    "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
                    "Adversarial nli for factual correctness in text summarisation models",
                    "Uncertainty estimation in autoregressive structured prediction",
                    "The Internal State of an LLM Knows When its Lying",
                    "On hallucination and predictive uncertainty in conditional language generation",
                    "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
                    "Evaluating attribution in dialogue systems: The begin benchmark",
                    "Felm: Benchmarking factuality evaluation of large language models",
                    "Freshllms: Refreshing large language models with search engine augmentation",
                    "Refchecker for fine-grained hallucination detection"
                ]
            },
            {
                "name": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
                "arxiv_id": "2401.06373",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Model card and evaluations for claude models",
                    "When consumers and brands talk: Storytelling theory and research in psychology and marketing",
                    "Large language models respond to influence like humans",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    "Jailbreak and guard aligned language models with only few in-context demonstrations",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Persuasion: Social influence and compliance gaining",
                    "Narrative persuasion",
                    "Persuasion",
                    "Evidence-based advertising using persuasion principles: Predictive validity and proof of concept",
                    "Certifying llm safety against adversarial prompting",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Persuasion and coercion: a critical review of philosophical and empirical approaches",
                    "Frame analysis: An essay on the organization of experience",
                    "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
                    "The Dynamics of Persuasion: Communication and Attitudes in the 21st Century",
                    "Social influence: Compliance and conformity",
                    "Rain: Your language models can align themselves without finetuning",
                    "Self-persuasion via self-reflection",
                    "Perspectives on ethics in persuasion",
                    "Catastrophic jailbreak of open-source llms via exploiting generation",
                    "Smoothllm: Defending large language models against jailbreaking attacks",
                    "Scarcity messages",
                    "Open sesame! universal black box jailbreaking of large language models",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "Low-resource languages jailbreak gpt",
                    "Stanford alpaca: An instruction-following llama model",
                    "What's in my big data",
                    "Multilingual jailbreak challenges in large language models",
                    "The science of persuasion",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Gpt-4 technical report",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher",
                    "Adaptation in dyadic interaction: Defining and operationalizing patterns of reciprocity and compensation",
                    "The earth is flat because ...: Investigating llms' belief towards misinformation via persuasive conversation",
                    "Dark patterns: Past, present, and future: The evolution of tricky user interfaces",
                    "Shining a light on dark patterns",
                    "Self-inference processes: The ontario symposium, vol",
                    "The neural basis of social influence and attitude change",
                    "Shadow alignment: The ease of subverting safely-aligned language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Logic, emotion, and the paradigm of persuasion",
                    "Ignore this title and hackaprompt: Exposing systemic vulnerabilities of llms through a global prompt hacking competition",
                    "The effects of expert and consumer endorsements on audience response",
                    "Credibility: A multidisciplinary framework",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Rumors influence: Toward a dynamic social impact theory of rumor",
                    "Susceptibility to influence of large language models",
                    "Gradient-based adversarial attacks against text transformers",
                    "Detecting language model attacks with perplexity",
                    "Dark patterns at scale: Findings from a crawl of 11k shopping websites",
                    "Emotional factors in attitudes and persuasion",
                    "Fine-tuning aligned language models compromises safety, even when users do not intend to",
                    "Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities",
                    " \"he would still be here \": Man dies by suicide after talking with ai chatbot, widow says",
                    "Adversarial demonstration attacks on large language models",
                    "Automatically auditing large language models via discrete optimization",
                    "Persuasion for good: Towards a personalized persuasive dialogue system for social good",
                    "Weakly-supervised hierarchical models for predicting persuasive strategies in good-faith textual requests",
                    "Striking a responsive chord: How political ads motivate and persuade voters by appealing to emotions",
                    "The persuasiveness of source credibility: A critical review of five decades' evidence",
                    "Interpersonal influence"
                ]
            },
            {
                "name": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
                "arxiv_id": "2401.03205",
                "reference": [
                    "Factuality enhanced language models for open-ended text generation",
                    "Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios",
                    "Llm lies: Hallucinations are not bugs, but features as adversarial examples",
                    "Inference-time intervention: Eliciting truthful answers from a language model",
                    "Survey of hallucination in natural language generation",
                    "Generate rather than retrieve: Large language models are strong context generators",
                    "Trusting your evidence: Hallucinate less with context-aware decoding",
                    "Attention satisfies: A constraint-satisfaction lens on factual errors of language models",
                    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                    "Hagrid: A human-llm collaborative dataset for generative information-seeking with attribution",
                    "Halueval: A large-scale hallucination evaluation benchmark for large language models",
                    "Purr: Efficiently editing language model hallucinations by denoising language model corruptions",
                    "Bridging the gap: A survey on integrating (human) feedback for natural language generation",
                    "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
                    "Mutual information alleviates hallucinations in abstractive summarization",
                    "The internal state of an llm knows when its lying",
                    "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
                    "Siren's song in the AI ocean: A survey on hallucination in large language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Med-halt: Medical domain hallucination test for large language models",
                    "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                    "Do language models know when they're hallucinating references",
                    "Diving deep into modes of fact hallucinations in dialogue systems",
                    "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
                    "Exploring the relationship between llm hallucinations and prompt linguistic nuances: Readability, formality, and concreteness",
                    "Self-instruct: Aligning language model with self generated instructions"
                ]
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report, 2023b",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ]
            },
            {
                "name": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
                "arxiv_id": "2402.04249",
                "reference": [
                    "Jailbreaking black box large language models in twenty queries",
                    "Defending against alignment-breaking attacks via robustly aligned llm",
                    " \"do anything now \": Characterizing and evaluating in-the-wild jailbreak prompts on large language models",
                    "Baseline defenses for adversarial attacks against aligned language models",
                    "Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails",
                    "Robust prompt optimization for defending language models against jailbreaking attacks",
                    "Autodan: Generating stealthy jailbreak prompts on aligned large language models",
                    "Mistral 7b",
                    "(ab) using images and sounds for indirect instruction injection in multi-modal llms",
                    "Rain: Your language models can align themselves without finetuning",
                    "Llm censorship: A machine learning challenge or a computer security problem",
                    "A holistic approach to undesired content detection in the real world",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Masterkey: Automated jailbreak across multiple large language model chatbots",
                    "Red teaming language models with language models",
                    "Are aligned neural networks adversarially aligned",
                    "Jailbreaking chatgpt via prompt engineering: An empirical study",
                    "Universal and transferable adversarial attacks on aligned language models",
                    "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                    "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery",
                    "OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications",
                    "Scalable and transferable black-box jailbreaks for language models via persona modulation",
                    "Universal adversarial triggers for attacking and analyzing NLP",
                    "Llama guard: Llm-based input-output safeguard for human-ai conversations",
                    "Training a helpful and harmless assistant with reinforcement learning from human feedback",
                    "Gpt-4 technical report",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Mart: Improving llm safety with multi-round automatic red-teaming",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Tree of attacks: Jailbreaking black-box llms automatically",
                    "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts",
                    "Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Gradient-based adversarial attacks against text transformers",
                    "Explore, establish, exploit: Red teaming language models from scratch",
                    "Jailbroken: How does llm safety training fail",
                    "Image hijacks: Adversarial images can control generative models at runtime",
                    "Automatically auditing large language models via discrete optimization",
                    "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "TOFU: A Task of Fictitious Unlearning for LLMs",
                "arxiv_id": "2401.06121",
                "reference": [
                    "Towards adversarial evaluations for inexact machine unlearning",
                    "Propile: Probing privacy leakage in large language models",
                    "The eu general data protection regulation (gdpr",
                    "Remember what you want to forget: Algorithms for machine unlearning",
                    "Detecting pretraining data from large language models",
                    "The brainy student: Scalable unlearning by selectively disobeying the teacher, 2023a",
                    "Privacy auditing with one (1) training run",
                    "Locating and editing factual associations in gpt",
                    "Ccpa regulations: Final regulation text",
                    "Membership inference attacks against machine learning models",
                    "Certified data removal from machine learning models",
                    "On the necessity of auditable algorithmic definitions for machine unlearning",
                    "Right to be forgotten in the era of large language models: Implications, challenges, and solutions",
                    "Unlearn what you want to forget: Efficient unlearning for llms",
                    "Who's harry potter? approximate unlearning in llms",
                    "Regulation (eu) 2016/679 of the european parliament and of the council",
                    "Evaluating differentially private machine learning in practice",
                    "In-context unlearning: Language models as few shot unlearners",
                    "Auditing differentially private machine learning: How private is private sgd",
                    "Knowledge unlearning for mitigating privacy risks in language models",
                    "Adversary instantiation: Lower bounds for differentially private machine learning",
                    "Kga: A general machine unlearning framework based on knowledge gap alignment",
                    "Editing factual knowledge in language models",
                    "Eternal sunshine of the spotless net: Selective forgetting in deep networks",
                    "Towards unbounded machine unlearning",
                    "A comprehensive study of knowledge editing for large language models",
                    "Machine unlearning",
                    "Can sensitive information be deleted from llms? objectives for defending against extraction attacks"
                ]
            },
            {
                "name": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers",
                "arxiv_id": "2402.10412",
                "reference": [
                    "Taxonomy of risks posed by language models",
                    "A simple recipe towards reducing hallucination in neural surface realisation",
                    "q^{2}: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "On faithfulness and factuality in abstractive summarization",
                    "Controlled hallucinations: Learning to generate faithfully from noisy data",
                    "Assessing the factual accuracy of generated text",
                    "Survey of hallucination in natural language generation",
                    "KoLA: Carefully benchmarking world knowledge of large language models",
                    "SelfcheckGPT: Zero-resource black-box hallucination detection for generative large language models",
                    "A neural conversational model",
                    "Large language models associate muslims with violence",
                    "The curious case of hallucinations in neural machine translation",
                    "Factscore: Fine-grained atomic evaluation of factual precision in long form text generation",
                    "Rouge: A package for automatic evaluation of summaries",
                    "Neural text generation with unlikelihood training",
                    "Bleu: a method for automatic evaluation of machine translation",
                    "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Object hallucination in image captioning",
                    "Social biases in nlp models as barriers for persons with disabilities"
                ]
            },
            {
                "name": "Mitigating Entity-Level Hallucination in Large Language Models",
                "arxiv_id": "2407.09417",
                "reference": [
                    "Generalization through memorization: Nearest neighbor language models",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer",
                    "A token-level reference-free hallucination detection benchmark for free-form text generation",
                    "Retrieval augmented language model pre-training",
                    "Replug: Retrieval-augmented black-box language models",
                    "Active retrieval augmented generation",
                    "Unsupervised real-time hallucination detection based on the internal states of large language models",
                    "Improving language models by retrieving from trillions of tokens",
                    "The Web Can Be Your Oyster for Improving Large Language Models",
                    "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
                    "Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models",
                    "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
                    "Leveraging passage retrieval with generative models for open domain question answering",
                    "Xlnet: Generalized autoregressive pretraining for language understanding",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "In-context retrieval-augmented language models"
                ]
            },
            {
                "name": "Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling",
                "arxiv_id": "2405.00611",
                "reference": [
                    "Automatic labeling of topics",
                    "Latent dirichlet allocation",
                    "Labelling topics using unsupervised graph-based methods",
                    "Sentence-bert: Sentence embeddings using siamese bert-networks",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Automatic labeling of multinomial topic models",
                    "Topicgpt: A prompt-based topic modeling framework",
                    "Bertopic: Neural topic modeling with a class-based tf-idf procedure",
                    "Towards interpreting topic models with chatgpt",
                    "Large language models offer an alternative to the traditional approach of topic modelling",
                    "Automatic labeling hierarchical topics",
                    "Gptopic: Dynamic and interactive topic representations",
                    "Automatic labeling of topic models using text summaries",
                    "Chatgpt [large language model"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
            "arxiv_id": "2311.04892",
            "isAPA": true,
            "abstract": "Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like 'Youare Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs'capabilities remains unclear. To fill this gap, we present the first extensive studyof the unintended side-effects of persona assignment on the ability of LLMs toperform basic reasoning tasks. Our study covers 24 reasoning datasets (spanningmathematics, law, medicine, morals, and more) , 4 LLMs (2 versions of ChatGPT3.5, GPT-4-Turbo, and Llama-2-70b-chat) , and 19 diverse personas (e.g., 'anAsian person') spanning 5 socio-demographic groups: race, gender, religion, disability, and political affiliation. Our experiments unveil that LLMs harbor deeprooted bias against various socio-demographics underneath a veneer of fairness.While they overtly reject stereotypes when explicitly asked ('Are Black people lessskilled at mathematics?') , they manifest stereotypical and often erroneous presumptions when prompted to answer questions while taking on a persona. Thesecan be observed as abstentions in the model's response, e.g., 'As a Black person, Iam unable to answer this question as it requires math knowledge', and generallyresult in a substantial drop in performance on reasoning tasks. Our experimentswith ChatGPT-3.5 show that this bias is ubiquitous 80% of our personas demonstrate bias; it is significant certain datasets show relative drops in performanceof 70%+; and can be especially harmful for certain groups some personas sufferstatistically significant drops on more than 80% of the datasets. Overall, we findthat all four LLMs exhibit persona-induced bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42%of the personas) . Further analysis shows that these persona-induced errors canbe hard-to-discern as they do not always manifest as explicit abstentions. Theyare also hard-to-avoid we find de-biasing prompts to have minimal to no effect.Our findings serve as a cautionary tale that the practice of assigning personas toLLMs a trend on the rise can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.1",
            "reference": [
                "Sunipa Dev, Tao Li, J. M. Phillips, and Vivek Srikumar. On measuring and mitigating biased inferences of word embeddings",
                "Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior",
                "John J. Horton. Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research",
                "Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. ArXiv, abs",
                "Myra Cheng, Esin Durmus, and Dan Jurafsky. Marked personas: Using natural language prompts to measure stereotypes in language models. ArXiv, abs",
                "Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them",
                "Yixin Wan, Jieyu Zhao, Nanyun Peng, Kai-Wei Chang, and Aman Chadha. Are personalized stochastic parrots more dangerous? Evaluating persona biases in dialogue systems. ArXiv, abs",
                "Peter A. Jansen. From words to wires: Generating functioning electronic devices from natural language descriptions. ArXiv, abs",
                "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs",
                "Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Y. Wang. A survey on fairness in large language models. ArXiv, abs",
                "Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context impersonation reveals large language models' strengths and biases. ArXiv, abs",
                "Joseph Henrich, Steven J. Heine, and Ara Norenzayan. Most people are not weird. Nature",
                "OpenAI. Custom instructions for chatgpt, 2023a. https://openai.com/blog/custom-instructions-for-chatgpt",
                "Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Kai-Wei Chang. Ethical-advice taker: Do language models understand natural language interventions? In Findings@ACL",
                "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? ArXiv, abs",
                "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs",
                "Jonas Freiknecht and Wolfgang Effelsberg. Procedural generation of interactive stories using language models",
                "Edwin B Wilson. Probable inference, the law of succession, and statistical inference. Journal of the American Statistical Association",
                "Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. Out of one, many: Using language models to simulate human samples. Political Analysis",
                "Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution",
                "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention",
                "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023b",
                "Emily Sheng, Josh Arnold, Zhou Yu, Kai-Wei Chang, and Nanyun Peng. Revealing persona biases in dialogue systems. ArXiv, abs",
                "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration",
                "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. ArXiv, abs",
                "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. ArXiv, abs",
                "Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings",
                "Gati Aher, Rosa I. Arriaga, and Adam T. Kalai. Using large language models to simulate multiple humans and replicate human subject studies",
                "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods",
                "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners",
                "Jeongeon Park and Daeun Choi. Audilens: Configurable llm-generated audiences for public speech practice. Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology",
                "Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Social simulacra: Creating populated prototypes for social computing systems",
                "Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs",
                "David Danks and Alex John London. Algorithmic bias in autonomous systems",
                "Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. ArXiv, abs",
                "Perttu H\u00e4m\u00e4l\u00e4inen, Mikke Tavast, and Anton Kunnari. Evaluating large language models in generating synthetic hci research data: a case study",
                "Solon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. The problem with bias: From allocative to representational harms in machine learning",
                "Tao Li, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Vivek Srikumar. Unqovering stereotypical biases via underspecified questions"
            ],
            "related work": "7Related WorkPersonas in LLMs: Personified LLMs have seen widespread usage in simulating human behavior.Park et al. (2023) created personas with detailed attributes and studied their evolution over time.Aher et al. (2023) used LLMs to replicate classic economic, psycho-linguistic, and social psychology experiments with some success.Argyle et al. (2023) showed some success in replicating the viewpoints of demographically varied U.S. sub-populations with GPT-3. Personas have also been used to create collaborative agents that collectively improve the LLM capability:Qian et al. (2023) used personas to create a virtual chat-powered software development company,Wang et al. (2023) used personas in a self-collaboration setting to improve the LLM performance on knowledge and reasoning tasks, andSalewski et al. (2023) showed that LLMs adopting expert personas can do better on vision and language tasks. Motivated by this emergence of personified LLMs, our work studies the impact of socio-demographic persona assignments on the reasoning abilities of LLMs.Biases in models:There is a vast amount of work on how bias in algorithms and systems can cause harm(Danks & London,2017; Barocas et al.,2017) . Our focus is specifically on measuring the bias in learned models. Biases have been extensively studied in vector representations(Bolukbasi et al.,2016) , task-specific models(Rudinger et al.,2018; Zhao et al.,2018) , and even language models(Li et al.,2023) via their behavior on tasks such as coreference resolution(Rudinger et al.,2018; Zhao et al.,2018) , entailment(Dev et al.,2019) , and question answering(Li et al.,2020) . In contrast to these works, our work specifically focuses on biases due to persona-assignment in LLMs.Persona Biases:Deshpande et al. (2023) demonstrated that personas can be used to surface toxic responses from ChatGPT.Cheng et al. (2023) showed that LLMs can generate stereotypical descriptions of socio-demographic personas.Sheng et al. (2021) studied the effect of persona on dialog systems with a focus on harmful text in their outputs.Wan et al. (2023) extended this study to personified LLMs (e.g. ChatGPT) with richer personas and more detailed analysis, however the focus was still on harmful text in generated outputs. Our work, to the best of our knowledge, is the first to use persona-assignment to study the impact of persona on reasoning performance of LLMs.",
            "date": "2023"
        },
        "topic": "Bias and Fairness in LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
                "arxiv_id": "2406.12033",
                "subtitles": [
                    "Mental Health Prediction",
                    "LLMs and Mental Health Applications"
                ],
                "reference": [
                    "Predicting depression via social media",
                    "Tram: Benchmarking temporal reasoning for large language models",
                    "A call to action on assessing and mitigating bias in artificial intelligence applications for mental health",
                    "Prominet: Prototype-based multi-view network for interpretable email response prediction",
                    "Bertsurv: Bert-based survival models for predicting outcomes of trauma patients",
                    "Detection of mental health from reddit via deep contextualized representations",
                    "Empirical quantitative analysis of covid-19 forecasting models",
                    "Sensemood: depression detection on social media",
                    "Deep learning and machine learning in psychiatry: a survey of current progress in depression detection, diagnosis and treatment",
                    "Metacognitive prompting improves understanding in large language models",
                    "Mental-llm: Leveraging large language models for mental health prediction via online text data",
                    "Phi-3 technical report: A highly capable language model locally on your phone",
                    "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis",
                    "Detection and classification of anxiety in university students through the application of machine learning",
                    "Artificial intelligence-based approaches for suicide prediction: Hope or hype",
                    "Are large language models ready for healthcare? a comparative study on clinical language understanding",
                    "Is chatgpt a good translator? a preliminary study",
                    "Mentalbert: Publicly available pretrained language models for mental healthcare",
                    "Improving language understanding by generative pre-training",
                    "Gpt-4 technical report",
                    "Detection of suicide ideation in social media forums using deep learning",
                    "Fairehr-clp: Towards fairness-aware clinical predictions with contrastive learning in multimodal electronic health records",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Depression detection from social networks data based on machine learning and deep learning techniques: An interrogative survey",
                    "Audibert: A deep transfer learning multimodal classification framework for depression screening",
                    "Machine learning models to detect anxiety and depression through social media: A scoping review",
                    "Enhancing transformer efficiency for multivariate time series classification",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Evaluation of chatgpt for nlp-based mental health applications",
                    "Artificial intelligence assisted tools for the detection of anxiety and depression leading to suicidal ideation in adolescents: a review",
                    "Gemini: a family of highly capable multimodal models",
                    "Mentallama: Interpretable mental health analysis on social media with large language models",
                    "Integrating physiological time series and clinical notes with transformer for early prediction of sepsis"
                ],
                "related_work": "2Related WorkIn this section, we delve into the existing literature on mental health prediction, followed by an overview of the latest research advancements in LLMs and their applications in mental health.2.1Mental Health PredictionExtensive studies have focused on identifying and predicting risks associated with various mental health issues such as anxiety(Ahmed et al.,2022; Bhatnagar et al.,2023) , depression(Squires et al.,2023; Hasib et al.,2023) , and suicide ideation(Menon and Vijayakumar,2023; Barua et al.,2024) over the past decade. Traditional methods initially relied on machine learning models, including SVMs(De Choudhury et al.,2013) , and deep learning approaches like LSTM-CNNs(Tadesse et al.,2019) to improve prediction accuracy. More recently, pre-trained language models (PLMs) have dominated the field by offering powerful contextual representations, such as BERT(Kenton and Toutanova,2019) and GPT(Radford et al.,) , across a variety of tasks, including text classification(Wang et al.,2022a,2023a) , time series analysis(Wang et al.,2022b) , and disease detection(Zhao et al.,2021a,b) . For mental health, attention-based models leveraging the contextual features of BERT have been developed for both user-level and post-level classification(Jiang et al.,2020) . Additionally, specialized PLMs like MentalBERT and MentalRoBERTa, trained on social media data, have been proposed(Ji et al.,2022b) . Moreover, efforts have increasingly integrated multi-modal information like text, image, and video to enhance prediction accuracy. For example, combining CNN and BERT for visual-textual methods(Lin et al.,2020) and Audio-Assisted BERT for audio-text embeddings(Toto et al.,2021) have improved performance in depression detection.2.2LLMs and Mental Health ApplicationsThe success of Transformer-based language models has motivated researchers and practitioners to advance towards larger and more powerful LLMs, containing tens to hundreds of billions of parameters, such as GPT-4(Achiam et al.,2023) , Llama2(Touvron et al.,2023) , Gemini(Team et al.,2023) , and Phi-3(Abdin et al.,2024) . Extensive evaluations have shown great potential in broad domains such as healthcare(Wang et al.,2023b) , machine translation(Jiao et al.,2023) , and complex reasoning(Wang and Zhao,2023c) . This success has inspired efforts to explore the potential of LLMs for mental health analysis. Some studies(Lamichhane,2023; Yang et al.,2023a) have tested the performance of ChatGPT on multiple classification tasks, such as stress, depression, and suicide detection, revealing initial potential for mental health applications but also highlighting significant room for improvement, with around 5-10% performance gaps. Additionally, instruction-tuning mental health LLMs, such as Mental-LLM(Xu et al.,2024) and MentaLLama(Yang et al.,2024) , has been proposed. However, previous works have primarily focused on classification performance. Given the sensitivity of this domain, particularly for serious mental health conditions like suicide detection, bias is a more critical issue(Wang and Zhao,2023b; Timmons et al.,2023; Wang et al.,2024) . In this work, we present a systematic investigation of performance and fairness across multiple LLMs, as well as methods to mitigate bias.",
                "abstract": "The advancement of large language models (LLMs) has demonstrated strong capabilities across various applications, including mental health analysis. However, existing studies have focused on predictive performance, leaving the critical issue of fairness underexplored, posing significant risks to vulnerable populations. Despite acknowledging potential biases, previous works have lacked thorough investigations into these biases and their impacts. To address this gap, we systematically evaluate biases across seven social factors (e.g., gender, age, religion) using ten LLMs with different prompting methods on eight diverse mental health datasets. Our results show that GPT-4 achieves the best overall balance in performance and fairness among LLMs, although it still lags behind domain-specific models like MentalRoBERTa in some cases. Additionally, our tailored fairness-aware prompts can effectively mitigate bias in mental health predictions, highlighting the great potential for fair analysis in this field."
            },
            {
                "name": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                "arxiv_id": "2403.05668",
                "subtitles": [
                    "Fairness in Recommender Systems",
                    "Leveraging Pre-trained LMs and Prompting for Recommender Systems"
                ],
                "reference": [
                    "Defining and measuring fairness in location recommendations",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
                    "Exploring artist gender bias in music recommendation",
                    "Pareto optimality for fairness-constrained collaborative filtering",
                    "Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems",
                    "The winner takes it all: geographic imbalance and provider (un) fairness in educational recommender systems",
                    "Balanced neighborhoods for multi-sided fairness in recommendation",
                    "Advertisement recommendation based on personal interests and ad push fairness",
                    "OpenP5: Benchmarking Foundation Models for Recommendation",
                    "Estimation of fair ranking metrics with incomplete judgments",
                    "Spot: Better frozen model adaptation through soft prompt transfer",
                    "TFROM: A Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers",
                    "Mitigating sentiment bias for recommender systems",
                    "Balancing between accuracy and fairness for interactive recommendation with reinforcement learning",
                    "A unifying and general account of fairness measurement in recommender systems",
                    "Fair sharing for sharing economy platforms",
                    "Fairness and discrimination in recommendation and retrieval",
                    "Exploiting personalized calibration and metrics for fairness recommendation",
                    "Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms",
                    "Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
                    "Towards universal sequence representation learning for recommender systems",
                    "Does fair ranking improve minority outcomes? understanding the interplay of human and algorithmic biases in online hiring",
                    "Fairness in recommender systems: research landscape and future directions",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "Two-sided fairness in rankings via Lorenz dominance",
                    "Interplay between upsampling and regularization for provider fairness in recommender systems",
                    "Defining and supporting narrative-driven recommendation",
                    "Fairness-aware news recommendation with decomposed adversarial learning",
                    "Towards long-term fairness in recommendation",
                    "Addressing marketing bias in product recommendations",
                    "Language models are few-shot learners",
                    "Fairness among new items in cold start recommender systems",
                    "A flexible framework for evaluating user and item fairness in recommender systems",
                    "Towards understanding and mitigating unintended biases in language model-driven conversational recommendation",
                    "User-item matching for recommendation fairness",
                    "A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News",
                    "The Unfairness of Active Users and Popularity Bias in Point-of-Interest Recommendation",
                    "An enhanced probabilistic fairness-aware group recommendation by incorporating social activeness",
                    "User-oriented fairness in recommendation",
                    "A fairness-aware hybrid recommender system"
                ],
                "related_work": "2.Related work In this section, we briefly review some related work on recommendation systems and LLM techniques. 2.1.Fairness in Recommender Systems In examining the landscape of FairRS research, we can categorize the literature along several dimensions: the stakeholder focus, the core recommender system model, the dynamics of fairness evaluation, and the granularity of fairness with respect to group and individual distinctions (Deldjoo et al., 2023). Table 1 provides an overview of how different papers are categorized by stakeholder focus consumer or producer  and RS models, whether traditional or employing recent RecLLM advances. This table further elucidates the attributes used to operationalize fairness considerations from both consumer and producer standpoints, underlining the relative scarcity of research focusing on RecLLMs within the FairRS domain. Table 2 instead provide an additional discourse by underscoring the technical nuances of fairness evaluation, differentiating between static and dynamic methodologies. It highlights the dominance of group and static evaluations in the current literature, pointing to potential areas for further investigation and development. 2.1.1.Core RS models and Stakeholder. In the current landscape of recommender systems (RS), in particular on FairRS, we observe a clear division between traditional models and those enhanced by recommendation-centric large language models (RecLLM). Traditional RS principally operates on collaborative filtering (CF) mechanisms, potentially supplemented by auxiliary user and item side information. These systems have been the subject of numerous studies aimed at developing benchmarks for fairness evaluation and strategies for bias mitigation, as studied in depth in  (Deldjoo et al., 2023; Ekstrand et al., 2019). Traditional RS models serve as the backbone of recommendation systems, albeit without the intricate natural language processing capabilities endowed by large language models (LLMs). The 'RecLLM' paradigm represents an innovative frontier in AI and RS research. Here they refer to models that integrate complex NLP methods, such as those derived from GPT-like architectures, into the recommendation process. While this integration promises enhanced personalization and a refined recommendation experience by comprehending user nuances, it simultaneously poses the risk of inheriting and perpetuating biases present in the extensive and unfiltered data used for LLM training. Concerns are particularly pronounced regarding biases related to race, gender, and brand recognition, which could result in unbalanced exposure for emerging entities or products. Consequently, a fundamental aspect of our ongoing research is the precise quantification of these biases, setting the stage for the formulation of strategies that can effectively neutralize the inadvertent propagation of these biases through (advanced) RecLLM systems. In summary: \u2022 Traditional RS. In 'traditional RS', the focus frequently lies on collaborative filtering (CF) algorithms that utilize historical datasets within a train-predict paradigm, occasionally supplemented by user and item metadata. Although these systems operate effectively, they lack the advanced natural language processing (NLP) capabilities seen in large language models (LLMs). Research work in this area are dedicated to enhancing frameworks for evaluating fairness and formulating strategies to mitigate biases inherent in these models. For example, Hao et al. (2021) tackle the issue of unfair discrimination by CF due to imbalanced data, proposing a multi-objective Optimization approach that seeks a Pareto optimal solution to balance subgroup performance without sacrificing overall accuracy. Farnadi et al. (2018) address inherent biases by introducing a hybrid fairness-aware recommender system that merges multiple similarity measures and demographic information to mitigate recommendation biases. Naghiaei et al. (2022) highlight the two-sided nature of recommender systems and present a re-ranking approach that integrates fairness constraints for both consumers and producers, showcasing the algorithm's ability to improve fairness without diminishing recommendation quality. Lastly, Wan et al. (2020) investigate the bias induced by marketing strategies in CF systems and propose a framework that enhances fairness across different market segments, achieving more equitable recommendation performance. \u2022 RecLLM. These model address the integration of language models with RS, signifying a shift towards using NLP techniques to refine the accuracy and pertinence of recommendations. Research in this domain is not limited to the application of LLMs for classical top- k  recommendation tasks but also extends to applications such as conversational recommendation systems, personalized explanation generation, and multi-modal recommendation scenarios. For instance, Shen et al. (2023) examine the unintended biases in language model-driven Conversational Recommendation Systems (CRSs), showing how biases can influence the category and price range of recommendations, and offer mitigation strategies that preserve recommendation quality. Li et al. (2023a) study the application of ChatGPT in personalized news recommendation task and find that the system is sensitive to input phrasing and signal the challenges in achieving provider fairness and fake news detection. They suggest that ongoing, dynamic evaluation of ChatGPT's recommendations is crucial for understanding and improving its performance in real-world tasks. Another study by Zhang et al. (2023) introduces the FaiRLLM benchmark, as pointed out earlier, specifically designed to evaluate the fairness of recommendations produced by RecLLM systems, highlighting the biases these models exhibit against certain sensitive user attributes in music and movie recommendations. Recently, Deldjoo et al. (Deldjoo, 2024) investigate prompt design strategies within ChatGPT-based RecLMMs and assess their effect on recommendation quality, and provider fairness. They find that assigning system role can mitigate popularity bias and enhance fairness, suggesting that combining these strategies with personalized models could lead to a more balanced recommendation experience. These examples underscore the evolving nature of RS technology and the importance of considering biases and fairness in the development of RecLLM systems. In general, the comparison of these two RS methodologies in Table 1 highlights a critical moment in the evolution of RS, where the quest for superior personalized recommendations must be meticulously weighed against the essential need for fairness and other harms in every facet of the recommendation process. 2.1.2.Stakeholder Considerations. Earlier discussions have established the importance of market orientation in classifying the corpus of group fairness research whether they address concerns from the consumer perspective, the provider's angle, or a combination of both. In delineating these categories, literature often focuses on certain sensitive attributes such as consumer demographics (including age and gender) and producer-related attributes such as item popularity. \u2022 Consumer Fairness. Research in this area aims to ensure equitable recommendations for consumers, where fairness is typically measure based on the relevance (or effectiveness) of recommendations for user groups, e.g., demographic groups. Typically, as illustrated in Table 1, consumer level of activity (e.g., active vs. inactive users), demographics, or other metrics (e.g., education) are utilized to identify protected groups. \u2022 Producer Fairness. This aims to achieve fairness for content or product creators within the recommender system. Fairness could be measured at the item level (e.g., popularity of items) or the producer level (such as artists, authors, brands), with popularity or recognition of artists/brands as examples. In technical terms, a producer can be seen as a higher-level grouping of items. Several attributes have been used, including popularity, demographics, and price/brand/location. \u2022 CP Fairness. This encompasses research that considers both consumer and producer fairness, endeavoring to achieve a balanced approach. Positioning the current work. While fairness in traditional recommendation systems is well-established, we observe a scarcity of research on fairness in LLMs. The present study seeks to address this gap by focusing on evaluating fairness and biases within RecLLMs. Our attention is particularly drawn to the consumer aspect of RecLLMs, building upon and refining previous works, especially since significant research has already been conducted on demographics from the consumer perspective. However, we also aim to propose a similar framework for the producer side and eventually explore a combined approach that integrates both consumer and producer perspectives. Our work also addresses the static aspects of fairness in recommender systems, as shown in Table 2. The purpose of presenting this table is to highlight the ample opportunities for further research in this field. 2.2.Leveraging Pre-trained LMs and Prompting for Recommender Systems The integration of natural language processing (NLP) techniques within RS, underscores the major role of LLMs in enhancing recommendation accuracy through deep semantic understanding. For instance, Hou et al. (2022) utilize natural language descriptions and tags as inputs into LLMs to create user representations for more effective recommendations. This contrasts with the narrative-driven recommendations (Bogers and Koolen, 2017) that rely on verbose descriptions of specific contextual needs. Regarding the evolution of prompting strategies, initial attempts often employed few-shot learning (Brown et al., 2020), guiding LLMs using exemplary cases to refine task-specific outcomes. Through the progress of prompt learning, tasks are adapted to align with LLM capabilities, rather than adapting LLMs to tasks, employing either discrete or continuous/soft prompts to improve performance across various tasks. This strategy has demonstrated effectiveness across a range of tasks, including recommendation tasks. At the core of these advances lies the personalization of LLMs for recommendation purposes. The P5 framework (Geng et al., 2022) and its iterations, such as OpenP5 (Xu et al., 2023), showcase the integration of multiple recommendation tasks into a unified LLM framework using personalized prompts. This approach reformulates recommendation tasks as sequence-to-sequence generation problems, showing the adaptability of LLMs to various recommendation contexts and emphasizing the importance of capturing user intent and personalized needs. Furthermore, exploring prompt transfer techniques, such as SPoT (Vu et al., 2021) and ATTEMPT (Asai et al., 2022), represents a major step in applying the learned knowledge from source tasks to target recommendation tasks. These methodologies, together with knowledge distillation techniques, contribute significantly to the development of more efficient and effective LLM-based recommendation models. They underscore the potential for intra-task prompt distillation and cross-task prompt transfer, enhancing the efficiency and effectiveness of LLM-based recommendation models. In sum, the integration of LLMs into recommender systems represents a paradigm shift towards leveraging advanced NLP and innovative prompting strategies for delivering highly personalized and contextually rich recommendations. These developments promise to reshape the landscape of recommender systems, making them more adaptable, intuitive, and user-centric.",
                "abstract": "In the evolving landscape of recommender systems, the integration of Large Language Models (LLMs) such as ChatGPT marks a new era, introducing the concept of Recommendation via LLM (RecLLM). While these advancements promise unprecedented personalization and efficiency, they also bring to the fore critical concerns regarding fairness, particularly in how recommendations might inadvertently perpetuate or amplify biases associated with sensitive user attributes. In order to address these concerns, our study introduces a comprehensive evaluation framework, CFaiRLLM, aimed at evaluating (and thereby mitigating) biases on the consumer side within RecLLMs.Our research methodically assesses the fairness of RecLLMs by examining how recommendations might vary with the inclusion of sensitive attributes such as gender, age, and their intersections, through both similarity alignment and true preference alignment. By analyzing recommendations generated under different conditions-including the use of sensitive attributes in user prompts-our framework identifies potential biases in the recommendations provided. A key part of our study involves exploring how different detailed strategies for constructing user profiles (random, top-rated, recent) impact the alignment between recommendations made without consideration of sensitive attributes and those that are sensitive-attribute-aware, highlighting the bias mechanisms within RecLLMs.The findings in our study highlight notable disparities in the fairness of recommendations, particularly when sensitive attributes are integrated into the recommendation process, either individually or in combination. The analysis demonstrates that the choice of user profile sampling strategy plays a significant role in affecting fairness outcomes, highlighting the complexity of achieving fair recommendations in the era of LLMs."
            },
            {
                "name": "Cognitive Bias in High-Stakes Decision-Making with LLMs",
                "arxiv_id": "2403.00811",
                "subtitles": [
                    "Bias in Large Language Models",
                    "Patterns of Cognitive Bias in LLMs"
                ],
                "reference": [
                    "Capturing failures of large language models via human cognitive biases",
                    "Ai-moderated decision-making: Capturing and balancing anchoring bias in sequential decision tasks",
                    "Fairpy: A toolkit for evaluation of social biases and their mitigation in large language models",
                    "Mind the biases: Quantifying cognitive biases in language model prompting",
                    "Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias",
                    "The framing of decisions and the psychology of choice",
                    "Challenging the appearance of machine intelligence: Cognitive bias in llms",
                    "Towards understanding and mitigating social biases in language models",
                    "Beyond accuracy: Behavioral testing of nlp models with checklist",
                    "Bias in word embeddings",
                    "Man is to computer programmer as woman is to homemaker? debiasing word embeddings"
                ],
                "related_work": "2Related Work2.1Bias in Large Language ModelsMany different social biasesLiang et al. (2021) and biases related to reasoning and decision-makingItzhak et al. (2023) have been detected in LLMs.Viswanath and Zhang (2023) propose a toolkit for evaluating social biases in LLMs, including evaluation metrics for detecting social biasesRibeiro et al. (2020) perform a test comprising a small set of neutral sentences with simple adjectives, label preserving perturbations to check if the behavior of the LLM differs, and test adding a sentiment to the template to check if the model predicts the opposite sentimentRibeiro et al. (2020) . Compared to their work, which focuses on the extent of biased decisions that are made towards protected groups, our work provides insight into decision patterns akin to human cognitive bias where we analyze systematic flaws of language models during a decision-making procedure.Existing evaluation metrics for societal bias are often based on word embeddingsBolukbasi et al. (2016) ; Papakyriakopoulos et al. (2020) ; Viswanath and Zhang (2023) , which is not directly applicable for evaluation of decision patterns akin to human cognitive bias. Cognitive bias is not necessarily embedded in specific tokens, but can be reflected in the entire currentTversky and Kahneman (1981) or previous contextEchterhoff et al. (2022) .2.2Patterns of Cognitive Bias in LLMsTo address the lack of evaluation metrics for cognitive bias,Lin and Ng (2023) propose metrics for availability and framing bias using dummy inputs and paraphrasing for classification tasks. In comparison, our work focuses on generative tasks. Previous work has tackled individual cognitive biases such as representativeness, base rate neglect, etc.Talboy and Fuller (2023) . However, they evaluate the biases as one-off questions and answers, which require further investigation on generalization.Jones and Steinhardt (2022) test for systematic qualitative errors of LLM responses with human cognitive bias using coding prompts. Cognitive bias can influence many scenarios, but is especially important for high-stakes decisions in human-AI collaboration, as humans might be influenced by the decision assistance that LLMs provide.",
                "abstract": "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. Given their training on human (created) data, LLMs have been shown to inherit societal biases against protected groups, as well as be subject to bias functionally resembling cognitive bias. Human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive science, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method utilising LLMs to debias their own prompts. Our analysis provides a comprehensive picture of the presence and effects of cognitive bias across commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigates model answers that display patterns akin to human cognitive bias without having to manually craft examples for each bias."
            },
            {
                "name": "Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One",
                "arxiv_id": "2402.12150",
                "subtitles": [
                    "Fairness Evaluation",
                    "Fairness Enhancement",
                    "Multi-Agent Collaboration"
                ],
                "reference": [
                    "Deep reinforcement learning from human preferences",
                    "Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity",
                    "Improving language model negotiation with self-play and in-context learning from ai feedback",
                    "A survey on fairness in large language models",
                    "Communicative agents for software development, 2023a",
                    "Improving factuality and reasoning in language models through multiagent debate",
                    "Multi-agent collaboration: Harnessing the power of intelligent llm agents",
                    "Llm harmony: Multi-agent communication for problem solving",
                    "Sparks of artificial general intelligence: Early experiments with gpt",
                    "Perturbation augmentation for fairer nlp",
                    "Biasasker: Measuring the bias in conversational ai system",
                    "Persistent anti-muslim bias in large language models",
                    "Experiential co-learning of software-developing agents, 2023b",
                    "Encouraging divergent thinking in large language models through multi-agent debate",
                    "Unqovering stereotyping biases via underspecified questions",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Bias and fairness in large language models: A survey",
                    "Measuring harmful sentence completion in language models for lgbtqia+ individuals",
                    "Language models are few-shot learners",
                    "Trustgpt: A benchmark for trustworthy and responsible large language models",
                    "In-context impersonation reveals large language models' strengths and biases",
                    "Collecting a large-scale gender bias dataset for coreference resolution and machine translation",
                    "Training socially aligned language models on simulated social interactions",
                    "Bbq: A hand-built bias benchmark for question answering",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related WorkFairness Evaluation.To evaluate LLMs' fairness, researchers have proposed a variety of evaluation methods and benchmarks(Brown et al.,2020; Levy et al.,2021; Qian et al.,2022; Parrish et al.,2021; Cui et al.,2023; Huang et al.,2023; Wan et al.,2023) . An increasing amount of research indicates that there is a widespread presence of unfairness in LLMs. Research byBrown et al. (2020) highlights that GPT-3 exhibits social biases, and a study byAbid et al. (2021) reveals that GPT-3 demonstrates a disproportionately higher violent bias against Muslims compared to other religious groups. Although existing fairness benchmarks achieve success to some degree, they still have inherent limitations. Existing survey(Gallegos et al.,2023) shows that many benchmarks are designed for specific tasks like Sentence Completion(Nozza et al.,2022) , Masked Tokens Prediction(Levy et al.,2021) which are inadequate for evaluating general conversational LLMs. Also, questions in the benchmarks for conversational LLMs are either too simple to elicit deeper bias(Li et al.,2020; Parrish et al.,2021) , or suffer from excessive lengthiness that contains much information irrelevant to discrimination and stereotypes(Cui et al.,2023) . To address the existing issues, BiasAsker(Wan et al.,2023) implements an automated pipeline to generate tens of thousands of questions based on collected demographic attributes and templates, to elicit and quantify the party-specific bias in conversational LLMs. However, BiasAsker focuses solely on close-ended questions, relying on the generation of a large volume of questions to assess fairness, which is highly inefficient.Fairness Enhancement.Large-scale LLMs can be debiased through methods like fine-tuning and prompt engineering. The effectiveness of fine-tuning these models on manually-crafted datasets to reduce bias has been widely proven. A notable approach is using Reinforcement Learning from Human Feedback (RLHF) (Christiano et al.,2017) for instruction fine-tuning. This technique has been effectively employed in projects such as InstructGPT(Ouyang et al.,2022) and Llama2-chat(Touvron et al.,2023) . However, such methods require substantial human intervention, which might introduce new biases into the trained models. In addition to the finetuning-based methods, prompt engineering is another effective strategy for reducing bias in large-scale LLMs. This approach involves crafting specific prompts to guide the model towards producing fairer outputs without the need for fine-tuning. For example,Bubeck et al. (2023) show that by adding the phrase 'in an inclusive way' to the prompts, they can alter GPT-4's responses to use a third-person pronoun instead of using either the female or male pronoun in the occupation recommendation task. This modification leads to a fairer outcome(Li et al.,2023b) .Multi-Agent Collaboration.Recently, multiple studies have focused on using collaborations between LLMs with different roles for performance enhancement.Salewski et al. (2023) discover that assigning different roles to agents can boost their effectiveness.Du et al. (2023) enhance factual correctness and reasoning accuracy through multi-agent debates.Liang et al. (2023) engage multiple agents in debates to address the degeneration-of-thought issue in self-reflection.Fu et al. (2023) show that multiple agents can enhance each other's capabilities in negotiation scenarios, such as buyer-seller interactions, through role-playing and learning from feedback. Lastly,Liu et al. (2023) design a simulated social interaction sandbox for social alignment in LLMs, andTalebirad & Nadiri (2023) introduce agents with unique attributes and roles to manage complex tasks in a black box environment.Qian et al. (2023a,b) present an innovative paradigm that utilizes LLMs in various roles, including programmers, reviewers, and testers, streamlining and unifying key processes through natural language communication across the entire software development process.Rasal (2024) introduce a novel multi-agent communication pipeline, to enhance LLMs' problem-solving capabilities.",
                "abstract": "The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performance."
            },
            {
                "name": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
                "arxiv_id": "2404.07990",
                "subtitles": [
                    "Pipeline with Foundation Models",
                    "Bias Mitigation in Generative Models"
                ],
                "reference": [
                    "Iti-gen: Inclusive text-to-image generation",
                    "Improving the fairness of deep generative models without retraining",
                    "Emerging properties in self-supervised vision transformers",
                    "Unbiased image synthesis via manifold-driven sampling in diffusion models",
                    "Fair diffusion: Instructing text-to-image generation models on fairness",
                    "Classifier-free diffusion guidance",
                    "Repfair-gan: Mitigating representation bias in gans using gradient clipping",
                    "Bias-to-text: Debiasing unknown visual biases through language interpretation",
                    "Vipergpt: Visual inference via python execution for reasoning",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Video chatcaptioner: Towards the enriched spatiotemporal descriptions",
                    "On the opportunities and risks of foundation models",
                    "Visual instruction tuning",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions",
                    "Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering",
                    "Learning transferable visual models from natural language supervision",
                    "Mitigating inappropriateness in image generation: Can there be value in reflecting the world's ugliness",
                    "Visual programming: Compositional visual reasoning without training",
                    "Dinov2: Learning robust visual features without supervision",
                    "Language models are few-shot learners",
                    "Llama: Open and efficient foundation language models",
                    "Reclip: A strong zero-shot baseline for referring expression comprehension",
                    "Sega: Instructing text-to-image models using semantic guidance",
                    "High-resolution image synthesis with latent diffusion models"
                ],
                "related_work": "2Related workPipeline with Foundation Models.We broadly refer to foundation models[5]as large-scale deep learning models trained on extensive data corpora, usually with a self-supervised objective[5]. This approach has been used across different modalities, such as text[63,9], vision[49,10,15]and multi-modal models[76,42,51]. These models can be fine-tuned on downstream tasks or applied in a zero-shot manner, generalizing to unseen tasks[9,68,60].Lately, several works combined different foundation models to solve complex tasks.[22,61]use an LLM to generate Python code that invokes vision-language models to produce results. TIFA[27]assesses the faithfulness of a generated image to a given text prompt, by querying a VQA model with questions produced by an LLM from the original caption. Similarly,[76,11]enhance image/video captioning by iteratively querying an LLM to ask questions to a VQA model. Differently,[35]identify spurious correlations in synthetic images via captioning and language interpretation, but without categorizing or quantifying bias.We share a similar motivation, i.e., we leverage powerful foundation models to build an automatic pipeline, tailored to the novel task of open-set bias discovery.OpenBias builds a knowledge base of biases leveraging the domain-specific knowledge fromrealcaptions and LLMs.Bias Mitigation in Generative Models.Bias mitigation is a long-studied topic in generative models. A substantial line of work focused on GAN-based methods. Some works improve fairness at inference time by altering the latent space semantic distribution[62]or by gradient clipping to control the gradient ensuring fairer representations for sensitive groups[33]. The advent of T2I generative models has directed research efforts towards fairness within this domain. FairDiffusion[17]guides Stable Diffusion[53]toward fairer generation in job-related contexts. It enhances classifier-free guidance[25]by adding a fair guidance term based on user-provided fair instructions. Similarly,[7]demonstrates that (negative) prompt and semantic guidance[6]mitigate inappropriateness generation in several T2I models. Given handwritten text as input,ITI-GEN[72]enhances the fairness of T2I generative models through prompt learning. To improve fairness,[59]guide generation using the data manifold of the training set, estimated via unsupervised learning.While yielding notable result, these bias mitigation methods rely on predefined lists of biases. Here, we argue that there may exist other biases not considered by these methods. Therefore, our proposed pipeline is orthogonal, providing a valuable tool to enhance their utility.",
                "abstract": "Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement."
            },
            {
                "name": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
                "arxiv_id": "2407.08441",
                "subtitles": [
                    "Fairness evaluation and bias benchmarking",
                    "Adversarial attacks via jailbreak prompting"
                ],
                "reference": [],
                "related_work": "2Related workSeveral studies have underscored the potential risks posed by societal biases, toxic language, or discriminatory outputs that can be generated by LLMs[11,9]. In addition, despite advances in safety strategies, research suggests that LLMs can still be manipulated to expose hidden biases through adversarial attacks[6,7]. This section reviews recent work in this area, focusing on fairness evaluation, bias benchmarking, and adversarial attacks using jailbreak prompts.2.0.1Fairness evaluation and bias benchmarking.Effective methods for identifying and mitigating bias are critical to ensuring the safety and responsible use of LLMs. The primary strategy concerns creating benchmark datasets and frameworks that allow to probe LLMs for potential biases[22,23], generally employing targeted prompts and metrics. Manerba et al.[13]presents SOFA (Social Fairness) , a fairness probing benchmark encompassing diverse identities and stereotypes, also introducing a perplexity-based score to measure the fairness of language models. Tedeschi et al.[14]introduce a novel safety risk taxonomy, also presenting ALERT, a comprehensive benchmark for red teaming LLMs. StereoSet[15]is another benchmark tackling stereotypical biases in gender, profession, race, and religion, providing a comprehensive evaluation of how LLMs perpetuate societal stereotypes across various demographic categories. Furthermore, several other benchmarks for assessing bias in LLMs have been proposed for specific types of bias, including cognitive[20], gender-occupational[19], religion[12], and racial[30].2.0.2Adversarial attacks via jailbreak prompting.Adversarial attacks on LLMs involve deliberately crafting inputs to expose their vulnerabilities. These attacks can be particularly insidious, as they may manipulate the model into generating biased, toxic, or undesirable outputs. Recent studies have focused on the development of adversarial techniques to test and improve the robustness of LLMs against such vulnerabilities. Among the most recent methods proposed in the literature, Chao et al. introduced PAIR[16], a systematically automated prompt-level jailbreak, which employs an attacker LLM to iteratively refine prompts, enhancing the chances of successfully bypassing the model's defenses. Similarly, TAP[25]leverages an attacker LLM but uses a tree-of-thought reasoning approach to iteratively refine candidate prompts, also pruning unlikely ones. Another approach is AutoDAN[17], which employs a hierarchical genetic algorithm that automatically generates malicious prompts. The process begins with an initial prompt formulated according to the DAN (Do Anything Now) attack template, designed to guide the model into bypassing its safety guardrails. Genetic algorithms are also used in OpenSesame[27], which combines the user's query with an optimized universal adversarial prompt to disrupt the model alignment, leading to unintended and potentially harmful outputs. Furthermore, GUARD[26]employs a role-playing attack strategy, which involves the simulation of specific roles to mimic real-world threats and vulnerabilities. In particular, additional language models are leveraged to simulate the behavior of malicious users attempting to jailbreak a target LLM.We build our study upon prior work by evaluating the safety of LLMs with the following key differences: We go beyond existing approaches by leveraging jailbreak prompts to examine bias categories initially deemed safe. This approach allows us to assess the true robustness and fairness of LLMs, ensuring that safety measures are not only present but effective across a broad spectrum of scenarios. By using jailbreak techniques to elicit bias and reveal vulnerabilities hidden in LLMs, we assess the effectiveness of various attacks at different model scales, exploring how changes in model size impact reasoning capabilities, filtering mechanisms, and model safety. We present a thorough analysis of LLM behavior under bias elicitation. Particularly, we introduce a safety score that jointly evaluates model's fairness and robustness, investigating its tendency to either decline or debias generated content and to prefer stereotypes or counterstereotypes in its outputs.",
                "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training data. These include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. This study explores the presence of these biases within the responses given by the most recent LLMs, analyzing the impact on their fairness and reliability. We also investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more sustainable and inclusive artificial intelligence."
            },
            {
                "name": "Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias",
                "arxiv_id": "2406.00020",
                "subtitles": [
                    "Trans and Non-Binary Dialects",
                    "Linguistics of Slurs",
                    "Gender Variance in NLP",
                    "Defining and Detecting Harmful Speech"
                ],
                "reference": [
                    "Historical perspectives on language and identity",
                    "Recognition of They/Them as Singular Personal Pronouns in Coreference Resolution",
                    "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
                    "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of\" Subjectivity\" and\" Identity Terms",
                    "An Analysis of WordNet's Coverage of Gender Identity Using Twitter and The National Transgender Discrimination Survey",
                    "Fighting Hate Speech, Silencing Drag Queens? Artificial Intelligence in Content Moderation and Risks to LGBTQ Voices Online",
                    "Revisiting Queer Minorities in Lexicons",
                    "Word norms and measures of linguistic reclamation for LGBTQ+ slurs",
                    " \"Building a thick skin for each other \": The use of 'reading' as an interactional practice of mock impoliteness in drag queen backstage talk",
                    "COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements",
                    "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
                    " \"I'm fully who I am \": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
                    "Cursing in English on twitter",
                    "Investigating the role of swear words in abusive language detection tasks",
                    "Challenges in discriminating profanity from hate speech",
                    "Practices of Slur Use",
                    "Hidden behind the obvious: Misleading keywords and implicitly abusive language on social media",
                    "They, Them, Theirs: Rewriting with Gender-Neutral English",
                    "Theories of  \"Gender \" in NLP Bias Research",
                    "Accounting for Offensive Speech as a Practice of Resistance",
                    "Male, Female, and Nonbinary Differences in UK Twitter Self-descriptions: A Fine-grained Systematic Exploration"
                ],
                "related_work": "2.Related Work2.1.Trans and Non-Binary DialectsAccording to a framework from the field of linguistics termed\"pragmatics\", socio-cultural factors centrally determine and profoundly influence the form and function of language(Joseph,[n.d.]) . For example, a 2021 UK study found that non-binary users are more likely than men or women to use words related to gender and sexuality on Twitter(Thelwall et al.,2021) . This distinctive pattern of word usage reflects the result of a non-binarydialectbecause, among the factors relevant to understanding the use of gender and sexuality terms, the speaker'ssocial group(as opposed to the time or medium of communication) emerges as a strong predictor of characteristic expression patterns.In addition to shifts in the distribution of tokens over a vocabulary, a dialect may also include more latent shifts in pragmatic intent. For example, many trans and non-binary communities use mock rudenessnotto harm the audience but rather to build in-group solidarity and resilience to future discriminatory experiences(McKinnon,2017) . Similarly, in gender-queer dialects, slurs that historically cause harm, such as  \"fag \" or  \"sissy \", may be repurposed by the in-group to serve nontoxic purposes like identifying oneself(Dias Oliva et al.,2021) . In this work, we primarily focus on slur use (as opposed to other features of gender-queer dialects) because they are easy to identify in corpora and may be incorrectly parsed by language models, leading to falsely labeling their use as harmful.2.2.Linguistics of SlursFunctionally, slurs both classify a person or groupandexpress a particularperspectivethat the speaker has towards that person or group. While publicly classifying someone as trans or non-binary can cause harm, especially to those who conceal their identity for their safety, it is the latent perspectives associated with slur that can make them uniquely harmful (relative to other classifications liketrans) . Frequently, these perspectives evoke feelings of disgust or hatred and have been associated with intimidation or violence.However, the perspectives that originated a slur do not represent the full range of contemporary uses and intentions. Quotes of others using a slur and discussing the slur itself seem to be more acceptable uses among outgroup members because in these contexts the speaker may be conveying a more neutral or even positive perspective towards the group that is subject to the slur(Hess,2020) . Additionally, slurs can take on new, non-derogatory senses through discussion and use by members of the group subject to the slur, through the process oflinguistic reclamation, described earlier(Edmondson,2021) . The extent that an individual of a marginalized group reclaims a may vary with age, gender identity, sexual orientation and relationship with the specific slur(Edmondson,2021) .Taken together, the boundary between harmful and non-harmful uses of slurs is sometimes (if not usually, given the stigmas often associated with slurs) determined by establishing the speaker's social identity. This determination is not straightforward or sometimes even possible, further confounding the interpretation of intent. In this work, we not only provide harm assessments across varying uses from people in slur target groups, but also study the extent to which LLMs are able to mimick this ability in the more controlled text domain.2.3.Gender Variance in NLPAccording to a comprehensive review of approximately 200 articles relating to gender bias in natural language processing (NLP) , almost no papers in this area conceptualize gender as non-binary(Devinney et al.,2022) . Concurrently, multiple works have found suboptimal performance by NLP systems when confronted by the singular pronoun use of 'they'(Baumler and Rudinger,2022; Ovalle et al.,2023) . This observation is further compounded by the finding that Wordnet 3.0 contains representations for only 39% of topical terms from the National Transgender Discrimination Survey(Hicks et al.,[n.d.]) . It appears as though NLP systems may often be constructed without deeply considering non-binary gender identities. Notably, popular English lexicons for inappropriate language fail to differentiate between pejorative and non-pejorative LGBTQ+ terminology(Ramesh et al.,2022) , even neglecting the difference between the terms 'gay' and 'fag'. Nonetheless, there is hope for decreasing bias in NLP systems, as evidenced by Seq2Seq model's ability to translate gendered pronouns to gender-neutral pronouns with minimal error(Sun et al.,2021) .2.4.Defining and Detecting Harmful SpeechDetermining harm is inherently subjective. Researchers have worked to mitigate this subjectivity by creating frameworks that include facets like target group, explicitness of abuse, speaker intent and power dynamics(Waseem et al.,2017; Zhou et al.,2023; Zhao et al.,2021) . Here we incorporate speaker identity as contextual information to help temper some of the subjectivity within our analysis.When classifiers falsely identify harm they run the risk of suppressing speech. One common contributor to false positives is the over-reliance on keywords rather than contextual clues (e.g. Davidson(Davidson et al.,2019; Yin and Zubiaga,2022) ) . According an empirical analysis, linear classifiers struggle to discern between hate speech and profanities(Malmasi and Zampieri,2018) . This concern is compounded by the frequency of profanities in online platforms like Twitter, where approximately one in thirteen tweets includes swear words(Wang et al.,2014) . Nonetheless, recent advancements hold promise for reducing the risk of false positives. Leveraging word-level annotations as features has been shown to alleviate some reliance on keywords for abusive language detection(Pamungkas et al.,2023) , and novel language classification frameworks have uncovered social positioning as crucial context in detecting offensive language(Diaz et al.,2022) .",
                "abstract": "Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed. Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1 <= 0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users."
            },
            {
                "name": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers",
                "arxiv_id": "2404.03192",
                "subtitles": [
                    "Ranking with LLMs",
                    "Fairness in LLMs",
                    "Fairness in Search and Ranking"
                ],
                "reference": [
                    "Matching code and law: Achieving algorithmic fairness with optimal transport",
                    "Reducing disparate exposure in ranking: A learning to rank approach",
                    "Gender bias and stereotypes in large language models",
                    "Ranking with fairness constraints",
                    "A meta-learning approach to fair ranking",
                    "BBQ: A hand-built bias benchmark for question answering",
                    "Sparks of artificial general intelligence: Early experiments with GPT",
                    "Beyond yes and no: Improving zero-shot LLM rankers via scoring fine-grained relevance labels",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Fine-tuning llama for multi-stage text retrieval",
                    "Fa*ir: A fair top-k ranking algorithm",
                    "Measuring fairness in ranked outputs",
                    "ifair: Learning individually fair data representations for algorithmic decision making",
                    "Holistic evaluation of language models",
                    "Designing fair ranking schemes",
                    "The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models",
                    "Marked personas: Using natural language prompts to measure stereotypes in language models",
                    "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
                    "Knowledge of cultural moral norms in large language models",
                    "Diversified subgraph query generation with group fairness",
                    "Auditing search query suggestion bias through recursive algorithm interrogation",
                    "Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search",
                    "Passage-specific prompt tuning for passage reranking in question answering with large language models",
                    "Improving passage retrieval with zero-shot question generation",
                    "Balanced ranking with diversity constraints",
                    "Multi-stage document ranking with BERT",
                    "Open-source large language models are strong zero-shot query likelihood models for document ranking",
                    "A unified meta-learning framework for fair ranking with curriculum learning",
                    "Persistent anti-muslim bias in large language models",
                    "Is ChatGPT good at search? investigating large language models as re-ranking agents",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Online set selection with fairness and diversity constraints",
                    "Large language models are effective text rankers with pairwise ranking prompting",
                    "Fairness in recommendation ranking through pairwise comparisons",
                    "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
                    "Large language models are not yet human-level evaluators for abstractive summarization",
                    "Zero-shot listwise document reranking with a large language model",
                    "Decodingtrust: A comprehensive assessment of trustworthiness in GPT models",
                    "Equity of attention: Amortizing individual fairness in rankings",
                    "Language models are few-shot learners",
                    "Nlpositionality: Characterizing design biases of datasets and models",
                    "Selection Problems in the Presence of Implicit Bias",
                    "Fairness in ranking, part i: Score-based ranking",
                    "Text-to-text multi-view learning for passage re-ranking",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Works 2.1Ranking with LLMs In document ranking with LLMs, methodologies could be categorized supervised Nogueira et al. (2019) ; Ju et al. (2021) ; Pradeep et al. (2021) ; Ma et al. (2023a) and unsupervisedLiang et al. (2022) ; Zhuang et al. (2023a) ; Sachan et al. (2022) ; Zhuang et al. (2023b) approaches. Supervised methods focus on fine-tuning LLMs with specific ranking datasets to enhance relevance assessment between queries and documents. For instance, RankLLaMaMa et al. (2023a) employs a decode-only strategy for relevance determination, proving effective particularly with smaller LLMs. Conversely, unsupervised techniques leverage LLMs' inherent capabilities for ranking without additional training. These include pointwise approaches for binary or nuanced relevance labeling Liang et al. (2022) ; Zhuang et al. (2023a) , and zero-shot methods Sachan et al. (2022) ; Wu et al. (2024) that utilize log-likelihood scores for relevance estimation. Despite promising developments, listwise ranking Sun et al. (2023) ; Ma et al. (2023b) ; Tang et al. (2023) has shown competitive results mainly with GPT-4 based methods, which are notably sensitive to document order. Additionally, pairwise strategiesQin et al. (2023) explore ranking documents relative to queries, further diversifying the approaches within this field. 2.2Fairness in LLMs Research on fairness in LLMs has gained considerable traction, driven by the realization that biases present in pretraining corpora can lead LLMs to generate content that is not only harmful but also offensive, often resulting in discrimination against marginalized groups. This heightened awareness has spurred increased research efforts aimed at understanding the origins of bias and addressing the detrimental aspects of LLMs Santy et al. (2023) ; Bubeck et al. (2023) . Initiatives like Reinforcement Learning from Human Feedback Ouyang et al. (2022) and Reinforcement Learning for AI Fairness Bai et al. (2022) seek to mitigate the reinforcement of existing stereotypes and the generation of demeaning content.Beyond existing literature, Kotek et al. (2023) test the presence of gender bias in LLMs and demonstrate the biased assumptions from LLMs. FaiRLLM Zhang et al. (2023) critically evaluates RecLLM's fairness, highlighting biases in ChatGPT recommendations by user attributes. Concurrently, efforts to refine LLM fairness assessments are gaining traction within the NLP community Cheng et al. (2023) ; Ramezani and Xu (2023) . Studies like Brown et al. (2020) andAbid et al. (2021) expose biases in GPT-3's content generation, with the latter noting a violent bias against Muslims.Shen et al. (2023) also found that LLMs may result in misleading and unreliable evaluations for abstractive summarization. Benchmarks such as BBQ Parrish et al. (2022) , CrowS-Pairs Nangia et al. (2020) , RealToxicityPromptsGehman et al. (2020) , and holistic evaluations Liang et al. (2022) further this analysis across various LLMs. DecodingTrustWang et al. (2023) extends this to a detailed fairness exploration in ChatGPT and GPT-4. 2.3Fairness in Search and RankingFair ranking models have been classified into score-based and supervised learning models, as outlined byZehlike et al. (2022) . Score-based models, proposed by researchers like Yang and Stoyanovich (2017) ,Yang et al. (2019) ,Celis et al. (2018) , and Stoyanovich et al. (2018) , intervene on score outcomes to enhance fairness.Kleinberg and Raghavan (2018) andAsudeh et al. (2019) designed models to correct training data biases and establish fair ranking functions.In supervised models, various approaches are employed at different stages.Lahoti et al. (2019) introduced pre-processing models for unbiased model training.Zehlike and Castillo (2020) developed DELTR, the first listwise LTR loss function, combining fairness and ranking metrics.Beutel et al. (2019) ,Ma et al. (2022) , andHaak and Schaer (2022) contributed to in-processing models, addressing exposure bias and query fairness. Chu et al. (2021) highlighted biases in neural architecture search methods. Post-processing models, like FA*IR by Zehlike et al. (2017) and CFA\\theta Zehlike et al. (2020) , re-rank outputs to meet fairness metrics.Biega et al. (2018) proposed an algorithm optimizing the equity of user attention.Wang et al. (2022) proposed a meta-learning approach to train an unbiased model with a meta-learner, and Wang et al. (2024) proposed a general fair ranking framework to learn progressively on the unbiased meta-dataset with a meta-learner. Despite these advancements, there is a lack of research specifically on the fairness of LLMs as rankers.",
                "abstract": "The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works (e.g., RankGPT) have also demonstrated that the LLMs exhibit better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker."
            },
            {
                "name": "Editable Fairness: Fine-Grained Bias Mitigation in Language Models",
                "arxiv_id": "2408.11843",
                "subtitles": [
                    "Bias Mitigation in Large Language Models",
                    "Knowledge Editing"
                ],
                "reference": [
                    "An empirical analysis of parameter-efficient methods for debiasing pre-trained language models",
                    "Causal analysis of syntactic agreement mechanisms in neural language models",
                    "Knowledge neurons in pretrained transformers",
                    "Large language model bias mitigation from the perspective of knowledge editing",
                    "Memory-based model editing at scale",
                    "Mass-editing memory in a transformer",
                    "Fast model debias with machine unlearning",
                    "Towards debiasing sentence representations",
                    "Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases",
                    "Mabel: Attenuating gender bias using textual entailment data",
                    "Locating and editing factual associations in gpt",
                    "Editing commonsense knowledge in gpt",
                    "Transformer-patcher: One mistake worth one neuron",
                    "Co 2 pt: Mitigating bias in pre-trained language models through counterfactual contrastive prompt tuning",
                    "Aging with grace: Lifelong model editing with discrete key-value adaptors",
                    "Null it out: Guarding protected attributes by iterative nullspace projection",
                    "Adept: A debiasing prompt framework",
                    "Learnable privacy neurons localization in language models",
                    "Fixing model bugs with natural language patches",
                    "Measuring and reducing gendered correlations in pre-trained models",
                    "Debiasing pre-trained contextualised embeddings",
                    "Calibrating factual knowledge in pretrained language models",
                    "Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs",
                    "Pmet: Precise model editing in a transformer",
                    "Auto-debias: Debiasing masked language models with automated biased prompts",
                    "Can we edit factual knowledge by in-context learning",
                    "Bias and fairness in large language models: A survey",
                    "Sustainable modular debiasing of language models",
                    "On measuring and mitigating biased inferences of word embeddings",
                    "Editable neural networks",
                    "Diverse adversaries for mitigating bias in training",
                    "Editing factual knowledge in language models",
                    "Fairfil: Contrastive neural debiasing method for pretrained text encoders",
                    "Fast model editing at scale",
                    "Towards a critical race methodology in algorithmic fairness",
                    "Investigating gender bias in language models using causal mediation analysis",
                    "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
                    "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology"
                ],
                "related_work": "2 Related WorksBias Mitigation in Large Language Models. Several approaches have been proposed for debiasing pre-trained language models, which can be grouped into two categories:(1) Fine-tuning. This branch includes additional pre-training on re-balanced corpora[78,70]or with a contrastive objective[33,10], projection-based methods[43,58,36,13]in the embedding space, in-training methods[29,33]and parameter-efficient fine-tuning[39,72]methods.(2) Prompt-tuning. Prompt-tuning[27,73,42,18]involve generating either discrete prompts or continuous prompts to mitigate social biases. There are alsopost-hocapproaches[61,7]that are deployed after the training phase to achieve effective debiasing. However, existing techniques treat social groups as interchangeable[22]and neutralize different social groups in model inputs or outputs, while ignoring or concealing distinct facts of different social groups[30]. In contrast, our method mitigates biases based on fine-grained individual biases, avoiding compromising other knowledge.Knowledge Editing.Knowledge or Model Editing[63,12,11]has been proposed to facilitate data-efficient modifications to model behavior while ensuring no detrimental impact on performance across other inputs. These approaches manipulate the model's output for specific cases either by integrating external models with the original, unchanged model[49,50,17,31,34,77], or by altering the model parameters responsible for undesirable output[48,28,32,46]. The most relevant line of work is locate and edit[46,47,11,41,9], which suggests identifying neurons crucial to a model's factual predictions[65,20,8]and subsequently updating the feed-forward weights to edit the output. Inspired by these works, we propose the first fine-grained bias mitigation framework, which enables nuancedly calibrating individual biases with minimal cost.",
                "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs. Code will be publicly available."
            },
            {
                "name": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models",
                "arxiv_id": "2408.12494",
                "subtitles": [
                    "Gender Bias Statement",
                    "Gender Bias in Large Language Models",
                    "Benchmarks for Gender Bias Assessment"
                ],
                "reference": [
                    "An analysis of gender bias studies in natural language processing",
                    " \"Like Lesbians Walking the Perimeter \": Experiences of U.S. LGBTQ+ Folks With Online Security, Safety, and Privacy Advice",
                    "French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English",
                    "Sex and gender analysis improves science and engineering",
                    "Quantifying Social Biases Using Templates is Unreliable",
                    "Queer In AI: A Case Study in Community-Led Participatory AI",
                    "Measuring and Mitigating Name Biases in Neural Machine Translation",
                    "Gender bias and stereotypes in Large Language Models",
                    "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                    "Convention on AI and Human Rights",
                    "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
                    " \"I'm fully who I am \": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
                    "'It's destroyed me completely': Kenyan moderators decry toll of training of AI models",
                    "Skilled or Gullible f Gender Stereotypes Related to Computer Security and Privacy",
                    "Evaluating Gender Bias in Machine Translation",
                    "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                    " \"Un-Equal Online Safety? \" A Gender Analysis of Security and Privacy Protection Advice and Behaviour Patterns",
                    "Large pre-trained language models contain human-like biases of what is right and wrong to do",
                    "Blueprint for an AI Bill of Rights",
                    "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
                    "Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
                    "Quantifying ChatGPT's gender bias",
                    "StereoSet: Measuring stereotypical bias in pretrained language models",
                    "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
                    "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
                    "Language (Technology) is Power: A Critical Survey of  \"Bias \" in NLP",
                    "On Evaluating and Mitigating Gender Biases in Multilingual Settings"
                ],
                "related_work": "2.Background and related work We delve into the pivotal research surrounding gender bias within the field of LLMs. We begin by articulating gender bias in the context of diverse gender identities (Sec. 2.1), followed by a review of the phenomena of gender bias (Sec. 2.2). Lastly, we analyze the current approaches for constructing benchmarks in gender bias assessment (Sec. 2.3). 2.1.Gender Bias Statement Before looking into the nuances of gender bias, it is essential to distinguish between 'sex' and 'gender.' 'Sex' refers to the biological differences between male and female bodies. In contrast, 'gender' encompasses a broader spectrum, including the array of identities beyond the male-female binary, such as transgender, genderqueer, non-binary, and more (Tannenbaum et al., 2019). This distinction is crucial in addressing gender bias, as it recognizes the varied and personal nature of gender identity, challenging traditional perceptions. With this understanding of gender, we can define gender bias as prejudicial attitudes or discriminatory actions based on an individual's gender identity. Gender bias manifests in harmful stereotypes and unequal treatment, affecting not just women and men but all genders across the spectrum. It can be both overt and subtle, embedded in societal norms and influencing perceptions across different communities (Costa-juss\u00e0, 2019). This broader perspective is essential for a comprehensive approach to gender bias, addressing the specific challenges faced by various gender identities, including marginalized transgender and non-binary (TGNB) identities. 2.2.Gender Bias in Large Language Models The gender bias in LLMs is highlighted in several studies (Felkner et al., 2023; Blodgett et al., 2020; Kapoor and Narayanan, 2023; Nadeem et al., 2021; Stanovsky et al., 2019; Schramowski et al., 2022; Ovalle et al., 2023), underscoring the risks associated with biased AI outputs. The emergence of gender bias within the realm of LLMs poses significant challenges, particularly when considering the diverse gender identities. LLMs exhibit biases against binary genders, predominantly in the form of reinforcing gender stereotypes. Research has shown that these models frequently associate professions, behaviors, and traits with specific genders based on outdated and culturally ingrained stereotypes (Si et al., 2022; Coopamootoo and Ng, 2023; Geeng et al., 2022; Wei et al., 2023). For instance, LLMs have been observed to link nursing and teaching predominantly with women, and engineering or leadership roles with men (Bolukbasi et al., 2016; Vashishtha et al., 2023; Guo et al., 2022). Such biases not only reflect societal prejudices but also perpetuate them, further entrenching gender stereotypes in digital interactions and decision-making processes (Kiritchenko and Mohammad, 2018; Wang et al., 2022; N\u00e9v\u00e9ol et al., 2022). Particularly, Kapoor and Narayanan (Kapoor and Narayanan, 2023) provide shocking evidence that mainstream LLMs reinforce gender stereotypes. They test GPT-3.5 and GPT-4 with the gender-biased dataset Winobias (Zhao et al., 2018) and find that an average of 34% in GPT-3.5's outputs and 26% of GPT-4's output reveal gender stereotypes or biased language. This challenge intensifies when considering non-binary and diverse gender identities. LLMs, primarily trained on datasets that lack representation of non-binary genders, struggle to adequately recognize and represent these identities. This results in the erasure or misrepresentation of non-binary individuals, contributing to their marginalization. Ovalle et al. (Ovalle et al., 2023) highlight that the text generated by LLMs fails to acknowledge the existence of genders beyond the male-female binary, leading to a lack of visibility and recognition for non-binary and genderqueer individuals. Furthermore, a notable survey by QueerInAI reveals that over 65% of respondents from the LGBTQIA+ community have experienced increased digital discrimination correlating with biased AI outputs (QueerInAI et al., 2023). These findings raise concerns about AI technology, as they could exacerbate harmful gender stereotypes and destabilize digital interactions across various domains. Such biases have the potential to deepen gender disparities and impede progress toward gender equality. In response, countries and regions are introducing legal frameworks to combat gender discrimination in algorithmic systems, such as the U.S.'s Blueprint for an AI Bill of Rights (House, 2023) and the EU's Convention on AI and Human Rights (Parliament and of the Council, 2023). This underscores the critical need for effective assessment and reduction of gender bias in LLMs, not just as a technical challenge but as a societal imperative to ensure equitable and respectful AI interactions. 2.3.Benchmarks for Gender Bias Assessment Assessing gender bias in LLMs is a multifaceted challenge. Current techniques for assessing gender bias are predominantly categorized into three strategies: template-based (Sec. 2.3.1), phrase-based (Sec. 2.3.2), and option-based (Sec. 2.3.3). While these methods have advanced our understanding and assessment of gender bias, they also exhibit limitations, especially when considering the public's aspiration for realistic and objective bias assessment. 2.3.1.Template-based benchmarks Template-based benchmarks in gender bias assessment involve the creation of datasets by modifying sentence templates to include different gender identities. This strategy (e.g., EEC (Kiritchenko and Mohammad, 2018), Winobias (Zhao et al., 2018), Winoqueer (Felkner et al., 2023)) is operationalized by altering specific elements in sentences to reflect various gender identities, thus enabling an assessment of the model's response to these changes. Specifically, EEC and Winobias primarily focus on identifying gender bias by altering pronouns and associated gender roles within sentences, revealing how models perceive gender in professional and social roles. Winoqueer extends this by including a wider range of gender identities beyond the binary, examining model responses to diverse gender expressions and roles. Template-based approaches offer a straightforward and simple way to manipulate gender variables within sentence structures. However, they come with notable limitations. One significant drawback is the lack of transparency in how templates are chosen and constructed. Additionally, these methods are often sensitive to changes in template structure, as exemplified in Fig. 1. For instance, when using the template  \"The situation makes [GENDER] feel [EMOTION WORD]\" with EEC, modifying the template while keeping its content intact can result in different outcomes. This highlights the limited ability of this approach to capture the intricacies and nuances of natural language, potentially leading to biased gender bias assessments (Seshadri et al., 2022). The rigid template structure may not accurately reflect the fluidity and diversity of real-world language usage, affecting the realism and applicability of assessment findings. 2.3.2.Phrase-based benchmarks Phrase-based approaches for evaluating gender bias in LLMs involve the use of seed phrases to initiate text generation by these LLMs. This strategy aims to mirror more natural language generation processes. A prominent example is the BOLD dataset (Dhamala et al., 2021), which is specifically designed to assess biases in open-ended text generation by providing LLMs with seed phrases and instructing them to complete these phrases. Its seed phrases are excerpted from Wikipedia, encompassing diverse domains and contexts that explicitly or implicitly relate to gender, thereby offering insights into the models' gender bias. The primary advantage of phrase-based approaches is their intuitive nature, closely aligning with natural language processes, thereby providing a more realistic setting for bias assessment. However, its one significant limitation is the potential biases inherent in the phrases themselves. For instance, as illustrated in Fig. 1, an analysis of the BOLD dataset reveals biases in the seed phrases. The dataset's division shows biased descriptions in the seed phrases for both gender groups. This raises concerns about the objectivity of the dataset, as the inherent biases in the prompts could lead to skewed results. Another limitation arises from the dataset's reliance on public resources like Wikipedia. According to Kotek et al. (Kotek et al., 2023), the complete original content corresponding to the seed phrase, extracted from the widely used public domain, may be included in the model's training data, which can subsequently affect the objectivity of the assessment results. 2.3.3.Option-based benchmarks Option-based approaches present statements with multiple response choices, including biased, neutral, and unrelated options. A notable example is StereoSet (Nadeem et al., 2021), a benchmark designed to evaluate bias in language models. Within this framework, language models are presented with statements and are asked to select responses that reveal their underlying biases or demonstrate a lack thereof. The primary objective is to assess the model's propensity towards biased responses in various scenarios, thereby shedding some light on its inherent biases. Option-based methods offer a substantial advantage by encompassing a broad spectrum of scenarios and biases, providing a comprehensive perspective on a model's inclinations. Nonetheless, the creation of such benchmarks necessitates extensive manual scrutiny and classification of options, starting from contextual statements to the selection of response choices. Particularly during the data curation phase, the manual review and selection of sentences entail significant human resources, rendering the process both time-consuming and costly. As highlighted by The Guardian's report (Guardian, 2023), content reviewers involved in AI systems, such as OpenAI, may experience psychological distress due to the nature of their work, often without sufficient warnings or support, and are typically compensated at relatively low rates. Furthermore, the reliance on crowdsourcing platforms for option classification introduces a high degree of subjectivity. Most importantly, this strategy struggles to directly measure biases in open-ended responses, limiting its ability to mimic real-world interactions. A significant gap apparent in these three strategies is their limited attention to transgender and non-binary (TGNB) identities, which tend to be overlooked in the construction of benchmarks. Except for the template-based strategy, the other two strategies notably lack a comprehensive framework for assessing bias related to TGNB gender identities. This omission poses a challenge to achieving a truly inclusive gender bias assessment. Existing methodologies underscore the necessity for establishing unified criteria that encompass the multifaceted nature of gender equality benchmarks, ensuring both the realism and objectivity of the assessment process. This leads to the development of more comprehensive and inclusive benchmarks, thereby advancing the field towards more realistic and equitable solutions in gender bias assessment within LLMs.",
                "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in natural language generation, but they have also been observed to magnify societal biases, particularly those related to gender. In response to this issue, several benchmarks have been proposed to assess gender bias in LLMs. However, these benchmarks often lack practical flexibility or inadvertently introduce biases. To address these shortcomings, we introduce GenderCARE, a comprehensive framework that encompasses innovative Criteria, bias Assessment, Reduction techniques, and Evaluation metrics for quantifying and mitigating gender bias in LLMs. To begin, we establish pioneering criteria for gender equality benchmarks, spanning dimensions such as inclusivity, diversity, explainability, objectivity, robustness, and realisticity. Guided by these criteria, we construct GenderPair, a novel pair-based benchmark designed to assess gender bias in LLMs comprehensively. Our benchmark provides standardized and realistic evaluations, including previously overlooked gender groups such as transgender and non-binary individuals. Furthermore, we develop effective debiasing techniques that incorporate counterfactual data augmentation and specialized fine-tuning strategies to reduce gender bias in LLMs without compromising their overall performance. Extensive experiments demonstrate a significant reduction in various gender bias benchmarks, with reductions peaking at over 90% and averaging above 35% across 17 different LLMs. Importantly, these reductions come with minimal variability in mainstream language tasks, remaining below 2%. By offering a realistic assessment and tailored reduction of gender biases, we hope that our GenderCARE can represent a significant step towards achieving fairness and equity in LLMs. More details are available atthis https URL."
            }
        ],
        "survey": {
            "name": "Fairness and Bias in Multimodal AI: A Survey",
            "arxiv_id": "2406.19097",
            "subtitles": [
                {
                    "name": "Introduction",
                    "key_history": [
                        {
                            "reference_title": "Bias and fairness in multimodal machine learning: A case study of automated video interviews",
                            "key_word": "Fairness and Bias in Multimodal Models"
                        },
                        {
                            "reference_title": "The dark side of dataset scaling: Evaluating racial classification in multimodal models.",
                            "key_word": "Criminal Prediction Bias"
                        },
                        {
                            "reference_title": "Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
                            "key_word": "Sexualized Image Generation"
                        },
                        {
                            "reference_title": "Fairness in language models beyond English: Gaps and challenges",
                            "key_word": "Quantifying Bias: Preuse, Intrinsic, and Extrinsic"
                        },
                        {
                            "reference_title": "50 years of test (un) fairness: Lessons for machine learning",
                            "key_word": "Fairness in AI"
                        }
                    ],
                    "references_in_this_section": [
                        "A survey on bias and fairness in machine learning",
                        "Taxonomy of risks posed by language models",
                        "Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
                        "Bipol: A novel multi-axes bias evaluation metric with explainability for nlp",
                        "50 years of test (un) fairness: Lessons for machine learning",
                        "A method of assessing bias in test items",
                        "State-of-the-art in open-domain conversational ai: A survey",
                        "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
                        "Fair generation through prior modification",
                        "Foundation and large language models: fundamentals, challenges, opportunities, and social impacts",
                        "Evaluating bias and fairness in gender-neutral pretrained vision-and-language models",
                        "The dark side of dataset scaling: Evaluating racial classification in multimodal models",
                        "The ethics of chatgpt in medicine and healthcare: a systematic review on large language models (llms",
                        "Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems",
                        "A survey on evaluation of large language models",
                        "Language (technology) is power: A critical survey of  \"bias\" in NLP",
                        "On the dangers of stochastic parrots: Can language models be too big",
                        "Towards understanding and mitigating social biases in language models",
                        "Bias and fairness in large language models: A survey",
                        "Auditing the ai auditors: A framework for evaluating fairness and bias in high stakes ai predictive models",
                        "Fairness evaluation within large language models through the lens of depression",
                        "Bias and unfairness in machine learning models: a systematic review on datasets, tools, fairness metrics, and identification and mitigation methods",
                        "Fairness in language models beyond English: Gaps and challenges",
                        "On measuring fairness in generative models",
                        "A survey on datasets for fairness-aware machine learning",
                        "Mitigating gender bias in natural language processing: Literature review",
                        "Bias and fairness in multimodal machine learning: A case study of automated video interviews",
                        "Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies"
                    ]
                },
                {
                    "name": "Fairness and Bias in AI",
                    "key_history": [
                        {
                            "reference_title": "Organizational justice: Yesterday, today, and tomorrow. Journal of management",
                            "key_word": "Justice Theory"
                        },
                        {
                            "reference_title": "What should be done with equity theory? new approaches to the study of fairness in social relationships",
                            "key_word": "Equity Theory"
                        },
                        {
                            "reference_title": "Objectification theory: Toward understanding women's lived experiences and mental health risks",
                            "key_word": "Objectification Theory"
                        },
                        {
                            "reference_title": "Objectification theory: Toward understanding women's lived experiences and mental health risks",
                            "key_word": "Consequences & Legal Perspectives"
                        }
                    ],
                    "references_in_this_section": [
                        "Equity theory revisited: Comments and annotated bibliography",
                        "A survey on bias and fairness in machine learning",
                        "Now you see me, now you don't: Detecting sexual objectification through a change blindness paradigm",
                        "Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
                        "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
                        "50 years of test (un) fairness: Lessons for machine learning",
                        "From women to objects: Appearance focus, target gender, and perceptions of warmth, morality and competence",
                        "Everyday sexism: Evidence for its incidence, nature, and psychological impact from three daily diary studies",
                        "What should be done with equity theory? new approaches to the study of fairness in social relationships",
                        "Differential validity and differential prediction of cognitive ability tests: Understanding test bias in the employment context",
                        "DebIE: A platform for implicit and explicit debiasing of word embedding spaces",
                        "Organizational justice: Yesterday, today, and tomorrow",
                        "Biases in large language models: origins, inventory, and discussion",
                        "Auditing the ai auditors: A framework for evaluating fairness and bias in high stakes ai predictive models",
                        "An empirical analysis of compute-optimal large language model training",
                        "Objectification theory: Toward understanding women's lived experiences and mental health risks",
                        "Language models are unsupervised multitask learners",
                        "Undesirable biases in nlp: Addressing challenges of measurement",
                        "Interpretability and fairness evaluation of deep learning models on mimic-iv dataset"
                    ]
                },
                {
                    "name": "Method",
                    "key_history": [
                        {
                            "reference_title": "Attention is all you need",
                            "key_word": "Vaswani"
                        },
                        {
                            "reference_title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
                            "key_word": "Influence of Transformers on Computer Vision"
                        }
                    ],
                    "references_in_this_section": [
                        "Attention is all you need",
                        "Lessons from applying the systematic literature review process within the software engineering domain",
                        "Academic plagiarism detection: a systematic literature review",
                        "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
                        "Procedures for performing systematic reviews"
                    ]
                },
                {
                    "name": "Fairness and Bias in LLM s and LMM s",
                    "key_history": [
                        {
                            "reference_title": "Language models are unsupervised multitask learners",
                            "key_word": "Bias in Language Models"
                        },
                        {
                            "reference_title": "American== white in multimodal language-and-image ai",
                            "key_word": "Bias in Multimodal Models"
                        },
                        {
                            "reference_title": "Afriwoz: Corpus for exploiting cross-lingual transfer for dialogue generation in low-resource, african languages",
                            "key_word": "Bias in Multilingual AI "
                        },
                        {
                            "reference_title": "Cultural differences in bias? origin and gender bias in pre-trained german and french word embeddings",
                            "key_word": "Bias in German and Dutch Language Models"
                        },
                        {
                            "reference_title": "Large pre-trained language models contain human-like biases of what is right and wrong to do",
                            "key_word": "Ethical Challenges in AI"
                        }
                    ],
                    "references_in_this_section": [
                        "Microsoft coco: Common objects in context",
                        "Vqgan-clip: Open domain image generation and editing with natural language guidance",
                        "Bloom: A 176b-parameter open-access multilingual language model",
                        "Mitigating biases in multimodal personality assessment",
                        "Foundation and large language models: fundamentals, challenges, opportunities, and social impacts",
                        "The pile: An 800gb dataset of diverse text for language modeling",
                        "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
                        "The chicago face database: A free stimulus set of faces and norming data",
                        "Deep residual learning for image recognition",
                        "Multi30K: Multilingual English-German image descriptions",
                        "Zero-shot text-to-image generation",
                        "Explaining first impressions: Modeling, recognizing, and explaining apparent personality from videos",
                        "Language models are unsupervised multitask learners",
                        "Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning",
                        "High-resolution image synthesis with latent diffusion models",
                        "Interpretability and fairness evaluation of deep learning models on mimic-iv dataset",
                        "A fairness-aware fusion framework for multimodal cyberbullying detection",
                        "Measuring bias in multimodal models: Multimodal composite association score",
                        "Bipol: A novel multi-axes bias evaluation metric with explainability for nlp",
                        "Multilingual Twitter corpus and baselines for evaluating demographic bias in hate speech recognition",
                        "Bias at a second glance: A deep dive into bias for german educational peer-review data modeling",
                        "Challenges in detoxifying language models",
                        "Counterfactually measuring and eliminating social bias in vision-language pre-training models",
                        "Glam: Efficient scaling of language models with mixture-of-experts",
                        "Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models",
                        "Assessing multilingual fairness in pre-trained multimodal representations",
                        "Vqa: Visual question answering",
                        "Data bias according to bipol: Men are naturally right and it is the role of women to follow their lead",
                        "Into the laion's den: Investigating hate in multimodal datasets",
                        "Cultural differences in bias? origin and gender bias in pre-trained german and french word embeddings",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "Whose opinions do language models reflect",
                        "Crossmodal-3600: A massively multilingual multimodal evaluation dataset",
                        "Roberta: A robustly optimized bert pretraining approach",
                        "Fairness in language models beyond English: Gaps and challenges",
                        "Harnessing large language models in nursing care planning: opportunities, challenges, and ethical considerations",
                        "A multidimensional analysis of social biases in vision transformers",
                        "A multi-dimensional study on bias in vision-language models",
                        "Bias and fairness in multimodal machine learning: A case study of automated video interviews",
                        "Semantics derived automatically from language corpora contain human-like biases",
                        "Voxceleb2: Deep speaker recognition",
                        "Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies",
                        "Measuring bias in contextualized word representations",
                        "Vernacular? i barely know her: Challenges with style control and stereotyping",
                        "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                        "Voxceleb: Large-scale speaker verification in the wild",
                        "Palm: scaling language modeling with pathways",
                        "Human-centric multimodal machine learning: Recent advances and testbed on ai-based recruitment",
                        "Evaluating bias and fairness in gender-neutral pretrained vision-and-language models",
                        "Mimic-iv, a freely accessible electronic health record dataset",
                        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
                        "Large pre-trained language models contain human-like biases of what is right and wrong to do",
                        "Gpt-4 technical report",
                        "Visual instruction tuning",
                        "Afriwoz: Corpus for exploiting cross-lingual transfer for dialogue generation in low-resource, african languages",
                        "Image as a foreign language: Beit pretraining for vision and vision-language tasks",
                        "Pali: A jointly-scaled multilingual language-image model",
                        "On the dangers of stochastic parrots: Can language models be too big",
                        "Bias and fairness in large language models: A survey",
                        "Llama 2: Open foundation and fine-tuned chat models",
                        "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
                        "On measuring social biases in sentence encoders",
                        "Demographic fairness in multimodal biometrics: A comparative analysis on audio-visual speaker recognition systems",
                        "On measuring fairness in generative models",
                        "Aurora-m: The first open source multilingual language model red-teamed according to the us executive order",
                        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                        "Hierarchical text-conditional image generation with clip latents",
                        "Slip: Self-supervision meets language-image pre-training",
                        "Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
                        "Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine",
                        "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
                        "RobBERT: a Dutch RoBERTa-based Language Model",
                        "Improved baselines with visual instruction tuning",
                        "Datacomp: In search of the next generation of multimodal datasets",
                        "GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models",
                        "Lamda: Language models for dialog applications",
                        "CCAligned: A massive collection of cross-lingual web-document pairs",
                        "What's in the box? an analysis of undesirable content in the common crawl corpus",
                        "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                        "The woman worked as a babysitter: On biases in language generation",
                        "Laion-5b: An open large-scale dataset for training next generation image-text models",
                        "The falcon series of open language models",
                        "A survey on evaluation of large language models",
                        "Mixtral of experts",
                        "American== white in multimodal language-and-image ai",
                        "Measuring gender bias in german language generation",
                        "Learning transferable visual models from natural language supervision",
                        "Bipol: Multi-axes evaluation of bias with explainability in benchmark datasets",
                        "Ctrl: A conditional transformer language model for controllable generation",
                        "Language models are few-shot learners",
                        "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
                        "RealToxicityPrompts: Evaluating neural toxic degeneration in language models"
                    ]
                },
                {
                    "name": "Discussion",
                    "key_history": [
                        {
                            "reference_title": "Semantics derived automatically from language corpora contain human-like biases",
                            "key_word": "Embedding Association Tests (EATs) in Fairness and Bias Evaluation"
                        },
                        {
                            "reference_title": "Gender bias in coreference resolution: Evaluation and debiasing methods.",
                            "key_word": "Counterfactual Data Augmentation (CDA) for Debiasing"
                        },
                        {
                            "reference_title": "Into the laion's den: Investigating hate in multimodal datasets",
                            "key_word": "Curate Over Crawl for Improved Dataset Quality"
                        },
                        {
                            "reference_title": "Mitigating biases in multimodal personality assessment",
                            "key_word": "Adversarial Learning for Fairness"
                        },
                        {
                            "reference_title": "Towards debiasing sentence representations",
                            "key_word": "Autoregressive Iterative Nullspace Projection (A-INLP) Method"
                        }
                    ],
                    "references_in_this_section": [
                        "A survey on bias and fairness in machine learning",
                        "Understanding undesirable word embedding associations",
                        "Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation",
                        "Fairness-aware structured pruning in transformers",
                        "Taxonomy of risks posed by language models",
                        "Distilling the knowledge in a neural network",
                        "Datasheets for datasets",
                        "Fairdistillation: mitigating stereotyping in language models",
                        "Honest: Measuring hurtful sentence completion in language models",
                        "BBQ: A hand-built bias benchmark for question answering",
                        "Multimodal biomedical ai",
                        "Multibench: Multiscale benchmarks for multimodal representation learning",
                        "Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
                        "Harvesting implicit group attitudes and beliefs from a demonstration web site",
                        "Bipol: A novel multi-axes bias evaluation metric with explainability for nlp",
                        "Towards debiasing sentence representations",
                        "Instruction makes a difference",
                        "Distributed representations of words and phrases and their compositionality",
                        "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
                        "Grad-cam: Visual explanations from deep networks via gradient-based localization",
                        "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
                        "Learning gender-neutral word embeddings",
                        "Multimodal bias: Assessing gender bias in computer vision models with nlp techniques",
                        "ROBBIE: Robust bias evaluation of large generative language models",
                        "Mitigating biases in multimodal personality assessment",
                        "The sexual objectification and emotion database: A free stimulus set and norming data of sexually objectified and non-objectified female targets expressing multiple emotions",
                        "Rethinking fairness: An interdisciplinary survey of critiques of hegemonic ml fairness approaches",
                        "Gender bias in coreference resolution: Evaluation and debiasing methods",
                        "Generative ai and teachers-for us or against us? a case study",
                        "Glove: Global vectors for word representation",
                        "On the limitations of large language models (llms): False attribution",
                        "Bold: Dataset and metrics for measuring biases in open-ended language generation",
                        "Evaluating bias and fairness in gender-neutral pretrained vision-and-language models",
                        "Improving gender fairness of pre-trained language models without catastrophic forgetting",
                        "Stereotype and skew: Quantifying gender bias in pre-trained and fine-tuned language models",
                        "The casual conversations v2 dataset : A diverse, large benchmark for measuring fairness and robustness in audio/vision/speech models",
                        "The dark side of dataset scaling: Evaluating racial classification in multimodal models",
                        "Data bias according to bipol: Men are naturally right and it is the role of women to follow their lead",
                        "On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning",
                        "Gender bias in coreference resolution",
                        "Gender shades: Intersectional accuracy disparities in commercial gender classification",
                        "Into the laion's den: Investigating hate in multimodal datasets",
                        "Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias",
                        "Cultural differences in bias? origin and gender bias in pre-trained german and french word embeddings",
                        "DebIE: A platform for implicit and explicit debiasing of word embedding spaces",
                        "Measuring and reducing gendered correlations in pre-trained models",
                        "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
                        "RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models",
                        "Getting gender right in neural machine translation",
                        "Auto-debias: Debiasing masked language models with automated biased prompts",
                        "Towards understanding and mitigating social biases in language models",
                        "Enriching word vectors with subword information",
                        "Sustainable modular debiasing of language models",
                        "Scaling up visual and vision-language representation learning with noisy text supervision",
                        "Overcoming catastrophic forgetting in neural networks",
                        "Learning transferable visual models from natural language supervision",
                        "Bipol: Multi-axes evaluation of bias with explainability in benchmark datasets",
                        "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
                        "Attenuating bias in word vectors",
                        "Parameter-efficient modularised bias mitigation via AdapterFusion",
                        "Mitigating unwanted biases with adversarial learning",
                        "Lessons from archives: Strategies for collecting sociocultural data in machine learning",
                        "AdapterFusion: Non-destructive task composition for transfer learning",
                        "On measuring social biases in sentence encoders",
                        "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
                        "Parameter-efficient transfer learning for NLP",
                        "On measuring fairness in generative models",
                        "Mitigating gender bias in natural language processing: Literature review",
                        "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
                        "Bias and fairness in multimodal machine learning: A case study of automated video interviews",
                        "Semantics derived automatically from language corpora contain human-like biases",
                        "Dear: Debiasing vision-language models with additive residuals",
                        "Yfcc100m: The new data in multimedia research",
                        "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
                        "Interpretability and fairness evaluation of deep learning models on mimic-iv dataset"
                    ]
                }
            ],
            "all_references": [
                "Microsoft coco: Common objects in context",
                "Multimodal biomedical ai",
                "Vqgan-clip: Open domain image generation and editing with natural language guidance",
                "Instruction makes a difference",
                "Beavertails: Towards improved safety alignment of llm via a human-preference dataset",
                "Bloom: A 176b-parameter open-access multilingual language model",
                "The sexual objectification and emotion database: A free stimulus set and norming data of sexually objectified and non-objectified female targets expressing multiple emotions",
                "Mitigating biases in multimodal personality assessment",
                "Foundation and large language models: fundamentals, challenges, opportunities, and social impacts",
                "What should be done with equity theory? new approaches to the study of fairness in social relationships",
                "On the limitations of large language models (llms): False attribution",
                "The pile: An 800gb dataset of diverse text for language modeling",
                "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
                "Differential validity and differential prediction of cognitive ability tests: Understanding test bias in the employment context",
                "The chicago face database: A free stimulus set of faces and norming data",
                "Deep residual learning for image recognition",
                "Measuring and reducing gendered correlations in pre-trained models",
                "Multi30K: Multilingual English-German image descriptions",
                "Zero-shot text-to-image generation",
                "Towards understanding and mitigating social biases in language models",
                "An empirical analysis of compute-optimal large language model training",
                "Attention is all you need",
                "Explaining first impressions: Modeling, recognizing, and explaining apparent personality from videos",
                "Language models are unsupervised multitask learners",
                "Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning",
                "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
                "High-resolution image synthesis with latent diffusion models",
                "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
                "Equity theory revisited: Comments and annotated bibliography",
                "Interpretability and fairness evaluation of deep learning models on mimic-iv dataset",
                "A fairness-aware fusion framework for multimodal cyberbullying detection",
                "Now you see me, now you don't: Detecting sexual objectification through a change blindness paradigm",
                "Taxonomy of risks posed by language models",
                "Fairness-aware structured pruning in transformers",
                "Measuring bias in multimodal models: Multimodal composite association score",
                "Bipol: A novel multi-axes bias evaluation metric with explainability for nlp",
                "Multilingual Twitter corpus and baselines for evaluating demographic bias in hate speech recognition",
                "Bias at a second glance: A deep dive into bias for german educational peer-review data modeling",
                "State-of-the-art in open-domain conversational ai: A survey",
                "From women to objects: Appearance focus, target gender, and perceptions of warmth, morality and competence",
                "Grad-cam: Visual explanations from deep networks via gradient-based localization",
                "Challenges in detoxifying language models",
                "Counterfactually measuring and eliminating social bias in vision-language pre-training models",
                "Glam: Efficient scaling of language models with mixture-of-experts",
                "Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models",
                "Everyday sexism: Evidence for its incidence, nature, and psychological impact from three daily diary studies",
                "Fair generation through prior modification",
                "Bold: Dataset and metrics for measuring biases in open-ended language generation",
                "Gender bias in coreference resolution: Evaluation and debiasing methods",
                "Assessing multilingual fairness in pre-trained multimodal representations",
                "Vqa: Visual question answering",
                "The dark side of dataset scaling: Evaluating racial classification in multimodal models",
                "Data bias according to bipol: Men are naturally right and it is the role of women to follow their lead",
                "Into the laion's den: Investigating hate in multimodal datasets",
                "Cultural differences in bias? origin and gender bias in pre-trained german and french word embeddings",
                "Organizational justice: Yesterday, today, and tomorrow",
                "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "Whose opinions do language models reflect",
                "Crossmodal-3600: A massively multilingual multimodal evaluation dataset",
                "Roberta: A robustly optimized bert pretraining approach",
                "Harnessing large language models in nursing care planning: opportunities, challenges, and ethical considerations",
                "Fairness in language models beyond English: Gaps and challenges",
                "A multidimensional analysis of social biases in vision transformers",
                "A multi-dimensional study on bias in vision-language models",
                "Bias and fairness in multimodal machine learning: A case study of automated video interviews",
                "Semantics derived automatically from language corpora contain human-like biases",
                "Voxceleb2: Deep speaker recognition",
                "Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies",
                "A survey on bias and fairness in machine learning",
                "Understanding undesirable word embedding associations",
                "Measuring bias in contextualized word representations",
                "Vernacular? i barely know her: Challenges with style control and stereotyping",
                "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                "Voxceleb: Large-scale speaker verification in the wild",
                "Towards debiasing sentence representations",
                "A method of assessing bias in test items",
                "Palm: scaling language modeling with pathways",
                "Human-centric multimodal machine learning: Recent advances and testbed on ai-based recruitment",
                "ROBBIE: Robust bias evaluation of large generative language models",
                "Generative ai and teachers-for us or against us? a case study",
                "Evaluating bias and fairness in gender-neutral pretrained vision-and-language models",
                "On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning",
                "Mimic-iv, a freely accessible electronic health record dataset",
                "Gender shades: Intersectional accuracy disparities in commercial gender classification",
                "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
                "Large pre-trained language models contain human-like biases of what is right and wrong to do",
                "Gpt-4 technical report",
                "RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models",
                "Visual instruction tuning",
                "Afriwoz: Corpus for exploiting cross-lingual transfer for dialogue generation in low-resource, african languages",
                "On the dangers of stochastic parrots: Can language models be too big",
                "Pali: A jointly-scaled multilingual language-image model",
                "Image as a foreign language: Beit pretraining for vision and vision-language tasks",
                "Bias and fairness in large language models: A survey",
                "Scaling up visual and vision-language representation learning with noisy text supervision",
                "Auditing the ai auditors: A framework for evaluating fairness and bias in high stakes ai predictive models",
                "Llama 2: Open foundation and fine-tuned chat models",
                "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
                "Demographic fairness in multimodal biometrics: A comparative analysis on audio-visual speaker recognition systems",
                "On measuring social biases in sentence encoders",
                "On measuring fairness in generative models",
                "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
                "Aurora-m: The first open source multilingual language model red-teamed according to the us executive order",
                "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
                "Hierarchical text-conditional image generation with clip latents",
                "Slip: Self-supervision meets language-image pre-training",
                "Honest: Measuring hurtful sentence completion in language models",
                "BBQ: A hand-built bias benchmark for question answering",
                "Contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
                "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
                "50 years of test (un) fairness: Lessons for machine learning",
                "Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine",
                "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
                "RobBERT: a Dutch RoBERTa-based Language Model",
                "Improved baselines with visual instruction tuning",
                "Datacomp: In search of the next generation of multimodal datasets",
                "GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models",
                "Gender bias in coreference resolution",
                "Lamda: Language models for dialog applications",
                "CCAligned: A massive collection of cross-lingual web-document pairs",
                "What's in the box? an analysis of undesirable content in the common crawl corpus",
                "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "Laion-5b: An open large-scale dataset for training next generation image-text models",
                "The woman worked as a babysitter: On biases in language generation",
                "The falcon series of open language models",
                "A survey on evaluation of large language models",
                "Mixtral of experts",
                "Auto-debias: Debiasing masked language models with automated biased prompts",
                "Measuring gender bias in german language generation",
                "American== white in multimodal language-and-image ai",
                "Learning transferable visual models from natural language supervision",
                "Fairness evaluation within large language models through the lens of depression",
                "Bipol: Multi-axes evaluation of bias with explainability in benchmark datasets",
                "Ctrl: A conditional transformer language model for controllable generation",
                "Language models are few-shot learners",
                "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
                "Objectification theory: Toward understanding women's lived experiences and mental health risks",
                "RealToxicityPrompts: Evaluating neural toxic degeneration in language models"
            ]
        },
        "topic_history": [
            {
                "name": "Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models",
                "arxiv_id": "2406.12033",
                "reference": [
                    "Predicting depression via social media",
                    "Tram: Benchmarking temporal reasoning for large language models",
                    "A call to action on assessing and mitigating bias in artificial intelligence applications for mental health",
                    "Prominet: Prototype-based multi-view network for interpretable email response prediction",
                    "Bertsurv: Bert-based survival models for predicting outcomes of trauma patients",
                    "Detection of mental health from reddit via deep contextualized representations",
                    "Empirical quantitative analysis of covid-19 forecasting models",
                    "Sensemood: depression detection on social media",
                    "Deep learning and machine learning in psychiatry: a survey of current progress in depression detection, diagnosis and treatment",
                    "Metacognitive prompting improves understanding in large language models",
                    "Mental-llm: Leveraging large language models for mental health prediction via online text data",
                    "Phi-3 technical report: A highly capable language model locally on your phone",
                    "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis",
                    "Detection and classification of anxiety in university students through the application of machine learning",
                    "Artificial intelligence-based approaches for suicide prediction: Hope or hype",
                    "Are large language models ready for healthcare? a comparative study on clinical language understanding",
                    "Is chatgpt a good translator? a preliminary study",
                    "Mentalbert: Publicly available pretrained language models for mental healthcare",
                    "Improving language understanding by generative pre-training",
                    "Gpt-4 technical report",
                    "Detection of suicide ideation in social media forums using deep learning",
                    "Fairehr-clp: Towards fairness-aware clinical predictions with contrastive learning in multimodal electronic health records",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Depression detection from social networks data based on machine learning and deep learning techniques: An interrogative survey",
                    "Audibert: A deep transfer learning multimodal classification framework for depression screening",
                    "Machine learning models to detect anxiety and depression through social media: A scoping review",
                    "Enhancing transformer efficiency for multivariate time series classification",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Evaluation of chatgpt for nlp-based mental health applications",
                    "Artificial intelligence assisted tools for the detection of anxiety and depression leading to suicidal ideation in adolescents: a review",
                    "Gemini: a family of highly capable multimodal models",
                    "Mentallama: Interpretable mental health analysis on social media with large language models",
                    "Integrating physiological time series and clinical notes with transformer for early prediction of sepsis"
                ]
            },
            {
                "name": "CFaiRLLM: Consumer Fairness Evaluation in Large-Language Model Recommender System",
                "arxiv_id": "2403.05668",
                "reference": [
                    "Defining and measuring fairness in location recommendations",
                    "Recommendation as language processing (rlp) : A unified pretrain, personalized prompt & predict paradigm (p",
                    "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
                    "Exploring artist gender bias in music recommendation",
                    "Pareto optimality for fairness-constrained collaborative filtering",
                    "Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems",
                    "The winner takes it all: geographic imbalance and provider (un) fairness in educational recommender systems",
                    "Balanced neighborhoods for multi-sided fairness in recommendation",
                    "Advertisement recommendation based on personal interests and ad push fairness",
                    "OpenP5: Benchmarking Foundation Models for Recommendation",
                    "Estimation of fair ranking metrics with incomplete judgments",
                    "Spot: Better frozen model adaptation through soft prompt transfer",
                    "TFROM: A Two-sided Fairness-Aware Recommendation Model for Both Customers and Providers",
                    "Mitigating sentiment bias for recommender systems",
                    "Balancing between accuracy and fairness for interactive recommendation with reinforcement learning",
                    "A unifying and general account of fairness measurement in recommender systems",
                    "Fair sharing for sharing economy platforms",
                    "Fairness and discrimination in recommendation and retrieval",
                    "Exploiting personalized calibration and metrics for fairness recommendation",
                    "Fairrec: Two-sided fairness for personalized recommendations in two-sided platforms",
                    "Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
                    "Towards universal sequence representation learning for recommender systems",
                    "Does fair ranking improve minority outcomes? understanding the interplay of human and algorithmic biases in online hiring",
                    "Fairness in recommender systems: research landscape and future directions",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "Two-sided fairness in rankings via Lorenz dominance",
                    "Interplay between upsampling and regularization for provider fairness in recommender systems",
                    "Defining and supporting narrative-driven recommendation",
                    "Fairness-aware news recommendation with decomposed adversarial learning",
                    "Towards long-term fairness in recommendation",
                    "Addressing marketing bias in product recommendations",
                    "Language models are few-shot learners",
                    "Fairness among new items in cold start recommender systems",
                    "A flexible framework for evaluating user and item fairness in recommender systems",
                    "Towards understanding and mitigating unintended biases in language model-driven conversational recommendation",
                    "User-item matching for recommendation fairness",
                    "A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News",
                    "The Unfairness of Active Users and Popularity Bias in Point-of-Interest Recommendation",
                    "An enhanced probabilistic fairness-aware group recommendation by incorporating social activeness",
                    "User-oriented fairness in recommendation",
                    "A fairness-aware hybrid recommender system"
                ]
            },
            {
                "name": "Cognitive Bias in High-Stakes Decision-Making with LLMs",
                "arxiv_id": "2403.00811",
                "reference": [
                    "Capturing failures of large language models via human cognitive biases",
                    "Ai-moderated decision-making: Capturing and balancing anchoring bias in sequential decision tasks",
                    "Fairpy: A toolkit for evaluation of social biases and their mitigation in large language models",
                    "Mind the biases: Quantifying cognitive biases in language model prompting",
                    "Instructed to bias: Instruction-tuned language models exhibit emergent cognitive bias",
                    "The framing of decisions and the psychology of choice",
                    "Challenging the appearance of machine intelligence: Cognitive bias in llms",
                    "Towards understanding and mitigating social biases in language models",
                    "Beyond accuracy: Behavioral testing of nlp models with checklist",
                    "Bias in word embeddings",
                    "Man is to computer programmer as woman is to homemaker? debiasing word embeddings"
                ]
            },
            {
                "name": "Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One",
                "arxiv_id": "2402.12150",
                "reference": [
                    "Deep reinforcement learning from human preferences",
                    "Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity",
                    "Improving language model negotiation with self-play and in-context learning from ai feedback",
                    "A survey on fairness in large language models",
                    "Communicative agents for software development, 2023a",
                    "Improving factuality and reasoning in language models through multiagent debate",
                    "Multi-agent collaboration: Harnessing the power of intelligent llm agents",
                    "Llm harmony: Multi-agent communication for problem solving",
                    "Sparks of artificial general intelligence: Early experiments with gpt",
                    "Perturbation augmentation for fairer nlp",
                    "Biasasker: Measuring the bias in conversational ai system",
                    "Persistent anti-muslim bias in large language models",
                    "Experiential co-learning of software-developing agents, 2023b",
                    "Encouraging divergent thinking in large language models through multi-agent debate",
                    "Unqovering stereotyping biases via underspecified questions",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Bias and fairness in large language models: A survey",
                    "Measuring harmful sentence completion in language models for lgbtqia+ individuals",
                    "Language models are few-shot learners",
                    "Trustgpt: A benchmark for trustworthy and responsible large language models",
                    "In-context impersonation reveals large language models' strengths and biases",
                    "Collecting a large-scale gender bias dataset for coreference resolution and machine translation",
                    "Training socially aligned language models on simulated social interactions",
                    "Bbq: A hand-built bias benchmark for question answering",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
                "arxiv_id": "2404.07990",
                "reference": [
                    "Iti-gen: Inclusive text-to-image generation",
                    "Improving the fairness of deep generative models without retraining",
                    "Emerging properties in self-supervised vision transformers",
                    "Unbiased image synthesis via manifold-driven sampling in diffusion models",
                    "Fair diffusion: Instructing text-to-image generation models on fairness",
                    "Classifier-free diffusion guidance",
                    "Repfair-gan: Mitigating representation bias in gans using gradient clipping",
                    "Bias-to-text: Debiasing unknown visual biases through language interpretation",
                    "Vipergpt: Visual inference via python execution for reasoning",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Video chatcaptioner: Towards the enriched spatiotemporal descriptions",
                    "On the opportunities and risks of foundation models",
                    "Visual instruction tuning",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions",
                    "Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering",
                    "Learning transferable visual models from natural language supervision",
                    "Mitigating inappropriateness in image generation: Can there be value in reflecting the world's ugliness",
                    "Visual programming: Compositional visual reasoning without training",
                    "Dinov2: Learning robust visual features without supervision",
                    "Language models are few-shot learners",
                    "Llama: Open and efficient foundation language models",
                    "Reclip: A strong zero-shot baseline for referring expression comprehension",
                    "Sega: Instructing text-to-image models using semantic guidance",
                    "High-resolution image synthesis with latent diffusion models"
                ]
            },
            {
                "name": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
                "arxiv_id": "2407.08441",
                "reference": []
            },
            {
                "name": "Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias",
                "arxiv_id": "2406.00020",
                "reference": [
                    "Historical perspectives on language and identity",
                    "Recognition of They/Them as Singular Personal Pronouns in Coreference Resolution",
                    "Understanding Abuse: A Typology of Abusive Language Detection Subtasks",
                    "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of\" Subjectivity\" and\" Identity Terms",
                    "An Analysis of WordNet's Coverage of Gender Identity Using Twitter and The National Transgender Discrimination Survey",
                    "Fighting Hate Speech, Silencing Drag Queens? Artificial Intelligence in Content Moderation and Risks to LGBTQ Voices Online",
                    "Revisiting Queer Minorities in Lexicons",
                    "Word norms and measures of linguistic reclamation for LGBTQ+ slurs",
                    " \"Building a thick skin for each other \": The use of 'reading' as an interactional practice of mock impoliteness in drag queen backstage talk",
                    "COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements",
                    "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
                    " \"I'm fully who I am \": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
                    "Cursing in English on twitter",
                    "Investigating the role of swear words in abusive language detection tasks",
                    "Challenges in discriminating profanity from hate speech",
                    "Practices of Slur Use",
                    "Hidden behind the obvious: Misleading keywords and implicitly abusive language on social media",
                    "They, Them, Theirs: Rewriting with Gender-Neutral English",
                    "Theories of  \"Gender \" in NLP Bias Research",
                    "Accounting for Offensive Speech as a Practice of Resistance",
                    "Male, Female, and Nonbinary Differences in UK Twitter Self-descriptions: A Fine-grained Systematic Exploration"
                ]
            },
            {
                "name": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers",
                "arxiv_id": "2404.03192",
                "reference": [
                    "Matching code and law: Achieving algorithmic fairness with optimal transport",
                    "Reducing disparate exposure in ranking: A learning to rank approach",
                    "Gender bias and stereotypes in large language models",
                    "Ranking with fairness constraints",
                    "A meta-learning approach to fair ranking",
                    "BBQ: A hand-built bias benchmark for question answering",
                    "Sparks of artificial general intelligence: Early experiments with GPT",
                    "Beyond yes and no: Improving zero-shot LLM rankers via scoring fine-grained relevance labels",
                    "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                    "Fine-tuning llama for multi-stage text retrieval",
                    "Fa*ir: A fair top-k ranking algorithm",
                    "Measuring fairness in ranked outputs",
                    "ifair: Learning individually fair data representations for algorithmic decision making",
                    "Holistic evaluation of language models",
                    "Designing fair ranking schemes",
                    "The expando-mono-duo design pattern for text ranking with pretrained sequence-to-sequence models",
                    "Marked personas: Using natural language prompts to measure stereotypes in language models",
                    "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
                    "Knowledge of cultural moral norms in large language models",
                    "Diversified subgraph query generation with group fairness",
                    "Auditing search query suggestion bias through recursive algorithm interrogation",
                    "Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search",
                    "Passage-specific prompt tuning for passage reranking in question answering with large language models",
                    "Improving passage retrieval with zero-shot question generation",
                    "Balanced ranking with diversity constraints",
                    "Multi-stage document ranking with BERT",
                    "Open-source large language models are strong zero-shot query likelihood models for document ranking",
                    "A unified meta-learning framework for fair ranking with curriculum learning",
                    "Persistent anti-muslim bias in large language models",
                    "Is ChatGPT good at search? investigating large language models as re-ranking agents",
                    "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation",
                    "Constitutional ai: Harmlessness from ai feedback",
                    "Online set selection with fairness and diversity constraints",
                    "Large language models are effective text rankers with pairwise ranking prompting",
                    "Fairness in recommendation ranking through pairwise comparisons",
                    "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
                    "Large language models are not yet human-level evaluators for abstractive summarization",
                    "Zero-shot listwise document reranking with a large language model",
                    "Decodingtrust: A comprehensive assessment of trustworthiness in GPT models",
                    "Equity of attention: Amortizing individual fairness in rankings",
                    "Language models are few-shot learners",
                    "Nlpositionality: Characterizing design biases of datasets and models",
                    "Selection Problems in the Presence of Implicit Bias",
                    "Fairness in ranking, part i: Score-based ranking",
                    "Text-to-text multi-view learning for passage re-ranking",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Editable Fairness: Fine-Grained Bias Mitigation in Language Models",
                "arxiv_id": "2408.11843",
                "reference": [
                    "An empirical analysis of parameter-efficient methods for debiasing pre-trained language models",
                    "Causal analysis of syntactic agreement mechanisms in neural language models",
                    "Knowledge neurons in pretrained transformers",
                    "Large language model bias mitigation from the perspective of knowledge editing",
                    "Memory-based model editing at scale",
                    "Mass-editing memory in a transformer",
                    "Fast model debias with machine unlearning",
                    "Towards debiasing sentence representations",
                    "Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases",
                    "Mabel: Attenuating gender bias using textual entailment data",
                    "Locating and editing factual associations in gpt",
                    "Editing commonsense knowledge in gpt",
                    "Transformer-patcher: One mistake worth one neuron",
                    "Co 2 pt: Mitigating bias in pre-trained language models through counterfactual contrastive prompt tuning",
                    "Aging with grace: Lifelong model editing with discrete key-value adaptors",
                    "Null it out: Guarding protected attributes by iterative nullspace projection",
                    "Adept: A debiasing prompt framework",
                    "Learnable privacy neurons localization in language models",
                    "Fixing model bugs with natural language patches",
                    "Measuring and reducing gendered correlations in pre-trained models",
                    "Debiasing pre-trained contextualised embeddings",
                    "Calibrating factual knowledge in pretrained language models",
                    "Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs",
                    "Pmet: Precise model editing in a transformer",
                    "Auto-debias: Debiasing masked language models with automated biased prompts",
                    "Can we edit factual knowledge by in-context learning",
                    "Bias and fairness in large language models: A survey",
                    "Sustainable modular debiasing of language models",
                    "On measuring and mitigating biased inferences of word embeddings",
                    "Editable neural networks",
                    "Diverse adversaries for mitigating bias in training",
                    "Editing factual knowledge in language models",
                    "Fairfil: Contrastive neural debiasing method for pretrained text encoders",
                    "Fast model editing at scale",
                    "Towards a critical race methodology in algorithmic fairness",
                    "Investigating gender bias in language models using causal mediation analysis",
                    "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
                    "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology"
                ]
            },
            {
                "name": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models",
                "arxiv_id": "2408.12494",
                "reference": [
                    "An analysis of gender bias studies in natural language processing",
                    " \"Like Lesbians Walking the Perimeter \": Experiences of U.S. LGBTQ+ Folks With Online Security, Safety, and Privacy Advice",
                    "French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English",
                    "Sex and gender analysis improves science and engineering",
                    "Quantifying Social Biases Using Templates is Unreliable",
                    "Queer In AI: A Case Study in Community-Led Participatory AI",
                    "Measuring and Mitigating Name Biases in Neural Machine Translation",
                    "Gender bias and stereotypes in Large Language Models",
                    "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
                    "Convention on AI and Human Rights",
                    "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
                    " \"I'm fully who I am \": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
                    "'It's destroyed me completely': Kenyan moderators decry toll of training of AI models",
                    "Skilled or Gullible f Gender Stereotypes Related to Computer Security and Privacy",
                    "Evaluating Gender Bias in Machine Translation",
                    "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
                    " \"Un-Equal Online Safety? \" A Gender Analysis of Security and Privacy Protection Advice and Behaviour Patterns",
                    "Large pre-trained language models contain human-like biases of what is right and wrong to do",
                    "Blueprint for an AI Bill of Rights",
                    "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
                    "Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
                    "Quantifying ChatGPT's gender bias",
                    "StereoSet: Measuring stereotypical bias in pretrained language models",
                    "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
                    "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
                    "Language (Technology) is Power: A Critical Survey of  \"Bias \" in NLP",
                    "On Evaluating and Mitigating Gender Biases in Multilingual Settings"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data",
            "arxiv_id": "2001.07966",
            "isAPA": false,
            "abstract": "In this paper, we introduce a new vision-language pre-trained model - ImageBERT - for image-textjoint embedding. Our model is a Transformer[1]-based model, which takes different modalities asinput and models the relationship between them. The model is pre-trained on four tasks simultaneously: Masked Language Modeling (MLM) , Masked Object Classification (MOC) , Masked RegionFeature Regression (MRFR) , and Image Text Matching (ITM) . To further enhance the pre-trainingquality, we have collected a Large-scale weAk-supervised Image-Text (LAIT) dataset from Web.We first pre-train the model on this dataset, then conduct a second stage pre-training on ConceptualCaptions[2] and SBU Captions[3]. Our experiments show that multi-stage pre-training strategyoutperforms single-stage pre-training. We also fine-tune and evaluate our pre-trained ImageBERTmodel on image retrieval and text retrieval[4] tasks, and achieve new state-of-the-art results on bothMSCOCO[5] and Flickr30k[6] datasets",
            "reference": [
                "Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual question answering",
                "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read",
                "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. ArXiv, abs",
                "Forrest N. Iandola, Matthew W. Moskewicz, Sergey Karayev, Ross B. Girshick, Trevor Darrell, and Kurt Keutzer. Densenet: Implementing efficient convnet descriptor pyramids. ArXiv, abs",
                "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
                "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. ArXiv, abs",
                "Pinghua Gong, Jieping Ye, and Changshui Zhang. Multi-stage multi-task feature learning. Advances in neural information processing systems",
                "Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs",
                "Ranjay Krishna, Yuke Zhu, Oliver Groth, J. M. Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision",
                "Kuang-Huei Lee, Xiao Dong Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching",
                "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. arXiv e-prints, page arXiv:1706.03762, Jun",
                "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv e-prints, page arXiv:1907.11692, Jul",
                "Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li, and Xin Fan. Position focused attention network for image-text matching",
                "Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics",
                "Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                "Botian Shi, Lei Ji, Pan Lu, Zhendong Niu, and Nan Duan. Knowledge aware semantic concept expansion for image-text matching",
                "Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training. arXiv e-prints, page arXiv:1908.06066, Aug",
                "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages",
                "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages",
                "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. arXiv e-prints, page arXiv:1707.07998, Jul",
                "Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From Recognition to Cognition: Visual Commonsense Reasoning. arXiv e-prints, page arXiv:1811.10830, Nov",
                "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. arXiv e-prints, page arXiv:1908.08530, Aug",
                "Luowei Zhou, Hamid Palangi, Lefei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. ArXiv, abs",
                "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv e-prints, page arXiv:1810.04805, Oct",
                "Hao Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers",
                "Amanpreet Singh, Vedanuj Goswami, Vivek Natarajan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Pythia-a platform for vision & language research",
                "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. ArXiv, abs",
                "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv e-prints, page arXiv:1609.08144, Sep",
                "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv e-prints, page arXiv:1504.00325, Apr",
                "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv e-prints, page arXiv:1906.08237, Jun",
                "Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification",
                "Andrej Karpathy and Li Fei-Fei. Deep Visual-Semantic Alignments for Generating Image Descriptions. arXiv e-prints, page arXiv:1412.2306, Dec",
                "Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh. VQA: Visual Question Answering. arXiv e-prints, page arXiv:1505.00468, May"
            ],
            "related work": "2Related WorkAfter Transformer[1]was proposed and widely used by cross-modal researches, the results on various tasks have been pushed to a new Everest in recent one year. Though almost all latest work are based on Transformer, they differ in various ways. We will review these work from different dimensions in below. Model architecture.BERT[10]model is pre-trained for NLP tasks whose input is one or two sentences. To apply BERT structure to cross-modal tasks, there can be many ways to deal with different modalities. ViLBERT[14]and LXMERT[15]applied a single-modal Transformer to image and sentence respectively, then combined the two modalities together with a cross-modal Transformer. Other work, such as VisualBERT[16], B2T2[17], Unicoder-VL[18], VL-BERT[19], Unified VLP[20], UNITER[21], etc., all concatenated image and sentence as a single input to the Transformer. It is hard to argue which model structure is better, since its performance really depends on the specific scenario. Image visual tokens.Almost all recent paper applied an object detection model to the images and treated the detected regions of interest (RoIs) as image descriptors, just as linguistic tokens. Different from other work which used a pre-trained detection model, VL-BERT trained the detection network together with its image-text joint embedding network, and it also added global image features into model training. We can see region-based image features are good image descriptors, and they form a sequence of visual tokens that can be directly fed into Transformer. Pre-train data.Unlike language model pre-training that can leverage tremendous natural language data, vision-language tasks require high quality image descriptions that are hard to obtain for free. Conceptual Captions[2]is the most widely used data for image-text pre-training, given that it has 3M image descriptions and is relatively larger than other datasets. UNITER[21]combines four datasets (Conceptual Captions[2], SBU Captions[3], Visual Genome[22]and MSCOCO[5]) together to form a 9.6M training corpus and achieved state-of-the-art results on many image-text cross-modal tasks. LXMERT[15]added some VQA training data into pre-training and obtained state-of-the-art results on VQA task. We can see that data quality and volume play important roles in model training, and should be paid more attention to when designing new models.",
            "date": "2020"
        },
        "topic": "Large Multi-Modal Language Models",
        "year_start": "2019",
        "year_end": "2024",
        "target_list": [
            {
                "name": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
                "arxiv_id": "2401.15947",
                "subtitles": [
                    "Large Vision-Language Models",
                    "Mixture of Experts in Multi-modal Learning"
                ],
                "reference": [
                    "Lisa: Reasoning segmentation via large language model",
                    "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
                    "Internlm: A multilingual language model with progressively enhanced capabilities",
                    "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
                    "Sparse upcycling: Training mixture-of-experts from dense checkpoints",
                    "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
                    "Gshard: Scaling giant models with conditional computation and automatic sharding",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Improved baselines with visual instruction tuning",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Cot-mote: Exploring contextual masked auto-encoder pre-training with mixture-of-textual-experts for passage retrieval",
                    "Multimodal contrastive learning with limoe: the language-image mixture of experts",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A survey on multimodal large language models",
                    "Eve: Efficient vision-language pre-training with masked prediction and modality-aware moe",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Moss: Training conversational language models from synthetic data",
                    "Kosmos-2: Grounding multimodal large language models to the world",
                    "Grounding language models to images for multimodal generation",
                    "Glamm: Pixel grounding large multimodal model",
                    "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
                    "Gpt-4 technical report",
                    "Bubogpt: Enabling visual grounding in multi-modal llms",
                    "Scaling vision-language models with sparse mixture of experts",
                    "Visual instruction tuning",
                    "Uni-perceiver-moe: Learning sparse generalist models with conditional moes",
                    "Swin transformer v2: Scaling up capacity and resolution",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
                    "Learning factored representations in a deep mixture of experts",
                    "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
                    "Baichuan 2: Open large-scale language models",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Detgpt: Detect what you need via reasoning",
                    "Pace: Unified multi-modal dialogue pre-training with progressive and compositional experts",
                    "Honeybee: Locality-enhanced projector for multimodal llm",
                    "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
                    "Video-llava: Learning united visual representation by alignment before projection",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Alpaca: A strong, replicable instruction-following model",
                    "Llama: Open and efficient foundation language models",
                    "Glm: General language model pretraining with autoregressive blank infilling",
                    "Adaptive mixtures of local experts",
                    "Mixture of cluster-conditional lora experts for vision-language instruction tuning",
                    "Beyond distillation: Task-level mixture-of-experts for efficient inference",
                    "Qwen technical report",
                    "Svit: Scaling up visual instruction tuning",
                    "Multiway-adapater: Adapting large-scale multi-modal models for scalable image-text retrieval",
                    "Flamingo: a visual language model for few-shot learning",
                    "St-moe: Designing stable and transferable sparse expert models",
                    "Rome: Role-aware mixture-of-expert transformer for text-to-video retrieval",
                    "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
                ],
                "related_work": "2Related Work 2.1Large Vision-Language Models Powerful LLMs (OpenAI, 2023; Touvron et al., 2023a; Wei et al., 2022; Touvron et al., 2023b; Zheng et al., 2023; Team, 2023; Sun et al., 2023; Du et al., 2021; Bai et al., 2023a; Yang et al., 2023; Penedo et al., 2023; Taori et al., 2023) with strong instruction-following and generalization capabilities have been applied to LVLMs. Early works such as BLIP-2 (Li et al., 2023b) and FROMAGe (Koh et al., 2023) encoded visual signals into a sequence of visual tokens, successfully adapting vision to LLMs through several projection layers. Subsequently, recent works have focused on improving performance through methods such as expanding the instruction-tuning dataset (Liu et al., 2023a, c; Zhang et al., 2023c; Zhao et al., 2023a; Chen et al., 2023d), optimizing training strategies (Bai et al., 2023b; Chen et al., 2023b), increasing resolution of image (Liu et al., 2023b; Bai et al., 2023b; Wang et al., 2023d) enhancing image encoders (Chen et al., 2023e; Zhang et al., 2023a; Bai et al., 2023b), aligning the input (Lin et al., 2023) and projection layers (Cha et al., 2023; Alayrac et al., 2022; Bai et al., 2023b; Dai et al., 2023; Ye et al., 2023; Zhao et al., 2023a). These works empowered LVLMs with powerful visual understanding capabilities by expanding the visual instruction fine-tuning datasets and model scales. Currently, some works have endowed LVLMs with fine-grained image understanding capabilities, such as region understanding (Chen et al., 2023c; Zhao et al., 2023b; Liu et al., 2023e), multi-region understanding (Wang et al., 2023c; Pi et al., 2023; Peng et al., 2023), and pixel-wise grounding (Rasheed et al., 2023; Lai et al., 2023). However, the cost of scaling up dense visual data and models is challenging to bear (Liu et al., 2022; Yin et al., 2023). In this work, we aim to make state-of-the-art LVLMs research more accessible by leveraging mixture of experts. 2.2Mixture of Experts in Multi-modal Learning Mixture of Experts (MoE) (Jacobs et al., 1991; Eigen et al., 2013) is a hybrid model consisting of multiple sub-models, known as experts, which are integrated together. The key concept of MoE is the use of a router to determine the token set that each expert handles, thereby reducing interference between different types of samples. Hard Routers. In the hard router mode, each expert is typically pre-defined as a specific pattern. This is because multi-modal data naturally exhibit gaps (Liang et al., 2022), making it difficult for soft routers to learn the optimal patterns for assigning tokens to different experts. A series of works (Bao et al., 2022; Long et al., 2023; Satar et al., 2022; Wang et al., 2022; Shen et al., 2023) naturally decouple experts based on modal categories and pre-define each expert to handle a specific modality. An important feature of these hard-based routers is that they do not require learning the router. This mode is also widely applied in the task-specific MoE (Li et al., 2023e; Zhu et al., 2022; Ma et al., 2023; Kudugunta et al., 2021). Soft Routers. Some works (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022; Komatsuzaki et al., 2022) in natural language process have explored the MoE based on soft routers. Soft routers enable dynamic allocation of data among different experts, allowing each expert to focus on its expertise and achieve model sparsity. Therefore, our main focus is on leveraging soft routers in the MoE. Small-scale (million-level) models based on soft routers have also been explored in the context of multi-modal learning, such as EVE (Chen et al., 2023a) and LIMoE (Mustafa et al., 2022), which attempt a fusion of data by using soft routers. The work most relevant to ours is MoCLE (Gou et al., 2023). However, MoCLE clusters different instruction sets and distributes them to different experts, which compromises the flexibility and autonomy of the experts. Differently, MoE-LLaVA relies on knowledge-rich routers to distribute tokens to different paths.",
                "abstract": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released atthis https URL."
            },
            {
                "name": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "arxiv_id": "2403.20330",
                "subtitles": [
                    "Large Vision-Language Models",
                    "Evaluations of LVLMs"
                ],
                "reference": [
                    "Phi2: The surprising power of small language models",
                    "A-okvqa: A benchmark for visual question answering using world knowledge",
                    "A diagram is worth a dozen images",
                    "Palm: Scaling language modeling with pathways",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Internlm: A multilingual language model with progressively enhanced capabilities",
                    "Mistral 7b",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Deepseek llm: Scaling open-source language models with longtermism",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "To see is to believe: Prompting gpt-4v for better visual instruction tuning",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Improved baselines with visual instruction tuning",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Multilayer perceptron (mlp",
                    "Can vision-language models think from a first-person perspective",
                    "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Yi: Open foundation models by 01. ai",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Cheap and quick: Efficient vision-language instruction tuning for large language models",
                    "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Mixtral of experts",
                    "Deepseek-vl: Towards real-world vision-language understanding",
                    "Visual instruction tuning",
                    "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Scaling up visual and vision-language representation learning with noisy text supervision",
                    "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                    "Learning transferable visual models from natural language supervision",
                    "Baichuan 2: Open large-scale language models",
                    "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Chatgpt",
                    "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
                    "Qwen technical report",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Gemini: a family of highly capable multimodal models",
                    "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
                    "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work Large Vision-Language Models. As large language models (LLMs) [8, 43, 43, 47, 42, 34, 36, 9] rapidly advance, a growing fraction of the research community is focusing on integrating visual content into LLMs to build a powerful intelligent assistant with more interactive ways. Central to these large vision-language models (LVLMs) are the seminal works in modality alignment within the vision-language learning area [37, 17]. The foundation work CLIP [37] exemplifies the alignment of vision and language modalities through contrastive learning on extensive image-text pairs. Built upon the CLIP image encoder which is somewhat aligned with the language modality, current LVLMs typically utilize vast image-text pairs to connect the vision encoder and LLM, enabling LLM to receive and understand visual content [54, 26, 24, 11, 52, 2, 48, 31, 5]. For example, MiniGPT4 [54] and LLaVA [26] directly connect the vision encoder and LLM with QFormer [22] and MLP [40], showing proficiency in multi-modal dialogues. Subsequent works have further enhanced LVLMs by improving the multi-modal instruction data [24, 48, 5, 44] and designing novel modules [2, 23, 45, 28, 15, 12] for more sufficient modality alignment. Evaluations of LVLMs. To probe the true capabilities of the emerging LVLMs, the research community has developed many multi-modal benchmarks encompassing a wide range of evaluation axes [27, 14, 38, 51, 39, 21, 26, 50, 46]. Early single-task benchmarks, such as VQA [16], MS-COCO [39], and OK-VQA [38], fail to holistically assess LVLMs' general multi-modal perception and reasoning capabilities. To address this issue, comprehensive multi-modal benchmarks have been constructed [26, 21, 51, 14, 27, 7, 46]. For example, SEED [21] and MMBench [27] cover 12 and 20 evaluation dimensions respectively, while MMMU [51] spans 30 college-level subjects, providing some competitive arenas for a comprehensive comparison of cutting-edge LVLMs. However, existing evaluations of LVLMs overlook some critical issues. On the one hand, they do not guarantee that all evaluation samples can not be correctly answered without the visual content. On the other hand, current evaluations consistently adhere to the process of inferring on given benchmarks and calculating scores for LVLMs, overlooking the possibility of data leakage during multi-modal training. This oversight can lead to unfair comparisons and misjudgments of the real gains in multi-modal capabilities brought by multi-modal training. ",
                "abstract": "Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.9% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks over 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain."
            },
            {
                "name": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
                "arxiv_id": "2403.14520",
                "subtitles": [
                    "Large Language Models",
                    "Vision Language Models",
                    "Vision Transformer",
                    "State Space Models"
                ],
                "reference": [
                    "Recent advances in natural language processing via large pre-trained language models: A survey",
                    "Liquid structural state-space models",
                    "Llava-phi: Efficient multi-modal assistant with small language model",
                    "Mobilevlm : A fast, strong and open vision language assistant for mobile devices",
                    "Mobilevlm v2: Faster and stronger baseline for vision language model",
                    "Stable lm",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "GLM: general language model pretraining with autoregressive blank infilling",
                    "Textbooks are all you need ii: phi",
                    "Hungry hungry hippos: Towards language modeling with state space models",
                    "Instance-aware prompt learning for language understanding and generation",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Improved baselines with visual instruction tuning",
                    "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama",
                    "Stanford alpaca: An instruction-following llama model",
                    "Gpt-4 technical report",
                    "It's raw! audio generation with state-space models",
                    "Simplified state space layers for sequence modeling",
                    "Redpajama: an open dataset for training large language models",
                    "Efficiently modeling long sequences with structured state spaces",
                    "Visual instruction tuning",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Tinyllama: An open-source small language model",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Learning transferable visual models from natural language supervision",
                    "Decision s4: Efficient sequence-based rl via state spaces layers",
                    "Structured state space models for in-context reinforcement learning",
                    "Llama: Open and efficient foundation language models",
                    "Diffusion models without attention",
                    "Flamingo: a visual language model for few-shot learning",
                    "Llama-adapter v2: Parameter-efficient visual instruction model",
                    "Textbooks are all you need"
                ],
                "related_work": "2Related work2.1Large Language ModelsWith the emergence of ChatGPT[43], also referred to as InstructGPT, Large Language Models (LLMs) have become the dominant force across various natural language processing tasks[42,26]. A defining characteristic of LLMs is their substantial model size, typically comprising billions of parameters, and their utilization of vast training corpora[53,51]. Although InstructGPT remains unreleased to the public, the community has actively pursued the development of open-source large language models. Notably, GLM[15], LLaMA[55], and the instruction-finetuned version of LLaMA such as Alpaca[52]and Vicuna[8], have been successively introduced, striving to achieve performance levels comparable to the InstructGPT model. Concurrently, due to the significant computational demands of LLMs, there has been a growing trend towards investigating small-scale alternatives. For instance, models like Stable LM[5], TinyLLaMA[60], and Phi[22,32]boast parameter counts of less than 3 billion. These studies advocate that superior quality data and feasible training methodologies can enable small-scale LLMs to achieve results on par with their larger counterparts.2.2Vision Language ModelsThe multimodal LLM is a foreseeable outcome deriving from the regular LLM since humans need to interact with multiple modality information in daily life. In specific, the vision data is another important data type besides the language. Thus, VLMs augmenting LLMs with the ability to handle with visual information are widely explored nowadays. OpenAI has designed the proprietary VLM GPT-4V and shown amazing visual comprehension ability. Flamingo[1]is another well-known VLM introduced by Google. To date, the community also contributed many exciting studies on VLM, such as LLaMA-adapter[17], OpenFlamingo[2], and LLaVA[38,37].A striking commonality among previous VLMs is that they all adapt the Transformer backbone to construct the dependencies alongside the sequential tokens. Despite its outstanding ability to capture potential relations in the data, the Transformer network has a quadratic computation complexity, which results in an inherent drawback hindering its deployment, especially when the model is large. To mitigate this problem, several studies have been carried out to present more compact and efficient VLMs. LLaVA-Phi[64]builds a multimodal foundation model taking the recent small-scale LLM called Phi-2 as the base model. TinyLLaVA introduces a unified training framework that is capable of combining visual information with several small-scale large-language models, such as TinyLLaMA[60], stable LM, and also Phi-2. MobileVLM[9,10]introduces MobileLLaMA as the base model which trains a family of small-scaled LLM base on LLaMA architecture. Besides, MobileVLM also presents a lightweight downsample projector (LDP) to decrease the sequence length of visual tokens further and thus alleviate the computation burden of the attention operation.2.3Vision TransformerTo build the VLM, the vision Transformer[14](ViT) is always applied to transfer the spatial image data into the sequential hidden tokens. ViT provides a powerful backbone for a variety of computer vision tasks. It takes as input a sequence of split image patches and produces sequential hidden representations through the Transformer network that has been proven a dominant paradigm across numerous deep learning tasks. ViT can be pre-trained via different pre-training schemes ranging from supervised learning to unsupervised objectives such as contrastive learning. Up to now, CLIP[45]has produced a sample-efficient ViT model via the contrastive loss between image and text and is widely used in multimodal fields.2.4State Space ModelsState space models (SSMs) have shown highly promising performance in long-range sequence modeling[50,24], image generation[57], and reinforcement learning[4,40]. The most attractive feature of SSM is that it can be written as a form of recurrence neural network (RNN) for efficient autoregressive inference or to process a whole input sequence in parallel for efficient training like the Attention-based Transformer. Despite the efficiency, SSMs also achieve effective results when tackling a range of sequence modeling tasks. In particular, Gu et al.[21]present the structured state-space sequence model (S4) to implement time series analysis. Goel et al.[18]applies the state space model to audio generation and obtains satisfactory performance. Furthermore, the H3 model[16]is proposed to fill the gap between SSMs and Transformers in language modeling.Over the past few months, a new selective state space model, namely Mamba[20], has been presented and is considered a strong competitor to the Transformer architecture. Compared to LLMs of the same capacity, Mamba-based language models show competitive performance, with faster inference speeds that scale linearly with time and constant memory usage.",
                "abstract": "In recent years, the application of multimodal large language models (MLLM) in various fields has achieved remarkable success. However, as the foundation model for many downstream tasks, current MLLMs are composed of the well-known Transformer network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective multi-modal Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra's linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction benchmarks show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at:this https URL."
            },
            {
                "name": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception",
                "arxiv_id": "2401.16158",
                "subtitles": [
                    "LLM-based Agent",
                    "Agent for Mobile Device"
                ],
                "reference": [
                    "Mm-react: Prompting chatgpt for multimodal reasoning and action",
                    "Appagent: Multimodal agents as smartphone users",
                    "Metagpt: Meta programming for multi-agent collaborative framework",
                    "Auto-gpt for online decision making: Benchmarks and additional opinions",
                    "Llava-plus: Learning to use tools for creating multimodal agents",
                    "Small llms are weak tool learners: A multi-llm agent",
                    "Gpt4tools: Teaching large language model to use tools via self-instruction",
                    "Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language",
                    "Controlllm: Augment language models with tools by searching on graphs",
                    "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                    "Visual chatgpt: Talking, drawing and editing with visual foundation models",
                    "Modelscope-agent: Building your customizable agent system with open-source large language models"
                ],
                "related_work": "4Related Work4.1LLM-based AgentWith the rapid advancement of Large Language Models (LLMs) , agents built upon these models have notched up impressive achievements across a burgeoning spectrum of tasksLi et al. (2023) ; Liu et al. (2023a,b,c) ; Shen et al. (2023) ; Wu et al. (2023) ; Yang et al. (2023a) ; Shen et al. (2024) ; Yang et al. (2023b) ; Hong et al. (2023) ; Yang et al. (2023c) . Functioning as the core, these agents adeptly interpret user instructions and deploy a versatile array of tools to execute intricate tasks. The expansive integration of diverse tools liberates LLMs from the confines of pure text processing. Currently, LLM-based agents are flourishing in diverse domains, showcasing prowess in tasks such as image and video editing, image generation, visual question answering, intelligent predictions, and more. This underscores the transformative impact of LLMs on the landscape of AI applications.4.2Agent for Mobile DeviceThe application of agents to operate terminal devices is becoming a hotspot. AppAgentYang et al. (2023d) is a mobile App assistant based on GPT-4V. They label manipulable regions of the app's UI with semi-transparent tags by invoking XML files from the Android system. The agent acquires operational capabilities through three methods: self-exploration, observing user video demos, and utilizing user documents. After a certain degree of exploration, the agent gains a sufficient understanding of operable regions, allowing it to execute correct operations based on instructions.",
                "abstract": "Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced atthis https URL."
            },
            {
                "name": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
                "arxiv_id": "2405.21075",
                "subtitles": [
                    "Advancements in MLLMs",
                    "MLLM Benchmarks"
                ],
                "reference": [
                    "Timechat: A time-sensitive multimodal large language model for long video understanding",
                    "A challenger to gpt-4v? early explorations of gemini in visual expertise",
                    "Pegasus-v1 technical report",
                    "Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models",
                    "Introducing our multimodal models",
                    "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Valley: Video assistant with large language model enhanced ability",
                    "Video-chatgpt: Towards detailed video understanding via large vision and language models",
                    "Sigmoid loss for language image pre-training",
                    "Hawkeye: Training video-text llms for grounding text in videos",
                    "A survey on multimodal large language models",
                    "Moviechat: From dense token to sparse memory for long video understanding",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Videochat: Chat-centric video understanding",
                    "Video-llama: An instruction-tuned audio-visual language model for video understanding",
                    "Value: A multi-task benchmark for video-and-language understanding evaluation",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models",
                    "Vtimellm: Empower llm to grasp video moments",
                    "Momentor: Advancing video large language model with fine-grained temporal reasoning",
                    "Visual instruction tuning",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                    "Learning transferable visual models from natural language supervision",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models",
                    "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
                    "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems",
                    "Pllava : Parameter-free llava extension from images to videos for video dense captioning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "Tempcompass: Do video llms really understand videos",
                    "Llama: Open and efficient foundation language models",
                    "Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models",
                    "Mvbench: A comprehensive multi-modal video understanding benchmark",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Flamingo: a visual language model for few-shot learning",
                    "Video-teller: Enhancing cross-modal generation with fusion and decoupling",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                    "Lita: Language instructed temporal-localization assistant"
                ],
                "related_work": "5Related WorkAdvancements in MLLMs.Recent advancements in MLLMs have seen notable progress[66,14]. MLLMs typically comprise three core modules: (i) a vision encoder for visual feature extraction, (ii) a modality alignment module to integrate visual features into the embedding space of the language model, and (iii) an LLM backbone for decoding multi-modal context. CLIP[52]and SigLIP[70]are widely-used for image encoding, while LLaMA[58]and Vicuna[10]serve as popular choices for LLMs. The alignment module varies from simple linear projections[33,75]to more complex architectures such as Q-Former[22,11], and gated cross-attention layers substantiated by Flamingo and IDEFICS[1,4]. Additionally, Fuyu-8B[6]introduces a novel framework mapping raw image pixels directly to the LLM embedding space. Regarding MLLMs for processing videos[23,71,43,57,32,44,19,20,64], the key difference lies in how they encode the video into vision tokens compatible with the LLMs. Representative work like Video-LLaMA[71]first uses a ViT[12]with an image Q-Former to encode individual frames and then employs a video Q-Former for temporal modeling. VideoChat2[24]utilizes a video transformer to encode video features and subsequently implements a Q-Former[22]to compress video tokens. To empower video MLLMs with temporal localization capability[17,51,60], TimeChat[56]constructs time-sensitive instruction tuning datasets and encodes timestamp knowledge into visual tokens. VTimeLLM[16]proposes a LLaVA-like three-stage training method. However, the potential of MLLMs in processing sequential visual data is still under-explored. Therefore, we introduce Video-MME for full-spectrum, multi-modal evaluation of MLLMs in video analysis.MLLM Benchmarks.Alongside advancements in architecture, significant efforts have been made to improve benchmarking for MLLMs, guiding the development of the next generation of these models. Previous studies have integrated various aspects of evaluation, such as perception and cognitive capabilities, to create comprehensive benchmarks for assessing image MLLMs[13,67,38]. As image MLLMs have demonstrated exceptional performance in general perception tasks, benchmarks regarding scientific understanding[28], multi-modal mathematical reasoning[41,73], and multi-disciplinary[69]capabilities have drawn increasing attention. For video MLLMs, similar efforts have been made to incorporate existing benchmarks[59,26]for evaluating video understanding[24,47]. Given the temporal nature of video modalities, specific benchmarks have been developed to address temporal understanding, highlighting the limitations of current video MLLMs in comprehending video content[29,39].In this work, we introduce a new high-quality video understanding benchmark, Video-MME. Compared to previous benchmarks, Video-MME includes a diverse set of videos of varying durations, supplemented with external modalities such as audios and subtitles. Additionally, it features human-annotated multi-level QA pairs, providing a comprehensive assessment framework for MLLMs. Our results indicate that open-source MLLMs still have a large gap with closed models. Our analysis and discussion further shed lights on the future development of MLLMs.",
                "abstract": "In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 254 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page:this https URL"
            },
            {
                "name": "Red Teaming Visual Language Models",
                "arxiv_id": "2401.12915",
                "subtitles": [
                    "Red Teaming and Safety",
                    "Visual Language Models"
                ],
                "reference": [
                    "Introducing our multimodal models",
                    "Pali-x: On scaling up a multilingual vision and language model",
                    "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
                    "A survey for in-context learning",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Aligning large multimodal models with factually augmented rlhf",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Reducing sentiment bias in language models via counterfactual evaluation",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Red teaming language models with language models",
                    "Improved baselines with visual instruction tuning",
                    "M{}^{3}: A large-scale dataset towards multi-modal multilingual instruction tuning",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
                    "Mmicl: Empowering vision-language model with multi-modal in-context learning",
                    "The secret sharer: Evaluating and testing unintended memorization in neural networks",
                    "True few-shot learning with language models",
                    "Visual instruction tuning",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Silkie: Preference distillation for large visual language models",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Flamingo: a visual language model for few-shot learning"
                ],
                "related_work": "4Related Work 4.1Red Teaming and Safety The concept ofRed Teamingoriginates in cyber-security, which involves employing advanced techniques to identify cyber-system vulnerabilities. In recent years, this term has gained prominence in the realm of natural language processing (NLP) , specifically referring to the methods and techniques used to test and attack language models (LMs) in order to uncover potential harms they can cause. These harms encompass offensive or harmful content, data leakage or privacy breaches(Carlini et al.,2019) , misinformation or disinformation(Lin et al.,2021) , and distributional or representational biases(Huang et al.,2020) .Within this realm of red teaming LMs, various previous works and studies have been conducted, which can be categorized into two approaches: manual red teaming and automated red teaming(Perez et al.,2022) . Manual red teaming involves human annotators or adversaries generating test cases and inputs to elicit potentially harmful outputs from LMs. On the other hand, automated red teaming methods leverage one LM to generate test cases for another LM, aiming to compel the targeted LM to produce harmful outputs. For instance, a study utilized automated red teaming techniques to reveal offensive and harmful behaviors displayed by LMs(Perez et al.,2022) . This research was based on methodologies previously introduced by(Perez et al.,2021) , where LMs were employed to generate test cases for dialogue systems and detect offensive responses.Researchers have also investigated scaling behaviors across different model sizes and explored various model types for red teaming purposes(Ganguli et al.,2022) . These include plain language models, models with rejection sampling, and models trained using reinforcement learning from human feedback. Furthermore, studies have delved into the security and safety implications of incorporating vision into LLMs, highlighting concerns about their vulnerability to visual adversarial attacks(Qi et al.,2023) . Specifically, VLMs such as Flamingo and GPT-4, which combine language and visual cues, have been examined. In this paper, the focus will extend to the Red Teaming of VLMs.4.2Visual Language ModelsThe advancements in LLMs have been a driving force in the evolution of VLMs. The pilot study Flamingo(Alayrac et al.,2022) , along with its open-source iterations(Awadalla et al.,2023; Lauren\u00e7on et al.,2023) , has effectively demonstrated the integration of LLMs with vision encoders. PaLI-X(Chen et al.,2023b) explores the impact of scaling vision and language components in greater depth. The Q-Former in BLIP-2(Li et al.,2023a) has been instrumental in narrowing the divide between visual and textual modalities. InstructBLIP(Dai et al.,2023) and MM-ICL(Zhao et al.,2023) have advanced the integration of instructional elements into the alignment process of visual and textual information, enhancing in-context learning capabilities(Dong et al.,2022) . MiniGPT-4(Zhu et al.,2023) and LLaVA-series(Liu et al.,2023c,b) perform instruction tuning on high-quality instruction tuning datasets synthesized by ChatGPT/GPT-4, exhibit encouraging outcomes in harmonizing visual encoders with LLMs. The recently introduced Qwen-VL(Bai et al.,2023) scales up multi-modal pre-training, while Fuyu(Bavishi et al.,2023) treats segmented pixel patches as visual tokens and trains the multimodal language model directly. LLaVA-RLHF(Sun et al.,2023) investigates RLHF may help VLMs achieve even better performance compared to its baseline models. VLFeedback(Li et al.,2023b) shows that direct preference optimization (DPO) (Rafailov et al.,2023) also brings significant improvements for VLMs with annotated preference on various instruction tuning sources(Li et al.,2023c; Liu et al.,2023a; Zhang et al.,2023b) .",
                "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-Hal, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-source."
            },
            {
                "name": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
                "arxiv_id": "2401.11170",
                "subtitles": [
                    "Large vision-language models (VLMs) ",
                    "Energy-latency manipulation"
                ],
                "reference": [
                    "Denial of service attacks in wireless networks: The case of jammers",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "The curious case of neural text degeneration",
                    "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                    "Slowlidar: Increasing the latency of lidar-based detection using adversarial examples",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Nicgslowdown: Evaluating the efficiency robustness of neural image caption generation models",
                    "The dark side of dynamic routing neural networks: Towards efficiency backdoor injection",
                    "Nmtsloth: understanding and testing efficiency degradation of neural machine translation systems",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A panda? no, it's a sloth: Slowdown attacks on adaptive multi-exit neural network inference",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Bottom-up and top-down attention for image captioning and visual question answering",
                    "Sponge examples: Energy-latency attacks on neural networks"
                ],
                "related_work": "2Related WorkLarge vision-language models (VLMs) .Recently, the advanced VLMs, such as BLIP(Li et al.,2022a) , BLIP-2(Li et al.,2023b) , InstructBLIP(Dai et al.,2023) , and MiniGPT-4(Zhu et al.,2023) , have achieved an enhanced zero-shot performance in various multi-modal tasks. Concretely, BLIP proposes a unified vision and language pre-training framework, while BLIP-2 introduces a query transformer to bridge the modality gap between a vision transformer and an LLM. Additionally, InstructBLIP and MiniGPT-4 both adopt instruction tuning for VLMs to improve the vision-language understanding performance. The integration of the vision modality into VLMs enables visual context-aware interaction, surpassing the capabilities of LLMs. However, this integration also introduces vulnerabilities arising from the manipulation of visual inputs. In our paper, we propose to craft verbose images to induce high energy-latency cost of VLMs.Energy-latency manipulation.The energy-latency manipulation(Chen et al.,2022b; Hong et al.,2021; Chen et al.,2023; Liu et al.,2023a) aims to slow down the models by increasing their energy computation and response time during the inference stage, a threat analogous to the denial-of-service (DoS) attacks(Pelechrinis et al.,2010) from the Internet. Specifically,Shumailov et al. (2021) first observe that a larger representation dimension calculation can introduce more energy-latency cost in LLMs. Hence, they propose to craft sponge samples to maximize the \\mathcal{L}_{2} norm of activation values across all layers, thereby introducing more representation calculation and energy-latency cost. NICGSlowDown(Chen et al.,2022c) proposes to increase the number of decoder calls,i.e., the length of the generated sequence, to increase the energy-latency of smaller-scale captioning models. They minimize the logits of both EOS token and output tokens to generate long sentences.However, these previous methods cannot be directly applied to VLMs for two main reasons. On one hand, they primarily focus on LLMs or smaller-scale models. Sponge samples are designed for LLMs for translations(Liu et al.,2019) and NICGSlowdown targets for RNNs or LSTMs combined with CNNs for image captioning(Anderson et al.,2018) . Differently, our verbose images are tailored for VLMs in multi-modal tasks. On the other hand, the objective of NICGSlowdown involves logits of specific output tokens. Nevertheless, current VLMs generate random output sequences for the same input sample, due to advanced sampling policies(Holtzman et al.,2020) , which makes it challenging to optimize objectives with specific output tokens. Therefore, it highlights the need for methods specifically designed for VLMs to induce high energy-latency cost.",
                "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available atthis https URL."
            },
            {
                "name": "Visual Hallucinations of Multi-modal Large Language Models",
                "arxiv_id": "2402.14683",
                "subtitles": [
                    "Hallucinations",
                    "VH Benchmarks in MLLMs",
                    "Mitigating VH in MLLMs"
                ],
                "reference": [
                    "Evaluating object hallucination in large vision-language models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                    "A survey on hallucination in large vision-language models",
                    "Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark",
                    "A survey of hallucination in large foundation models",
                    "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
                    "Mitigating hallucination in large multi-modal models via robust instruction tuning",
                    "Eyes wide shut? exploring the visual shortcomings of multimodal llms",
                    "Visual evidence prompting mitigates hallucinations in multimodal large language models",
                    "Mass-producing failures of multimodal systems with language models",
                    "Woodpecker: Hallucination correction for multimodal large language models",
                    "Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites",
                    "Survey of hallucination in natural language generation"
                ],
                "related_work": "5Related WorkHallucinationsHallucinations are well-known issues for generative AI, including LLMsJi et al. (2023) ; Huang et al. (2023a) , MLLMsLiu et al. (2024b) ; Rawte et al. (2023) ; Tong et al. (2024) , and text-to-image generative modelTong et al. (2023) . In general, hallucination refers to a generative model imagines factually incorrect details in its response for a given input. VH occurs when an MLLM imagines incorrect details about an image in visual question answering.VH Benchmarks in MLLMsPrior works have tried to benchmark MLLMs' VHsLi et al. (2023) ; Liu et al. (2024a) ; Fu et al. (2023) ; Tong et al. (2024) . However, they collect VH images only from existing image datasets. This limits the diversity of VH images. Moreover, existing image datasets may have been used to pre-train an MLLM, leading to data contaminationJacovi et al. (2023) ; Sainz et al. (2023) . Our VHTest can generate a diverse set of new VH images that do not appear in existing benchmarks. Moreover, the shape and size VH modes are formulated by us for the first time.Mitigating VH in MLLMsExisting works on mitigating VHs in MLLMs can be categorized intofine-tuning-phaseandtesting-phasemitigation. Fine-tuning-phase mitigation focuses on improving the fine-tuning data qualityWang et al. (2024) ; Liu et al. (2024a) and/or model structureTong et al. (2024) . These works typically freeze the vision encoder during fine-tuning, following the standard fine-tuning setting of LLaVA-1.5. We find that fine-tuning the vision encoder together reduces VHs in MLLMs. Testing-phase mitigation leverages prompt engineering with more visual evidenceLi et al. (2024) or correction tools for hallucinated responsesYin et al. (2023) . Testing-phase mitigation is complementary to fine-tuning-phase mitigation.",
                "abstract": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available:this https URL."
            },
            {
                "name": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
                "arxiv_id": "2402.10884",
                "subtitles": [
                    "MLLMs and Visual Instruction Tuning",
                    "Mitigating Modality Conflict in MLLMs",
                    "Distillation-based Instruction Tuning",
                    "Preference Alignment",
                    "Distilling AI Feedback for Preference Alignment"
                ],
                "reference": [
                    "Mm-react: Prompting chatgpt for multimodal reasoning and action",
                    "Finetuned language models are zero-shot learners",
                    "Multimodal-gpt: A vision and language model for dialogue with humans",
                    "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                    "Visual chatgpt: Talking, drawing and editing with visual foundation models",
                    "Aligning large multimodal models with factually augmented rlhf",
                    "The false promise of imitating proprietary llms",
                    "Zephyr: Direct distillation of lm alignment",
                    "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification",
                    "Visual instruction tuning",
                    "Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Learning transferable visual models from natural language supervision",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Llama: Open and efficient foundation language models",
                    "Otter: A multi-modal model with in-context instruction tuning",
                    "Flamingo: a visual language model for few-shot learning",
                    "Training language models to follow instructions with human feedback"
                ],
                "related_work": "2Related Work 2.1MLLMs and Visual Instruction Tuning Incorporating another modality into large language models represents a natural evolution for these systems. Modality expansion can be achieved through system-level enhancements at inference time, with approaches such as Mm-react [5], Visual ChatGPT [6], and HuggingGPT [1] enabling the LLM to invoke off-the-shelf vision models and APIs. An alternative strand of research involves the training of end-to-end MLLMs. To avoid the prohibitive costs associated with pre-training from scratch, these models often integrate pre-trained vision models with large language models, applying various degrees of modality adaptation. Mini-GPT4 [7] focuses solely on training a linear projection matrix to connect CLIP-based [8] vision representations with the LLaMA model [9]; BLIP-2 introduces a cross-attention module to extract vision tokens relevant to the query. Both LLaVA [10] and mPlug-OWL [11] feature cross-modality connectors between the vision and language domains, but they also fine-tune the LLM and vision encoder, respectively. Flamingo [12], in contrast, incorporates new cross-attention layers directly into the LLM. In the language domain, Wei et al. [13] discovered that fine-tuning a base LLM with instructions described in natural language enhances the model's ability to follow those instructions. In a similar vein, MLLMs are typically fine-tuned with instructions; Mini-GPT4 [7] utilized template instructions based on image-text pairs, while InstructBLIP [14], Otter [15], and LLaVA [10] employed human-written visual question-answers or synthetically generated question-answer pairs by prompting GPT-4 with COCO captions and bounding boxes [10]. However, considering that both LLaVA and Instruct-BLIP utilize Vicuna [16] an instruction-tuned LLaMA it remains a topic of debate whether their steps of visual instruction tuning genuinely add to the model's instruction-following capabilities or merely conform to the instruction-following format used in Vicuna's training. 2.2Mitigating Modality Conflict in MLLMs To preserve the ability to follow language instructions, mPLUG-OWL [17] and LLaVA 1.5 [10] incorporate language-only instruction data back into their mixed visual-language instruction datasets, specifically ShareGPT. It is noteworthy that the LLM backbone of LLaVA 1.5, Vicuna, had been previously trained on this identical ShareGPT dataset. Further investigation reveals that, despite the integrated dataset, LLaVA 1.5 exhibits degradation in language instruction-following capabilities; the MT-Bench score for LLaVA-1.5-13b is notably lower than that for Vicuna-V1.5-7b. While mPLUG-OWL-2 [17] presents promising solutions to the challenges of modality conflict, and has shown superior performance on text-based benchmarks, it also introduces increased parameter count and more complex implementation, which poses practical challenges. Specifically, mPLUG-OWL-2 implements modality-adaptive modules that include distinct layer normalization, as well as separate key and value projection matrices for text and visual tokens, whilst maintaining a shared structure for query projection matrices. The model is fine-tuned from the non-instruction-tuned LLaMA-2-7B base model, incorporating 548K textual instruction data samples from a total of 1.2M, derived from both SlimOrca [18] and ShareGPT. This approach has enabled mPLUG-OWL-2 to excel in language and visual-language tasks, outperforming RLHF-augmented LLaMA-2 Chat on text-centric evaluations such as MMLU and BBH [17]. However, the introduction of modality-specific modules has led to an increase in the model's parameter count from 7.2 billion to 8.2 billion. Additionally, the separate processing paths for visual and language tokens have resulted in a more intricate compute graph, complicating the utilization of fused GPU kernels to achieve efficient inference. The methodologies discussed so far are reliant on standard fine-tuning practices, necessitating a significant augmentation of computational resources to integrate text instruction data effectively, with the aim of resolving the challenges posed by modality conflict. 2.3Distillation-based Instruction Tuning Leveraging the output of large proprietary models, smaller open-source models such as Vicuna [16], Alpaca, and more recently ShareGPT4V [19], have been fine-tuned, although this approach has limitations in terms of generalization capabilities. Gudibande et al. [20] observed that models fine-tuned through instruction tuning by imitation barely bridge the performance gap in tasks beyond the scope of the training data. They contend that imitation as a strategy is a false promise, asserting that only a significant volume of imitation data or a larger base model can close the disparity between open and closed-source models [20]. While recognizing the utility of the expansive GPT4V dataset like ShareGPT4V [19], it is posited that such scaling of distillation-based instruction tuning primarily extends the model's competency within the distribution it was trained on rather than its out-of-distribution generalizability. Further research indicated that distillation instruction tuning on a smaller scale tends to skew the model's performance towards a niche subset and significantly impair its broader applicability. This was evidenced by a baseline experiment in which fine-tuning LLaVA with a 6k VQA dataset, sourced from Gemini Pro-generated answers, resulted in pronounced performance declines across both textual and visual benchmarks. 2.4Preference Alignment The Instruct-GPT series [21] has shown that merely employing supervised fine-tuning (SFT) on Large Language Models (LLMs) is insufficient for aligning them with human preferences. The technique of Reinforcement Learning from Human Feedback (RLHF) [21] addresses this by constructing a reward model that encapsulates human preferences and then applying reinforcement learning to maximize this reward. The Direct Preference Optimization (DPO) approach posits that directly tuning the preference dataset can serve as an effective substitute for reward modeling, offering the added benefit of reduced computational complexity. Another novel method, known as rejection sampling SteerLM, has recently been identified to achieve performance akin to RLHF by incorporating human-annotated quality metrics before generation, serving as a conditional SFT-based strategy for alignment [22]. Our experiments with DPO, SteerLM, and rejection sampling reference the prior work on LLaVA-RLHF [23], using it as a benchmark for RLHF performance. 2.5Distilling AI Feedback for Preference Alignment In the realm of alignment methods, reliance on human-annotated preference annotations is common. While effective on a large scale, this approach incurs substantial costs and operational complexities [24]. The effectiveness of reward models based on pairwise ranking is constrained by the inherent subjectivity of human preferences, with LLaMA's reward model achieving an accuracy range of 64.3-70.6%, and the LLaVA-RLHF model reaching 67%. In response to these limitations, Zephyr [25] and UltraFeedback [26] have utilized preference annotations distilled from GPT-4 to train models with 7B parameters, achieving performance levels comparable to those of 70B parameter models. Motivated to adapt this distillation-preference alignment approach for MLLMs, our work introduces a nuanced chain-of-thought prompting technique, coupled with a detailed annotation guide, spanning five assessment metrics. ",
                "abstract": "In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This enhancement in textual instruction proficiency correlates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning."
            },
            {
                "name": "Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models",
                "arxiv_id": "2401.00988",
                "subtitles": [
                    "Language-driving datasets and models",
                    "Multimodal Large Language Models"
                ],
                "reference": [
                    "Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario",
                    "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
                    "Clip2point: Transfer clip to point cloud classification with image-depth pre-training",
                    "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                    "Drama: Joint risk localization and captioning in driving",
                    "Referring multi-object tracking",
                    "Talk2bev: Language-enhanced bird's-eye view maps for autonomous driving",
                    "Talk2car: Taking control of your self-driving car",
                    "Drivegpt4: Interpretable end-to-end autonomous driving via large language model",
                    "Videochat: Chat-centric video understanding",
                    "Video-llama: An instruction-tuned audio-visual language model for video understanding",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Hilm-d: Towards high-resolution understanding in multimodal large language models for autonomous driving",
                    "Gpt-driver: Learning to drive with gpt",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Gpt-4 technical report",
                    "Perceptiongpt: Effectively fusing visual perception into llm",
                    "3d-llm: Injecting the 3d world into large language models",
                    "Object referring in videos with language and human gaze",
                    "G-llava: Solving geometric problem with multi-modal large language model",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Llama: Open and efficient foundation language models",
                    "Detgpt: Detect what you need via reasoning, 2023a",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Flamingo: a visual language model for few-shot learning",
                    "Planning-oriented autonomous driving",
                    "Etc: Temporal boundary expand then clarify for weakly supervised video grounding with multimodal large language model"
                ],
                "related_work": "2Related Work Language-driving datasets and models. CityScapes-Ref [45], Talk2Car [9] perform language-grounding tasks. ReferKITTI [46] and NuPrompt [41] leverage temporal data for 2D or 3D referring object detection and tracking. Nuscenes-QA [41] offers numerous question-answer pairs for multi-view perception tasks in driving scenes. Some advancements, e.g., DRAMA [33] and HiLM-D [11], generating text descriptions for localizing risk objects. Beyond perception, DriveGPT4 [48] and GPT-Driver [34] leverage LLMs for interpreting vehicle actions and planning, respectively. Talk2BEV [10] formulate BEV into a JSON file and input it into ChatGPT [35] to conduct autonomous driving understanding. Despite these advancements, a common limitation persists: most datasets and models address only part of the autonomous driving tasks with incomplete information. As shown in Table 1 and Fig. 1, in this paper, we propose a challenging dataset containing various tasks that require holistic information, i.e., temporal, multi-view, spatial and so on, to address. Multimodal Large Language Models. Leveraging the capabilities of pre-trained LLMs like LLaMA [44] and Vicuna [6], Multimodal LLMs (MLLMs) are expanding their application spectrum, handling inputs from images [39, 23, 24, 40, 13, 50, 1, 5], videos [49, 25, 22], and 3D data [15, 18] to medical data [20]. In the domain of autonomous driving, DriveGPT4 [48] and Talk2BEV [10] have integrated MLLMs for comprehension. However, these approaches have limitations; DriveGPT4 is confined to single-view inputs, and Talk2BEV lacks temporal dynamics and an end-to-end framework. Addressing these gaps, our BEV-InMLLM model assimilates comprehensive temporal, multi-view, and spatial data, for reliable decisions.",
                "abstract": "The rise of multimodal large language models (MLLMs) has spurred interest in language-based driving tasks. However, existing research typically focuses on limited tasks and often omits key multi-view and temporal information which is crucial for robust autonomous driving. To bridge these gaps, we introduce NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17 subtasks, where each task demands holistic information (e.g., temporal, multi-view, and spatial), significantly elevating the challenge level. To obtain NuInstruct, we propose a novel SQL-based method to generate instruction-response pairs automatically, which is inspired by the driving logical progression of humans. We further present BEV-InMLLM, an end-to-end method for efficiently deriving instruction-aware Bird's-Eye-View (BEV) features, language-aligned for large language models. BEV-InMLLM integrates multi-view, spatial awareness, and temporal semantics to enhance MLLMs' capabilities on NuInstruct tasks. Moreover, our proposed BEV injection module is a plug-and-play method for existing MLLMs. Our experiments on NuInstruct demonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g. around 9% improvement on various tasks. We plan to release our NuInstruct for future research development."
            }
        ],
        "survey": {
            "name": "The Revolution of Multimodal Large Language Models: A Survey",
            "arxiv_id": "2402.12451",
            "subtitles": [
                {
                    "name": "Empowering LLMs with Multimodal Capabilities",
                    "key_history": [
                        {
                            "reference_title": "Language models are few-shot learners",
                            "key_word": "Large Language Models"
                        },
                        {
                            "reference_title": "Warp: Word-level adversarial reprogramming",
                            "key_word": "PEFT"
                        },
                        {
                            "reference_title": "Flamingo: a Visual Language Model for Few-Shot Learning",
                            "key_word": "Towards Multimodal LLMs"
                        },
                        {
                            "reference_title": "Learning transferable visual models from natural language supervision",
                            "key_word": "Visual Encoder"
                        },
                        {
                            "reference_title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
                            "key_word": "Linear and MLP Projections"
                        },
                        {
                            "reference_title": "LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
                            "key_word": "Q-Former"
                        },
                        {
                            "reference_title": "Flamingo: a Visual Language Model for Few-Shot Learning",
                            "key_word": "Additional Cross-Attention Layers"
                        },
                        {
                            "reference_title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
                            "key_word": "Single-Stage Training"
                        },
                        {
                            "reference_title": "Visual Instruction Tuning",
                            "key_word": "Two-Stage Training"
                        },
                        {
                            "reference_title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
                            "key_word": "Training Data"
                        }
                    ],
                    "references_in_this_section": [
                        "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
                        "PixelLM: Pixel Reasoning with Large Multimodal Model",
                        "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
                        "Introducing ChatGPT",
                        "ChatterBox: Multi-round Multimodal Referring and Grounding",
                        "Warp: Word-level adversarial reprogramming",
                        "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
                        "Llama 2: Open foundation and fine-tuned chat models",
                        "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
                        "CogVLM: Visual Expert for Pretrained Language Models",
                        "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
                        "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
                        "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
                        "MiniGPT-v2: Large Language Model As a Unified Interface for Vision-Language Multi-task Learning",
                        "Pixel Aligned Language Models",
                        "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                        "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
                        "Contextual Object Detection with Multimodal Large Language Models",
                        "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
                        "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
                        "Opt: Open pre-trained transformer language models",
                        "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
                        "LAION-5B: An open large-scale dataset for training next generation image-text models",
                        "Self-instruct: Aligning language model with self generated instructions",
                        "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
                        "Visual Instruction Tuning with Polite Flamingo",
                        "Exploring the limits of transfer learning with a unified text-to-text transformer",
                        "Palm: Scaling language modeling with pathways",
                        "Masked Autoencoders Are Scalable Vision Learners",
                        "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
                        "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                        "GPT-4 Technical Report",
                        "Flamingo: a Visual Language Model for Few-Shot Learning",
                        "mT5: A massively multilingual pre-trained text-to-text transformer",
                        "Training language models to follow instructions with human feedback",
                        "Stanford Alpaca: An Instruction-Following LLaMA Model",
                        "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
                        "Microsoft COCO: Common Objects in Context",
                        "LoRA: Low-Rank Adaptation of Large Language Models",
                        "Unifying Language Learning Paradigms",
                        "Qlora: Efficient finetuning of quantized llms",
                        "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                        "LLaFS: When Large-Language Models Meet Few-Shot Segmentation",
                        "LLaMA: Open and Efficient Foundation Language Models",
                        "Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs",
                        "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
                        "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
                        "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
                        "LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model",
                        "GSVA: Generalized Segmentation via Multimodal Large Language Models",
                        "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
                        "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
                        "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",
                        "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models",
                        "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
                        "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
                        "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
                        "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
                        "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
                        "Language Is Not All You Need: Aligning Perception with Language Models",
                        "Scaling Instruction-Finetuned Language Models",
                        "Qwen technical report",
                        "LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
                        "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                        "Prefix-tuning: Optimizing continuous prompts for generation",
                        "Robust Fine-Tuning of Zero-Shot Models",
                        "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs",
                        "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
                        "NExT-Chat: An LMM for Chat, Detection and Segmentation",
                        "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning",
                        "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
                        "SVIT: Scaling up Visual Instruction Tuning",
                        "Mixtral of Experts",
                        "Improved Baselines with Visual Instruction Tuning",
                        "The power of scale for parameter-efficient prompt tuning",
                        "Visual Instruction Tuning",
                        "Eva: Exploring the limits of masked visual representation learning at scale",
                        "Training compute-optimal large language models",
                        "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
                        "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
                        "Language models are few-shot learners",
                        "GPT understands, too",
                        "LISA: Reasoning Segmentation via Large Language Model",
                        "VL-Mamba: Exploring State Space Models for Multimodal Learning",
                        "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
                        "Grounding Language Models to Images for Multimodal Inputs and Outputs",
                        "Perceiver: General perception with iterative attention",
                        "Honeybee: Locality-enhanced Projector for Multimodal LLM",
                        "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
                        "Learning transferable visual models from natural language supervision",
                        "DetGPT: Detect What You Need via Reasoning",
                        "Shikra: Unleashing Multimodal LLM\u2019s Referential Dialogue Magic",
                        "Multimodal c4: An open, billion-scale corpus of images interleaved with text",
                        "Datacomp: In search of the next generation of multimodal datasets",
                        "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models",
                        "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
                        "Generalizable Entity Grounding via Assistance of Large Language Model",
                        "Lenna: Language enhanced reasoning detection assistant",
                        "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
                        "GLaMM : Pixel Grounding Large Multimodal Model",
                        "Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models",
                        "Magneto: A Foundation Transformer",
                        "VILA: On Pre-training for Visual Language Models",
                        "COYO-700M: Image-Text Pair Dataset"
                    ]
                },
                {
                    "name": "Tackling Visual Tasks with MLLMs",
                    "key_history": [
                        {
                            "reference_title": "Shikra: Unleashing Multimodal LLM\u2019s Referential Dialogue Magic",
                            "key_word": "Region-as-Text"
                        },
                        {
                            "reference_title": "GLaMM : Pixel Grounding Large Multimodal Model",
                            "key_word": "Embedding-as-Region"
                        },
                        {
                            "reference_title": "DetGPT: Detect What You Need via Reasoning",
                            "key_word": "Text-to-Grounding"
                        },
                        {
                            "reference_title": "Generating Images with Multimodal Language Models",
                            "key_word": "Connecting MLLMs with Diffusion Models"
                        },
                        {
                            "reference_title": "Generative Pretraining in Multimodality",
                            "key_word": "End-to-End Pipelines"
                        },
                        {
                            "reference_title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
                            "key_word": "Video Understanding"
                        },
                        {
                            "reference_title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
                            "key_word": "Any-Modality Models"
                        },
                        {
                            "reference_title": "Kosmos-2.5: A Multimodal Literate Model",
                            "key_word": "Domain-Specific MLLMs"
                        }
                    ],
                    "references_in_this_section": [
                        "Grounding DINO: Marrying dino with grounded pre-training for open-set object detection",
                        "PixelLM: Pixel Reasoning with Large Multimodal Model",
                        "ChatterBox: Multi-round Multimodal Referring and Grounding",
                        "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
                        "CogVLM: Visual Expert for Pretrained Language Models",
                        "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
                        "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models",
                        "MiniGPT-v2: Large Language Model As a Unified Interface for Vision-Language Multi-task Learning",
                        "Pixel Aligned Language Models",
                        "Generating Images with Multimodal Language Models",
                        "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
                        "Making LLaMA SEE and Draw with SEED Tokenizer",
                        "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
                        "Kosmos-2.5: A Multimodal Literate Model",
                        "Contextual Object Detection with Multimodal Large Language Models",
                        "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
                        "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
                        "Segment Anything",
                        "Generative Pretraining in Multimodality",
                        "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding",
                        "PaLM-E: An Embodied Multimodal Language Model",
                        "Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens",
                        "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
                        "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
                        "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action",
                        "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                        "Taming Transformers for High-Resolution Image Synthesis",
                        "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
                        "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst",
                        "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
                        "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
                        "LLaFS: When Large-Language Models Meet Few-Shot Segmentation",
                        "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
                        "LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model",
                        "GSVA: Generalized Segmentation via Multimodal Large Language Models",
                        "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",
                        "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models",
                        "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
                        "One For All: Video Conversation is Feasible Without Video Instruction Tuning",
                        "RegionCLIP: Region-based Language-Image Pretraining",
                        "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
                        "Masked-Attention Mask Transformer for Universal Image Segmentation",
                        "mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding",
                        "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought",
                        "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
                        "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning",
                        "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",
                        "NExT-Chat: An LMM for Chat, Detection and Segmentation",
                        "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning",
                        "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
                        "A simple framework for open-vocabulary segmentation and detection",
                        "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer",
                        "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
                        "LLMGA: Multimodal Large Language Model based Generation Assistant",
                        "PandaGPT: One Model To Instruction-Follow Them All",
                        "Semantic-SAM: Segment and recognize anything at any granularity",
                        "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                        "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning",
                        "SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models",
                        "High-Resolution Image Synthesis with Latent Diffusion Models",
                        "LISA: Reasoning Segmentation via Large Language Model",
                        "Unified Language-Vision Pretraining with Dynamic Discrete Visual Tokenization",
                        "Integrally Pre-Trained Transformer Pyramid Networks",
                        "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
                        "Generative Multimodal Models are In-Context Learners",
                        "Planting a SEED of Vision in Large Language Model",
                        "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",
                        "DetGPT: Detect What You Need via Reasoning",
                        "Shikra: Unleashing Multimodal LLM\u2019s Referential Dialogue Magic",
                        "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
                        "U-Net: Convolutional Networks for Biomedical Image Segmentation",
                        "Generalizable Entity Grounding via Assistance of Large Language Model",
                        "Lenna: Language enhanced reasoning detection assistant",
                        "NExT-GPT: Any-to-Any Multimodal LLM",
                        "VideoChat: Chat-Centric Video Understanding",
                        "GLaMM : Pixel Grounding Large Multimodal Model",
                        "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
                        "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model",
                        "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                        "Jointly Training Large Autoregressive Multimodal Models",
                        "Neural Discrete Representation Learning",
                        "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
                        "Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"
                    ]
                }
            ],
            "all_references": [
                "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
                "Grounding DINO: Marrying dino with grounded pre-training for open-set object detection",
                "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions",
                "OneLLM: One Framework to Align All Modalities with Language",
                "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                "PixelLM: Pixel Reasoning with Large Multimodal Model",
                "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
                "Gemini: A Family of Highly Capable Multimodal Models",
                "Visual Storytelling",
                "Introducing ChatGPT",
                "ChatterBox: Multi-round Multimodal Referring and Grounding",
                "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                "Warp: Word-level adversarial reprogramming",
                "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
                "Llama 2: Open foundation and fine-tuned chat models",
                "Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories",
                "CogVLM: Visual Expert for Pretrained Language Models",
                "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
                "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
                "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
                "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
                "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models",
                "MiniGPT-v2: Large Language Model As a Unified Interface for Vision-Language Multi-task Learning",
                "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
                "Pixel Aligned Language Models",
                "TouchStone: Evaluating Vision-Language Models by Language Models",
                "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "Generating Images with Multimodal Language Models",
                "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback",
                "Evaluating Object Hallucination in Large Vision-Language Models",
                "Ocr-vqa: Visual question answering by reading text in images",
                "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
                "Making LLaMA SEE and Draw with SEED Tokenizer",
                "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
                "Kosmos-2.5: A Multimodal Literate Model",
                "Contextual Object Detection with Multimodal Large Language Models",
                "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model",
                "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
                "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
                "Opt: Open pre-trained transformer language models",
                "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
                "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
                "Tiny LVLM-eHub: Early Multimodal Experiments with Bard",
                "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
                "Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare",
                "InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language",
                "Segment Anything",
                "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                "LAION-5B: An open large-scale dataset for training next generation image-text models",
                "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding",
                "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
                "PaLM-E: An Embodied Multimodal Language Model",
                "Generative Pretraining in Multimodality",
                "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
                "Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens",
                "Self-instruct: Aligning language model with self generated instructions",
                "Learning 3D Representations From 2D Pre-Trained Models via Image-to-Point Masked Autoencoders",
                "CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios",
                "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
                "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
                "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
                "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing",
                "Visual Instruction Tuning with Polite Flamingo",
                "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "Palm: Scaling language modeling with pathways",
                "PointLLM: Empowering Large Language Models to Understand Point Clouds",
                "Masked Autoencoders Are Scalable Vision Learners",
                "3D-LLM: Injecting the 3D World into Large Language Models",
                "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
                "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
                "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action",
                "TextCaps: A Dataset for Image Captioning with Reading Comprehension",
                "Kosmos-2: Grounding Multimodal Large Language Models to the World",
                "GPT-4 Technical Report",
                "Flamingo: a Visual Language Model for Few-Shot Learning",
                "Taming Transformers for High-Resolution Image Synthesis",
                "mT5: A massively multilingual pre-trained text-to-text transformer",
                "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
                "M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models",
                "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
                "DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding",
                "Generation and Comprehension of Unambiguous Object Descriptions",
                "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning",
                "Dolphins: Multimodal Language Model for Driving",
                "Training language models to follow instructions with human feedback",
                "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
                "Stanford Alpaca: An Instruction-Following LLaMA Model",
                "Microsoft COCO: Common Objects in Context",
                "SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions",
                "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
                "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering",
                "LoRA: Low-Rank Adaptation of Large Language Models",
                "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst",
                "Unifying Language Learning Paradigms",
                "Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models",
                "Qlora: Efficient finetuning of quantized llms",
                "VQA: Visual Question Answering",
                "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
                "OtterHD: A High-Resolution Multi-modality Model",
                "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
                "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
                "LLaMA: Open and Efficient Foundation Language Models",
                "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs",
                "LLaFS: When Large-Language Models Meet Few-Shot Segmentation",
                "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
                "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
                "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
                "LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model",
                "GSVA: Generalized Segmentation via Multimodal Large Language Models",
                "Point-BERT: Pre-Training 3D Point Cloud Transformers With Masked Point Modeling",
                "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
                "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
                "LLMBind: A Unified Modality-Task Integration Framework",
                "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
                "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",
                "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models",
                "REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory",
                "A Survey on Multimodal Large Language Models",
                "One For All: Video Conversation is Feasible Without Video Instruction Tuning",
                "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
                "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
                "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
                "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
                "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
                "RegionCLIP: Region-based Language-Image Pretraining",
                "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning",
                "nocaps: novel object captioning at scale",
                "VizWiz Grand Challenge: Answering Visual Questions From Blind People",
                "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
                "Language Is Not All You Need: Aligning Perception with Language Models",
                "Masked-Attention Mask Transformer for Universal Image Segmentation",
                "Scaling Instruction-Finetuned Language Models",
                "mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding",
                "Qwen technical report",
                "Modeling Context in Referring Expressions",
                "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought",
                "The Open Images Dataset V4: Unified Image Classification, Object Detection, and Visual Relationship Detection at Scale",
                "LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
                "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                "Deep visual-semantic alignments for generating image descriptions",
                "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
                "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning",
                "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",
                "Prefix-tuning: Optimizing continuous prompts for generation",
                "ImageBind: One Embedding Space To Bind Them All",
                "Robust Fine-Tuning of Zero-Shot Models",
                "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs",
                "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
                "Scannet: Richly-Annotated 3D Reconstructions of Indoor Scenes",
                "FoodLMM: A Versatile Food Assistant using Large Multi-modal Model",
                "NExT-Chat: An LMM for Chat, Detection and Segmentation",
                "See, Say, and Segment: Teaching LMMs to Overcome False Premises",
                "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World",
                "SVIT: Scaling up Visual Instruction Tuning",
                "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer",
                "Mixtral of Experts",
                "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
                "A simple framework for open-vocabulary segmentation and detection",
                "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
                "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning",
                "Improved Baselines with Visual Instruction Tuning",
                "Visual7W: Grounded Question Answering in Images",
                "LLMGA: Multimodal Large Language Model based Generation Assistant",
                "PandaGPT: One Model To Instruction-Follow Them All",
                "The power of scale for parameter-efficient prompt tuning",
                "Semantic-SAM: Segment and recognize anything at any granularity",
                "Visual Instruction Tuning",
                "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
                "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
                "Eva: Exploring the limits of masked visual representation learning at scale",
                "CIDEr: Consensus-Based Image Description Evaluation",
                "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
                "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning",
                "Training compute-optimal large language models",
                "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
                "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
                "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
                "FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models",
                "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
                "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models",
                "Aligning Large Multi-Modal Model with Robust Instruction Tuning",
                "Language models are few-shot learners",
                "GPT understands, too",
                "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                "High-Resolution Image Synthesis with Latent Diffusion Models",
                "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",
                "LISA: Reasoning Segmentation via Large Language Model",
                "VL-Mamba: Exploring State Space Models for Multimodal Learning",
                "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
                "Multimodal Large Language Models: A Survey",
                "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
                "Unified Language-Vision Pretraining with Dynamic Discrete Visual Tokenization",
                "Grounding Language Models to Images for Multimodal Inputs and Outputs",
                "Integrally Pre-Trained Transformer Pyramid Networks",
                "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
                "Perceiver: General perception with iterative attention",
                "Generative Multimodal Models are In-Context Learners",
                "MMBench: Is Your Multi-modal Model an All-around Player",
                "Honeybee: Locality-enhanced Projector for Multimodal LLM",
                "Planting a SEED of Vision in Large Language Model",
                "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
                "Learning transferable visual models from natural language supervision",
                "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",
                "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding",
                "DetGPT: Detect What You Need via Reasoning",
                "Shikra: Unleashing Multimodal LLM\u2019s Referential Dialogue Magic",
                "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
                "Multimodal c4: An open, billion-scale corpus of images interleaved with text",
                "Datacomp: In search of the next generation of multimodal datasets",
                "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models",
                "Visual Spatial Reasoning",
                "MLLM-Protector: Ensuring MLLM\u2019s Safety without Hurting Performance",
                "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
                "Towards VQA Models That Can Read",
                "Generalizable Entity Grounding via Assistance of Large Language Model",
                "U-Net: Convolutional Networks for Biomedical Image Segmentation",
                "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
                "Lenna: Language enhanced reasoning detection assistant",
                "NExT-GPT: Any-to-Any Multimodal LLM",
                "VideoChat: Chat-Centric Video Understanding",
                "GLaMM : Pixel Grounding Large Multimodal Model",
                "Grounded language-image pre-training",
                "Emerging Properties in Self-Supervised Vision Transformers",
                "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
                "VisualBERT: A Simple and Performant Baseline for Vision and Language",
                "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model",
                "UnIVAL: Unified Model for Image, Video, Audio and Language Tasks",
                "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model",
                "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                "Panoptic scene graph generation",
                "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
                "Attention is all you need",
                "Jointly Training Large Autoregressive Multimodal Models",
                "Woodpecker: Hallucination correction for multimodal large language models",
                "Neural Discrete Representation Learning",
                "Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models",
                "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
                "Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
                "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
                "Magneto: A Foundation Transformer",
                "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness",
                "LVIS: A dataset for large vocabulary instance segmentation",
                "VILA: On Pre-training for Visual Language Models",
                "COYO-700M: Image-Text Pair Dataset",
                "VIGC: Visual instruction generation and correction",
                "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model",
                "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
                "ReferItGame: Referring to Objects in Photographs of Natural Scenes"
            ]
        },
        "topic_history": [
            {
                "name": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
                "arxiv_id": "2401.15947",
                "reference": [
                    "Lisa: Reasoning segmentation via large language model",
                    "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
                    "Judging llm-as-a-judge with mt-bench and chatbot arena",
                    "The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only",
                    "Internlm: A multilingual language model with progressively enhanced capabilities",
                    "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
                    "Sparse upcycling: Training mixture-of-experts from dense checkpoints",
                    "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
                    "Gshard: Scaling giant models with conditional computation and automatic sharding",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Improved baselines with visual instruction tuning",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Cot-mote: Exploring contextual masked auto-encoder pre-training with mixture-of-textual-experts for passage retrieval",
                    "Multimodal contrastive learning with limoe: the language-image mixture of experts",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A survey on multimodal large language models",
                    "Eve: Efficient vision-language pre-training with masked prediction and modality-aware moe",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Moss: Training conversational language models from synthetic data",
                    "Kosmos-2: Grounding multimodal large language models to the world",
                    "Grounding language models to images for multimodal generation",
                    "Glamm: Pixel grounding large multimodal model",
                    "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
                    "Gpt-4 technical report",
                    "Bubogpt: Enabling visual grounding in multi-modal llms",
                    "Scaling vision-language models with sparse mixture of experts",
                    "Visual instruction tuning",
                    "Uni-perceiver-moe: Learning sparse generalist models with conditional moes",
                    "Swin transformer v2: Scaling up capacity and resolution",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
                    "Learning factored representations in a deep mixture of experts",
                    "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
                    "Baichuan 2: Open large-scale language models",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Detgpt: Detect what you need via reasoning",
                    "Pace: Unified multi-modal dialogue pre-training with progressive and compositional experts",
                    "Honeybee: Locality-enhanced projector for multimodal llm",
                    "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
                    "Video-llava: Learning united visual representation by alignment before projection",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Alpaca: A strong, replicable instruction-following model",
                    "Llama: Open and efficient foundation language models",
                    "Glm: General language model pretraining with autoregressive blank infilling",
                    "Adaptive mixtures of local experts",
                    "Mixture of cluster-conditional lora experts for vision-language instruction tuning",
                    "Beyond distillation: Task-level mixture-of-experts for efficient inference",
                    "Qwen technical report",
                    "Svit: Scaling up visual instruction tuning",
                    "Multiway-adapater: Adapting large-scale multi-modal models for scalable image-text retrieval",
                    "Flamingo: a visual language model for few-shot learning",
                    "St-moe: Designing stable and transferable sparse expert models",
                    "Rome: Role-aware mixture-of-expert transformer for text-to-video retrieval",
                    "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer"
                ]
            },
            {
                "name": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
                "arxiv_id": "2403.20330",
                "reference": [
                    "Phi2: The surprising power of small language models",
                    "A-okvqa: A benchmark for visual question answering using world knowledge",
                    "A diagram is worth a dozen images",
                    "Palm: Scaling language modeling with pathways",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Internlm: A multilingual language model with progressively enhanced capabilities",
                    "Mistral 7b",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Deepseek llm: Scaling open-source language models with longtermism",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "To see is to believe: Prompting gpt-4v for better visual instruction tuning",
                    "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition",
                    "Improved baselines with visual instruction tuning",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Multilayer perceptron (mlp",
                    "Can vision-language models think from a first-person perspective",
                    "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Yi: Open foundation models by 01. ai",
                    "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                    "Cheap and quick: Efficient vision-language instruction tuning for large language models",
                    "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Mixtral of experts",
                    "Deepseek-vl: Towards real-world vision-language understanding",
                    "Visual instruction tuning",
                    "Learn to explain: Multimodal reasoning via thought chains for science question answering",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Scaling up visual and vision-language representation learning with noisy text supervision",
                    "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                    "Learning transferable visual models from natural language supervision",
                    "Baichuan 2: Open large-scale language models",
                    "Seed-bench: Benchmarking multimodal llms with generative comprehension",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Chatgpt",
                    "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
                    "Qwen technical report",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Gemini: a family of highly capable multimodal models",
                    "Q-bench: A benchmark for general-purpose foundation models on low-level vision",
                    "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
                "arxiv_id": "2403.14520",
                "reference": [
                    "Recent advances in natural language processing via large pre-trained language models: A survey",
                    "Liquid structural state-space models",
                    "Llava-phi: Efficient multi-modal assistant with small language model",
                    "Mobilevlm : A fast, strong and open vision language assistant for mobile devices",
                    "Mobilevlm v2: Faster and stronger baseline for vision language model",
                    "Stable lm",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "GLM: general language model pretraining with autoregressive blank infilling",
                    "Textbooks are all you need ii: phi",
                    "Hungry hungry hippos: Towards language modeling with state space models",
                    "Instance-aware prompt learning for language understanding and generation",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Improved baselines with visual instruction tuning",
                    "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama",
                    "Stanford alpaca: An instruction-following llama model",
                    "Gpt-4 technical report",
                    "It's raw! audio generation with state-space models",
                    "Simplified state space layers for sequence modeling",
                    "Redpajama: an open dataset for training large language models",
                    "Efficiently modeling long sequences with structured state spaces",
                    "Visual instruction tuning",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Tinyllama: An open-source small language model",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Learning transferable visual models from natural language supervision",
                    "Decision s4: Efficient sequence-based rl via state spaces layers",
                    "Structured state space models for in-context reinforcement learning",
                    "Llama: Open and efficient foundation language models",
                    "Diffusion models without attention",
                    "Flamingo: a visual language model for few-shot learning",
                    "Llama-adapter v2: Parameter-efficient visual instruction model",
                    "Textbooks are all you need"
                ]
            },
            {
                "name": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception",
                "arxiv_id": "2401.16158",
                "reference": [
                    "Mm-react: Prompting chatgpt for multimodal reasoning and action",
                    "Appagent: Multimodal agents as smartphone users",
                    "Metagpt: Meta programming for multi-agent collaborative framework",
                    "Auto-gpt for online decision making: Benchmarks and additional opinions",
                    "Llava-plus: Learning to use tools for creating multimodal agents",
                    "Small llms are weak tool learners: A multi-llm agent",
                    "Gpt4tools: Teaching large language model to use tools via self-instruction",
                    "Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language",
                    "Controlllm: Augment language models with tools by searching on graphs",
                    "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                    "Visual chatgpt: Talking, drawing and editing with visual foundation models",
                    "Modelscope-agent: Building your customizable agent system with open-source large language models"
                ]
            },
            {
                "name": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
                "arxiv_id": "2405.21075",
                "reference": [
                    "Timechat: A time-sensitive multimodal large language model for long video understanding",
                    "A challenger to gpt-4v? early explorations of gemini in visual expertise",
                    "Pegasus-v1 technical report",
                    "Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models",
                    "Introducing our multimodal models",
                    "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Valley: Video assistant with large language model enhanced ability",
                    "Video-chatgpt: Towards detailed video understanding via large vision and language models",
                    "Sigmoid loss for language image pre-training",
                    "Hawkeye: Training video-text llms for grounding text in videos",
                    "A survey on multimodal large language models",
                    "Moviechat: From dense token to sparse memory for long video understanding",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Videochat: Chat-centric video understanding",
                    "Video-llama: An instruction-tuned audio-visual language model for video understanding",
                    "Value: A multi-task benchmark for video-and-language understanding evaluation",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models",
                    "Vtimellm: Empower llm to grasp video moments",
                    "Momentor: Advancing video large language model with fine-grained temporal reasoning",
                    "Visual instruction tuning",
                    "An image is worth 16x16 words: Transformers for image recognition at scale",
                    "Mm-vet: Evaluating large multimodal models for integrated capabilities",
                    "Learning transferable visual models from natural language supervision",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models",
                    "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
                    "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems",
                    "Pllava : Parameter-free llava extension from images to videos for video dense captioning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "Tempcompass: Do video llms really understand videos",
                    "Llama: Open and efficient foundation language models",
                    "Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models",
                    "Mvbench: A comprehensive multi-modal video understanding benchmark",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Flamingo: a visual language model for few-shot learning",
                    "Video-teller: Enhancing cross-modal generation with fusion and decoupling",
                    "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
                    "Lita: Language instructed temporal-localization assistant"
                ]
            },
            {
                "name": "Red Teaming Visual Language Models",
                "arxiv_id": "2401.12915",
                "reference": [
                    "Introducing our multimodal models",
                    "Pali-x: On scaling up a multilingual vision and language model",
                    "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
                    "A survey for in-context learning",
                    "Truthfulqa: Measuring how models mimic human falsehoods",
                    "Aligning large multimodal models with factually augmented rlhf",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Reducing sentiment bias in language models via counterfactual evaluation",
                    "Visual adversarial examples jailbreak aligned large language models",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Red teaming language models with language models",
                    "Improved baselines with visual instruction tuning",
                    "M{}^{3}: A large-scale dataset towards multi-modal multilingual instruction tuning",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
                    "Mmicl: Empowering vision-language model with multi-modal in-context learning",
                    "The secret sharer: Evaluating and testing unintended memorization in neural networks",
                    "True few-shot learning with language models",
                    "Visual instruction tuning",
                    "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
                    "Openflamingo: An open-source framework for training large autoregressive vision-language models",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Silkie: Preference distillation for large visual language models",
                    "Direct preference optimization: Your language model is secretly a reward model",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Flamingo: a visual language model for few-shot learning"
                ]
            },
            {
                "name": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
                "arxiv_id": "2401.11170",
                "reference": [
                    "Denial of service attacks in wireless networks: The case of jammers",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "The curious case of neural text degeneration",
                    "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                    "Slowlidar: Increasing the latency of lidar-based detection using adversarial examples",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Nicgslowdown: Evaluating the efficiency robustness of neural image caption generation models",
                    "The dark side of dynamic routing neural networks: Towards efficiency backdoor injection",
                    "Nmtsloth: understanding and testing efficiency degradation of neural machine translation systems",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A panda? no, it's a sloth: Slowdown attacks on adaptive multi-exit neural network inference",
                    "Roberta: A robustly optimized bert pretraining approach",
                    "Bottom-up and top-down attention for image captioning and visual question answering",
                    "Sponge examples: Energy-latency attacks on neural networks"
                ]
            },
            {
                "name": "Visual Hallucinations of Multi-modal Large Language Models",
                "arxiv_id": "2402.14683",
                "reference": [
                    "Evaluating object hallucination in large vision-language models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions",
                    "A survey on hallucination in large vision-language models",
                    "Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark",
                    "A survey of hallucination in large foundation models",
                    "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
                    "Mitigating hallucination in large multi-modal models via robust instruction tuning",
                    "Eyes wide shut? exploring the visual shortcomings of multimodal llms",
                    "Visual evidence prompting mitigates hallucinations in multimodal large language models",
                    "Mass-producing failures of multimodal systems with language models",
                    "Woodpecker: Hallucination correction for multimodal large language models",
                    "Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites",
                    "Survey of hallucination in natural language generation"
                ]
            },
            {
                "name": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
                "arxiv_id": "2402.10884",
                "reference": [
                    "Mm-react: Prompting chatgpt for multimodal reasoning and action",
                    "Finetuned language models are zero-shot learners",
                    "Multimodal-gpt: A vision and language model for dialogue with humans",
                    "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
                    "Visual chatgpt: Talking, drawing and editing with visual foundation models",
                    "Aligning large multimodal models with factually augmented rlhf",
                    "The false promise of imitating proprietary llms",
                    "Zephyr: Direct distillation of lm alignment",
                    "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM",
                    "Ultrafeedback: Boosting language models with high-quality feedback",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification",
                    "Visual instruction tuning",
                    "Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Learning transferable visual models from natural language supervision",
                    "mplug-owl: Modularization empowers large language models with multimodality",
                    "Sharegpt4v: Improving large multi-modal models with better captions",
                    "Llama: Open and efficient foundation language models",
                    "Otter: A multi-modal model with in-context instruction tuning",
                    "Flamingo: a visual language model for few-shot learning",
                    "Training language models to follow instructions with human feedback"
                ]
            },
            {
                "name": "Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models",
                "arxiv_id": "2401.00988",
                "reference": [
                    "Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario",
                    "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
                    "Clip2point: Transfer clip to point cloud classification with image-depth pre-training",
                    "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
                    "Drama: Joint risk localization and captioning in driving",
                    "Referring multi-object tracking",
                    "Talk2bev: Language-enhanced bird's-eye view maps for autonomous driving",
                    "Talk2car: Taking control of your self-driving car",
                    "Drivegpt4: Interpretable end-to-end autonomous driving via large language model",
                    "Videochat: Chat-centric video understanding",
                    "Video-llama: An instruction-tuned audio-visual language model for video understanding",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Hilm-d: Towards high-resolution understanding in multimodal large language models for autonomous driving",
                    "Gpt-driver: Learning to drive with gpt",
                    "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
                    "Gpt-4 technical report",
                    "Perceptiongpt: Effectively fusing visual perception into llm",
                    "3d-llm: Injecting the 3d world into large language models",
                    "Object referring in videos with language and human gaze",
                    "G-llava: Solving geometric problem with multi-modal large language model",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Llama: Open and efficient foundation language models",
                    "Detgpt: Detect what you need via reasoning, 2023a",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Flamingo: a visual language model for few-shot learning",
                    "Planning-oriented autonomous driving",
                    "Etc: Temporal boundary expand then clarify for weakly supervised video grounding with multimodal large language model"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
            "arxiv_id": "2306.00978",
            "isAPA": true,
            "abstract": "Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users'privacy. However, the astronomical model size and the limited hardware resource pose significant deploymentchallenges. We propose Activation-aware Weight Quantization (AWQ) , a hardware-friendly approach for LLMlow-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protectingonly 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we shouldrefer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization,we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employsan equivalent transformation to scale the salient weight channels to protect them. The scale is determined bycollecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, soit generalizes to different domains and modalities without overfitting the calibration set. AWQ outperformsexisting work on various language modeling and domain-specific benchmarks (coding and math) . Thanks tobetter generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the firsttime, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference frameworktailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offersmore than 3\u00d7 speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It alsodemocratizes the deployment of the 70B Llama-2 model on mobile GPUs",
            "reference": [
                "Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts",
                "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b",
                "Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F., and Liu, X. Outlier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint arXiv:2209.13325, 2022b",
                "Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8() : 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv",
                "Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A., Mao, H., Kautz, J., Shoeybi, M., and Han, S. Vila: On pre-training for visual language models",
                "Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv",
                "Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size quantization. arXiv preprint arXiv",
                "Koh, J. Y., Salakhutdinov, R., and Fried, D. Grounding language models to images for multimodal generation. arXiv preprint arXiv",
                "Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023b",
                "Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv",
                "Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u00e8, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., Gall\u00e9, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv",
                "Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems",
                "Choi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srinivasan, V., and Gopalakrishnan, K. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv",
                "Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people",
                "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a",
                "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems",
                "Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv",
                "Feng, S., Hou, B., Jin, H., Lin, W., Shao, J., Lai, R., Ye, Z., Zheng, L., Yu, C. H., Yu, Y., and Chen, T. TensorIR: An Abstraction for Automatic Tensorized Program Optimization",
                "Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv",
                "Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering",
                "Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv",
                "Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and Gu, S. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv",
                "Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv",
                "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems",
                "Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. TVM: An Automated End-to-End Optimizing Compiler for Deep Learning",
                "Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems",
                "Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023c",
                "Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
                "Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv",
                "Tillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate language and compiler for tiled neural network computations",
                "Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs",
                "Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "Nagel, M., Baalen, M. v., Blankevoort, T., and Welling, M. Data-free quantization through weight equalization and bias correction",
                "Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read",
                "Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems",
                "Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. 2023a",
                "Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F., and Liu, X. Outlier suppression: Pushing the limit of low-bit transformer language models, 2022a. URL https://arxiv.org/abs",
                "Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv",
                "Wei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., and Liu, X. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv",
                "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners",
                "Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Jitsev, J., Kornblith, S., Koh, P. W., Ilharco, G., Wortsman, M., and Schmidt, L. Openflamingo, March 2023. URL https://doi.org/10.5281/zenodo",
                "Wang, K., Liu, Z., Lin, Y., Lin, J., and Han, S. HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
                "Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv",
                "Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023b",
                "Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv",
                "Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems",
                "Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks for efficient integer-arithmetic-only inference",
                "Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers, 2022. URL https://arxiv.org/abs",
                "MLC-Team. MLC-LLM, 2023. URL https://github.com/mlc-ai/mlc-llm",
                "Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv",
                "Lin, J., Chen, W.-M., Lin, Y., Gan, C., Han, S., et al. Mcunet: Tiny deep learning on iot devices. Advances in Neural Information Processing Systems",
                "Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a",
                "Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture with cascade token and head pruning. CoRR, abs/2012.09852, 2020. URL https://arxiv.org/abs",
                "Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv",
                "Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv",
                "Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv",
                "Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv",
                "Bengio, Y., L\u00e9onard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv",
                "Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y., Van Baalen, M., and Blankevoort, T. A white paper on neural network quantization. arXiv preprint arXiv",
                "Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models",
                "Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models",
                "Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, D. Y., Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E., et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv",
                "Klimt, B. and Yang, Y. The enron corpus: A new dataset for email classification research",
                "Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023d",
                "Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv",
                "Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca",
                "Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv",
                "Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv",
                "Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna",
                "Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? adaptive rounding for post-training quantization",
                "Kim, Y. J., Henry, R., Fahim, R., and Awadalla, H. H. Who says elephants can't run: Bringing large scale moe models into cloud scale production. arXiv preprint arXiv"
            ],
            "related work": "2Related WorkModel quantization methods.Quantization reduces the bit-precision of deep learning modelsHan et al. (2016) ; Jacob et al. (2018) ; Nagel et al. (2019) ; Wang et al. (2019) ; Nagel et al. (2020) ; Lin et al. (2020) , which helps to reduce the model size and accelerate inference. Quantization techniques generally fall into two categories: quantization-aware training (QAT, which relies on backpropagation to update the quantized weights) Bengio et al. (2013) ; Gholami et al. (2021) ; Nagel et al. (2021) ; Choi et al. (2018) and post-training quantizationJacob et al. (2018) ; Nagel et al. (2019;2020) (PTQ, usually training-free) . The QAT methods cannot easily scale up to large models like LLMs. Therefore, people usually use PTQ methods to quantize LLMs.Quantization of LLMs.People study two settings for LLM quantization: (1) W8A8 quantization, where both activation and weights are quantized to INT8Dettmers et al. (2022) ; Xiao et al. (2022) ; Yao et al. (2022) ; Wei et al. (2022a;2023) ; (2) Low-bit weight-only quantization (e.g., W4A16) , where only weights are quantized into low-bit integersFrantar et al. (2022) ; Dettmers & Zettlemoyer (2022) ; Sheng et al. (2023) ; Park et al. (2022) . We focus on the second setting in this work since it not only reduces the hardware barrier (requiring a smaller memory size) but also speeds up the token generation (remedies memory-bound workload) . Apart from the vanilla round-to-nearest baseline (RTN) , GPTQFrantar et al. (2022) is the closest to our work. However, the reconstruction process of GPTQ leads to an over-fitting issue to the calibration set and may not preserve the generalist abilities of LLMs for other modalities and domains. It also requires a reordering trick to work for some models (e.g., LLaMA-7BTouvron et al. (2023a) and OPT-66BZhang et al. (2022) ) . Apart from quantiztion methods designed for general-purporse hardware, SpAttenWang et al. (2020) designs a progressive approach to gradually increase the number of bits used in softmax calculation.System support for low-bit quantized LLMs.Low-bit quantized LLMs have been a popular setting to reduce inference costs. There are some system supports to achieve a practical speed-up. GPTQFrantar et al. (2022) provides INT3 kernels for OPT models andGPTQ-for-LLaMAextends kernel support for INT4 reordered quantization with the help of TritonTillet et al. (2019) . FlexGenSheng et al. (2023) ,llama.cpp***https://github.com/ggerganov/llama.cppandexllama\u2020\u2020\u2020https://github.com/turboderp/exllamaperform group-wise INT4 quantization to reduce I/O costs and offloading. FasterTransformer implements FP16\u00d7\\times\u00d7INT4 GEMM for weight-only per-tensor quantization but does not support group quantization. LUT-GEMMPark et al. (2022) performs bitwise computation on GPU CUDA cores with the help of lookup tables. Our concurrent work, MLC-LLMMLC-Team (2023) offers strong results on multiple edge CPU and GPU platforms thanks to the powerful TVMChen et al. (2018) ; Feng et al. (2023) backend.",
            "date": "2023"
        },
        "topic": "Acceleration for LLMs",
        "year_start": "2022",
        "year_end": "2024",
        "target_list": [
            {
                "name": "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge",
                "arxiv_id": "2402.10787",
                "subtitles": [
                    "Efficient Large Language Models",
                    "Quantization for LLMs"
                ],
                "reference": [
                    "Specializing smaller language models towards multi-step reasoning",
                    "Llm-qat: Data-free quantization aware training for large language models",
                    "Sparsegpt: Massive language models can be accurately pruned in one-shot",
                    "Bloom: A 176b-parameter open-access multilingual language model",
                    "Awq: Activation-aware weight quantization for llm compression and acceleration",
                    "Llama: Open and efficient foundation language models",
                    "Gpt-4 technical report",
                    "Opt: Open pre-trained transformer language models",
                    "Gptq: Accurate post-training quantization for generative pre-trained transformers",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers",
                    "Lora: Low-rank adaptation of large language models"
                ],
                "related_work": "2Related Work2.1Efficient Large Language ModelsRecent advancements in LLMs like GPT-4Achiam et al. (2023) have significantly improved NLP capabilities at the cost of massive computations and energy, limiting their accessibility and applications. This has led to the emergence of efficient and lightweight LLMs to address these limitations without compromising performance. Models such as LLaMATouvron et al. (2023) , OPTZhang et al. (2022) , and BLOOMWorkshop et al. (2022) offer a wide range of sizes, from as few as 125M to as many as 176B parameters, providing versatile options for various applications. To further enhance the efficiency of LLMs, multiple compression techniques have been developedHu et al. (2021) ; Frantar et al. (2022) ; Frantar and Alistarh (2023) ; Fu et al. (2023) . These methods aim to reduce model size and computational demands, enabling deployment on resource-constrained platforms such as edge devices. This shift towards more manageable models facilitates real-time NLP applications like virtual assistants and language translation, broadening the accessibility and utility of advanced NLP technologies.2.2Quantization for LLMsQuantization reduces DNN bit-precision, leading to smaller models and faster inference. Current methods are divided into PTQ and QAT, each offering distinct advantages and facing unique challenges. PTQ generally results in low accuracy, especially in low-bit quantizations. To address this, SmoothquantXiao et al. (2023) achieves W8A8 precision by smoothing activation outliers, while ZeroQuantYao et al. (2022) employs a layer-by-layer knowledge distillation algorithm to enhance low-bit quantization performance. Different from PTQ, QAT presents a promising avenue for better performance, requiring massive data and resources for fine-tuning, which is especially hard for LLMs.Most PTQ and QAT works focus on weight-only quantization, and the weight-activation quantization to quantize both weights and activations is less explored. GPTQFrantar et al. (2022) and AWQLin et al. (2023) focus on reducing the precision of weights while maintaining full-precision activations. Thus, their speedups may be limited due to the computational costs with full-precision activations. Only LLM-QATLiu et al. (2023) employs data-free distillation methods to quantize the weights, activations, and KV caches for large models like LLaMA-7B, which can hardly be deployed on edge devices. The exploration of weight-activation quantization with QAT for lightweight LLMs facilitating deployment on the edge is still an open field.",
                "abstract": "Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information distortion in quantized attention maps, demonstrated by the different distributions in quantized query and key of the self-attention mechanism. Then, the entropy and distribution guided QAT is proposed to mitigate the information distortion. Moreover, we design a token importance-aware adaptive method to dynamically quantize the tokens with different bit widths for further optimization and acceleration. Our extensive experiments verify the substantial improvements with our framework across various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x compared with its FP16 counterparts across multiple edge devices, signaling a groundbreaking advancement."
            },
            {
                "name": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models",
                "arxiv_id": "2406.07528",
                "subtitles": [
                    "Efficient Context Computation",
                    "Context Length Extrapolation",
                    "Memory-based Approaches"
                ],
                "reference": [
                    "Efficient memory management for large language model serving with pagedattention",
                    "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory",
                    "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                    "Yarn: Efficient context window extension of large language models",
                    "CLEX: continuous length extrapolation for large language models",
                    "Flashdecoding++: Faster large language model inference on gpus",
                    "Generating long sequences with sparse transformers",
                    "LLM maybe longlm: Self-extend LLM context window without tuning",
                    "Memorizing transformers",
                    "Memory networks",
                    "Linformer: Self-attention with linear complexity",
                    "Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache",
                    "End-to-end memory networks",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Transformer-xl: Attentive language models beyond a fixed-length context",
                    "Key-value memory networks for directly reading documents",
                    "Efficient streaming language models with attention sinks",
                    "Snapkv: Llm knows what you are looking for before generation",
                    "Efficiently modeling long sequences with structured state spaces",
                    "Roformer: Enhanced transformer with rotary position embedding",
                    "Compressive transformers for long-range sequence modelling",
                    "Flashattention-2: Faster attention with better parallelism and work partitioning",
                    "Reformer: The efficient transformer",
                    "Generalization through memorization: Nearest neighbor language models",
                    "Focused transformer: Contrastive training for context scaling",
                    "Neural turing machines",
                    "Clex: Continuous length extrapolation for large language models",
                    "Explicit sparse transformer: Concentrated attention through explicit selection",
                    "Transformers are rnns: Fast autoregressive transformers with linear attention",
                    "Etc: Encoding long and structured inputs in transformers",
                    "Unlimiformer: Long-range transformers with unlimited length input",
                    "Longformer: The long-document transformer",
                    "Train short, test long: Attention with linear biases enables input length extrapolation",
                    "Big bird: Transformers for longer sequences",
                    "Fast transformer decoding: One write-head is all you need"
                ],
                "related_work": "2Related WorksEfficient Context Computation.The computational and memory demands of LLM training often limit it to short sequences. Using LLMs directly on long sequences presents challenges such as out-of-domain issues and distractions from lengthy and noisy inputsLin et al. (2024) ; Tworkowski et al. (2024a) ; Li et al. (2024d) . As a result, context length extrapolation has emerged as a method to extend LLMs' sequence length without additional training. Early approaches have designed new relative positional encoding mechanisms during pre-trainingPress et al. (2022) ; Tworkowski et al. (2024b) . The following research has focused on the extensively adopted rotary position embedding (RoPE) Su et al. (2023) , suggesting extending the length by interpolating positions to introduce non-integer positionsChen et al. (2024) ; Peng et al. (2023) ; Jin et al. (2024) ; Chen et al. (2023) . To process extremely long sequences, Stream-LLM(Xiao et al.,2024b) and LM-Infinite(Lin et al.,2024) utilize the sliding window attention mechanism and discard distant contexts. Additionally, InfLLMXiao et al. (2024a) leverages a context memory to furnish LLMs with pertinent contextual information. Yet, the objective of these models during long-text reading is inherently ambiguous, and it can become distracting when reading extensive articles. In this work, we introduce the Query-aware Context Lookup mechanism, enabling the model to effectively retrieve information relevant to the query from lengthy texts.Context Length Extrapolation.The computational complexity of attention layers, which grows quadratically, is a significant bottleneck restricting LLMs' capability to handle lengthy sequences. Consequently, numerous researchers have devised efficient attention mechanisms, including sparse attentionZaheer et al. (2021) ; Beltagy et al. (2020) ; Child et al. (2019) ; Ainslie et al. (2020) ; Zhao et al. (2019) , approximate attention computations using kernel functionsKitaev et al. (2020) ; Wang et al. (2020) ; Katharopoulos et al. (2020) , and replacing attention layers with state-space models of linear complexityGu et al. (2022) ; Gu and Dao (2023) . These approaches necessitate architectural modifications, requiring retraining of the models. Concurrently, many scholars have tackled this challenge from an infrastructural angle by optimizing the memory usage of attention computations to mitigate the computational resource requirements of the modelDao et al. (2022) ; Dao (2023) ; Hong et al. (2024) ; Shazeer (2019) ; Kwon et al. (2023) . Given the training-free nature of our method, it can be seamlessly integrated to further expedite LLM inference.Memory-based Approaches.Memory networks have been extensively researched for decades and have demonstrated effectiveness in enhancing models with additional information storage capabilitiesGraves et al. (2014) ; Weston et al. (2015) ; Sukhbaatar et al. (2015) ; Miller et al. (2016) . With the rise of pre-trained models, memory layers have gradually found application in the training stage of recurrent transformer layers, enabling models to recursively process long sequencesDai et al. (2019) ; Rae et al. (2020) ; Khandelwal et al. (2020) ; Wu et al. (2022) ; Bertsch et al. (2023) . These approaches segment sequences, encoding each segments individually, and utilize memory to retain context information from preceding segments. Yet, they necessitate architectural modifications and are typically incorporated during the pre-training phase. In contrast, our objective is to explore the intrinsic properties of LLMs and introduce a training-free Query-aware Context Lookup mechanism for long-text comprehension.",
                "abstract": "The capacity of Large Language Models (LLMs) to comprehend and reason over long contexts is pivotal for advancements in diverse fields. Yet, they still stuggle with capturing long-distance dependencies within sequences to deeply understand semantics. To address this issue, we introduce Query-aware Inference for LLMs (Q-LLM), a system designed to process extensive sequences akin to human cognition. By focusing on memory data relevant to a given query, Q-LLM can accurately capture pertinent information within a fixed window size and provide precise answers to queries. It doesn't require extra training and can be seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) can read Harry Potter within 30s and accurately answer the questions. On widely recognized benchmarks, Q-LLM improved by 7.17% compared to the current state-of-the-art on LLaMA3, and by 3.26% on Mistral on the $\\infty$-bench. In the Needle-in-a-Haystack and BABILong task, Q-LLM improved upon the current SOTA by 7.0% and 6.1%. Our code can be found inthis https URL."
            },
            {
                "name": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models",
                "arxiv_id": "2408.08554",
                "subtitles": [
                    "Weight-only quantization",
                    "Weight-activation quantization"
                ],
                "reference": [
                    "Spqr: A sparse-quantized representation for near-lossless llm weight compression",
                    "Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Billm: Pushing the limit of post-training quantization for llms",
                    "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models",
                    "Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks",
                    "Qa-lora: Quantization-aware low-rank adaptation of large language models",
                    "Omniquant: Omnidirectionally calibrated quantization for large language models",
                    "Extreme compression of large language models via additive quantization",
                    "Pb-llm: Partially binarized large language models",
                    "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration",
                    "Low-Rank Quantization-Aware Training for LLMs",
                    "LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale",
                    "Affinequant: Affine transformation quantization for large language models",
                    "Quip: 2-bit quantization of large language models with guarantees",
                    "PeQA: A Massive Persian Question-Answering and Chatbot Dataset",
                    "Qlora: Efficient finetuning of quantized llms",
                    "Gptq: Accurate post-training quantization for generative pre-trained transformers",
                    "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
                ],
                "related_work": "2Related WorkLLM quantization can be broadly divided into weight-only quantization and weight-activation quantization.Weight-only quantization.To alleviate computational burdens, some studies focus on weight-only quantization. LLM.int8() (Dettmers et al.2022) achieves accurate INT8 quantization by retaining significant channels. GPTQ(Frantar et al.2022) uses Hessian-based error compensation to reduce quantization errors in LLMs, enabling 3-bit quantization. AWQ(Lin et al.2024a) and OWQ(Lee et al.2024) significantly enhance quantized model performance by considering the impact of activation outliers on weight quantization. Methods like QuIP(Chee et al.2024) , QuIP#(Tseng et al.2024) , and AQLM(Egiazarian et al.2024) facilitate 2-bit quantization through learnable codebooks or additional fine-tuning. Approaches such as(Dettmers et al.2023; Shang et al.2023; Huang et al.2024) improve PTQ performance through unstructured mixed-precision fine-grained weight grouping. Additionally, research such as(Dettmers et al.2024; Xu et al.2023b; Arshia et al.2022; Bondarenko, Del Chiaro, and Nagel2024) employs efficient parameter fine-tuning (PEFT) techniques to compress weights through fine-tuning.Weight-activation quantization.Weight-activation quantization differs from weight-only quantization by quantizing both weights and activation (including KV caches) to accelerate LLM inference. The main challenge in quantizing activation is handling outliers, which can cause significant quantization errors. To address this issue, ZeroQuant(Yao et al.2022) proposes a fine-grained, hardware-friendly quantization scheme for weights and activation. SmoothQuant(Xiao et al.2023) shifts the quantization difficulty from activation to weights through mathematically equivalent transformations, achieving W8A8 quantization.(Shao et al.2023; Ma et al.2024b; Hu et al.2024a) enhances performance by training quantization parameters. Limited by GPU platform instruction limitations, these jobs can only use W8A8 to perform actual inference, even if they achieve lower quantization bit-widths (e.g., W6A6) .",
                "abstract": "Large Language Models (LLMs) have revolutionized natural language processing tasks. However, their practical application is constrained by substantial memory and computational demands. Post-training quantization (PTQ) is considered an effective method to accelerate LLM inference. Despite its growing popularity in LLM model compression, PTQ deployment faces two major challenges. First, low-bit quantization leads to performance degradation. Second, restricted by the limited integer computing unit type on GPUs, quantized matrix operations with different precisions cannot be effectively accelerated. To address these issues, we introduce a novel arbitrary-bit quantization algorithm and inference framework, ABQ-LLM. It achieves superior performance across various quantization settings and enables efficient arbitrary-precision quantized inference on the GPU. ABQ-LLM introduces several key innovations: (1) a distribution correction method for transformer blocks to mitigate distribution differences caused by full quantization of weights and activations, improving performance at low bit-widths. (2) the bit balance strategy to counteract performance degradation from asymmetric distribution issues at very low bit-widths (e.g., 2-bit). (3) an innovative quantization acceleration framework that reconstructs the quantization matrix multiplication of arbitrary precision combinations based on BTC (Binary TensorCore) equivalents, gets rid of the limitations of INT4/INT8 computing units. ABQ-LLM can convert each component bit width gain into actual acceleration gain, maximizing performance under mixed precision(e.g., W6A6, W2A8). Based on W2*A8 quantization configuration on LLaMA-7B model, it achieved a WikiText2 perplexity of 7.59 (2.17$\\downarrow $ vs 9.76 in AffineQuant). Compared to SmoothQuant, we realized 1.6$\\times$ acceleration improvement and 2.7$\\times$ memory compression gain."
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "subtitles": [
                    "Multimodal LLMs",
                    "Evaluating Multimodal LLMs",
                    "Visual Encoders",
                    "Ambiguities in Embedding Models"
                ],
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ],
                "related_work": "5Related WorksMultimodal LLMs.We study the limitations of Multimodal LLMs[40,13,30,31,8]and explore possible ways to improve these models. Multimodal LLMs build from pretrained Large Language Models[41,3,58,59,69]and CLIP vision encoder[43,54]. These systems then use an adapter, such as MLPs[30,31], Q-Former[26,8], and gated attention[2,25], to integrate the pretrained CLIP vision encoder into LLMs. More recently, instructBLIP[8], LLaVA-1.5[30]highlight the importance of high-quality training data. Yet, there is a scarcity of research focusing on the impact of visual encoders, which is an important gap our work aims to address through a systematic study.Evaluating Multimodal LLMs.MMVP assesses MLLMs using a set of simple yet critical Visual Question Answering (VQA) questions constructed from CLIP-blind pairs. Previous benchmarks such as TextVQA[52], VQAv2[15], and GQA[21]have centered on traditional VQA queries. Recently, there are works like MM-Vet[64], POPE[27], and MM-Bench[32]designed to specifically evaluate multimodal LLMs including hallucination, reasoning, and robustness. The previous benchmarks and evaluations have shown that Multimodal LLMs can suffer from hallucination[29,28], catastrophic forgetting[67]and lack of robustness[11]. In taking a step back to the fundamentals, our work uncovers that even the most advanced multimodal LLMs, such as GPT-4V[40], Gemini[14], Bard[30], and LLaVA-1.5[30], are not immune to stumbling over elementary visual questions. We also identified part of the problem as being the incapable visual encoder.Visual Encoders.MMVP-VLM provides a detailed analysis of the visual capabilities of various CLIP variants[43,54,62,66]. These models mostly follow the method proposed inRadford et al.[43]that uses contrastive loss to train on large volumes of image-text pairs. They differ in training data[62], training recipes[54], and objective functions[66]. Nonetheless, our studies show that all of these CLIP variants struggle with simple visual patterns such as  \"orientation \",  \"count \",  \"presence of specific features \",etc. Another line of research focuses on vision-only self-supervised learning (SSL) . This category includes contrastive SSL[7,16,5,17]and mask-based SSL[70,18,4]. SLIP[39]explores the synergy between CLIP and contrastive SSL, but focusing primarily on standard classification tasks. In fact, a common practice to evaluate the quality of these vision models is through linear probing or fine-tuning on ImageNet[47,45]. Although current evaluation methods provide a basic level of assessment on representation quality, our findings indicate a growing detachment from the needs of recent use cases. As demonstrated in the MoF experiments in Section4, the CLIP vision model and the vision-only SSL models learn complementary features. However, the linear probing accuracy on ImageNet alone provides a limited understanding of feature utility in MLLMs. This observation suggests the need for more diverse evaluations[61]in visual representation learning, to better align with current and emerging applications.Ambiguities in Embedding Models.Our work exploits CLIP-blind pairs within the CLIP vision embedding space to generate examples of failures in CLIP models and subsequently MLLMs. This concept has ties to previous research focused on documenting failure modes in text embedding models[12,36,55]. More recently,Thrush et al.[56],Yuksekgonul et al.[65]andHsieh et al.[19]study the binding problems CLIP faces in processing text queries, noting that CLIP models treat text input as a bag of words.Tong et al.[57]examines the implications for downstream text-guided generative models.Tschannen et al.[60]suggests image captioners as promising alternatives to CLIP for improving attribute binding. Our work focuses on the visual patterns.",
                "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems."
            },
            {
                "name": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
                "arxiv_id": "2407.20042",
                "subtitles": [
                    "LLM-based Code Generation",
                    "Efficient Inference of LLMs"
                ],
                "reference": [
                    "Code completion by modeling flattened abstract syntax trees as graphs",
                    "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
                    "Exploring continual learning for code generation models",
                    "Efficient Memory Management for Large Language Model Serving with PagedAttention",
                    "Magicoder: Source code is all you need",
                    "Accelerating Code Search with Deep Hashing and Code Classification",
                    "Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems",
                    "Towards more realistic evaluation of LLM-based code generation: an experimental study and beyond",
                    "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
                    "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                    "Lever: Learning to verify language-to-code generation with execution",
                    "Structured Chain-of-Thought Prompting for Code Generation",
                    "Refining ChatGPT-generated code: Characterizing and mitigating code quality issues",
                    "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
                    "Latent predictor networks for code generation",
                    "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
                    "Fast inference from transformers via speculative decoding",
                    "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                    "Planning with large language models for code generation",
                    "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
                    "Deep learning for source code modeling and generation: Models, applications, and challenges",
                    "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards Efficient Code Generation",
                    "Improving Code Generation by Dynamic Temperature Sampling",
                    "Phind-CodeLlama-34B-v",
                    "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding",
                    "Retrieval-based neural code generation",
                    "AceCoder: An Effective Prompting Technique Specialized in Code Generation",
                    "Large Language Model-Aware In-Context Learning for Code Generation"
                ],
                "related_work": "7.Related Work7.1.LLM-based Code GenerationCode generation has been extensively studied in recent years(Le et al.,2020; Wang and Li,2021; Hayati et al.,2018; Ling et al.,2016) and currently the paradigm has been shifted to LLM-based code generation(Zheng et al.,2024; Wang et al.,2024; Guo et al.,2022; Luo et al.,2023; Jain et al.,2023; Yadav et al.,2023; Li et al.,2023c; Liu et al.,2023; Li et al.,2023b; Zhang et al.,2023b) . Some works(Phind,2023; Luo et al.,2023; Jain et al.,2023; Yadav et al.,2023) boost the code generation ability through Supervised Fine-Tuning (SFT) . For instance, Phind-CodeLlama(Phind,2023) achieves superior performance over GPT-4 through SFT on high-quality code datasets based on the CodeLlama-34B model. Besides, some works(Li et al.,2023c; Liu et al.,2023; Li et al.,2023b; Zhang et al.,2023b; Wei et al.,2023) propose effective prompt techniques to enhance the code generation ability of Code LLMs. For instance, Li et al.(Li et al.,2024) propose AceCoder, which retrieves programs related to given requirements to create a prompt, enabling the model to learn from these examples and generate high-quality code. Other works(Zhu et al.,2023; Ni et al.,2023) propose decoding strategies for Code LLMs to improve the performance of code generation. For example, Zhu et al.(Zhu et al.,2023) propose AdapT sampling to improve the performance of code generation through adaptive adjusting the decoding temperature.7.2.Efficient Inference of LLMsRecently, many studies attempt to improve the inference efficiency of LLMs. Some works(Kwon et al.,2023; Dao et al.,2022; Dao,2023) increase the inference speed of LLMs by optimizing memory management and data access. Dao et al.(Dao et al.,2022; Dao,2023) propose FlashAttention to accelerate inference by reorganizing attention computation through a tiling approach, which significantly decreases GPU memory read/write operations. Other works(Leviathan et al.,2023; Zhang et al.,2023e) accelerate the inference of LLMs by decreasing the computational time of predicting each token. Leviathan et al.(Leviathan et al.,2023) propose speculative decoding, which employs a small model to predict each token and a large model to verify it. This approach not only enhances the generation speed but also ensures the quality of the outputs. Besides, many works attempt to increase the inference efficiency of LLMs in code intelligence tasks(Sun et al.,2023,2024a,2024b; Gu et al.,2022) . For example, Sun et al.(Sun et al.,2023) propose an efficient inference approach for Code LLMs. This method utilizes a Transformer-based estimator to assess the quality of prompts and prevent the completion of low-quality prompts in advance. Compared with these works,CodeFastimproves inference efficiency for Code LLMs by preventing the generation of excess tokens. Our approach is complementary to existing efficient inference approaches, allowing for the synergy to further enhance inference efficiency for Code LLMs. However, these current technologies, while effective at enhancing the inference speed of large language models, have not been tailored specifically to the unique characteristics of code generation tasks.",
                "abstract": "Code generation aims to automatically generate code snippets that meet given natural language requirements and plays an important role in software development. Although Code LLMs have shown excellent performance in this domain, their long generation time poses a signification limitation in practice use. In this paper, we first conduct an in-depth preliminary study with different Code LLMs on code generation tasks and identify a significant efficiency issue, i.e., continual generation of excess tokens. It harms the developer productivity and leads to huge computational wastes. To address it, we introduce CodeFast, an inference acceleration approach for Code LLMs on code generation. The key idea of CodeFast is to terminate the inference process in time when unnecessary excess tokens are detected. First, we propose an automatic data construction framework to obtain training data. Then, we train a unified lightweight model GenGuard applicable to multiple programming languages to predict whether to terminate inference at the current step. Finally, we enhance Code LLM with GenGuard to accelerate its inference in code generation tasks. We conduct extensive experiments with CodeFast on five representative Code LLMs across four widely used code generation datasets. Experimental results show that (1) CodeFast can significantly improve the inference speed of various Code LLMs in code generation, ranging form 34% to 452%, without compromising the quality of generated code. (2) CodeFast is stable across different parameter settings and can generalize to untrained datasets. Our code and data are available atthis https URL"
            },
            {
                "name": "Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight",
                "arxiv_id": "2407.15819",
                "subtitles": [
                    "Multi-modal large language models",
                    "Efficient model pre-training",
                    "Multi-scale hierarchy in vision"
                ],
                "reference": [
                    "Feature pyramid networks for object detection",
                    "Flamingo: a visual language model for few-shot learning",
                    "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                    "Backpropagation applied to handwritten zip code recognition",
                    "Mar: Masked autoencoders for efficient action recognition",
                    "Opt: Open pre-trained transformer language models",
                    "Scaling language-image pre-training via masking",
                    "Fully convolutional networks for semantic segmentation",
                    "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
                    "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
                    "Zero: Memory optimizations toward training trillion parameter models",
                    "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models",
                    "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
                    "Mm1: Methods, analysis & insights from multimodal llm pre-training",
                    "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Introducing meta llama 3: The most capable openly available llm to date",
                    "Swin transformer: Hierarchical vision transformer using shifted windows",
                    "Deepseek llm: Scaling open-source language models with longtermism",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Deep residual learning for image recognition",
                    "Multiscale vision transformers",
                    "Improving language understanding by generative pre-training",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Visual instruction tuning",
                    "Imagenet classification with deep convolutional neural networks",
                    "Pali: A jointly-scaled multilingual language-image model",
                    "Multimodal few-shot learning with frozen language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
                    "Flashattention-2: Faster attention with better parallelism and work partitioning",
                    "Moe-llava: Mixture of experts for large vision-language models",
                    "Llava-next: Improved reasoning, ocr, and world knowledge, January",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Honeybee: Locality-enhanced projector for multimodal llm",
                    "Attention is all you need",
                    "Llama: Open and efficient foundation language models",
                    "You only look once: Unified, real-time object detection",
                    "Qwen technical report",
                    "Exploring plain vision transformer backbones for object detection",
                    "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                    "Robocodex: Multimodal code generation for robotic behavior synthesis",
                    "Salmonn: Towards generic hearing abilities for large language models"
                ],
                "related_work": "4Related workMulti-modal large language models.Since the introduction of the Transformer arhictecture[93]and large-scale pre-training[24,78], language models have been advancing rapidly[90,91,107,79,69,5,3]. Recently, they are shown to be able to handle various types of data, such as vision[71,57,43,2]and audio[65,88], leading to a series of multi-modal language models (MLLMs) [4,11,98,108]. The visual capabilities of MLLMs are mainly enabled through transforming visual features into visual tokens, which can be roughly categorized into two types. One uses linear projection to feed image patches into LLMs[57,10,15,92,96], and the other uses learnable prompts and cross-attentions to aggregate information from the whole feature map[43,4,2,98,49]. Alternatively, Honeybee[9]proposes a convolutional model for combining the benefit of both. Most existing approaches use an identical number of visual tokens throughout pre-training and fine-tuning. Though some of the recent works have exploited raising the visual tokens during fine-tuning with increased resolution to enhance downstream performance[49,54,56,68], the large set of visual tokens for each image still presents a major bottleneck for the pre-training stage.Efficient model pre-training.As the model size consistently expands, the efficiency of training large models has become increasingly important. Beyond efforts in the system optimizations[80,81,23,22], the pre-training of large models can be accelerated by sparse computation, such as masking[45,76]or mixture of experts[27,50]. Our approach presents a novel perspective for accelerating pre-training for MLLMs by reducing visual tokens required.Multi-scale hierarchy in vision.Multi-scale hierarchy is a fundamental property in vision, which has led to the introduction and evolution of convolutional networks[29,41,39,32]as well as its application in various vision problems[60,82,52,12]. Recently, transformers are also shown to benefit from multi-scale hierarchy[97,59,26,46]. This work extends multi-scale hierarchy to language models for stronger visual capabilities and higher training efficiency.",
                "abstract": "This paper introduces Chain-of-Sight, a vision-language bridge module that accelerates the pre-training of Multimodal Large Language Models (MLLMs). Our approach employs a sequence of visual resamplers that capture visual details at various spacial scales. This architecture not only leverages global and local visual contexts effectively, but also facilitates the flexible extension of visual tokens through a compound token scaling strategy, allowing up to a 16x increase in the token count post pre-training. Consequently, Chain-of-Sight requires significantly fewer visual tokens in the pre-training phase compared to the fine-tuning phase. This intentional reduction of visual tokens during pre-training notably accelerates the pre-training process, cutting down the wall-clock training time by ~73%. Empirical results on a series of vision-language benchmarks reveal that the pre-train acceleration through Chain-of-Sight is achieved without sacrificing performance, matching or surpassing the standard pipeline of utilizing all visual tokens throughout the entire training process. Further scaling up the number of visual tokens for pre-training leads to stronger performances, competitive to existing approaches in a series of benchmarks."
            },
            {
                "name": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
                "arxiv_id": "2406.15486",
                "subtitles": [
                    "Approximate Attention",
                    "KV Cache Compression"
                ],
                "reference": [
                    "Pixelated butterfly: Simple and efficient sparse training for neural network models",
                    "Atom: Low-bit quantization for efficient and accurate llm serving",
                    "Skvq: Sliding-window key and value cache quantization for large language models",
                    "Generating long sequences with sparse transformers",
                    "H2o: Heavy-hitter oracle for efficient generative inference of large language models",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Linformer: Self-attention with linear complexity",
                    "Sparq attention: Bandwidth-efficient llm inference",
                    "Efficient streaming language models with attention sinks",
                    "Model tells you what to discard: Adaptive kv cache compression for llms",
                    "Faster causal attention over large sequences through sparse flash attention",
                    "Hyperattention: Long-context attention in near-linear time",
                    "Efficient content-based sparse attention with routing transformers",
                    "Biformer: Vision transformer with bi-level routing attention",
                    "Reformer: The efficient transformer",
                    "Longnet: Scaling transformers to 1,000,000,000 tokens",
                    "Transformers are rnns: Fast autoregressive transformers with linear attention",
                    "Longformer: The long-document transformer",
                    "Big bird: Transformers for longer sequences",
                    "Rethinking attention with performers",
                    "Scatterbrain: Unifying sparse and low-rank attention",
                    "ETC: Encoding long and structured inputs in transformers"
                ],
                "related_work": "2Related WorkApproximate Attention.Plenty of works have been proposed to approximate quadratic attention with lower complexity[18,19,20,21,22,23,24,25,26,27,28,29,30,31,42,40,25]. For example, BigBird[20]combines window-, global- and random-attention to capture long range dependency. Reformer[21]reduces computional cost via locality-sensitive hashing. LongNet[22]replaces full attention with dilated attention. Linformer[27]employs low-rank matrix to approximate attention. HyperAttention[26]utilizes locality sensitive hashing to identify important entries on attention map. However, these approaches uses either static or coarse-grained sparse pattern, and often overlook the head-specific sparsity pattern. They cannot be losslessly applied in pretrained LLMs without additional finetuning or training.KV Cache Compression.Long sequence comes with substantial KV cache memory consumption. StreamingLLM[37]keeps attention sinks and several recent tokens for infinite length generation. H2O[39]dynamically retains a balance of recent and heavy hitter tokens according to attention score during decoding. FastGen[43]adaptively construct KV cache according to observed head-specific policies. Recent efforts also quantize KV cache to lower precision to reduce memory consumption[44,45,46]. These works target on reducing the memory consumption of KV cache, while SampleAttention focuses on mitigating the long context computation overhead. SampleAttention can be combined with these approaches to further reduce memory consumption of KV cache.",
                "abstract": "Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention."
            },
            {
                "name": "Lossless Acceleration of Large Language Model via Adaptive N-gram Parallel Decoding",
                "arxiv_id": "2404.08698",
                "subtitles": [
                    "Inference systems",
                    "Compression",
                    "Speculative Execution"
                ],
                "reference": [
                    "Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding",
                    "Accelerating large language model decoding with speculative sampling",
                    "Gpt3. int8 () : 8-bit matrix multiplication for transformers at scale",
                    "Flexgen: High-throughput generative inference of large language models with a single gpu",
                    "Orca: A distributed serving system for {{\\{{Transformer-Based}}\\}} generative models",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Token dropping for efficient bert pretraining",
                    "Distilling task-specific knowledge from bert into simple neural networks",
                    "Training data-efficient image transformers & distillation through attention",
                    "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
                    "H \\_2 o: Heavy-hitter oracle for efficient generative inference of large language models",
                    "Deja vu: Contextual sparsity for efficient llms at inference time",
                    "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale",
                    "Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding",
                    "Accelerating inference for pretrained language models by unified multi-perspective early exiting",
                    "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification",
                    "Fast inference from transformers via speculative decoding",
                    "Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale",
                    "Confident adaptive language modeling",
                    "Speculative computation, parallelism, and functional programming",
                    "Random-ltd: Random and layerwise token dropping brings efficient training for large-scale transformers",
                    "Tensorrt-llm: NVIDIA tensorrt for large language models",
                    "Inference with reference: Lossless acceleration of large language models",
                    "Gptq: Accurate post-training quantization for generative pre-trained transformers",
                    "Massive language models can be accurately pruned in one-shot",
                    "Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference"
                ],
                "related_work": "2Related Work Inference systems. The development of specialized inference systems for Large Language Models (LLMs), such as NVIDIA's TensorRT-LLM (NVIDIA, 2023), Orca (Yu et al., 2022), FlexGen (Sheng et al., 2023), and DeepSpeed Inference (Aminabadi et al., 2022), represents a notable advancement in the field. Despite progress, there is still a gap in the careful co-design of algorithms and systems, which is necessary to fully harness the potential of the hardware. Compression. Efficient LLM inference is facilitated by techniques such as quantization (Han et al., 2015; Frantar et al., 2022; Dettmers et al., 2022; Xiao et al., 2023), pruning (Bansal et al., 2023; Frantar and Alistarh, 2023; Liu et al., 2023), distillation (Tang et al., 2019; Touvron et al., 2021), and exit early strategies (Schuster et al., 2022; Kong et al., 2022; Yang et al., 2023b; Bae et al., 2023; Del Corro et al., 2023) suggest that some tokens can be accurately generated using only a fraction of the model layers. Token Prunings (Hou et al., 2022; Yao et al., 2022; Zhang et al., 2023b) reduce memory and computational demand to accelerate the inference process by prioritizing crucial tokens. These methods enhance efficiency but may necessitate model alterations, re-training, and potentially reduce accuracy. Speculative Execution. Speculative execution (Burton, 1985), adapted as speculative decoding in LLMs (Chen et al., 2023; Leviathan et al., 2023), has improved inference speeds by preempting computations. SpecInfer (Miao et al., 2023) leverages existing distilled, quantized, and pruned variants of an LLM, to build a small speculative model pool to guide speculation. However, these approaches require a high-quality draft model, and increase the memory footprint. Leviathan et al. (2023) also mentioned that unigram and bigram can be used as draft models, but they did not propose a method on how to build a bigram model for the actual running LLMs. Yang et al. (2023a) presented a method of copying reference tokens to the decoder, though its utility is limited by a dependency on repeated text. These techniques increase resource use and compel specialized training, such as distillation, for the draft model to ensure compatibility with the primary model.",
                "abstract": "While Large Language Models (LLMs) have shown remarkable abilities, they are hindered by significant resource consumption and considerable latency due to autoregressive processing. In this study, we introduce Adaptive N-gram Parallel Decoding (ANPD), an innovative and lossless approach that accelerates inference by allowing the simultaneous generation of multiple tokens. ANPD incorporates a two-stage approach: it begins with a rapid drafting phase that employs an N-gram module, which adapts based on the current interactive context, followed by a verification phase, during which the original LLM assesses and confirms the proposed tokens. Consequently, ANPD preserves the integrity of the LLM's original output while enhancing processing speed. We further leverage a multi-level architecture for the N-gram module to enhance the precision of the initial draft, consequently reducing inference latency. ANPD eliminates the need for retraining or extra GPU memory, making it an efficient and plug-and-play enhancement. In our experiments, models such as LLaMA and its fine-tuned variants have shown speed improvements up to 3.67x, validating the effectiveness of our proposed ANPD."
            },
            {
                "name": "BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models",
                "arxiv_id": "2401.12522",
                "subtitles": [
                    "LLM Acceleration",
                    "Speculative Decoding",
                    "Prompt Tuning"
                ],
                "reference": [
                    "Breaking the sequential dependency of llm inference using lookahead decoding, November",
                    "Efficient memory management for large language model serving with pagedattention",
                    "Draft & verify: Lossless large language model acceleration via self-speculative decoding",
                    "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                    "Rest: Retrieval-based speculative decoding",
                    "Distilling the knowledge in a neural network",
                    "Accelerating large language model decoding with speculative sampling",
                    "A survey of quantization methods for efficient neural network inference",
                    "Accelerating llm inference with staged speculative decoding",
                    "Non-autoregressive neural machine translation",
                    "A survey on non-autoregressive generation for neural machine translation and beyond",
                    "Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation",
                    "Speed: Speculative pipelined execution for efficient decoding",
                    "Pass: Parallel speculative sampling",
                    "Gpt understands, too",
                    "Blockwise parallel decoding for deep autoregressive models",
                    "Prefix-tuning: Optimizing continuous prompts for generation",
                    "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification",
                    "Fast inference from transformers via speculative decoding",
                    "Spectr: Fast speculative decoding via optimal transport",
                    "Lightseq: A high performance inference library for transformers",
                    "Towards efficient generative large language model serving: A survey from algorithms to systems",
                    "Online speculative decoding",
                    "Rethinking the value of network pruning",
                    "The power of scale for parameter-efficient prompt tuning",
                    "Accelerating transformer inference for translation via parallel decoding",
                    "Medusa: Simple framework for accelerating llm generation with multiple decoding heads"
                ],
                "related_work": "2Related Work 2.1LLM Acceleration LLM acceleration can be approached through various dimensions, including model compression Hinton et al. (2015); Liu et al. (2018), architecture simplification Dao et al. (2022), quantization Gholami et al. (2022), memory management Kwon et al. (2023), kernel optimization Wang et al. (2021), inference scheduling Kwon et al. (2023), efficient decoding Santilli et al. (2023), and more. These techniques span from cutting-edge algorithmic modifications to groundbreaking changes in system designs, finding widespread applications in practical scenarios Miao et al. (2023a). In this paper, a specific emphasis is placed on SAR decoding as one of the typical methods for efficient decoding. SAR decoding, derived from non-autoregressive (NAR) decoding Gu et al. (2018), is initially introduced for machine translation Stern et al. (2018). It diverges from the conventional AR generation paradigm by decoding output tokens in parallel, with the goal of attaining AR output quality through post-processing strategies Xiao et al. (2023). 2.2Speculative Decoding Speculative decoding stands out as another typical efficient decoding method, involving the anticipation of token distribution of corresponding AR models in a speculative manner. An early method Stern et al. (2018) generates future predictions as drafts by auxiliary prediction heads, then validate them by a scoring model. Recent studies Leviathan et al. (2023); Chen et al. (2023) utilize external draft models for token distribution sampling from large target models. SpecDec Xia et al. (2023) explores the designing principles for efficient draft models. OSD Liu et al. (2023b) enhances draft models through online retraining. Without employing external draft models, SPEED Hooper et al. (2023) designs a faster speculative pipeline with cyclic parameter sharing. While Self-SpecDec Zhang et al. (2023) expedites drafting by selectively skipping specific intermediate layers. Medusa Cai et al. (2023) adopts multiple additional prediction heads, akin to literature Stern et al. (2018). PaSS Monea et al. (2023) obtains SAR drafts by means of  \"look-ahead\" embeddings. REST He et al. (2023) utilizes the knowledge retrieval. Lookahead Fu et al. (2023) relies solely on n-grams generated by LLMs as speculative draft candidates. Optimizing verification is another way. SpecInfer Miao et al. (2023b) uses a draft candidate token tree for parallel verification. SSD Spector and Re (2023) restructures drafts into a tree and conducts batch decoding. SpecTr Sun et al. (2023) seeks an optimal tradeoff between more draft candidates and the associated cost. Our method belongs to speculative decoding that operates without external draft models. The recent study, Medusa Cai et al. (2023), shares similarities with BiTA in generating future tokens without altering original model parameters. However, a notable distinction lies in structure: BiTA employs soft embeddings, whereas Medusa utilizes multiple heads. Another recent study closely aligned with BiTA is PaSS Monea et al. (2023), using  \"look-ahead\" embeddings (referred as  \"mask tokens\" in BiTA) for future predictions, while BiTA incorporates additional prompt tokens, which prove beneficial in experiments. Moreover, both works require calling the model a second time to validate draft candidates, while BiTA seamlessly conducts speculative generation and verification. 2.3Prompt Tuning As a widely adopted parameter-efficient tuning (PET) technique, Prompt Tuning Lester et al. (2021), along with various subsequent methods Li and Liang (2021); Liu et al. (2023a), optimizes pretrained transformers by updating a minimal set of prompt tokens, enhancing model customization for specific tasks, domains, or requirements. In this study, we leverage benefits of prompt tuning, introducing deep soft prompting from prefix tuning Li and Liang (2021) to effectively adapt AR language models for SAR decoding without modifying the original model parameters.",
                "abstract": "Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieves a 2.7$\\times$ speedup on the MT-Bench benchmark. Extensive experiments confirm our method surpasses state-of-the-art acceleration techniques."
            },
            {
                "name": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
                "arxiv_id": "2407.02490",
                "subtitles": [
                    "Sparse Attention",
                    "Scaling Context Windows of LLMs",
                    "Long-Context LLM Inference"
                ],
                "reference": [
                    "You only cache once: Decoder-decoder architectures for language models",
                    "RWKV: Reinventing RNNs for the transformer era",
                    "Gqa: Training generalized multi-query transformer models from multi-head checkpoints",
                    "Dynamic sparse attention for scalable transformer acceleration",
                    "QUEST: Query-aware sparsity for efficient long-context LLM inference",
                    "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory",
                    "Yarn: Efficient context window extension of large language models",
                    "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
                    "Generating long sequences with sparse transformers",
                    "Iceformer: Accelerated inference with long-sequence transformers on CPUs",
                    "H2o: Heavy-hitter oracle for efficient generative inference of large language models",
                    "Samba: Simple hybrid state space models for efficient unlimited context language modeling",
                    "Keyformer: Kv cache reduction through key tokens selection for efficient generative inference",
                    "Sparsebert: Rethinking the importance analysis in self-attention",
                    "Mistral 7b",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "A unified implicit attention formulation for gated-linear recurrent sequence models",
                    "Compressing context to enhance inference efficiency of large language models",
                    "Phi-3 technical report: A highly capable language model locally on your phone",
                    "Ring attention with blockwise transformers for near-infinite context",
                    "Data engineering for scaling language models to 128k context",
                    "Efficient streaming language models with attention sinks",
                    "Model tells you what to discard: Adaptive kv cache compression for llms",
                    "Block pruning for faster transformers",
                    "Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time",
                    "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
                    "Sequence can secretly tell you what to discard",
                    "Challenges in deploying long-context transformers: A theoretical peak performance analysis",
                    "Snapkv: Llm knows what you are looking for before generation",
                    "Xgen-7b technical report",
                    "Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality",
                    "Swe-bench: Can language models resolve real-world github issues",
                    "Dynamic memory compression: Retrofitting LLMs for accelerated inference",
                    "Efficiently modeling long sequences with structured state spaces",
                    "LM-infinite: Zero-shot extreme length generalization for large language models",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression",
                    "Efficient content-based sparse attention with routing transformers",
                    "Leave no context behind: Efficient infinite context transformers with infini-attention",
                    "Get more with LESS: Synthesizing recurrence with KV cache compression for efficient LLM inference",
                    "Extending context window of large language models via positional interpolation",
                    "Reformer: The efficient transformer",
                    "Retentive network: A successor to transformer for large language models",
                    "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression",
                    "Block transformer: Global-to-local language modeling for fast inference",
                    "Focused transformer: Contrastive training for context scaling",
                    "Longnet: Scaling transformers to 1,000,000,000 tokens",
                    "Transformers are rnns: Fast autoregressive transformers with linear attention",
                    "Unlimiformer: Long-range transformers with unlimited length input",
                    "Longformer: The long-document transformer",
                    "Sparq attention: Bandwidth-efficient LLM inference",
                    "Train short, test long: Attention with linear biases enables input length extrapolation",
                    "Big bird: Transformers for longer sequences",
                    "Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding",
                    "Transformers are multi-state rnns",
                    "Llmlingua: Compressing prompts for accelerated inference of large language models",
                    "Jamba: A hybrid transformer-mamba language model",
                    "LongroPE: Extending LLM context window beyond 2 million tokens",
                    "Fast transformer decoding: One write-head is all you need"
                ],
                "related_work": "5Related WorksSparse AttentionDue to the quadratic complexity of the attention mechanism, many previous works have focused on sparse attention to improve the efficiency of Transformers. These methods include static sparse patterns, cluster-based sparse approaches, and dynamic sparse attention. Static sparse patterns include techniques such as sliding windows[30,2], dilated attention[8,72,16], and mixed sparse patterns[6,87,38]. Cluster-based sparse methods include hash-based[36]and kNN-based[68,54]methods. All of the above methods require pre-training the model from scratch, which makes them infeasible to be directly used as a plugin for reay-to-use LLMs. Recently, there has been work[14,85]to unify state space models[23,22,14], and linear attention[37,70]into structured masked attention. Additionally, some works[81,47,63]leverage the dynamic nature of attention to predict sparse patterns dynamically. However, these approaches often focus on low-rank hidden states during the dynamic pattern approximation or use post-statistical methods to obtain the sparse mask, which introduce substantial overhead in the estimation step, making them less useful for long-context LLMs.Scaling Context Windows of LLMsRecent research has focused on expanding the context window of pre-trained LLMs, that enables LLMs to handle more complex real-life applications[34,58]. These methods can be categorized into: 1) Staged pre-training[55,20]; 2) Modifying or interpolating position embeddings[61,10,60,19]; 3) Utilizing external memory modules for context storage[4,77,83]; 4) Expanding computations across multiple devices in a distributed manner[50]. However, these methods do not alleviate the high inference costs in long-context processing.Long-Context LLM InferenceRecent studies[21]have tackled the high computational cost of attention and substantial KV cache storage in long-context scenarios from two angles: pre-filling and decoding. Pre-filling optimizations are primarily categorized as State Space Models[23,22], linear attention methods[70,57], memory-based methods[53], hybrid methods[44,27,64], and prompt compression methods[41,32,33,62]. However, these approaches require training from scratch or additional overhead and are difficult to implement directly in pretrained long-context LLMs. Recently, some studies[52,83]have focused on using kNN or cluster-based sparse attention to accelerate LLM inference. However, these methods often lead to reduced accuracy, limited speedup, or are restricted to CPU scenarios.In contrast, optimizations for the decoding stage are divided into: 1) Reusing attention KV to reduce KV cache storage[73,3,71,12]; 2) Static KV cache compression patterns[82,29]; 3) Dynamic KV cache compression patterns, including completely discarding the KV cache after compression[89,42,25,56], and offloading-based methods[63,43,15]; 4) Cluster-based KV cache compression methods[54,78]; 5) Methods for restoring performance loss due to KV cache compression[1,18]; 6) Hierarchical speculative decoding methods[69]. Nevertheless, these methods do not address the heavy computational burden of the attention in the pre-filling stage.",
                "abstract": "The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available atthis https URL."
            }
        ],
        "survey": {
            "name": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
            "arxiv_id": "2409.03384",
            "subtitles": [
                {
                    "name": "FPGA-based accelerators",
                    "key_history": [
                        {
                            "reference_title": "Ftrans: Energy-efficient acceleration of transformers using fpga",
                            "key_word": "Acceleration of transformer-based large scale language representations"
                        },
                        {
                            "reference_title": "Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer",
                            "key_word": "Multi-Head Attention"
                        },
                        {
                            "reference_title": "Npe: An fpga-based overlay processor for natural language processing",
                            "key_word": "FPGA NPE"
                        },
                        {
                            "reference_title": "Accelerating transformer-based deep learning models on fpgas using column balanced block pruning",
                            "key_word": "Column Balanced Block Pruning"
                        },
                        {
                            "reference_title": "Accelerating transformer-based deep learning models on fpgas using column balanced block pruning",
                            "key_word": "Compressed Block Row"
                        },
                        {
                            "reference_title": "Via: A novel vision-transformer accelerator based on fpga",
                            "key_word": "Vision Transformers"
                        },
                        {
                            "reference_title": "Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation",
                            "key_word": "transformer networks used in LLMs"
                        },
                        {
                            "reference_title": "An efficient hardware accelerator for sparse transformer neural networks",
                            "key_word": "High computational demands of transformer models"
                        },
                        {
                            "reference_title": "Transformer-opu: An fpga-based overlay processor for transformer networks",
                            "key_word": "FPGA OPU"
                        },
                        {
                            "reference_title": "Hardware acceleration of transformer networks using fpgas",
                            "key_word": "FPGA acceleration of Transformer networks"
                        },
                        {
                            "reference_title": "A fast and flexible fpga-based accelerator for natural language processing neural networks",
                            "key_word": "FlexRun"
                        },
                        {
                            "reference_title": "Hpta: A high performance transformer accelerator based on fpga",
                            "key_word": "High-Performance Transformer Accelerator (HPTA) "
                        },
                        {
                            "reference_title": "An efficient fpga-based accelerator for swin transformer",
                            "key_word": "Swin Transformer"
                        },
                        {
                            "reference_title": "An fpga-based transformer accelerator using output block stationary dataflow for object recognition applications",
                            "key_word": "Output Block Storing (OBS) "
                        },
                        {
                            "reference_title": "A cost-efficient fpga implementation of tiny transformer model using neural ode",
                            "key_word": "ODE-based acceleration"
                        },
                        {
                            "reference_title": "Beta: Binarized energy-efficient transformer accelerator at the edge",
                            "key_word": "Beta, Computational flow subtraction"
                        },
                        {
                            "reference_title": "Me-vit: A single-load memory-efficient fpga accelerator for vision transformers",
                            "key_word": "Me-Vit"
                        },
                        {
                            "reference_title": "Transaxx: Efficient transformers with approximate computing",
                            "key_word": "TransAxx, Approximation computing"
                        },
                        {
                            "reference_title": "A cost-efficient fpga implementation of tiny transformer model using neural ode",
                            "key_word": "Neural Ordinary Differential Equation (Neural ODE) "
                        },
                        {
                            "reference_title": "Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration",
                            "key_word": "SSR, Balance between latency and performance"
                        }
                    ],
                    "references_in_this_section": [
                        "Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer",
                        "Accelerating transformer-based deep learning models on fpgas using column balanced block pruning",
                        "Transaxx: Efficient transformers with approximate computing",
                        "Me-vit: A single-load memory-efficient fpga accelerator for vision transformers",
                        "Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation",
                        "Ftrans: Energy-efficient acceleration of transformers using fpga",
                        "Hpta: A high performance transformer accelerator based on fpga",
                        "Transformer-opu: An fpga-based overlay processor for transformer networks",
                        "A cost-efficient fpga implementation of tiny transformer model using neural ode",
                        "Npe: An fpga-based overlay processor for natural language processing",
                        "An fpga-based transformer accelerator using output block stationary dataflow for object recognition applications",
                        "An efficient fpga-based accelerator for swin transformer",
                        "Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration",
                        "A fast and flexible fpga-based accelerator for natural language processing neural networks",
                        "Beta: Binarized energy-efficient transformer accelerator at the edge",
                        "Hardware acceleration of transformer networks using fpgas",
                        "Via: A novel vision-transformer accelerator based on fpga",
                        "An efficient hardware accelerator for sparse transformer neural networks"
                    ]
                },
                {
                    "name": "CPU and GPU-based Accelerators",
                    "key_history": [
                        {
                            "reference_title": "Turbotransformers: an efficient gpu serving system for transformer models",
                            "key_word": "TurboTransformer"
                        },
                        {
                            "reference_title": "Accelerating transformer networks through recomposing softmax layers,",
                            "key_word": "transformer networks"
                        },
                        {
                            "reference_title": "Accelerating transformer networks through recomposing softmax layers",
                            "key_word": "SoftMax"
                        },
                        {
                            "reference_title": "Lightseq2: Accelerated training for transformer-based models on gpus",
                            "key_word": "LightSeq2"
                        },
                        {
                            "reference_title": "Simplifying transformer blocks",
                            "key_word": "Simplified Transformer Networks"
                        },
                        {
                            "reference_title": "Inference with reference: Lossless acceleration of large language models",
                            "key_word": "LLMA"
                        },
                        {
                            "reference_title": "Efficient memory management for large language model serving with pagedattention",
                            "key_word": "vLLMs, PagedAttention"
                        },
                        {
                            "reference_title": "Alisa: Accelerating large language model inference via sparsity-aware kv caching",
                            "key_word": "Alisa, Dynamic scheduling"
                        }
                    ],
                    "references_in_this_section": [
                        "Efficient memory management for large language model serving with pagedattention",
                        "Lightseq2: Accelerated training for transformer-based models on gpus",
                        "Inference with reference: Lossless acceleration of large language models",
                        "Simplifying transformer blocks",
                        "Accelerating transformer networks through recomposing softmax layers",
                        "Alisa: Accelerating large language model inference via sparsity-aware kv caching",
                        "Turbotransformers: an efficient gpu serving system for transformer models"
                    ]
                },
                {
                    "name": "ASIC Accelerators",
                    "key_history": [
                        {
                            "reference_title": "A3: Accelerating attention mechanisms in neural networks with approximation",
                            "key_word": "Hardware acceleration"
                        },
                        {
                            "reference_title": "Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",
                            "key_word": "Hardware-software co-design"
                        },
                        {
                            "reference_title": "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
                            "key_word": "Algorithmic optimizations"
                        },
                        {
                            "reference_title": "Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture",
                            "key_word": "Sparse attention"
                        },
                        {
                            "reference_title": "Salo: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
                            "key_word": "Spatial accelerator"
                        },
                        {
                            "reference_title": "Acceltran: A sparsity-aware accelerator for dynamic inference with transformers",
                            "key_word": "Matrix compression"
                        },
                        {
                            "reference_title": "Dtqatten: Leveraging dynamic token-based quantization for efficient attention architecture",
                            "key_word": "Dynamic quantization"
                        },
                        {
                            "reference_title": "Energon: Toward efficient acceleration of transformers using dynamic sparse attention",
                            "key_word": "Dynamic sparse attention"
                        },
                        {
                            "reference_title": "H3d-transformer: A heterogeneous 3d (h3d) computing platform for transformer model acceleration on edge devices",
                            "key_word": "Compute-in-memory"
                        },
                        {
                            "reference_title": "Hardware-software co-design enabling static and dynamic sparse attention mechanisms",
                            "key_word": "Static and dynamic sparsity"
                        }
                    ],
                    "references_in_this_section": [
                        "Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",
                        "A3: Accelerating attention mechanisms in neural networks with approximation",
                        "Salo: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
                        "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
                        "Energon: Toward efficient acceleration of transformers using dynamic sparse attention",
                        "Dtqatten: Leveraging dynamic token-based quantization for efficient attention architecture",
                        "Hardware-software co-design enabling static and dynamic sparse attention mechanisms",
                        "Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture",
                        "Acceltran: A sparsity-aware accelerator for dynamic inference with transformers",
                        "H3d-transformer: A heterogeneous 3d (h3d) computing platform for transformer model acceleration on edge devices"
                    ]
                },
                {
                    "name": "In-Memory Hardware Accelerators",
                    "key_history": [
                        {
                            "reference_title": "Att: A fault-tolerant reram accelerator for attention-based neural networks",
                            "key_word": "In-memory acceleration"
                        },
                        {
                            "reference_title": "Retransformer: Reram-based processing-in-memory architecture for transformer acceleration",
                            "key_word": "In-memory architecture"
                        },
                        {
                            "reference_title": "In-memory computing based accelerator for transformer networks for long sequences",
                            "key_word": "In-memory computing"
                        },
                        {
                            "reference_title": "Transpim: A memory-based acceleration via software-hardware co-design for transformer",
                            "key_word": "Software-hardware co-design"
                        },
                        {
                            "reference_title": "X-former: In-memory acceleration of transformers",
                            "key_word": "Hardware-software co-design"
                        },
                        {
                            "reference_title": "X-former: In-memory acceleration of transformer",
                            "key_word": "Hybrid in-memory accelerator"
                        },
                        {
                            "reference_title": "Trancim: Full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes",
                            "key_word": "Digital CIM accelerator"
                        },
                        {
                            "reference_title": "H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention",
                            "key_word": "Analog-digital in-memory computing"
                        },
                        {
                            "reference_title": "Primate: Processing in memory acceleration for dynamic token-pruning transformers",
                            "key_word": "Dynamic token pruning"
                        },
                        {
                            "reference_title": "Hardsea: Hybrid analog-reram clustering and digital-sram in-memory computing accelerator for dynamic sparse self-attention in transformer",
                            "key_word": "Hybrid analog-digital"
                        }
                    ],
                    "references_in_this_section": [
                        "Trancim: Full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes",
                        "H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention",
                        "Retransformer: Reram-based processing-in-memory architecture for transformer acceleration",
                        "Hardsea: Hybrid analog-reram clustering and digital-sram in-memory computing accelerator for dynamic sparse self-attention in transformer",
                        "Transpim: A memory-based acceleration via software-hardware co-design for transformer",
                        "Att: A fault-tolerant reram accelerator for attention-based neural networks",
                        "Primate: Processing in memory acceleration for dynamic token-pruning transformers",
                        "X-former: In-memory acceleration of transformers",
                        "In-memory computing based accelerator for transformer networks for long sequences"
                    ]
                },
                {
                    "name": "Quantitative comparison",
                    "key_history": [
                        {
                            "reference_title": "Ftrans: Energy-efficient acceleration of transformers using fpga",
                            "key_word": "FTRANS"
                        },
                        {
                            "reference_title": "Acceltran: A sparsity-aware accelerator for dynamic inference with transformers",
                            "key_word": "AccelTran (server) "
                        },
                        {
                            "reference_title": "An efficient fpga-based accelerator for swin transformer",
                            "key_word": "Swin-T"
                        },
                        {
                            "reference_title": "Retransformer: Reram-based processing-in-memory architecture for transformer acceleration",
                            "key_word": "ReTransformer"
                        },
                        {
                            "reference_title": "Trancim: Full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes",
                            "key_word": "TranCIM"
                        }
                    ],
                    "references_in_this_section": [
                        "Retransformer: Reram-based processing-in-memory architecture for transformer acceleration",
                        "Ftrans: Energy-efficient acceleration of transformers using fpga",
                        "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
                        "H3d-transformer: A heterogeneous 3d (h3d) computing platform for transformer model acceleration on edge devices",
                        "X-former: In-memory acceleration of transformers",
                        "H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention",
                        "Me-vit: A single-load memory-efficient fpga accelerator for vision transformers",
                        "Matrix multiplication in vhdl",
                        "Acceltran: A sparsity-aware accelerator for dynamic inference with transformers",
                        "Beta: Binarized energy-efficient transformer accelerator at the edge",
                        "Transpim: A memory-based acceleration via software-hardware co-design for transformer",
                        "Trancim: Full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes",
                        "Scaling equations for the accurate prediction of CMOS device performance from 180 nm to 7 nm",
                        "Hardsea: Hybrid analog-reram clustering and digital-sram in-memory computing accelerator for dynamic sparse self-attention in transformer",
                        "A3: Accelerating attention mechanisms in neural networks with approximation",
                        "An efficient fpga-based accelerator for swin transformer",
                        "Via: A novel vision-transformer accelerator based on fpga",
                        "Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture",
                        "An efficient hardware accelerator for sparse transformer neural networks"
                    ]
                }
            ],
            "all_references": [
                "H3d-transformer: A heterogeneous 3d (h3d) computing platform for transformer model acceleration on edge devices",
                "In-memory computing based accelerator for transformer networks for long sequences",
                "Energon: Toward efficient acceleration of transformers using dynamic sparse attention",
                "A comprehensive performance study of large language models on novel ai accelerators",
                "Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer",
                "Primate: Processing in memory acceleration for dynamic token-pruning transformers",
                "Accelerating transformer networks through recomposing softmax layers",
                "Me-vit: A single-load memory-efficient fpga accelerator for vision transformers",
                "An efficient fpga-based accelerator for swin transformer",
                "Dtqatten: Leveraging dynamic token-based quantization for efficient attention architecture",
                "Acceltran: A sparsity-aware accelerator for dynamic inference with transformers",
                "Lightseq2: Accelerated training for transformer-based models on gpus",
                "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
                "Hpta: A high performance transformer accelerator based on fpga",
                "Accelerating transformer-based deep learning models on fpgas using column balanced block pruning",
                "Hardware acceleration of transformer networks using fpgas",
                "An efficient hardware accelerator for sparse transformer neural networks",
                "Turbotransformers: an efficient gpu serving system for transformer models",
                "Scaling equations for the accurate prediction of CMOS device performance from 180 nm to 7 nm",
                "Transformer-opu: An fpga-based overlay processor for transformer networks",
                "An fpga-based transformer accelerator using output block stationary dataflow for object recognition applications",
                "Retransformer: Reram-based processing-in-memory architecture for transformer acceleration",
                "Salo: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
                "Transpim: A memory-based acceleration via software-hardware co-design for transformer",
                "Via: A novel vision-transformer accelerator based on fpga",
                "Simplifying transformer blocks",
                "Beta: Binarized energy-efficient transformer accelerator at the edge",
                "Transaxx: Efficient transformers with approximate computing",
                "Hardware-software co-design enabling static and dynamic sparse attention mechanisms",
                "Trancim: Full-digital bitline-transpose cim-based sparse transformer accelerator with pipeline/parallel reconfigurable modes",
                "Hardsea: Hybrid analog-reram clustering and digital-sram in-memory computing accelerator for dynamic sparse self-attention in transformer",
                "A cost-efficient fpga implementation of tiny transformer model using neural ode",
                "Inference with reference: Lossless acceleration of large language models",
                "Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",
                "Att: A fault-tolerant reram accelerator for attention-based neural networks",
                "Alisa: Accelerating large language model inference via sparsity-aware kv caching",
                "Npe: An fpga-based overlay processor for natural language processing",
                "A fast and flexible fpga-based accelerator for natural language processing neural networks",
                "H3datten: Heterogeneous 3-d integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self-attention",
                "Matrix multiplication in vhdl",
                "Ftrans: Energy-efficient acceleration of transformers using fpga",
                "A3: Accelerating attention mechanisms in neural networks with approximation",
                "Hardware-friendly compression and hardware acceleration for transformer: A survey",
                "X-former: In-memory acceleration of transformers",
                "Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation",
                "Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture",
                "Efficient memory management for large language model serving with pagedattention",
                "Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration",
                "Transformer-based models and hardware acceleration analysis in autonomous driving: A survey"
            ]
        },
        "topic_history": [
            {
                "name": "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge",
                "arxiv_id": "2402.10787",
                "reference": [
                    "Specializing smaller language models towards multi-step reasoning",
                    "Llm-qat: Data-free quantization aware training for large language models",
                    "Sparsegpt: Massive language models can be accurately pruned in one-shot",
                    "Bloom: A 176b-parameter open-access multilingual language model",
                    "Awq: Activation-aware weight quantization for llm compression and acceleration",
                    "Llama: Open and efficient foundation language models",
                    "Gpt-4 technical report",
                    "Opt: Open pre-trained transformer language models",
                    "Gptq: Accurate post-training quantization for generative pre-trained transformers",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers",
                    "Lora: Low-rank adaptation of large language models"
                ]
            },
            {
                "name": "QuickLLaMA: Query-aware Inference Acceleration for Large Language Models",
                "arxiv_id": "2406.07528",
                "reference": [
                    "Efficient memory management for large language model serving with pagedattention",
                    "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory",
                    "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                    "Yarn: Efficient context window extension of large language models",
                    "CLEX: continuous length extrapolation for large language models",
                    "Flashdecoding++: Faster large language model inference on gpus",
                    "Generating long sequences with sparse transformers",
                    "LLM maybe longlm: Self-extend LLM context window without tuning",
                    "Memorizing transformers",
                    "Memory networks",
                    "Linformer: Self-attention with linear complexity",
                    "Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache",
                    "End-to-end memory networks",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "Transformer-xl: Attentive language models beyond a fixed-length context",
                    "Key-value memory networks for directly reading documents",
                    "Efficient streaming language models with attention sinks",
                    "Snapkv: Llm knows what you are looking for before generation",
                    "Efficiently modeling long sequences with structured state spaces",
                    "Roformer: Enhanced transformer with rotary position embedding",
                    "Compressive transformers for long-range sequence modelling",
                    "Flashattention-2: Faster attention with better parallelism and work partitioning",
                    "Reformer: The efficient transformer",
                    "Generalization through memorization: Nearest neighbor language models",
                    "Focused transformer: Contrastive training for context scaling",
                    "Neural turing machines",
                    "Clex: Continuous length extrapolation for large language models",
                    "Explicit sparse transformer: Concentrated attention through explicit selection",
                    "Transformers are rnns: Fast autoregressive transformers with linear attention",
                    "Etc: Encoding long and structured inputs in transformers",
                    "Unlimiformer: Long-range transformers with unlimited length input",
                    "Longformer: The long-document transformer",
                    "Train short, test long: Attention with linear biases enables input length extrapolation",
                    "Big bird: Transformers for longer sequences",
                    "Fast transformer decoding: One write-head is all you need"
                ]
            },
            {
                "name": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models",
                "arxiv_id": "2408.08554",
                "reference": [
                    "Spqr: A sparse-quantized representation for near-lossless llm weight compression",
                    "Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Billm: Pushing the limit of post-training quantization for llms",
                    "I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models",
                    "Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks",
                    "Qa-lora: Quantization-aware low-rank adaptation of large language models",
                    "Omniquant: Omnidirectionally calibrated quantization for large language models",
                    "Extreme compression of large language models via additive quantization",
                    "Pb-llm: Partially binarized large language models",
                    "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration",
                    "Low-Rank Quantization-Aware Training for LLMs",
                    "LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale",
                    "Affinequant: Affine transformation quantization for large language models",
                    "Quip: 2-bit quantization of large language models with guarantees",
                    "PeQA: A Massive Persian Question-Answering and Chatbot Dataset",
                    "Qlora: Efficient finetuning of quantized llms",
                    "Gptq: Accurate post-training quantization for generative pre-trained transformers",
                    "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
                ]
            },
            {
                "name": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
                "arxiv_id": "2401.06209",
                "reference": [
                    "When and why vision-language models behave like bags-of-words, and what to do about it",
                    "Masked autoencoders are scalable vision learners",
                    "Palm 2 technical report",
                    "Imagenet large scale visual recognition challenge",
                    "iBOT: Image BERT pre-training with online tokenizer",
                    "Slip: Self-supervision meets language-image pre-training",
                    "GQA: A new dataset for real-world visual reasoning and compositional question answering",
                    "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
                    "Mmbench: Is your multi-modal model an all-around player",
                    "Obelisc: An open web-scale filtered dataset of interleaved image-text documents",
                    "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
                    "Mass-producing failures of multimodal systems with language models",
                    "Winoground: Probing vision and language models for visio-linguistic compositionality",
                    "Aligning large multi-modal model with robust instruction tuning",
                    "Gpt-4 technical report",
                    "Image captioners are scalable vision learners too",
                    "Sigmoid loss for language image pre-training",
                    "Bootstrap your own latent-a new approach to self-supervised learning",
                    "Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy",
                    "Towards VQA models that can read",
                    "Improved baselines with visual instruction tuning",
                    "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "LLaMA: Open and efficient foundation language models",
                    "Instructblip: Towards general-purpose vision-language models with instruction tuning",
                    "A simple framework for contrastive learning of visual representations",
                    "Evaluating object hallucination in large vision-language models",
                    "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
                    "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
                    "Imagenet-21k pretraining for the masses",
                    "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                    "Investigating the catastrophic forgetting in multimodal large language models",
                    "Demystifying CLIP data",
                    "GPT-4V(ision) System Card",
                    "Bard",
                    "Visual instruction tuning",
                    "Gemini",
                    "Learning transferable visual models from natural language supervision",
                    "Self-supervised learning from images with a joint-embedding predictive architecture",
                    "LLaMA 2: Open foundation and fine-tuned chat models",
                    "Mme: A comprehensive evaluation benchmark for multimodal large language models",
                    "On measuring social biases in sentence encoders",
                    "MM-Vet: Evaluating large multimodal models for integrated capabilities",
                    "EVA-CLIP: Improved training techniques for clip at scale",
                    "Mitigating gender bias in natural language processing: Literature review",
                    "Flamingo: a visual language model for few-shot learning",
                    "Momentum contrast for unsupervised visual representation learning",
                    "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for GPT-4V (ision) , LLaVA-1.5, and other multi-modality models"
                ]
            },
            {
                "name": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
                "arxiv_id": "2407.20042",
                "reference": [
                    "Code completion by modeling flattened abstract syntax trees as graphs",
                    "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
                    "Exploring continual learning for code generation models",
                    "Efficient Memory Management for Large Language Model Serving with PagedAttention",
                    "Magicoder: Source code is all you need",
                    "Accelerating Code Search with Deep Hashing and Code Classification",
                    "Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems",
                    "Towards more realistic evaluation of LLM-based code generation: an experimental study and beyond",
                    "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
                    "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                    "Lever: Learning to verify language-to-code generation with execution",
                    "Structured Chain-of-Thought Prompting for Code Generation",
                    "Refining ChatGPT-generated code: Characterizing and mitigating code quality issues",
                    "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
                    "Latent predictor networks for code generation",
                    "LLM-Assisted Code Cleaning For Training Accurate Code Generators",
                    "Fast inference from transformers via speculative decoding",
                    "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                    "Planning with large language models for code generation",
                    "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference",
                    "Deep learning for source code modeling and generation: Models, applications, and challenges",
                    "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards Efficient Code Generation",
                    "Improving Code Generation by Dynamic Temperature Sampling",
                    "Phind-CodeLlama-34B-v",
                    "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding",
                    "Retrieval-based neural code generation",
                    "AceCoder: An Effective Prompting Technique Specialized in Code Generation",
                    "Large Language Model-Aware In-Context Learning for Code Generation"
                ]
            },
            {
                "name": "Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight",
                "arxiv_id": "2407.15819",
                "reference": [
                    "Feature pyramid networks for object detection",
                    "Flamingo: a visual language model for few-shot learning",
                    "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                    "Backpropagation applied to handwritten zip code recognition",
                    "Mar: Masked autoencoders for efficient action recognition",
                    "Opt: Open pre-trained transformer language models",
                    "Scaling language-image pre-training via masking",
                    "Fully convolutional networks for semantic segmentation",
                    "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
                    "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
                    "Zero: Memory optimizations toward training trillion parameter models",
                    "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration",
                    "Monkey: Image resolution and text label are important things for large multi-modal models",
                    "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models",
                    "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
                    "Mm1: Methods, analysis & insights from multimodal llm pre-training",
                    "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
                    "Qwen-vl: A frontier large vision-language model with versatile abilities",
                    "Introducing meta llama 3: The most capable openly available llm to date",
                    "Swin transformer: Hierarchical vision transformer using shifted windows",
                    "Deepseek llm: Scaling open-source language models with longtermism",
                    "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
                    "Shikra: Unleashing multimodal llm's referential dialogue magic",
                    "Cogvlm: Visual expert for pretrained language models",
                    "Deep residual learning for image recognition",
                    "Multiscale vision transformers",
                    "Improving language understanding by generative pre-training",
                    "Exploring the limits of transfer learning with a unified text-to-text transformer",
                    "Visual instruction tuning",
                    "Imagenet classification with deep convolutional neural networks",
                    "Pali: A jointly-scaled multilingual language-image model",
                    "Multimodal few-shot learning with frozen language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
                    "Flashattention-2: Faster attention with better parallelism and work partitioning",
                    "Moe-llava: Mixture of experts for large vision-language models",
                    "Llava-next: Improved reasoning, ocr, and world knowledge, January",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
                    "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
                    "Honeybee: Locality-enhanced projector for multimodal llm",
                    "Attention is all you need",
                    "Llama: Open and efficient foundation language models",
                    "You only look once: Unified, real-time object detection",
                    "Qwen technical report",
                    "Exploring plain vision transformer backbones for object detection",
                    "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
                    "Robocodex: Multimodal code generation for robotic behavior synthesis",
                    "Salmonn: Towards generic hearing abilities for large language models"
                ]
            },
            {
                "name": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
                "arxiv_id": "2406.15486",
                "reference": [
                    "Pixelated butterfly: Simple and efficient sparse training for neural network models",
                    "Atom: Low-bit quantization for efficient and accurate llm serving",
                    "Skvq: Sliding-window key and value cache quantization for large language models",
                    "Generating long sequences with sparse transformers",
                    "H2o: Heavy-hitter oracle for efficient generative inference of large language models",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Linformer: Self-attention with linear complexity",
                    "Sparq attention: Bandwidth-efficient llm inference",
                    "Efficient streaming language models with attention sinks",
                    "Model tells you what to discard: Adaptive kv cache compression for llms",
                    "Faster causal attention over large sequences through sparse flash attention",
                    "Hyperattention: Long-context attention in near-linear time",
                    "Efficient content-based sparse attention with routing transformers",
                    "Biformer: Vision transformer with bi-level routing attention",
                    "Reformer: The efficient transformer",
                    "Longnet: Scaling transformers to 1,000,000,000 tokens",
                    "Transformers are rnns: Fast autoregressive transformers with linear attention",
                    "Longformer: The long-document transformer",
                    "Big bird: Transformers for longer sequences",
                    "Rethinking attention with performers",
                    "Scatterbrain: Unifying sparse and low-rank attention",
                    "ETC: Encoding long and structured inputs in transformers"
                ]
            },
            {
                "name": "Lossless Acceleration of Large Language Model via Adaptive N-gram Parallel Decoding",
                "arxiv_id": "2404.08698",
                "reference": [
                    "Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding",
                    "Accelerating large language model decoding with speculative sampling",
                    "Gpt3. int8 () : 8-bit matrix multiplication for transformers at scale",
                    "Flexgen: High-throughput generative inference of large language models with a single gpu",
                    "Orca: A distributed serving system for {{\\{{Transformer-Based}}\\}} generative models",
                    "Smoothquant: Accurate and efficient post-training quantization for large language models",
                    "Token dropping for efficient bert pretraining",
                    "Distilling task-specific knowledge from bert into simple neural networks",
                    "Training data-efficient image transformers & distillation through attention",
                    "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
                    "H \\_2 o: Heavy-hitter oracle for efficient generative inference of large language models",
                    "Deja vu: Contextual sparsity for efficient llms at inference time",
                    "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale",
                    "Predictive pipelined decoding: A compute-latency trade-off for exact llm decoding",
                    "Accelerating inference for pretrained language models by unified multi-perspective early exiting",
                    "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification",
                    "Fast inference from transformers via speculative decoding",
                    "Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale",
                    "Confident adaptive language modeling",
                    "Speculative computation, parallelism, and functional programming",
                    "Random-ltd: Random and layerwise token dropping brings efficient training for large-scale transformers",
                    "Tensorrt-llm: NVIDIA tensorrt for large language models",
                    "Inference with reference: Lossless acceleration of large language models",
                    "Gptq: Accurate post-training quantization for generative pre-trained transformers",
                    "Massive language models can be accurately pruned in one-shot",
                    "Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference"
                ]
            },
            {
                "name": "BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models",
                "arxiv_id": "2401.12522",
                "reference": [
                    "Breaking the sequential dependency of llm inference using lookahead decoding, November",
                    "Efficient memory management for large language model serving with pagedattention",
                    "Draft & verify: Lossless large language model acceleration via self-speculative decoding",
                    "Flashattention: Fast and memory-efficient exact attention with io-awareness",
                    "Rest: Retrieval-based speculative decoding",
                    "Distilling the knowledge in a neural network",
                    "Accelerating large language model decoding with speculative sampling",
                    "A survey of quantization methods for efficient neural network inference",
                    "Accelerating llm inference with staged speculative decoding",
                    "Non-autoregressive neural machine translation",
                    "A survey on non-autoregressive generation for neural machine translation and beyond",
                    "Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation",
                    "Speed: Speculative pipelined execution for efficient decoding",
                    "Pass: Parallel speculative sampling",
                    "Gpt understands, too",
                    "Blockwise parallel decoding for deep autoregressive models",
                    "Prefix-tuning: Optimizing continuous prompts for generation",
                    "Specinfer: Accelerating generative llm serving with speculative inference and token tree verification",
                    "Fast inference from transformers via speculative decoding",
                    "Spectr: Fast speculative decoding via optimal transport",
                    "Lightseq: A high performance inference library for transformers",
                    "Towards efficient generative large language model serving: A survey from algorithms to systems",
                    "Online speculative decoding",
                    "Rethinking the value of network pruning",
                    "The power of scale for parameter-efficient prompt tuning",
                    "Accelerating transformer inference for translation via parallel decoding",
                    "Medusa: Simple framework for accelerating llm generation with multiple decoding heads"
                ]
            },
            {
                "name": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
                "arxiv_id": "2407.02490",
                "reference": [
                    "You only cache once: Decoder-decoder architectures for language models",
                    "RWKV: Reinventing RNNs for the transformer era",
                    "Gqa: Training generalized multi-query transformer models from multi-head checkpoints",
                    "Dynamic sparse attention for scalable transformer acceleration",
                    "QUEST: Query-aware sparsity for efficient long-context LLM inference",
                    "Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory",
                    "Yarn: Efficient context window extension of large language models",
                    "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
                    "Generating long sequences with sparse transformers",
                    "Iceformer: Accelerated inference with long-sequence transformers on CPUs",
                    "H2o: Heavy-hitter oracle for efficient generative inference of large language models",
                    "Samba: Simple hybrid state space models for efficient unlimited context language modeling",
                    "Keyformer: Kv cache reduction through key tokens selection for efficient generative inference",
                    "Sparsebert: Rethinking the importance analysis in self-attention",
                    "Mistral 7b",
                    "Mamba: Linear-time sequence modeling with selective state spaces",
                    "A unified implicit attention formulation for gated-linear recurrent sequence models",
                    "Compressing context to enhance inference efficiency of large language models",
                    "Phi-3 technical report: A highly capable language model locally on your phone",
                    "Ring attention with blockwise transformers for near-infinite context",
                    "Data engineering for scaling language models to 128k context",
                    "Efficient streaming language models with attention sinks",
                    "Model tells you what to discard: Adaptive kv cache compression for llms",
                    "Block pruning for faster transformers",
                    "Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time",
                    "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",
                    "Sequence can secretly tell you what to discard",
                    "Challenges in deploying long-context transformers: A theoretical peak performance analysis",
                    "Snapkv: Llm knows what you are looking for before generation",
                    "Xgen-7b technical report",
                    "Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality",
                    "Swe-bench: Can language models resolve real-world github issues",
                    "Dynamic memory compression: Retrofitting LLMs for accelerated inference",
                    "Efficiently modeling long sequences with structured state spaces",
                    "LM-infinite: Zero-shot extreme length generalization for large language models",
                    "Generative agents: Interactive simulacra of human behavior",
                    "Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression",
                    "Efficient content-based sparse attention with routing transformers",
                    "Leave no context behind: Efficient infinite context transformers with infini-attention",
                    "Get more with LESS: Synthesizing recurrence with KV cache compression for efficient LLM inference",
                    "Extending context window of large language models via positional interpolation",
                    "Reformer: The efficient transformer",
                    "Retentive network: A successor to transformer for large language models",
                    "Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression",
                    "Block transformer: Global-to-local language modeling for fast inference",
                    "Focused transformer: Contrastive training for context scaling",
                    "Longnet: Scaling transformers to 1,000,000,000 tokens",
                    "Transformers are rnns: Fast autoregressive transformers with linear attention",
                    "Unlimiformer: Long-range transformers with unlimited length input",
                    "Longformer: The long-document transformer",
                    "Sparq attention: Bandwidth-efficient LLM inference",
                    "Train short, test long: Attention with linear biases enables input length extrapolation",
                    "Big bird: Transformers for longer sequences",
                    "Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding",
                    "Transformers are multi-state rnns",
                    "Llmlingua: Compressing prompts for accelerated inference of large language models",
                    "Jamba: A hybrid transformer-mamba language model",
                    "LongroPE: Extending LLM context window beyond 2 million tokens",
                    "Fast transformer decoding: One write-head is all you need"
                ]
            }
        ]
    },
    {
        "source_paper": {
            "name": "Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks",
            "arxiv_id": "2310.10508",
            "isAPA": true,
            "abstract": "The advancements in Large Language Models (LLMs) have opened up new opportunities for AutomatedSoftware Engineering (ASE) . Two orthogonal approaches have been widely used to customize LLMs for ASEtasks, i.e., prompt engineering and fine-tuning. Prompt engineering-based approaches leverage differentprompting strategies to query LLMs (e.g., ChatGPT) to automate a task such as code generation, while finetuning-based approaches further train the pre-trained models (e.g., CodeBERT) on customized data related tothe down-stream task (in this example, code generation datasets) . However, to date, there is no comprehensiveand in-depth analysis of these two orthogonal approaches, in the ASE literature.In this paper, we investigate the effectiveness of state-of-the-art LLM, i.e., GPT-4, with three differentprompting engineering techniques (i.e., basic prompting, in-context learning, and task-specific prompting) against 18 fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code summarization, and codetranslation. Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks. Forcomment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformedthe first-ranked fine-tuned model by 8.33% points on average in BLEU. However, for code generation, thefirst-ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points, on averagein BLEU. For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on differenttranslation tasks. To explore the impact of different prompting strategies, we conducted a user study with 27graduate students and 10 industry practitioners. From our qualitative analysis, we find that the GPT-4 withconversational prompts (i.e., when a human provides feedback and instructions back and forth with a modelto achieve best results) showed drastic improvement compared to GPT-4 with automatic prompting strategies.Moreover, we observe that participants tend to request improvements, add more context, or give specificinstructions as conversational prompts, which goes beyond typical and generic prompting strategies. Ourstudy suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks,but fully automated prompt engineering with no human in the loop requires more study and improvement.",
            "reference": [
                "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
                "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv",
                "Hugo Touvron et al. 2023a. Llama 2: Open Foundation and Fine-Tuned Chat Models.  arXiv:2307.09288 [cs.CL",
                "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv",
                "Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation",
                "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.  arXiv:2302.13971 [cs.CL",
                "Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code",
                "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems",
                "Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. 2020. Unit test case generation with transformers and focal context. arXiv preprint arXiv",
                "Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. 2022. Natural Language to Code Translation with Execution",
                "Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation",
                "Rohan Anil et al. 2023b. PaLM 2 Technical Report.  arXiv:2305.10403 [cs.CL",
                "Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming",
                "Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval",
                "Weisong Sun, Chunrong Fang, Yuchen Chen, Guanhong Tao, Tingxu Han, and Quanjun Zhang. 2022. Code search based on context-aware code translation",
                "Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, and Nick Haber. 2023. Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions.  arXiv:2212.10561 [cs.CL",
                "Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. 2023. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.  arXiv:2308.00352 [cs.AI",
                "Barbara Kitchenham and Shari Lawrence Pfleeger. 2002. Principles of survey research: part 5: populations and samples. ACM SIGSOFT Software Engineering Notes",
                "Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. 2023. Lever: Learning to verify language-to-code generation with execution",
                "Stephen V Stehman. 1997. Selecting and interpreting measures of thematic classification accuracy. Remote sensing of Environment",
                "Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation",
                "Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
                "Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Anibal, Alec Peltekian, and Yanfang Ye. 2021. Cotext: Multi-task learning with code-text transformer. arXiv preprint arXiv",
                "Junaed Younus Khan and Gias Uddin. 2022. Automatic code documentation generation using gpt",
                "Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv",
                "Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. 2023. Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
                "Kyunghyun Cho, B van Merrienboer, Caglar Gulcehre, F Bougares, H Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation",
                "Hui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang. 2019. Deep learning based code smell detection. IEEE transactions on Software Engineering",
                "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT",
                "Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv",
                "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.  arXiv:2005.14165 [cs.CL",
                "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.  arXiv:2203.02155 [cs.CL",
                "Samia Kabir, David N Udo-Imeh, Bonan Kou, and Tianyi Zhang. 2023. Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. arXiv preprint arXiv",
                "Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. 2016. Deep learning code fragments for code clone detection",
                "Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, et al. 2022. Hyperprompt: Prompt-based task-conditioning of transformers",
                "Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022. UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
                "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Comput. Surveys",
                "Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction",
                "Jian Li, Yue Wang, Michael R Lyu, and Irwin King. 2018. Code completion with neural attention and pointer networks",
                "[n.d.]. Online Appendix, https://anonymous.4open.science/r/gpt4_ase_tasks-6BF",
                "Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual training for software engineering",
                "Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, and Michael R Lyu. 2023. Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study",
                "Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                "Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning.  arXiv:2303.11366 [cs.AI",
                "Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv preprint arXiv",
                "OpenAI. 2023. GPT-4 Technical Report.  arXiv:2303.08774 [cs.CL",
                "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation",
                "Yi Li, Shaohua Wang, and Tien N Nguyen. 2022. Dear: A novel deep learning-based approach for automated program repair",
                "Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer Zhou, Biao Cheng, Daxin Jiang, Jiusheng Chen, Ruofei Zhang, Houqiang Li, and Nan Duan. 2021. ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation",
                "Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs",
                "Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Cheung, and Jeff Kramer. 2023. Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting",
                "Sijie Shen, Xiang Zhu, Yihong Dong, Qizhi Guo, Yankun Zhen, and Ge Li. 2022. Incorporating domain knowledge through task augmentation for front-end JavaScript code generation",
                "Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang. 2023. LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation. arXiv preprint arXiv",
                "Sidong Feng and Chunyang Chen. 2023. Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models",
                "Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen tau Yih, Daniel Fried, and Sida I. Wang. 2022. Coder Reviewer Reranking for Code Generation.  arXiv:2211.16490 [cs.LG",
                "Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search",
                "Sindhu Tipirneni, Ming Zhu, and Chandan K. Reddy. 2023. StructCoder: Structure-Aware Transformer for Code Generation.  arXiv:2206.05239 [cs.LG",
                "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv",
                "Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. CodeT: Code Generation with Generated Tests.  arXiv:2207.10397 [cs.CL",
                "Aakanksha Chowdhery et al. 2022. PaLM: Scaling Language Modeling with Pathways.  arXiv:2204.02311 [cs.CL",
                "Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, and Sandro Gabriele Tiddia. 2023. Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction. arXiv preprint arXiv"
            ],
            "related work": "2.Background and related work2.1.Language Models in Software Engineering and Fine-TuningThe field of automated software engineering (ASE) has shifted significantly towards using deep learning models, especially language models (LMs) to model source code(Wang et al.,2016; White et al.,2016; Yin and Neubig,2018; Liu et al.,2019; Allamanis et al.,2018) . These methods have been found to offer substantial advantages over traditional approaches such as domain-specific language-guided models, probabilistic grammars, and simple neural language models. A common architecture for language modeling is the encoder-decoder architecture(Cho et al.,2014) . In recent years, LMs have been applied to different varieties of ASE applications. These include code completion(Li et al.,2018) , code search(Gu et al.,2018) , code generation(Shen et al.,2022) , unit test case generation(Tufano et al.,2020) , code summarization(Hu et al.,2018) , code translation(Sun et al.,2022) , automated program repair(Li et al.,2022) , etc.Pre-trained code models can learn general-purpose code representations that capture various code properties such as lexical, syntactic, semantic, and structural information. Fine-tuning can adapt these models to specific tasks by updating the pre-trained parameters with task-specific data. By leveraging this new technique, models could outperform existing baselines by a huge margin. To exploit the new technology in the ASE domain, a plethora of studies have applied pre-training models to source code and natural language corpora and fine-tune downstream tasks, e.g. code search, comment generation, variable-misues classification, wrong binary operator detection, function-docstring mismatch prediction, exception type classification, code generation, code completion, code translation, etc.(Feng et al.,2020; Kanade et al.,2020; Guo et al.,2022; Ahmad et al.,2021; Ahmed and Devanbu,2022) .2.2.Prompt engineering in Software EngineeringPrompt engineering is an alternative to fine-tuning that also adapts pre-trained LMs as fine-tuned language models. However, they do not rely on the fine-tuning phase with the supervised dataset. Instead, they provide prompts to the pre-trained LMs to consider all different kinds of tasks to a generation problem. These models are generally larger in terms of the size of corpora that are trained on and the number of parameters that the model learns from. Due to this distinction, they are called Large Language Models (LLMs) . The advent of LLMs and prompt engineering brought the LM tasks to a new level of performance(Brown et al.,2020; Ouyang et al.,2022; OpenAI,2023; Touvron et al.,2023; et al.,2023a,2022,b) . There have been numerous studies that exploited LLMs and prompt engineering to tackle ASE tasks(Khan and Uddin,2022; Gao et al.,2023; Wei et al.,2023; Feng and Chen,2023; Geng et al.,2024) . They have proposed different methods of prompting strategies, i.e. basic prompting, in-context learning, task-specific prompting, chain-of-thought prompting, auto-prompting, soft prompting, etc.(Liu et al.,2023,2022; He et al.,2022; Carta et al.,2023; Wei et al.,2022; Shin et al.,2020; Hambardzumyan et al.,2021) .Gao et al.(Gao et al.,2023) empirically investigated the three key factors in in-context learning in code intelligence tasks: selection, order, and number of examples. They found that both similarity and diversity in example selection are important in both performance and stability in predictions. They also find that the order and the number of examples have an impact on their performance. Li et al.(Li et al.,2023) investigatedChatGPT's ability to find correct failure-inducing test cases for buggy source code. From their initial finding,ChatGPThad a low success rate but after guiding it to focus with correct nuances, they were able to drastically improve the performance. Feng et al.(Feng and Chen,2023) proposedAdbGPT, a novel approach that uses LLMs to automatically reproduce bugs from bug reports, without any training or hard-coding effort. They designed prompts that leverage few-shot learning and chain-of-thought reasoning, to elicit LLMs' knowledge and logical reasoning for bug replay. Geng et al.(Geng et al.,2024) investigated the LLMs' performance in generating code comments with multiple intents regarding their properties. They adopted the in-context learning paradigm and designed customized strategies for example selection and re-ranking techniques to enhance the performance. Kabir et al.(Kabir et al.,2023) did an analysis onChatGPT's responses to Stack Overflow (SO) questions in ASE tasks. They analyzed 517 SO questions, linguistic analysis, and a user study and found more than half of the answers were incorrect and 77% of them were verbose. However, users still preferredChatGPT39.34% of the time due to the comprehensiveness and the style of language.Although the research in prompt engineering is picking up, there have not been many studies that have compared the fine-tuning models and the prompt-engineered LLMs comprehensively on various ASE tasks. To mitigate this research gap, this paper compares the former fine-tuning paradigm with the prompt-engineered LLMs in quantitative and qualitative approaches.",
            "date": "2023"
        },
        "topic": "LLMs for Software Engineering",
        "year_start": "2021",
        "year_end": "2024",
        "target_list": [
            {
                "name": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
                "arxiv_id": "2404.00971",
                "subtitles": [
                    "Code Generation with LLMs",
                    "Hallucination in LLMs",
                    "Evaluation of LLM Generated Code"
                ],
                "reference": [
                    "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
                    "No need to lift a finger anymore? assessing the quality of code generation by chatgpt",
                    "An empirical evaluation of github copilot's code suggestions",
                    "Starcoder: may the source be with you",
                    "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
                    "Survey of hallucination in natural language generation",
                    "Ds-1000: A natural and reliable benchmark for data science code generation",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Codereval: A benchmark of pragmatic code generation with generative pre-trained models",
                    "Refining chatgpt-generated code: Characterizing and mitigating code quality issues",
                    "Evaluating large language models trained on code",
                    "Competition-level code generation with alphacode",
                    "Large language models and simple, stupid bugs",
                    "Measuring coding challenge competence with apps",
                    "Program synthesis with large language models",
                    "How often do single-statement bugs occur? the manysstubs4j dataset",
                    "Codegen2: Lessons for training llms on programming and natural languages",
                    "Bugs in large language models generated code",
                    "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
                    "Siren's song in the ai ocean: A survey on hallucination in large language models",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Amazon codewhisperer",
                    "Chatgpt: Optimizing language models for dialogue"
                ],
                "related_work": "IIBackground&Related WorkII-ACode Generation with LLMsLarge Language Models (LLMs) have demonstrated remarkable achievements across various tasks in recent years. A significant number of LLMs for code-related tasks, especially for code generation, have been proposed[4,29,20,31]. Codex[4]is the earlier representative work to use large generative pre-trained models with up to 12 billion parameters to generate code snippets. It enabled Copilot to deliver real-time coding suggestions, revolutionizing the coding experience. The success of Codex has captured the interest of both academia and industry groups in this particular field. As a consequence, various models have emerged. DeepMind proposed AlphaCode[21], which is trained for generating code in real-world programming competitions. Meta proposed InCoder[7]and Code Llama[31], Salesforce proposed CodeRL[18]and CodeGen[28,27], Amazon provided CodeWhisperer[1], BigCode project proposed StarCoder[20], and OpenAI further introduces GPT and ChatGPT series, which are fine-tuned using Reinforcement Learning from Human Feedback (RLHF) , significantly surpassing prior methods. The emergence of these models yields remarkable enhancements in the effectiveness of code generation. To evaluate the performance of the generated code, various benchmarks are proposed. Notable examples of these benchmarks include HumanEval[4], DS-1000[17], MBPP[2], APPS[10], CoderEval[37],etc. These benchmarks typically consist of multiple test cases, and are usually accompanied by a few instances to aid in understanding the task description.II-BHallucination in LLMsThe term  \"hallucination \" has been widely used within the natural language processing (NLP) community to describe the generation of text that is nonsensical or deviates from the original source content[14]. In the field of NLP, hallucinations are categorized into two main types:intrinsic hallucinations, where the generated content contradicts the source content, andextrinsic hallucinations, where the generated content cannot be verified from the source input. Considering the versatility of LLMs,Zhang et al. [39]further refines the definition by categorizing hallucination within the context of LLMs as follows: (1) Input-conflicting hallucination, occurring when the generated content deviates from the original source input; (2) Context-conflicting hallucination, where the generated content contradicts previously generated information; (3) Fact-conflicting hallucination, arising when LLMs produce content that lacks fidelity to established world knowledge.However, there is a relatively limited amount of research focusing on hallucination in the context of code generation. Hallucination issues in the Code LLMs can hurt the overall quality of the generated code, potentially affecting performance and maintainability, and even resulting in unexpected errors and security vulnerabilities.II-CEvaluation of LLM Generated CodeWith the emergence of code LLMs, many researchers began to examine the quality of the LLM-generated code from different aspects, including security, usability, and especially correctness[13,23,36,33,32].Jesse et al. [13]explore the extent to which code LLMs are inclined to produce simple, stupid bugs[16].Nguyen and Nadi [26]assess the correctness and comprehensibility of GitHub Copilot's code suggestion.Tambon et al. [32]analyzed the bug patterns in LLM-generated code and their prevalence.Liu et al. [22]further conducted an empirical study of ChatGPT-generated code to evaluate its quality and reliability, which also includes an exploration of ChatGPT's self-debugging capability. Similarly,Liu et al. [23]examined the code snippets generated by ChatGPT, with a specific focus on three aspects: correctness, understandability, and security.Yeti\u015ftiren et al. [36]conducted a comprehensive analysis to compare the performance of AI-assisted code generation tools, in terms of code quality metrics, such as validity, correctness, security, reliability, and maintainability.In contrast to existing research, we conducted the first comprehensive analysis from the perspective of hallucinations to examine the deviations inherent in the LLM-generated code, and also analyzed the code quality issues/bugs that can arise from these hallucinations, encompassing most of the quality issues identified in current research[22,32].",
                "abstract": "The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future."
            },
            {
                "name": "Agentless: Demystifying LLM-based Software Engineering Agents",
                "arxiv_id": "2407.01489",
                "subtitles": [
                    "LLMs for code",
                    "Benchmarking for LLM-based coding tasks",
                    "Agent-based software development"
                ],
                "reference": [
                    "Programming-by-demonstration for long-horizon robot tasks",
                    "The rise and potential of large language model based agents: A survey",
                    "Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt",
                    "https://github.com/swe-bench/experiments/blob/main/evaluation/lite/20240612_IBM_Research_Agent101/README.md",
                    "Livecodebench: Holistic and contamination free evaluation of large language models for code",
                    "Unsupervised translation of programming languages",
                    "Leveraging automated unit tests for unsupervised code translation",
                    "Repairagent: An autonomous, llm-based agent for program repair",
                    "SWE-bench: Can language models resolve real-world github issues",
                    "Starcoder 2 and the stack v2: The next generation",
                    "https://opencsg.com/product?class=StarShip/",
                    "Magicoder: Source code is all you need",
                    "Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt",
                    "The living review on automated program repair",
                    "How to understand whole software repository",
                    "Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models",
                    "Starcoder: may the source be with you",
                    "Universal fuzzing via large language models",
                    "Automated program repair in the era of large pre-trained language models",
                    "Ds-1000: A natural and reliable benchmark for data science code generation",
                    "https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240622_Lingma_Agent",
                    "Mapping language to code in programmatic context",
                    "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x",
                    "https://www.marscode.com",
                    "Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models",
                    "Wizardcoder: Empowering code large language models with evol-instruct",
                    "Natural language to code generation in interactive data science notebooks",
                    "Evaluating large language models trained on code",
                    "https://www.swebench.com/lite.html",
                    "Competition-level code generation with alphacode",
                    "Measuring coding challenge competence with apps",
                    "https://aws.amazon.com/q/developer",
                    "Program synthesis with large language models",
                    "Autocoderover: Autonomous program improvement",
                    "https://www.cognition.ai/introducing-devin",
                    "Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation",
                    "Repocoder: Repository-level code completion through iterative retrieval and generation",
                    "https://github.com/OpenDevin/OpenDevin/",
                    "Patch generation with language models: Feasibility and scaling behavior",
                    "Code llama: Open foundation models for code",
                    "Deepseek-coder: When the large language model meets programming-the rise of code intelligence",
                    "Lost in translation: A study of bugs introduced by large language models while translating code",
                    "Repobench: Benchmarking repository-level code auto-completion systems",
                    "Aider is ai pair programming in your terminal",
                    "Large language models are few-shot testers: Exploring llm-based general bug reproduction",
                    "Opencodeinterpreter: Integrating code generation with execution and refinement",
                    "Multipl-e: A scalable and polyglot approach to benchmarking neural code generation",
                    "Coder: Issue resolving with multi-agent and task graphs",
                    "https://www.factory.ai",
                    "Swe-agent: Agent-computer interfaces enable automated software engineering"
                ],
                "related_work": "6Related WorkLLMs for code.LLMs have become the default choice for various coding tasks, due to the impressive results achieved by LLMs in both code generation and understanding[18]. Developers and researchers have applied on software engineering tasks, such as program synthesis[47;18;35;25], code translation[46;50;51], program repair[59;58;42;31;15], and test generation[19;60;20;33;30]. Apart from using general-purpose LLMs, code-specific LLMs have been built by further training LLMs using large amounts of open-source code snippets. Examples of code LLMs includeCodex[18], CodeLlama[52], StarCoder[34;39], DeepSeek-Coder[22], etc. Furthermore, researchers have also developed instruction-following code-specific LLMs using instruction-tuning methods. Examples of such LLMs include CodeLlama-Inst[52], DeepSeek-Coder-Inst[22], WizardCoder[40], Magicoder[54], and OpenCodeInterpreter[67].Benchmarking for LLM-based coding tasks.To evaluate the capability of LLMs on code, various benchmark has been proposed.HumanEval[18]and MBPP[14]are two of the most widely-used handcrafted code generation benchmarks complete with test cases to check for the correctness of LLM outputs. Furthermore, other benchmarks have been proposed with more robust test[37], additional programming languages[66;16], and other programming domains[26;23;36;32;62].More recently, instead of evaluating on self-contained coding problems, researchers have developed benchmarks focus on solving real-world software engineering issues by operating on an entire coding repository[28;63;38]. One such popular benchmark is SWE-bench[28], containing problems where the goal is to modify the repository and resolve a real-world GitHub issue. The authors of SWE-bench have since published a smaller filtered subset of SWE-bench Lite[11], containing 300 total problems, focused on bug fixing issues that only modify a single file in the ground truth patch. In this work, we conduct a detailed classification and analysis of the problems in SWE-bench Lite. We found that some problems lack sufficient information in the problem description to correctly solve the problem. Furthermore, there are also problems containing misleading patches, which can confuse the model. Recognizing these limitations, we further filter SWE-bench Lite to remove such problems and construct SWE-bench Lite-S that can serve as a more rigorous set of problems to evaluate different tools.Agent-based software development.With the emergence and popularity of agent-based frameworks[56], recently researchers and industry practitioners have begun developing agent-based approaches to solve software engineering tasks. Devin[4](and OpenDevin[10], open-source alternative) , is one of the first end-to-end LLM agent-based framework. Devin uses agents to first perform planning based on user requirement, then allows the agent to use file editor, terminal, and web search engine tools to iteratively perform the task. SWE-agent[61]designs a custom agent-computer interface (ACI) that allows the LLM agent to interact with the repository environment with actions such as reading, editing file, and running bash commands. AutoCodeRover[65]is another agent-based approach that provide the LLM agent with specific APIs (e.g., searching methods in certain class) to effectively find the locations that need to be modified to solve the issue. In addition to these highlighted examples, there has been a plethora of other agent-based approaches developed in both open-source[21]and close-source/commercial products[15;17;41;7;5;6;9;8;1]. Compared to these agent-based techniques,Agentlessoffers a simple and cost-effective solution to tackle real-world software engineering issues.Agentlessdemonstrates for the first time that anagentlessapproach can achieve similar performance, without the additional baggage of having to providing excessive tools or modeling complex environment behavior/feedback.",
                "abstract": "Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic two-phase process of localization followed by repair, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (27.33%) and lowest cost (\\$0.34) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction."
            },
            {
                "name": "Prioritizing Software Requirements Using Large Language Models",
                "arxiv_id": "2405.01564",
                "subtitles": [
                    "Background: Generative AI and Its Application in Software Requirements Prioritization"
                ],
                "reference": [
                    "The scope of chatgpt in software engineering: A thorough investigation",
                    "Prius: Applying gamification to user stories prioritization",
                    "Navigating complexity in software engineering: A prototype for comparing gpt-n solutions",
                    "Large language model evaluation via multi ai agents: Preliminary results",
                    "Artificial Intelligence-driven web development and agile project management using OpenAI API and GPT technology: A detailed report on technical integration and implementation of GPT models in CMS with API and agile web development for quality user-centered AI chat service experience",
                    "Autonomous agents in software development: A vision paper",
                    "Is chatgpt leading generative ai? what is beyond expectations? What is beyond expectations",
                    "Self-collaboration code generation via chatgpt",
                    "Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis",
                    "Not all requirements prioritization criteria are equal at all times: A quantitative analysis",
                    "Regulating chatgpt and other large generative ai models",
                    "Investigating code generation performance of chat-gpt with crowdsourcing social data",
                    "Towards human-bot collaborative software architecting with chatgpt",
                    "Gpt understands, too"
                ],
                "related_work": "2Background And Related Work2.1Background:Generative AI and Its Application in Software Requirements PrioritizationGenerative AI, aims to produce new material by mimicking human-like outputs in a variety of fields, including computer vision, NLP, and the creation of images and videos[15]. Text creation, machine translation, dialog systems, and code generation have all been transformed by this technology, especially through the use of autoregressive language models such as GPT and Generative Adversarial Networks (GANs) [16]. The transformer architecture, which greatly improves NLP skills by capturing contextual relationships inside text, is the foundation of the GPT model, which is well-known for its human-like text production[7,17].The exceptional efficiency and adaptability of GPT models, as evidenced by recent advances, indicate that they have tremendous potential to revolutionize SE processes[18,9]. These models can streamline the software development lifecycle by automating tasks like error detection, code snippet generation, and documentation creation after being trained on large-scale code repositories[19,20]. In addition to speeding up code generation and program development, the incorporation of GPT models into SE has improved software development's overall quality and efficiency[21,22].Software engineering (SE) has witnessed a rapid evolution of generative AI, demonstrating its transformational power. A major change that promises to accelerate and improve software development is the integration of GPT models into SE. These approaches transform not just the software development process but also the way software is developed and maintained. Most importantly, they improve the prioritizing of requirements, which is an essential development stage. With the use of AI, developers can more efficiently prioritize and order work, making sure that important features match user requirements and organizational goals. The development and maintenance of software will be drastically changed by this method, which represents a shift towards more agile, user-focused, and efficient software development.2.2Related WorkAn approach to needs prioritization in agile software development that is gamified is examined in the paper  \"PRIUS: Applying Gamification to User Stories Prioritization \". To increase stakeholder participation, it blends gaming aspects with the Wiegers Matrix prioritizing technique[23]. Positive effects on engagement levels during requirements prioritizing were observed in an academic setting during the development and testing of a system that supports this technique.Several studies (e.g.,[24],[25],[12]) have explored GPT models in SE and in requirements prioritization techniques. The application of Large Language Models (LLMs) to software development, specifically for needs prioritization, is a significant advancement that holds the potential to increase both efficiency and effectiveness. However, the problems with hallucinations and the constraints of natural language identify an important research need. A solution that can not only understand broad client needs and create user stories on its own but also uses AI agents to systematically prioritize tasks is desperately needed. The latest software development approaches that emphasize agility and the creative use of LLM agents must be embraced by this instrument. By closing this gap, we hope to improve needs prioritization accuracy and dependability while also fully utilizing AI's capabilities within agile software engineering frameworks. This presents a substantial opportunity for innovative research and development in the sector.Some established techniques and methods prioritize requirements in software development and project management. Understanding these methods matters for effective decision-making and resource allocation in enterprise-level software projects. While there are other notable techniques, we focus on those we use in our prioritization process. The Analytic Hierarchy Process (AHP) stands out. It blends mathematics and psychology, using pairwise comparisons and expert judgments to create priority scales. This process is known for its mathematical rigor and its ability to mix subjective and objective decision-making elements. Similarly, the MoSCoW method offers a straightforward yet effective framework for prioritizing requirements by sorting them into four categories: Must have, Should have, Could have, and Won't have. This method helps clarify the importance and urgency of each requirement, supporting more informed decision-making.",
                "abstract": "Large Language Models (LLMs) are revolutionizing Software Engineering (SE) by introducing innovative methods for tasks such as collecting requirements, designing software, generating code, and creating test cases, among others. This article focuses on requirements engineering, typically seen as the initial phase of software development that involves multiple system stakeholders. Despite its key role, the challenge of identifying requirements and satisfying all stakeholders within time and budget constraints remains significant. To address the challenges in requirements engineering, this study introduces a web-based software tool utilizing AI agents and prompt engineering to automate task prioritization and apply diverse prioritization techniques, aimed at enhancing project management within the agile framework. This approach seeks to transform the prioritization of agile requirements, tackling the substantial challenge of meeting stakeholder needs within set time and budget limits. Furthermore, the source code of our developed prototype is available on GitHub, allowing for further experimentation and prioritization of requirements, facilitating research and practical application."
            },
            {
                "name": "EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation",
                "arxiv_id": "2408.11198",
                "subtitles": [
                    "LLMs for Code Generation",
                    "Prompt Engineering for LLMs",
                    "Prompt Engineering of LLMs for Code Generation"
                ],
                "reference": [
                    "React: Synergizing reasoning and acting in language models",
                    "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
                    "Language models are few-shot learners",
                    "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
                    "A systematic survey of prompt engineering in large language models: Techniques and applications",
                    "Instoptima: Evolutionary multi-objective instruc- tion optimization via large language model-based instruction opera- tors",
                    "Automatic prompt optimization with\" gradient descent\" and beam search",
                    "Automatic code generation using pre-trained language models",
                    "Promptbreeder: Self-referential self-improvement via prompt evolution",
                    "A comprehensive review of State-of-The-Art methods for Java code generation from Natural Language Text",
                    "Code llama: Open foundation models for code",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Automatic chain of thought prompting in large language models",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Spell: Semantic prompt evolution based on a llm",
                    "Language models are unsupervised multitask learners",
                    "Evaluating large language models trained on code",
                    "Magicoder: Source Code Is All You Need",
                    "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
                    "Generated knowledge prompting for commonsense reasoning",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
                    "Language agent tree search unifies reasoning acting and planning in language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Large language models are human-level prompt engineers",
                    "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
                    "Reflexion: Language Agents with Verbal Reinforcement Learning",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Codesearchnet challenge: Evaluating the state of semantic code search",
                    "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization",
                    "Codexglue: A machine learning benchmark dataset for code understanding and generation"
                ],
                "related_work": "2Background and Related Work In this section, we briefly explain the background of LLMs for code generation and prompt engineering, then report the most related work in the context of prompt engineering of LLM for code. 2.1LLMs for Code Generation Models for code generation can be divided into two categories, RNN-based models and transformer-based models [1]. RNN-based models are older models that are outperformed by transformers. Transformer-based code generation models are transformers pre-trained on code-related datasets. For instance, CodeBERT [2] is a transformer model based on BERT [3], specifically pre-trained on the CodeSearchNet dataset [4]. Another example is CodeGPT-2, which builds upon GPT-2 [5] and is pre-trained on the same dataset. Lu et al. [6] evaluated the performance of both GPT-2 and CodeGPT-2 in the code generation task and later, Perez et al. [7], fine-tuned GPT-2 using their dataset for code generation. Newer versions of LLMs, such as GPT-4, have demonstrated improved performance compared to their predecessors across diverse tasks, including code generation. For instance, Codex [8] is a language model developed by OpenAI based on the GPT architecture, similar to GPT-3. However, it's specifically designed for code generation and understanding. Codex is trained on a diverse range of publicly available code repositories and strongly focuses on understanding and generating programming code. Another model, CodeLlama [9], is a family of large language models for code based on Llama 2 [10] that is available in 7B, 13B, and 34B parameters. DeepSeek Coder is another state-of-the-art LLM that has been used in software tasks such as Code Generation. This LLM has various sizes, ranging from 1B to 33B. Magicoder [11] is another model that has been fine-tuned on both CodeLlama-7b-Python and deepseek-coder-6.7b. In this research, we employed two different types of LLMs: GPT-4o, as an example of closed-source large models, and MagicCoder as an example of open-source smaller models. We selected GPT-4o due to its high performance among the closed-source LLMs. MagicCoder was chosen as the most cost-effective small-size open-source model, at the time of designing our experiments. 2.2Prompt Engineering for LLMs A prompt is an input or query provided to a model to generate a response or perform a task. Prompt engineering refers to the process of designing and refining prompts to achieve desired outcomes when using LLMs. There are multiple categories of prompt engineering, including approaches without training, reasoning and logic, reducing hallucination, and evolutionary-based methods [38]. Zero-shot [5] and few-shot [33] prompting fall under the category of approaches without training. Techniques such as chain-of-thought (CoT) prompting [12], Automatic Chain-of-Thought (Auto-CoT) [39], Self-Consistency [40], and knowledge prompting [13] exemplify reasoning and logic-based methods. To reduce hallucination for prompt engineering, techniques such as Retrieval Augmented Generation (RAG) [35], ReAct Prompting [41], and Chain-of-Knowledge (CoK) Prompting [42] are employed. Evolutionary algorithms are utilized to optimize prompts, as demonstrated by EvoPrompt [16] and PromptBreeder [25]. There are other solutions such as Automated Prompt Engineering [14] and Prompt Optimization with Textual Gradients (ProTeGi) [15]. Zero-shot prompting, as described in [5], eliminates the necessity for extensive training data by employing accurately designed prompts to direct the model toward executing new tasks. The model receives a task description without labeled training data and employs its pre-existing knowledge to generate predictions based on the prompt. Few-shot prompting [33] uses a limited number of input-output examples to help models understand tasks, unlike zero-shot prompting which provides no examples. However, this method requires additional tokens, which can be impractical for longer texts. The selection and composition of examples can also significantly influence model behavior and introduce biases. CoT [12] improves the reasoning abilities of large language models by incorporating intermediate reasoning steps within prompts. This technique breaks down complex tasks into smaller sub-tasks, mimicking human problem-solving. It significantly enhances performance in arithmetic, commonsense, and symbolic reasoning, especially with larger models. Auto-CoT  [39] is an approach designed to enhance the reasoning capabilities of LLMs by automating the generation of intermediate reasoning steps. It automates the creation of reasoning chains and demonstrations using diversity-based sampling to generate multiple reasoning paths. Automated prompt engineering approaches use an agent or a similar automated engine to interact with LLM and typically get some feedback to improve the prompt. Zhou et al. [14] proposed APE, a framework designed for the automatic generation and selection of instructions. In this framework, they used an LLM to generate instruction candidates and then search for candidate solutions to maximize a selected score function, treating prompt engineering as an optimization problem. Pryzan et al. [15] proposed Prompt Optimization with Textual Gradients (ProTeGi), which improves prompts for LLMs using a non-parametric approach inspired by numerical gradient descent. It uses natural language gradients from training data to critique and edit prompts, employing a process guided by beam search and bandit selection. This approach improved the efficiency of previous prompt editing techniques across various NLP tasks, including the novel challenge of LLM jailbreak detection. Evolutionary algorithms have been used in the past for prompt engineering of LLMs in generic NLP tasks. For example, EvoPrompt [16], a framework designed to automate the prompt engineering process for LLMs. EvoPrompt begins with an initial population of prompts and iteratively generates new ones using evolutionary operators such as mutation and crossover, which are performed by LLMs. Another similar approach is PromptBreeder [25], which employs task prompts to condition the LLM and mutation prompts to modify task prompts. The interplay between task and mutation prompts drives iterative improvement in PromptBreeder. Similar approaches for prompt enhancement using LLMs through EAs are also proposed in [26] and [27]. In contrast to existing evolutionary prompt engineering techniques, EPiC is particularly tailored for prompt engineering in coding tasks, with a fitness function defined based on the pass rate of test cases. In addition, it focuses on cost-efficiency throughout the process. To achieve this, it minimizes the calls to external LLMs and implements the mutation operator using local lightweight word embeddings libraries. 2.3Prompt Engineering of LLMs for Code Generation Zelikman et al. [17] introduced Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. Using Parsel, they break down algorithmic tasks into structured descriptions written in natural language, then explore various combinations of function implementations using tests. DyLAN [19] is a framework designed to boost the performance of Large Language Model (LLM) agents by assembling them into a dynamic team that adapts to different tasks. Unlike traditional methods with fixed agent sets, DyLAN's architecture adjusts dynamically to the task query, enhancing its versatility. It employs features like inference-time agent selection and early stopping to improve efficiency and effectiveness. Furthermore, DyLAN incorporates an automatic agent team optimization algorithm based on an unsupervised metric called Agent Importance Score, which selects the most effective agents for each task. Empirical results show DyLAN's success in tasks like reasoning and code generation, achieving significant improvements over single LLM executions. Reflexion [20] is a reinforcement-based framework for this problem where language agents learn from linguistic feedback rather than weight updates. Agents reflect on task feedback verbally and maintain their own reflective text in memory, which helps them make better decisions in subsequent trials. Reflexion is adaptable to different types and sources of feedback signals and shows significant improvements over baseline agents across various tasks. LATS (Language Agent Tree Search) [21] is another search-based framework that combines LLMs' abilities in planning, acting, and reasoning. LATS draws inspiration from the Monte Carlo tree search and repurposes LLMs' strengths as agents, value functions, and optimizers. Crucially, LATS incorporates an environment for external feedback to enable more deliberate and adaptive problem-solving beyond existing techniques' limitations. AgentCoder [22] utilizes a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. The programmer agent focuses on code generation and refinement based on feedback from the test executor agent, while the test designer agent generates test cases for the code. The test executor agent runs the code with the test cases and provides feedback to the programmer. This collaborative system aims to improve code generation efficiency, outperforming single-agent models and previous strategies. Zhong et al. [23] introduced Large Language Model Debugger (LDB), a framework that segments programs into basic blocks and tracks intermediate variable values during runtime. LDB enables LLMs to focus on simpler code units, verify their correctness block by block, and pinpoint errors effectively. In contrast to existing methods and to the best of our knowledge, EPiC is the first search-based prompt engineering method for code generation. It employs a lightweight process to identify the optimal solution in a cost-efficient manner. To achieve this, EPiC utilizes a local embedding function to implement mutation operators on text, to reduce the cost of iterative prompt engineering for code generation. It also guides the search over iterations using the fitness function in Section 4.4, which helpsfindg the most effective prompts.",
                "abstract": "Large Language Models (LLMs) have seen increasing use in various software development tasks, especially in code generation. The most advanced recent methods attempt to incorporate feedback from code execution into prompts to help guide LLMs in generating correct code, in an iterative process. While effective, these methods could be costly and time-consuming due to numerous interactions with the LLM and the extensive token usage. To address this issue, we propose an alternative approach named Evolutionary Prompt Engineering for Code (EPiC), which leverages a lightweight evolutionary algorithm to evolve the original prompts toward better ones that produce high-quality code, with minimal interactions with LLM. Our evaluation against state-of-the-art (SOTA) LLM-based code generation models shows that EPiC outperforms all the baselines in terms of cost-effectiveness."
            },
            {
                "name": "No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair",
                "arxiv_id": "2409.03267",
                "subtitles": [
                    "Large Language Models",
                    "Code Search",
                    "Code Generation",
                    "Program Repair"
                ],
                "reference": [
                    "Pre-Trained Model-Based Automated Software Vulnerability Repair: How Far are We",
                    "A Survey of Learning-based Automated Program Repair",
                    "A Survey on Large Language Models for Code Generation",
                    "Automated Program Repair in the Era of Large Pre-trained Language Models",
                    "Teaching Large Language Models to Self-Debug",
                    "Gamma: Revisiting Template-Based Automated Program Repair Via Mask Prediction",
                    "Impact of Code Language Models on Automated Program Repair",
                    "A syntax-guided edit decoder for neural program repair",
                    "ChatGPT",
                    "Software Testing With Large Language Models: Survey, Landscape, and Vision",
                    "An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation",
                    "Is Self-Repair a Silver Bullet for Code Generation",
                    "CoCoNuT: combining context-aware neural translation models using ensemble for program repair",
                    "A Systematic Literature Review on Large Language Models for Automated Program Repair",
                    "Apache Lucene",
                    "SkCoder: A Sketch-based Approach for Automatic Code Generation",
                    "Code Llama: Open Foundation Models for Code",
                    "GPT-4 Technical Report",
                    "A Survey on Large Language Models for Software Engineering",
                    "CodeAgent: Collaborative Agents for Software Engineering",
                    "A Survey of Source Code Search: A 3-Dimensional Perspective",
                    "AceCoder: An Effective Prompting Technique Specialized in Code Generation",
                    "GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search"
                ],
                "related_work": "2.Background & Related Work2.1.Large Language ModelsIn recent years, LLMs have attracted increasing attention from the industry and academia for their impressive capability of processing natural and programming languages(Zhang et al.,2023a) . LLMs are generally based on the Transformer architecture that has two key components: the encoder and the decoder. The former encodes the input into a vector representation for the model to understand the input, while the latter transforms the encoded representation to the output sequence. Such LLMs have shown outstanding performance in code-related tasks, such as program repair and code generation(Wang et al.,2024) . For example, ChatGPT(OpenAI,2024) and GPT-4(OpenAI,2023) released by OpenAI are known for their ability of conducting dialogues with human beings. They can take prompts in natural language and generate relevant code accordingly. CodeLlama(Rozi\u00e8re et al.,2023) is a family of open-source LLMs specifically trained for source code, and can solve programming problems in a zero-shot situation. Details about the application of LLMs in SE can be found in recent survey papers(Zhang et al.,2023a; Wang et al.,2024) . However, when programming, such LLMs typically struggle to learn the latest knowledge and interact with external coding tools. In this work, we aim to improve the programming capabilities of off-the-shelf LLMs by integrating them with code search, code generation, and program repair techniques.2.2.Code SearchA common action that developers take while programming is searching for existing code with similar requirements to reuse. A variety of code search techniques have been proposed to facilitate the retrieval process, and they can be generally classified into two types: IR-based and DL-based(Sun et al.,2024) . IR-based code search techniques usually involve indexing the codebases and using scoring algorithms to calculate the similarity between the query and the target code. For example, Lucene(Foundation,2024) is a typical IR-based search engine whose default scoring algorithm is BM25, which considers the word frequency and the lengths of documents to rank candidates in the retrieval corpus. DL-based techniques leverage deep learning models to encode code snippets into vectors, and retrieve similar code based on the cosine similarity between the vectors. For example, GraphSearchNet(Liu et al.,2023) is a neural network framework based on bidirectional GGNN to map queries and source code by learning the structural information from them. For a more comprehensive study on code search, please refer to the work of Sunet al.(Sun et al.,2024) . In this work, we implementCreamwith two simple yet effective code searchers,i.e.,IR-based and DL-based sterategies.2.3.Code GenerationCode generation is a popular task that LLMs are applied to because of its great potential to improve the coding efficiency of developers. For example, AceCoder(Li et al.,2024) retrieves similar code and remove redundant retrieval results to boost the effectiveness of code generation. SkCoder(Li et al.,2023) simulates developers' coding behavior by constructing code sketches from the retrieved similar code and turning the sketch into complete code with an encoder-decoder model. CodeAgent(Tang et al.,2024) proposes a novel repo-level code generation framework that integrates different programming tools including information retrieval tools with the purpose of gathering relevant resources so that LLMs can better understand the problems. Please refer to the of Jianget al.(Jiang et al.,2024) for a more comprehensive survey. In this work, we construct prompts augmented by the code searcher to query CodeLlama and ChatGPT as the code generator.2.4.Program RepairProgram repair aims to automatically fix software bugs, thereby reducing the efforts for manual debugging(Zhang et al.,2023a) . Existing repair techniques can be broadly categorized into traditional and learning-based ones. Traditional program repair approaches include heuristic-based, constraint-based, and template-based techniques. With recent advancements in DL, a variety of learning-based repair approaches have been proposed(Tufano et al.,2019; Lutellier et al.,2020; Zhu et al.,2021) . Such learning-based techniques leverage Neural Machine Translation (NMT) models to understand the semantics of the bugs and transform them into the correct code. For example, CoCoNut(Lutellier et al.,2020) utilizes a context-aware NMT architecture to represent the buggy source code and its surrounding context separately.Recently, LLMs are increasingly being utilized for repair tasks(Zhang et al.,2023b,2024c; Xia et al.,2023; Jiang et al.,2023) . For example, Zhanget al.(Zhang et al.,2024c) investigate the potential of fine-tuning LLMs in repairing security vulnerabilities. Xiaet al.(Jiang et al.,2023) evaluate the fixing capabilities of LLMs for Java single-hunk semantic bugs. Detailed summarization of program repair studies can be found in recent work(Zhang et al.,2024a,b) . However, unlike traditional repair techniques, LLMs' powerful natural language capabilities enable them to incorporate external runtime information, thus facilitating iterative patch generation(Olausson et al.,2023; Chen et al.,2023) . In this work, motivated by the self-critical capability of LLMs, we leverage execution feedback to integrate program repair into the programming process.",
                "abstract": "Automatic programming attempts to minimize human intervention in the generation of executable code, and has been a long-standing challenge in the software engineering community. To advance automatic programming, researchers are focusing on three primary directions: (1) code search that reuses existing code snippets from external databases; (2) code generation that produces new code snippets from natural language; and (3) program repair that refines existing code snippets by fixing detected bugs. Despite significant advancements, the effectiveness of state-of-the-art techniques is still limited, such as the usability of searched code and the correctness of generated code.Motivated by the real-world programming process, where developers usually use various external tools to aid their coding processes, such as code search engines and code testing tools, in this work, we propose \\toolname{}, an automatic programming framework that leverages recent large language models (LLMs) to integrate the three research areas to address their inherent limitations. In particular, our framework first leverages different code search strategies to retrieve similar code snippets, which are then used to further guide the code generation process of LLMs. Our framework further validates the quality of generated code by compilers and test cases, and constructs repair prompts to query LLMs for generating correct patches. We conduct preliminary experiments to demonstrate the potential of our framework, \\eg helping CodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a generic framework, \\toolname{} can integrate various code search, generation, and repair tools, combining these three research areas together for the first time. More importantly, it demonstrates the potential of using traditional SE tools to enhance the usability of LLMs in automatic programming."
            },
            {
                "name": "Towards Integrating Emerging AI Applications in SE Education",
                "arxiv_id": "2405.18062",
                "subtitles": [
                    "Identifying AI Trends in Research and Education",
                    "Related Work"
                ],
                "reference": [
                    "Chatbots applications in education: A systematic review",
                    "Artificial intelligence in education: A review",
                    "Computing education in the era of generative AI",
                    "Systematic review of research on artificial intelligence applications in higher education-where are the educators",
                    "How chatgpt will change software engineering education",
                    "Positive artificial intelligence in education (P-AIED) : A roadmap",
                    "Toward computer science curricular guidelines 2023 (cs",
                    "Artificial intelligence meets software engineering in computing education",
                    "Interacting with educational chatbots: A systematic review",
                    "SWEBOK Guide Version 4.0 beta",
                    "A review of artificial intelligence (AI) in education from 2010 to 2020"
                ],
                "related_work": "2   Study Design & Related WorkIn this section, we first provide a brief motivation for the need to systematically introduce AI in SE education, and further discuss existing related work in this area.2.1Identifying AI Trends in Research and EducationTo gain better insights into current trends - particularly within the past year - we started a structured literature analysis for both research and educational venues in software engineering. We specifically chose to include both research and education venues, hoping this will provide a more comprehensive understanding of current research trends and the extent to which they have been integrated into SE education. Furthermore, we aim to document challenges educators need to be aware of and, as a result, potentially adapt didactic concepts and/or teaching materials. In this short paper, we report on our work in progress and the first results of this analysis. For this purpose, we discuss initial findings (cf. Section3) and potential applications and opportunities (cf. Section4) .To enable a more structured analysis, we further aim to categorize our findings alongside the different SE activities and areas. For SE, numerous taxonomies and classification schemes have been proposed, most notably the Software Engineering Body of Knowledge (SEBoK) [17], with its 18 knowledge areas. As our primary focus, however, is on SE education, we have chosen to group our findings according to the ACM Computer Science Curriculum Guidelines version \"Gamma \"(CS2023) , comprising 17 different knowledge areas, which also incorporate - to a very large extent - the areas proposed in the SEBoK. The ACM Computer Science Curriculum Guidelines are designed to assist educators in the area of computer science, describing the general structure, as well as listing important topics to cover, core and elective CS courses[32]. Most notably, the most recent version, CS2023, currently discussed by the ACM task force, will take into account competency-based education, incorporating a competency model, comprising required skills, and knowledge to be acquired by students. Introducing Learning Analytics and competency-based education in classes typically requires a significant effort and is also something we see huge potential for leveraging AI and alleviating the burden of, for example, manually assessing if all teaching materials correspond to specified competencies (cf. Section4) .While CS2023 addresses a broad and comprehensive education, covering 17 different knowledge areas, ranging from computer science foundations, to AI, and parallel computing, we focus on two main areas: Software Development Fundamentals (SDF) that cover basic programming education, and second, Software Engineering (SE) . A brief overview of the sub-categories for these two areas is shown in Table1. The goal of our work is to identify existing work and approaches for each sub-category of the two areas and collect different means of how AI can be incorporated, for example in teaching aspects of software architecture, such as design patterns, or architectural styles. For this purpose, as a starting point, to gain an initial understanding of current research and trends, we collected an initial set of research papers from 2023, of two prime software engineering conferences: the International Conf. of Software Engineering, and the International Conf. on the Foundations of Software Engineering, as well as four educational conferences/tracks: Software Engineering Education and Training Track@ICSE, the International Conf. on Software Engineering Education and Training, the Hawaii International Conf. on System Sciences, and the Conf. on Innovation and Technology in Computer Science Education.The goal was to identify trends and hot topics and uncover areas that might be covered by research but not yet as part of the CS and SE education. In total, we started with a set of 51 publications (33 research-related and 18 education-related) , which was extended to 71 after performing an extra round of snowballing. We focused on publications that presented AI applications or educational concepts in the area of SE and programming, but excluded any kind of study, analysis, or review that did not present a specific approach. After carefully reading the titles and abstracts, we excluded 17 papers that were deemed out of scope (e.g. tertiary studies) , with a final set of 54 papers remaining.After selecting our set of papers, we divided them randomly, with four researchers evaluating the papers independently. For the research papers, we focused on the purpose (e.g., test case generation) , the technology used, and the specific application area. (Please note, that for the purpose of this paper, and for the sake of brevity, we mainly focus on the intent, and the relation to the CS2023 categories, but future work will broaden the scope of this analysis) . For education papers, we also focused on the CS2023 categories, and tried to relate them to the research areas - and ultimately identify gaps and areas that are not yet considered in education. Based on this analysis, we then selected the respective ACM curriculum categories (i.e., Knowledge Units) where the proposed approach, tool, or research could be relevant or applied. After this step, we consolidated all assessments, discussed and resolved conflicts until mutual agreement was achieved, and tried to identify clusters of tools and approaches, as well as gaps. The final goal of the study is to derive a set of concrete actionable guidelines for different courses part of the CS curriculum, with a set of tools and approaches that could be used, and aspects that need to be considered. In the following sections, we discuss the initial assignment of approaches to the CS2023 Knowledge Units and potential application scenarios.2.2Related WorkSeveral systematic reviews have investigated and classified aspects of AI applications in education. Bittencourtet al.[4]conducted an analysis exploring the intersection of positive psychology and AI in education (P-AIED) , identifying P-AIED as a new global movement focusing on positive emotion and engagement related to AI-supported teaching and learning. Kuhailet al.[21]and Okonkwo and Ade-Ibijola[29]performed studies related to the application of chatbots in education. Kuhailet al.analyzed work on educational chatbots and found that these are primarily used in computer science, language learning, general education, and to a lesser extent in fields like engineering and mathematics. Most chatbots were web-based and more than half functioned as teaching agents, while over a third acted as peer agents. Okonkwo and Ade-Ibijola analyze 53 primary studies and identify an increasing trend in the application of chatbots in education.Zhaiet al.[41]analyzed 100 papers focusing on the application of AI in the education sector from 2010 to 2020. They classified primary studies into a development layer (classification, matching, recommendation, and deep learning) , an application layer (feedback, reasoning, and adaptive learning) , and an integration layer (affection computing, role-playing, immersive learning, and gamification) . Chenet al.[6]performed a study investigating the impact of AI on education, focusing on its application and effects in administrative, instructional, and learning contexts. Similarly, Zawacki-Richteret al.[40]conducted a systematic review of research on the application of AI in higher education. The majority of research comes from the fields of Computer Science and STEM with AI being mainly used in areas of student profiling and predictive analytics, as well as intelligent tutoring systems.Several studies have focused on AI in computer science education. Dennyet al.[9]explore the challenges and opportunities presented by recent advances in AI, particularly code generation models, and their potential impact on computing education. Kalles[19]reports on the experience of using AI systems as the basis of educating IT students. The application areas were mainly decision tree lifecycle management and board game learning mechanisms. Daun and Brings[8]discuss the role of generative AI technologies, such as ChatGPT, in SE education. They discuss potential risks associated with students' use of generative AI but also identify several opportunities that these technologies can offer in enhancing educational practices such as individualization of education and personalized feedback.Current work in this regard primarily addresses only parts of SE education, for example, programming. With our work, we aim to cover a broader spectrum of SE education, leveraging the ACM Computer Science Curriculum Guidelines.",
                "abstract": "Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas."
            },
            {
                "name": "A Qualitative Study on Using ChatGPT for Software Security: Perception vs. Practicality",
                "arxiv_id": "2408.00435",
                "subtitles": [
                    "AI-Based Chatbots for Software Security",
                    "Vulnerability Detection Using Transformer-based Language Models"
                ],
                "reference": [
                    "Considerations for evaluating large language models for cybersecurity tasks",
                    "Transformer-based vulnerability detection in code at edittime: Zero-shot, few-shot, or fine-tuning",
                    "When chatgpt meets smart contract vulnerability detection: How far are we",
                    "Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities",
                    "Apibot: question answering bot for api documentation",
                    "New tricks to old codes: can ai chatbots replace static code analysis tools",
                    "Data quality for software vulnerability datasets",
                    "An empirical comparison of transformer-based models in vulnerability prediction",
                    "Transformer-based language models for software vulnerability detection",
                    "Conversational devbots for secure programming: An empirical study on skf chatbot",
                    "A new approach to web application security: Utilizing gpt language models for source code inspection",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Can large language models identify and reason about security vulnerabilities? not yet",
                    "Llbezpeky: Leveraging large language models for vulnerability detection",
                    "Breaking the silence: the threats of using llms in software engineering",
                    "Investigating user perceptions of conversational agents for software-related exploratory web search",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Prompt-enhanced software vulnerability detection using chatgpt"
                ],
                "related_work": "IIRelated WorkII-AAI-Based Chatbots for Software Security.Chatbots have been demonstrated to be suitable for software engineering tasks such as information retrieval[5]and API usage[16]. However, the use of chatbots for software security is limited. To date, the SKF-Chatbot (Secure Knowledge Framework) is one of the leading software security chatbots, which was developed by the OWASP Foundation222https://owasp.org/to provide access to Software Vulnerability (SV) information via a chat interface. However, its capabilities are limited due to its lack of contextual understanding[8]. Recently, AI-based chatbots have been able to overcome these limitations via the LLMs that power them. ChatGPT has thus far demonstrated remarkable capabilities for semantic comprehension and providing tailored solutions for given tasks[10].II-BVulnerability Detection Using Transformer-based Language Models.Prior studies have explored the use of transformer-based language models to identify software vulnerabilities using a variety of fine-tuning methodologies. For example, Thapa et al.[12]evaluated transformer-based language models (i.e., BERTBase, GPT-2 Base) against recurrent neural network (RNN) -based models (i.e., BiLSTM, BiGRU) for vulnerability detection in software datasets featuring C/C++ source code. According to their findings, transformer-based language models outperformed RNN-based models on all evaluation metrics. In another study, Kalouptsoglou et al.[17]fine-tuned various transformer-based models (i.e., BERT variants, GPT-2, BART) and carried out a comparative analysis to determine which of these models is the most appropriate for vulnerability detection. Furthermore, Chan et al.[18]created a vulnerability detection model targeted at detecting vulnerabilities in incomplete code snippets. They leveraged common learning approaches such as fine-tuning on three pre-trained LLMs, namely CodeBERT, code-davinci-002, and text-davinci-003.II-CVulnerability Detection Using ChatGPT.Several research efforts have explored the efficacy of ChatGPT in vulnerability detection. Chen et al.[19]exclusively probed ChatGPT's capability in identifying smart contract vulnerabilities. Szab\u00f3 et al.[20]focused on identifying vulnerabilities associated with CWE-653 (Improper Isolation or Compartmentalization) using multiple GPT models. Ozturk et al.[21]assessed ChatGPT's effectiveness in detecting the top 10 OWASP vulnerability categories in web applications. Zhang et al.,[22]through comprehensive testing on two Java and C/C++ vulnerability datasets, demonstrated that prompt-enhanced approaches could strengthen ChatGPT's vulnerability detection capability. In a recent study, Fu et al.[15]compared the capabilities of ChatGPT (i.e., GPT-3.5 and GPT-4) against three other LLMs (e.g., CodeBERT[23]) across various vulnerability tasks. The LLMs used in this study were fine-tuned explicitly for code-related tasks.There are two main ways in which our study differs from prior studies:1.Prior research has primarily focused on assessing[21,12,17,15]and enhancing ChatGPT's vulnerability detection performance through prompt engineering methodologies[24,25,22]or the integration of supplementary elements[26]within the LLM pipeline. We instead strive to focus more on insights through qualitative analysis of existing real-world uses and outputs. This shift aims to illuminate the disparity between practitioners' perceived value and the practicality of this technology in real-world settings. We provide further details in SectionIV.2.Prior studies that utilised datasets from existing literature[17,22,19,15]or open-source projects[12,18,20]ignored the possibility of data leakage (i.e., overlap between training and test data) . This factor could artificially inflate the reported performance of AI models' (e.g., ChatGPT) [27,28,29]. Consequently, we curated a high-quality vulnerability dataset tailored to our evaluation criteria from unseen data. SectionIII-B2discusses this in more detail.",
                "abstract": "Artificial Intelligence (AI) advancements have enabled the development of Large Language Models (LLMs) that can perform a variety of tasks with remarkable semantic understanding and accuracy. ChatGPT is one such LLM that has gained significant attention due to its impressive capabilities for assisting in various knowledge-intensive tasks. Due to the knowledge-intensive nature of engineering secure software, ChatGPT's assistance is expected to be explored for security-related tasks during the development/evolution of software. To gain an understanding of the potential of ChatGPT as an emerging technology for supporting software security, we adopted a two-fold approach. Initially, we performed an empirical study to analyse the perceptions of those who had explored the use of ChatGPT for security tasks and shared their views on Twitter. It was determined that security practitioners view ChatGPT as beneficial for various software security tasks, including vulnerability detection, information retrieval, and penetration testing. Secondly, we designed an experiment aimed at investigating the practicality of this technology when deployed as an oracle in real-world settings. In particular, we focused on vulnerability detection and qualitatively examined ChatGPT outputs for given prompts within this prominent software security task. Based on our analysis, responses from ChatGPT in this task are largely filled with generic security information and may not be appropriate for industry use. To prevent data leakage, we performed this analysis on a vulnerability dataset compiled after the OpenAI data cut-off date from real-world projects covering 40 distinct vulnerability types and 12 programming languages. We assert that the findings from this study would contribute to future research aimed at developing and evaluating LLMs dedicated to software security."
            },
            {
                "name": "Harnessing the Power of LLMs: Automating Unit Test Generation for High-Performance Computing",
                "arxiv_id": "2407.05202",
                "subtitles": [
                    "Research in parallel and high performance software unit testing",
                    "Evaluation of the LLMs in code generation",
                    "Application of the LLMs for unit testing"
                ],
                "reference": [
                    "Towards automatic and flexible unit test generation for legacy hpc code",
                    "A study on robustness and reliability of large language model code generation",
                    "Unit test case generation with transformers and focal context",
                    "Integrating mpi-based numerical software into an advanced parallel computing environment",
                    "No more manual tests? evaluating and improving chatgpt for unit test generation",
                    "Towards test driven development for computational science with pfunit",
                    "Optimizing the parallel scheme of the poisson solver for the reduced kinetic code teresa",
                    "Evaluating large language models trained on code",
                    "Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code",
                    "A systematic evaluation of large language models of code",
                    "Asleep at the keyboard? assessing the security of github copilot's code contributions",
                    "Test-driven development in hpc science: A case study",
                    "Assessing the quality of github copilot's code generation",
                    "Large language models and simple, stupid bugs",
                    "Do users write more insecure code with ai assistants"
                ],
                "related_work": "8Related WorkThe related work can be broadly categorized into the following areas.Research in parallel and high performance software unit testingAlthough TDD (Test-driven development) and unit testing framework are well established practice in software engineering, adopting them in the context of high-performance computing environments often faces unique requirements and challenges. High performance computing developers may encounter problems that differ from other application areas of programmingNanthaamornphong and Carver (2018) . For instance, a specific need in parallel and high performance computing is to develop tools for supporting unit tests that can be integrated with certain legacy HPC codeHovy and Kunkel (2016) . Another effort is to create unit testing framework tailored for the HPC environments and applications, for instance extension of unit testing framework to support parallelization techniques in high performance applications such as OpenMP and MPI.Evaluation of the LLMs in code generationAnother area of related work is evaluation of the LLMs for code generationChen et al. (2021a) ; Xu et al. (2022) . A particular topic that has attracted large amount of attention from the research community is to evaluate the code quality and security aspect of the LLM generated codePearce et al. (2021) ; Perry et al. (2022) . For instance, inYetistiren et al. (2022) , the authors have assessed the quality of the LLM generated code from multiple aspects such as compilation, functional correctness and code efficiency. Researchers also conducted studies of the bugs in the LLM generated code, which shows that there is a high chance that the LLMs may introduce difficult to detect bugs in the generated codeJesse et al. (2023) . Besides bugs in the LLM generated code, researchers have scrutinized and assessed the code generated by the LLMs for security vulnerabilitiesPearce et al. (2021) . Quality of the LLM generated code is also investigated from the reliability and robustness perspectiveZhong and Wang (2023) .Application of the LLMs for unit testingLast but not the least, researchers have explored and evaluated applicability of the LLMs for unit test generationYuan et al. (2023) . For instance, Barei\u00df et al.Barei\u00df et al. (2022) examined the performance of Codex in Java code test generation. They applied a few-shot learning approach, where the model was provided with a function to be tested, an example of another function, and an associated test. This allowed the model to learn the expected structure of a test. In another studyTufano et al. (2020) , the authors evaluated test coverage of a BART transformer model fine-tuned on a training dataset consisting of functions and tests.High-Performance Computing (HPC) has become a pivotal tool for scientific and engineering advancements, enabling the resolution of intricate problems that demand substantial computational prowessD'Ambra et al. (2003) . As these challenges intensify, the imperative for robust and efficient software solutions becomes evident. This realization has spurred a growing inclination towards the incorporation of established software engineering practices, such as Test-Driven Development (TDD) and unit testing, within the high performance realmRilee and Clune (2014) . However, the transition isn't straightforward. The unique requirements of parallel and high performance domain, like managing vast datasets, safeguarding data integrity across distributed systems, and optimizing for performance on dedicated hardware, pose distinct challengesCartier-Michaud et al. (2013) .This work distinguishes from all the related work with its focus on high performance related applications and parallel code. To the best of our knowledge, this is the first step investigating and evaluating the LLMs in the context of parallel and high performance programming and testing. Although the study only scratches the surface of this potentially rich area of endeavor, it shows initial results that could pave the way for further research and investigation.",
                "abstract": "Unit testing is crucial in software engineering for ensuring quality. However, it's not widely used in parallel and high-performance computing software, particularly scientific applications, due to their smaller, diverse user base and complex logic. These factors make unit testing challenging and expensive, as it requires specialized knowledge and existing automated tools are often ineffective.To address this, we propose an automated method for generating unit tests for such software, considering their unique features like complex logic and parallel processing. Recently, large language models (LLMs) have shown promise in coding and testing. We explored the capabilities of Davinci (text-davinci-002) and ChatGPT (gpt-3.5-turbo) in creating unit tests for C++ parallel programs. Our results show that LLMs can generate mostly correct and comprehensive unit tests, although they have some limitations, such as repetitive assertions and blank test cases."
            },
            {
                "name": "Studying LLM Performance on Closed- and Open-source Data",
                "arxiv_id": "2402.15100",
                "subtitles": [
                    "LLMs in SE",
                    "LLMs Generalization"
                ],
                "reference": [
                    "Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction",
                    "CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models",
                    "Memorization and generalization in neural code intelligence models",
                    "Learning to Represent Programs with Graphs",
                    "code2seq: Generating sequences from structured representations of code",
                    "Impact of Code Language Models on Automated Program Repair",
                    "code2vec: Learning distributed representations of code",
                    "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
                    "Few-shot training LLMs for project-specific code-summarization",
                    "Typewriter: Neural type prediction with search-based validation",
                    "Evaluating large language models trained on code",
                    "Better patching using LLM prompting, via Self-Consistency",
                    "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Copiloting the copilots: Fusing large language models with completion engines for automated program repair",
                    "Codegen2: Lessons for training llms on programming and natural languages",
                    "A systematic evaluation of large language models of code",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Automated repair of programs from large language models",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Attention is all you need",
                    "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization",
                    "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
                    "Towards Autonomous Testing Agents via Conversational Large Language Models",
                    "Global relational models of source code"
                ],
                "related_work": "7.Related Work7.1.LLMs in SELLMs are widely used in software engineering tasks, including code summarization(Chen et al.,2021; Ahmed and Devanbu,2022a; Ahmed et al.,2024) , code generation(Chen et al.,2021; Xu et al.,2022; Nijkamp et al.,2022,2023; Roziere et al.,2023) , program repair(Wei et al.,2023; Nashid et al.,2023; Ahmed and Devanbu,2023; Jiang et al.,2023; Fan et al.,2023a) , and testing(Kang et al.,2023; Feldt et al.,2023) . Several new ideas are evolving, such as chain-of-thoughts(Wei et al.,2022) , self-consistency(Wang et al.,2022) , and retrieval augmented generation (RAG) (Nashid et al.,2023) . Few-shot learning has mostly been used for in-context learning, and other ideas are still under consideration. However, no study has been performed comparing OSS and closed-source data in different OSS datasets. Studying OSS and closed-source data addresses two questions: i) Can LLMs perform as well on closed-source samples not seen in the training data? ii) Can OSS data help (viain-context learning) improve performance on closed-source data? In this paper, we have tried to answer these two questions. Though we could not precisely justify the reasons for differences, we provided some possible directions for future studies.7.2.LLMs GeneralizationIn prior studies with deep learning models, it was found that the model's performance on open-source and proprietary samples was comparable(Pradel et al.,2020) . However, these earlier models had relatively smaller training datasets that were easily accessible. With the emergence of Large Language Models (LLMs) , which are trained on billions of tokens, it becomes challenging to assess their generalization ability. Existing datasets such as HumanEval(Chen et al.,2021) , HUMANEVAL +(Liu et al.,2023) , and CoderEval(Yu et al.,2023) are used to evaluate the model's code synthesis capability. Still, they are often limited in size (a few hundred samples) and primarily consist of self-contained algorithmic problems, which may not represent real-world software development scenarios. Establishing a connection between open-source and proprietary projects is crucial to building trust with industry users. Rabin et al. conducted experiments to measure the extent of memorization in models by introducing random noise to the original training dataset(Rabin et al.,2023) . They used various metrics to quantify the impact of noise on different aspects of training and testing. The findings indicated that all models showed some degree of memorization. This could be concerning for code intelligence tasks that rely on noisy and repetitive data sources, such as code from GitHub. However, it's important to note that the models evaluated in their study (Code2Seq(Alon et al.,2018) , Code2Vec(Alon et al.,2019) , GGNN(Allamanis et al.,2018b) , GREAT(Hellendoorn et al.,2019) , Transformers(Vaswani et al.,2017) , CodeBERT(Feng et al.,2020) ) significantly differ from the recent LLMs' size and nature. Moreover, conducting such studies without access to the LLMs' training data is not feasible.",
                "abstract": "Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS --> proprietary code, but does significantly reduce for C++; we find that this difference is attributable to differences in identifiers. We also find that some performance degradation, in some cases, can be ameliorated efficiently by in-context learning."
            },
            {
                "name": "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios",
                "arxiv_id": "2403.19287",
                "subtitles": [
                    "Large Language Models for Software Engineering",
                    "Coder Benchmark"
                ],
                "reference": [
                    "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                    "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation",
                    "Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction",
                    "StarCoder: may the source be with you",
                    "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
                    "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
                    "MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation",
                    "CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models",
                    "Defects4J: a database of existing faults to enable controlled testing studies for Java programs",
                    "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis",
                    "Code Llama: Open Foundation Models for Code",
                    "Attention is All you Need",
                    "Program Synthesis with Large Language Models",
                    "Evaluating Large Language Models Trained on Code",
                    "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"
                ],
                "related_work": "2.Background and Related Work2.1.Large Language Models for Software EngineeringRecently, many code LLMs(Chen et al.,2021; Nijkamp et al.,2023; Li et al.,2023; Rozi\u00e8re et al.,2023) have been proposed to solve software engineering tasks. These models typically leverage the Transformer Decoder(Vaswani et al.,2017) architecture and undergo extensive training on large-scale code databases(Chen et al.,2021; Nijkamp et al.,2023; Li et al.,2023) , expecting excellence in executing code-intensive tasks.A prominent example within this realm is CodeX(Chen et al.,2021) . For the first time, they increased the model parameter size to 12 billion (B) and used 159 gigabytes (GB) of Python code data for pre-training. They also proposed the widely adopted benchmark HumanEval and the pass@k for evaluating the quality of generated code, in which CodeX achieved a pass@k\\text{=}1 of 28.81 on HumanEval. Subsequently, StarCoderBase(Li et al.,2023) further scaled both the parameter size to 15B and the training dataset to 799.37 GB code corpus, which contained various programming languages. We designate those LLMs that have not undergone specialized training for particular tasks (e.g., Python programming challenges or question-and-answer activities) as  \"Base LLM \".As mentioned, to enhance Python-specific performance on tasks like HumanEval, researchers continued pre-training the base model StarCoderBase on an additional 35B tokens of Python code, yielding a Python-enhanced version of the model StarCoder-Python. Ultimately, StarCoderBase and StarCoder-Python achieved a pass@k\\text{=}1 of 30.4 and 33.6 on HumanEval, respectively. We designate those LLMs that have continued pre-training in specific programming language corpus as  \"Specific Programming Language (PL) Base \".Moreover, the team behind WizardCoder(Luo et al.,2023) found that fine-tuning base LLMs with high-quality instruction samples (i.e., more diverse and more challenging coding problems) further boosted the LLMs programming performance. They used an evolutionary instruction-based data collection strategy to collect 80k instruction fine-tuning samples from ChatGPT. Utilizing these novel datasets, they fine-tuned a  \"Instruction Tuned \" LLM WizardCoder, significantly enhancing the HumanEval pass@k\\text{=}1 rate from 33.57 to an impressive 58.12.Despite the progress, the prevailing methods of evaluating these code LLMs tend to focus on simple Python-based programming puzzles like HumanEval and lack a comprehensive assessment of other programming tasks in software engineering. This limited evaluation does not fully capture the advancements made in code LLMs, nor does it comprehensively evaluate the impact of specialized training processes such as continued pre-training in a specific language code (mostly in Python) and instructional fine-tuning on various practical software engineering scenarios. In response to these limitations, our study introduces a multi-programming task, executable and real-world programming scenario-compliant evaluation benchmark, and provides an in-depth study to address the questions mentioned above.2.2.Coder BenchmarkRecent scholarly efforts have proposed many benchmarks(Chen et al.,2021; Austin et al.,2021; Cassano et al.,2023; Yu et al.,2023; Khan et al.,2023) to evaluate the programmatic skill of LLMs as presents in Table1. The HumanEval(Chen et al.,2021) benchmark stands out in this field, consisting of 164 manually designed Python programming questions. Each question provides function signatures, comments, function bodies, and multiple unit tests. LLMs are tasked with crafting function bodies informed by the given signatures and comments. Subsequently, the generated functions are executed and evaluated regarding their success in passing the unit tests tied to each question. We refer to such non-test-case code generation questions as functional code generation tasks. Later, CoderEval noticed that the questions in HumanEval are simple  \"self-contained \"(Yu et al.,2023) function generation tasks that each function only has language built-in dependency and do not match the actual development scenarios that rely on multiple public libraries and project files. Therefore, they introduced 460 Java and Python code generation questions derived from real projects on GitHub that are more aligned with actual development settings.In addition to mainstream functional code generation tasks, there are other types of datasets designed to evaluate LLMs. For example, Defects4j(Just et al.,2014) , a seminal benchmark in automated program repair, catalogs 835 defects across 17 authentic Java projects from Github. It offers both the defective and fixed versions of the code alongside related test cases. In this evaluative phase, the fix code generated by LLMs is executed, and the accompanying test cases verify the correctness of the fix. Based on Defects4j, Libro(Kang et al.,2023) has developed a benchmark for generating issue-specific tests. This requires LLMs to generate test cases that trigger the corresponding defects based on a given issue report. Additionally, the ChatTester(Yuan et al.,2023b) benchmark, designated for code-based test generation tasks, collects 1,000 test generation questions from open-source Java projects on GitHub. Each test question contains the code needed for testing, a natural language description of the task, and a test case. LLMs need to generate test cases based on the code under test and the natural language description.Besides the single-task benchmark mentioned above, previous researchers have proposed two multi-programming task benchmarks, CodeXGLUE(Lu et al.,2021) and XCodeEval(Khan et al.,2023) , that incorporate a broad range of questions and tasks, establishing a more substantial framework for evaluating LLMs. However, CodeXGLUE still uses textual similarity metrics, such as BLEU and CodeBLEU(Ren et al.,2020) , thereby falling short of actual code execution. XCodeEval focuses on programming competition questions, which, like HumanEval, are not  \"Project-Runnable \" and do not match the actual development scenarios. Therefore, CoderUJB fills the current gap in the field as a benchmark that simultaneously includes multiple programming tasks that are executable and match real-world development scenarios, thus broadening the scope of evaluation for LLMs across various practical programming tasks.",
                "abstract": "In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering."
            }
        ],
        "survey": {
            "name": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
            "arxiv_id": "2408.02479",
            "subtitles": [
                {
                    "name": "EXISTING WORKS AND THE SURVEY STRUCTURE",
                    "key_history": [
                        {
                            "reference_title": "Large language models for software engineering: Survey and open problems",
                            "key_word": "LLMs in Requirement Engineering"
                        },
                        {
                            "reference_title": "Chain-of-thought prompting elicits reasoning in large language models",
                            "key_word": "LLMs and SE Task Surveys"
                        },
                        {
                            "reference_title": "The rise and potential of large language model based agents: A survey",
                            "key_word": "Methodology in LLM-based Agents Survey"
                        }
                    ],
                    "references_in_this_section": [
                        "The rise and potential of large language model based agents: A survey",
                        "Robustness, security, privacy, explainability, efficiency, and usability of large language models for code",
                        "Large language models for software engineering: A systematic literature review",
                        "Generative artificial intelligence for software engineering - a research agenda",
                        "Generative software engineering",
                        "Large language models for software engineering: Survey and open problems",
                        "Towards an understanding of large language models in software engineering tasks",
                        "Lms: Understanding code syntax and semantics for code analysis",
                        "Chain-of-thought prompting elicits reasoning in large language models"
                    ]
                },
                {
                    "name": "Preliminaries",
                    "key_history": [
                        {
                            "reference_title": "Foundations of statistical natural language processing",
                            "key_word": "Historical Development of LLMs in NLP"
                        },
                        {
                            "reference_title": "Codet5+: Open code large language models for code understanding and generation",
                            "key_word": "Model Architecture"
                        },
                        {
                            "reference_title": "The evolution of computing: Alphago",
                            "key_word": "LLM-Based Agents and Embodied Systems"
                        },
                        {
                            "reference_title": "Llm-powered data augmentation for enhanced cross-lingual performance",
                            "key_word": "Data Augmentation Techniques for LLMs"
                        },
                        {
                            "reference_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                            "key_word": "Prompt Engineering for LLMs"
                        },
                        {
                            "reference_title": "Survey of hallucination in natural language generation",
                            "key_word": "Single-Agent Systems"
                        },
                        {
                            "reference_title": "Nissist: An incident mitigation copilot based on troubleshooting guides",
                            "key_word": "Multi-Agent Systems and Collaboration"
                        },
                        {
                            "reference_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
                            "key_word": "LLM Applications in Software Engineering"
                        },
                        {
                            "reference_title": "Repairagent: An autonomous, llm-based agent for program repair",
                            "key_word": "Emerging Applications of LLMs in Software Testing and Repair"
                        },
                        {
                            "reference_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
                            "key_word": "LLMs in Software Security"
                        }
                    ],
                    "references_in_this_section": [
                        "Llm-powered data augmentation for enhanced cross-lingual performance",
                        "The rise and potential of large language model based agents: A survey",
                        "More agents is all you need",
                        "Repairagent: An autonomous, llm-based agent for program repair",
                        "Opt: Open pre-trained transformer language models",
                        "Palm: Scaling language modeling with pathways",
                        "React: Synergizing reasoning and acting in language models",
                        "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
                        "Foundations of statistical natural language processing",
                        "Survey of hallucination in natural language generation",
                        "Gpt-3: Its nature, scope, limits, and consequences",
                        "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
                        "Long short-term memory",
                        "The evolution of computing: Alphago",
                        "Alpacafarm: A simulation framework for methods that learn from human feedback",
                        "Voyager: An open-ended embodied agent with large language models",
                        "Expel: Llm agents are experiential learners",
                        "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                        "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                        "Bert: Pre-training of deep bidirectional transformers for language understanding",
                        "Language models are few-shot learners",
                        "Attention is all you need",
                        "Nissist: An incident mitigation copilot based on troubleshooting guides",
                        "Llama: Open and efficient foundation language models",
                        "Codet5+: Open code large language models for code understanding and generation",
                        "Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain",
                        "Chatgpt: Optimizing language models for dialogue"
                    ]
                },
                {
                    "name": "Requirement Engineering and and Documentation",
                    "key_history": [
                        {
                            "reference_title": "Prcbert: Prompt learning for requirement classification using bert-based pretrained language models",
                            "key_word": "Requirement Classification"
                        },
                        {
                            "reference_title": "Evaluation of chatgpt on requirements information retrieval under zero-shot setting",
                            "key_word": "Requirement Elicitation"
                        },
                        {
                            "reference_title": "Using llms in software requirements specifications: An empirical evaluation",
                            "key_word": "SRS Generation"
                        },
                        {
                            "reference_title": "Specgen: Automated generation of formal program specifications via large language models",
                            "key_word": "Specification Generation"
                        },
                        {
                            "reference_title": "ChatGPT Prompt Patterns for Improving Code Quality",
                            "key_word": "Requirement Generation"
                        },
                        {
                            "reference_title": "Improving requirements completeness: Automated assistance through large language models",
                            "key_word": "Requirement Completeness Enhancement"
                        },
                        {
                            "reference_title": "Identification of intra-domain ambiguity using transformer-based machine learning",
                            "key_word": "Ambiguity Detection in Requirement Documents"
                        },
                        {
                            "reference_title": "Chatgpt as a tool for user story quality evaluation: Trustworthy out of the box",
                            "key_word": "User Story Quality Evaluation"
                        },
                        {
                            "reference_title": "Experimenting a new programming practice with llms",
                            "key_word": "Autonomous Software Development"
                        },
                        {
                            "reference_title": "Engineering safety requirements for autonomous driving with large language models",
                            "key_word": "Safety Requirement Generation for Autonomous Driving"
                        }
                    ],
                    "references_in_this_section": [
                        "Improving requirements completeness: Automated assistance through large language models",
                        "Experimenting a new programming practice with llms",
                        "Impact of large language models on generating software specifications",
                        "Norbert: Transfer learning for requirements classification",
                        "Investigating chatgpt's potential to assist in requirements elicitation processes",
                        "Tabasco: A transformer based contextualization toolkit",
                        "Llm-based agents for automating the enhancement of user story quality: An early report",
                        "Using llms in software requirements specifications: An empirical evaluation",
                        "Specgen: Automated generation of formal program specifications via large language models",
                        "Engineering safety requirements for autonomous driving with large language models",
                        "Leveraging transformer-based language models to automate requirements satisfaction assessment",
                        "Chatgpt as a tool for user story quality evaluation: Trustworthy out of the box",
                        "Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain",
                        "Houdini, an annotation assistant for esc/java",
                        "Identification of intra-domain ambiguity using transformer-based machine learning",
                        "Evaluation of chatgpt on requirements information retrieval under zero-shot setting",
                        "Advancing requirements engineering through generative ai: Assessing the role of llms",
                        "Prcbert: Prompt learning for requirement classification using bert-based pretrained language models",
                        "Bert: Pre-training of deep bidirectional transformers for language understanding",
                        "ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
                        "Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain"
                    ]
                },
                {
                    "name": "Code Generation and Software Development",
                    "key_history": [
                        {
                            "reference_title": "Evaluating large language models trained on code",
                            "key_word": "Codex for Code Generation and Debugging"
                        },
                        {
                            "reference_title": "Sql-palm: Improved large language model adaptation for text-to-sql (extended) ",
                            "key_word": "Text-to-SQL"
                        },
                        {
                            "reference_title": "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x",
                            "key_word": "Multilingual Code Generation"
                        },
                        {
                            "reference_title": "Leveraging print debugging to improve code generation in large language models",
                            "key_word": "Print Debugging"
                        },
                        {
                            "reference_title": "The impact of ai on developer productivity: Evidence from github copilot",
                            "key_word": "Code Completion"
                        },
                        {
                            "reference_title": "Codegen: An open large language model for code with multi-turn program synthesis",
                            "key_word": "Multi-turn Program Synthesis"
                        },
                        {
                            "reference_title": "Cycle: Learning to self-refine the code generation",
                            "key_word": "Code Self-Improvement"
                        },
                        {
                            "reference_title": "Self-collaboration code generation via chatgpt",
                            "key_word": "Self-Collaboration Framework with Multi-Agents"
                        },
                        {
                            "reference_title": "Metagpt: Meta programming for a multi-agent collaborative framework",
                            "key_word": "Automated Software Development"
                        },
                        {
                            "reference_title": "Toolformer: Language models can teach themselves to use tools",
                            "key_word": "API Integration in Code Generation"
                        }
                    ],
                    "references_in_this_section": [
                        "When llm-based code generation meets the software development process",
                        "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                        "The impact of ai on developer productivity: Evidence from github copilot",
                        "Experimenting a new programming practice with llms",
                        "L2MAC: Large language model automatic computer for extensive code generation",
                        "Cycle: Learning to self-refine the code generation",
                        "Toolformer: Language models can teach themselves to use tools",
                        "Incoder: A generative model for code infilling and synthesis",
                        "Autonomous agents in software development: A vision paper",
                        "Self-collaboration code generation via chatgpt",
                        "Agentcoder: Multi-agent-based code generation with iterative testing and optimisation",
                        "Evaluating large language models trained on code",
                        "Leveraging print debugging to improve code generation in large language models",
                        "L2ceval: Evaluating language-to-code generation capabilities of large language models",
                        "Alpacafarm: A simulation framework for methods that learn from human feedback",
                        "Codecompose: A large-scale industrial deployment of ai-assisted code authoring",
                        "Metagpt: Meta programming for a multi-agent collaborative framework",
                        "Github copilot: Your ai pair programmer",
                        "Codegen: An open large language model for code with multi-turn program synthesis",
                        "Codet5+: Open code large language models for code understanding and generation",
                        "Opencodeinterpreter: Integrating code generation with execution and refinement",
                        "Self-planning code generation with large language models",
                        "Codepori: Large scale model for autonomous software development by using multi-agents",
                        "Sql-palm: Improved large language model adaptation for text-to-sql (extended",
                        "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x"
                    ]
                },
                {
                    "name": "Autonomous Learning and Decision Making",
                    "key_history": [
                        {
                            "reference_title": "Large language models can self-improve",
                            "key_word": "Voting Inference"
                        },
                        {
                            "reference_title": "Teaching large language models to self-debug",
                            "key_word": "Autonomous Debugging"
                        },
                        {
                            "reference_title": "On the creativity of large language models",
                            "key_word": "Creativity and Decision-Making"
                        },
                        {
                            "reference_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
                            "key_word": "LLMs as Judges in Evaluation Tasks"
                        },
                        {
                            "reference_title": "Rethinking the bounds of llm reasoning: Are multi-agent discussions the key",
                            "key_word": "Multi-Agent Reasoning"
                        },
                        {
                            "reference_title": "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
                            "key_word": "Multi-Modal Decision Making"
                        },
                        {
                            "reference_title": "Reflexion: Language agents with verbal reinforcement learning",
                            "key_word": "Self-Reflection and Language Feedback"
                        },
                        {
                            "reference_title": "React: Synergizing reasoning and acting in language models",
                            "key_word": "Autonomous Learning"
                        },
                        {
                            "reference_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
                            "key_word": "Multi-Agent Framework"
                        },
                        {
                            "reference_title": "Camel: Communicative agents for \" mind \" exploration of large language model society",
                            "key_word": "Autonomous Agent Collaboration"
                        },
                        {
                            "reference_title": "Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents",
                            "key_word": "Multi-Agent Coordination"
                        },
                        {
                            "reference_title": "Self: Self-evolution with language feedback",
                            "key_word": "Self-Evolution and Human Trust Simulation with LLM-based Agents"
                        }
                    ],
                    "references_in_this_section": [
                        "More agents is all you need",
                        "Llm agents can autonomously exploit one-day vulnerabilities",
                        "Judging llm-as-a-judge with mt-bench and chatbot arena",
                        "React: Synergizing reasoning and acting in language models",
                        "Reflexion: Language agents with verbal reinforcement learning",
                        "Getting pwn'd by ai: Penetration testing with large language models",
                        "Agentlite: A lightweight library for building and advancing task-oriented llm agent system",
                        "On the creativity of large language models",
                        "Tree of thoughts: Deliberate problem solving with large language models",
                        "Can large language model agents simulate human trust behaviors",
                        "Rethinking the bounds of llm reasoning: Are multi-agent discussions the key",
                        "Camel: Communicative agents for",
                        "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
                        "Expel: Llm agents are experiential learners",
                        "Large language models in law: A survey",
                        "Large language models can self-improve",
                        "Explainable automated debugging via large language model-driven scientific debugging",
                        "Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents",
                        "Language agents as optimizable graphs",
                        "Are more llm calls all you need? towards scaling laws of compound inference systems",
                        "Combining fine-tuning and llm-based agents for intuitive smart contract auditing with justifications",
                        "Towards autonomous testing agents via conversational large language models",
                        "Teaching large language models to self-debug",
                        "Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis",
                        "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
                        "Self: Self-evolution with language feedback"
                    ]
                },
                {
                    "name": "Software Design and Evaluation",
                    "key_history": [
                        {
                            "reference_title": "Evalullm: Llm assisted evaluation of generative outputs",
                            "key_word": "Reference-based Evaluation"
                        },
                        {
                            "reference_title": "Software/hardware co-design for llm and its application for design verification",
                            "key_word": "Chrysalis Dataset for HLS Debugging"
                        },
                        {
                            "reference_title": "Data-driven prototyping via natural-language-based gui retrieval",
                            "key_word": "Data-driven GUI Prototyping"
                        },
                        {
                            "reference_title": "Communicative agents for software development",
                            "key_word": "Virtual Chat-Driven Software Development"
                        },
                        {
                            "reference_title": "Software engineering using autonomous agents: Are we there yet",
                            "key_word": "Software Design"
                        },
                        {
                            "reference_title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face",
                            "key_word": "Orchestrating AI Tasks"
                        },
                        {
                            "reference_title": "Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments",
                            "key_word": "Evaluating in Dynamic Multi-Agent Systems"
                        },
                        {
                            "reference_title": "Flows: Building blocks of reasoning and collaborating ai",
                            "key_word": "Human-AI Collaboration"
                        },
                        {
                            "reference_title": "Large language models as software components: A taxonomy for llm-integrated applications",
                            "key_word": "LLM-Integrated Applications"
                        },
                        {
                            "reference_title": "Agent-driven automatic software improvement",
                            "key_word": "Software Maintenance"
                        }
                    ],
                    "references_in_this_section": [
                        "More agents is all you need",
                        "Data-driven prototyping via natural-language-based gui retrieval",
                        "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                        "Chatgpt and software testing education: promises & perils",
                        "Exploring llm-based agents for root cause analysis",
                        "Software/hardware co-design for llm and its application for design verification",
                        "Large language models as software components: A taxonomy for llm-integrated applications",
                        "Toolformer: Language models can teach themselves to use tools",
                        "Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments",
                        "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face",
                        "On the creativity of large language models",
                        "Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences",
                        "Communicative agents for software development",
                        "Software engineering using autonomous agents: Are we there yet",
                        "Rethinking the bounds of llm reasoning: Are multi-agent discussions the key",
                        "Elicitron: An llm agent-based simulation framework for design requirements elicitation",
                        "Advancing requirements engineering through generative ai: Assessing the role of llms",
                        "Language agents as optimizable graphs",
                        "Software engineering education must adapt and evolve for an llm environment",
                        "Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents",
                        "Chatgpt: A study on its utility for ubiquitous software engineering tasks",
                        "Opencodeinterpreter: Integrating code generation with execution and refinement",
                        "Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis",
                        "Llm-based nlg evaluation: Current status and challenges",
                        "Flows: Building blocks of reasoning and collaborating ai",
                        "Agent-driven automatic software improvement",
                        "Evalullm: Llm assisted evaluation of generative outputs",
                        "Self: Self-evolution with language feedback",
                        "Batch prompting: Efficient inference with large language model apis"
                    ]
                },
                {
                    "name": "Software Test Generation",
                    "key_history": [
                        {
                            "reference_title": "Unit test case generation with transformers and focal context",
                            "key_word": "Unit test case generation"
                        },
                        {
                            "reference_title": "How well does llm generate security tests",
                            "key_word": "security test generation"
                        },
                        {
                            "reference_title": "Prompting is all you need: Automated android bug replay with large language models",
                            "key_word": "automated bug reproduction"
                        },
                        {
                            "reference_title": "Large language models are few-shot testers: Exploring llm-based general bug reproduction",
                            "key_word": "Bug Reproduction"
                        },
                        {
                            "reference_title": "Fuzz4all: Universal fuzzing with large language models",
                            "key_word": "LLM-Driven Universal Fuzz Testing"
                        },
                        {
                            "reference_title": "Code-aware prompting: A study of coverage guided test generation in regression setting using llm",
                            "key_word": "Code-Aware Prompting for Test Coverage"
                        },
                        {
                            "reference_title": "Coverup: Coverage-guided llm-based test generation",
                            "key_word": "High-Coverage Regression Testing"
                        },
                        {
                            "reference_title": "Llm-powered test case generation for detecting tricky bugs",
                            "key_word": "LLMs with Differential Testing for Fault Detection"
                        },
                        {
                            "reference_title": "Large language models as test case generators: Performance evaluation and enhancement",
                            "key_word": "Test Generation"
                        },
                        {
                            "reference_title": "Xuat-copilot: Multi-agent collaborative system for automated user acceptance testing with large language model",
                            "key_word": "Multi-Agent System for UAT Testing"
                        }
                    ],
                    "references_in_this_section": [
                        "Coverup: Coverage-guided llm-based test generation",
                        "Large language models as test case generators: Performance evaluation and enhancement",
                        "Large language models are few-shot testers: Exploring llm-based general bug reproduction",
                        "Towards autonomous testing agents via conversational large language models",
                        "Prompting is all you need: Automated android bug replay with large language models",
                        "A unified debugging approach via llm-based multi-agent synergy",
                        "Llm-powered test case generation for detecting tricky bugs",
                        "Unit test case generation with transformers and focal context",
                        "How well does llm generate security tests",
                        "Agentcoder: Multi-agent-based code generation with iterative testing and optimisation",
                        "Code-aware prompting: A study of coverage guided test generation in regression setting using llm",
                        "Xuat-copilot: Multi-agent collaborative system for automated user acceptance testing with large language model",
                        "Evaluating large language models trained on code",
                        "Fuzz4all: Universal fuzzing with large language models",
                        "Test mimicry to assess the exploitability of library vulnerabilities"
                    ]
                },
                {
                    "name": "Software Security and Maintenance",
                    "key_history": [
                        {
                            "reference_title": "Pentestgpt: An llm-empowered automatic penetration testing tool",
                            "key_word": "Automatic Penetration Testing Tool"
                        },
                        {
                            "reference_title": "Ritfis: Robust input testing framework for llms-based intelligent software",
                            "key_word": "Robust Input Testing Framework"
                        },
                        {
                            "reference_title": "Navrepair: Node-type aware c/c++ code vulnerability repair",
                            "key_word": "Vulnerability Repair"
                        },
                        {
                            "reference_title": "Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning",
                            "key_word": "Vulnerability Detection Using Code Property Graphs"
                        },
                        {
                            "reference_title": "Vulnerability detection with code language models: How far are we",
                            "key_word": "Code Language Models"
                        },
                        {
                            "reference_title": "Multi-objective fine-tuning for enhanced program repair with llms",
                            "key_word": "Automated Program Repair"
                        },
                        {
                            "reference_title": "A unified debugging approach via llm-based multi-agent synergy",
                            "key_word": "Multi-Agent Automated Debugging System"
                        },
                        {
                            "reference_title": "Repairagent: An autonomous, llm-based agent for program repair",
                            "key_word": "Dynamic Automated Program Repair"
                        },
                        {
                            "reference_title": "Combining fine-tuning and llm-based agents for intuitive smart contract auditing with justifications",
                            "key_word": "Smart Contract Auditing "
                        },
                        {
                            "reference_title": "Ldb: A large language model debugger via verifying runtime execution step-by-step",
                            "key_word": "Debugger for Enhanced Debugging"
                        }
                    ],
                    "references_in_this_section": [
                        "Ldb: A large language model debugger via verifying runtime execution step-by-step",
                        "Debug like a human: A large language model debugger via verifying runtime execution step-by-step",
                        "Ritfis: Robust input testing framework for llms-based intelligent software",
                        "Repairagent: An autonomous, llm-based agent for program repair",
                        "Llm agents can autonomously exploit one-day vulnerabilities",
                        "Llm-powered test case generation for detecting tricky bugs",
                        "Evaluation of chatgpt model for vulnerability detection",
                        "Multi-objective fine-tuning for enhanced program repair with llms",
                        "The hitchhiker's guide to program analysis: A journey with large language models",
                        "Repair is nearly generation: Multilingual program repair with llms",
                        "Navrepair: Node-type aware c/c++ code vulnerability repair",
                        "Getting pwn'd by ai: Penetration testing with large language models",
                        "An empirical evaluation of pre-trained large language models for repairing declarative formal specifications",
                        "Transformer-based language models for software vulnerability detection",
                        "Vulnerability detection with code language models: How far are we",
                        "Large language model-powered smart contract vulnerability detection: New perspectives",
                        "A novel approach for automatic program repair using round-trip translation with large language models",
                        "Engineering safety requirements for autonomous driving with large language models",
                        "Generative ai for pentesting: the good, the bad, the ugly",
                        "Acfix: Guiding llms with mined common rbac practices for context-aware repair of access control vulnerabilities in smart contracts",
                        "A unified debugging approach via llm-based multi-agent synergy",
                        "How far can we go with practical function-level program repair",
                        "Pentestgpt: An llm-empowered automatic penetration testing tool",
                        "Pydex: Repairing bugs in introductory python assignments using llms",
                        "Finetuning large language models for vulnerability detection",
                        "Combining fine-tuning and llm-based agents for intuitive smart contract auditing with justifications",
                        "Qlora: Efficient finetuning of quantized llms",
                        "Neftune: Noisy embeddings improve instruction finetuning",
                        "Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning",
                        "Concept-guided llm agents for human-ai safety codesign"
                    ]
                }
            ],
            "all_references": [
                "Vulnerability detection with code language models: How far are we",
                "Large language models for software engineering: Survey and open problems",
                "A prompt pattern catalog to enhance prompt engineering with chatgpt",
                "How well does llm generate security tests",
                "Codegen: An open large language model for code with multi-turn program synthesis",
                "Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond",
                "Reflexion: Language agents with verbal reinforcement learning",
                "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
                "Pentestgpt: An llm-empowered automatic penetration testing tool",
                "Chain-of-thought prompting elicits reasoning in large language models",
                "Exploring llm-based agents for root cause analysis",
                "Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis",
                "Experience grounds language",
                "Prcbert: Prompt learning for requirement classification using bert-based pretrained language models",
                "Large language models in law: A survey",
                "Jigsaw: large language models meet program synthesis",
                "Impact of large language models on generating software specifications",
                "Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences",
                "Attention is all you need",
                "Finetuning large language models for vulnerability detection",
                "Batch prompting: Efficient inference with large language model apis",
                "Navrepair: Node-type aware c/c++ code vulnerability repair",
                "Using llms in software requirements specifications: An empirical evaluation",
                "Teaching large language models to self-debug",
                "Software engineering using autonomous agents: Are we there yet",
                "Large language models as software components: A taxonomy for llm-integrated applications",
                "Codet5+: Open code large language models for code understanding and generation",
                "Opt: Open pre-trained transformer language models",
                "Generative software engineering",
                "Agent-driven automatic software improvement",
                "Llm-powered data augmentation for enhanced cross-lingual performance",
                "L2ceval: Evaluating language-to-code generation capabilities of large language models",
                "Multi-objective fine-tuning for enhanced program repair with llms",
                "Transformer-based language models for software vulnerability detection",
                "Data-driven prototyping via natural-language-based gui retrieval",
                "Leveraging transformer-based language models to automate requirements satisfaction assessment",
                "The rise and potential of large language model based agents: A survey",
                "Tabasco: A transformer based contextualization toolkit",
                "Norbert: Transfer learning for requirements classification",
                "React: Synergizing reasoning and acting in language models",
                "Sql-palm: Improved large language model adaptation for text-to-sql (extended",
                "Generative artificial intelligence for software engineering - a research agenda",
                "Are more llm calls all you need? towards scaling laws of compound inference systems",
                "Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents",
                "Chatgpt and software testing education: promises & perils",
                "Neftune: Noisy embeddings improve instruction finetuning",
                "Expel: Llm agents are experiential learners",
                "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                "Llm-based nlg evaluation: Current status and challenges",
                "Llm-based agents for automating the enhancement of user story quality: An early report",
                "Toolformer: Language models can teach themselves to use tools",
                "Self-planning code generation with large language models",
                "Elicitron: An llm agent-based simulation framework for design requirements elicitation",
                "Language agents as optimizable graphs",
                "Language models are few-shot learners",
                "Chatgpt: A study on its utility for ubiquitous software engineering tasks",
                "Ldb: A large language model debugger via verifying runtime execution step-by-step",
                "Ritfis: Robust input testing framework for llms-based intelligent software",
                "Metagpt: Meta programming for a multi-agent collaborative framework",
                "Large language models as test case generators: Performance evaluation and enhancement",
                "An empirical evaluation of pre-trained large language models for repairing declarative formal specifications",
                "Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments",
                "Llm agents can autonomously exploit one-day vulnerabilities",
                "Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain",
                "Assured llm-based software engineering",
                "Cycle: Learning to self-refine the code generation",
                "When llm-based code generation meets the software development process",
                "Llm-powered test case generation for detecting tricky bugs",
                "Lms: Understanding code syntax and semantics for code analysis",
                "Camel: Communicative agents for",
                "Codepori: Large scale model for autonomous software development by using multi-agents",
                "Chatgpt: Optimizing language models for dialogue",
                "Debug like a human: A large language model debugger via verifying runtime execution step-by-step",
                "Toolllm: Facilitating large language models to master 16000+ real-world apis",
                "Nissist: An incident mitigation copilot based on troubleshooting guides",
                "Test mimicry to assess the exploitability of library vulnerabilities",
                "Identification of intra-domain ambiguity using transformer-based machine learning",
                "Leveraging print debugging to improve code generation in large language models",
                "Artificial Intelligence: A Modern Approach",
                "Foundations of statistical natural language processing",
                "Codecompose: A large-scale industrial deployment of ai-assisted code authoring",
                "Unit test case generation with transformers and focal context",
                "Code-aware prompting: A study of coverage guided test generation in regression setting using llm",
                "Generative ai for pentesting: the good, the bad, the ugly",
                "Evaluation of chatgpt on requirements information retrieval under zero-shot setting",
                "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
                "Towards an understanding of large language models in software engineering tasks",
                "Repair is nearly generation: Multilingual program repair with llms",
                "The impact of ai on developer productivity: Evidence from github copilot",
                "Explainable automated debugging via large language model-driven scientific debugging",
                "Evaluating large language models trained on code",
                "Investigating chatgpt's potential to assist in requirements elicitation processes",
                "Self-collaboration code generation via chatgpt",
                "Software engineering education must adapt and evolve for an llm environment",
                "Evaluation of chatgpt model for vulnerability detection",
                "How far can we go with practical function-level program repair",
                "Alpacafarm: A simulation framework for methods that learn from human feedback",
                "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
                "Survey of hallucination in natural language generation",
                "On the creativity of large language models",
                "Chatgpt as a tool for user story quality evaluation: Trustworthy out of the box",
                "Repairagent: An autonomous, llm-based agent for program repair",
                "Xuat-copilot: Multi-agent collaborative system for automated user acceptance testing with large language model",
                "A survey on large language model based autonomous agents",
                "Opencodeinterpreter: Integrating code generation with execution and refinement",
                "Can large language model agents simulate human trust behaviors",
                "Coverup: Coverage-guided llm-based test generation",
                "Agentlite: A lightweight library for building and advancing task-oriented llm agent system",
                "Robustness, security, privacy, explainability, efficiency, and usability of large language models for code",
                "Voyager: An open-ended embodied agent with large language models",
                "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
                "Incoder: A generative model for code infilling and synthesis",
                "Combining fine-tuning and llm-based agents for intuitive smart contract auditing with justifications",
                "Large language model-powered smart contract vulnerability detection: New perspectives",
                "The evolution of computing: Alphago",
                "Getting pwn'd by ai: Penetration testing with large language models",
                "Pydex: Repairing bugs in introductory python assignments using llms",
                "Software/hardware co-design for llm and its application for design verification",
                "Fuzz4all: Universal fuzzing with large language models",
                "Rethinking the bounds of llm reasoning: Are multi-agent discussions the key",
                "Acfix: Guiding llms with mined common rbac practices for context-aware repair of access control vulnerabilities in smart contracts",
                "Judging llm-as-a-judge with mt-bench and chatbot arena",
                "Evalullm: Llm assisted evaluation of generative outputs",
                "ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
                "L2MAC: Large language model automatic computer for extensive code generation",
                "Communicative agents for software development",
                "A unified debugging approach via llm-based multi-agent synergy",
                "Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning",
                "A survey of agent-oriented software engineering",
                "Engineering safety requirements for autonomous driving with large language models",
                "Gpt-3: Its nature, scope, limits, and consequences",
                "Houdini, an annotation assistant for esc/java",
                "The hitchhiker's guide to program analysis: A journey with large language models",
                "A novel approach for automatic program repair using round-trip translation with large language models",
                "Palm: Scaling language modeling with pathways",
                "Concept-guided llm agents for human-ai safety codesign",
                "Autonomous agents in software development: A vision paper",
                "Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents",
                "Long short-term memory",
                "Llama: Open and efficient foundation language models",
                "Tree of thoughts: Deliberate problem solving with large language models",
                "Llm based multi-agent generation of semi-structured documents from semantic templates in the public administration domain",
                "Large language models can self-improve",
                "More agents is all you need",
                "Large language models for software engineering: A systematic literature review",
                "Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x",
                "Experimenting a new programming practice with llms",
                "Large language models are few-shot testers: Exploring llm-based general bug reproduction",
                "Specgen: Automated generation of formal program specifications via large language models",
                "Towards autonomous testing agents via conversational large language models",
                "Qlora: Efficient finetuning of quantized llms",
                "Requirements engineering for machine learning: Perspectives from data scientists",
                "Prompting is all you need: Automated android bug replay with large language models",
                "Improving requirements completeness: Automated assistance through large language models",
                "Flows: Building blocks of reasoning and collaborating ai",
                "Advancing requirements engineering through generative ai: Assessing the role of llms",
                "Bugram: bug detection with n-gram language models",
                "Agentcoder: Multi-agent-based code generation with iterative testing and optimisation",
                "Github copilot: Your ai pair programmer",
                "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "Self: Self-evolution with language feedback",
                "Long-context llms struggle with long in-context learning",
                "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face"
            ]
        },
        "topic_history": [
            {
                "name": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
                "arxiv_id": "2404.00971",
                "reference": [
                    "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
                    "No need to lift a finger anymore? assessing the quality of code generation by chatgpt",
                    "An empirical evaluation of github copilot's code suggestions",
                    "Starcoder: may the source be with you",
                    "Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt",
                    "Survey of hallucination in natural language generation",
                    "Ds-1000: A natural and reliable benchmark for data science code generation",
                    "Incoder: A generative model for code infilling and synthesis",
                    "Codereval: A benchmark of pragmatic code generation with generative pre-trained models",
                    "Refining chatgpt-generated code: Characterizing and mitigating code quality issues",
                    "Evaluating large language models trained on code",
                    "Competition-level code generation with alphacode",
                    "Large language models and simple, stupid bugs",
                    "Measuring coding challenge competence with apps",
                    "Program synthesis with large language models",
                    "How often do single-statement bugs occur? the manysstubs4j dataset",
                    "Codegen2: Lessons for training llms on programming and natural languages",
                    "Bugs in large language models generated code",
                    "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models",
                    "Siren's song in the ai ocean: A survey on hallucination in large language models",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Amazon codewhisperer",
                    "Chatgpt: Optimizing language models for dialogue"
                ]
            },
            {
                "name": "Agentless: Demystifying LLM-based Software Engineering Agents",
                "arxiv_id": "2407.01489",
                "reference": [
                    "Programming-by-demonstration for long-horizon robot tasks",
                    "The rise and potential of large language model based agents: A survey",
                    "Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt",
                    "https://github.com/swe-bench/experiments/blob/main/evaluation/lite/20240612_IBM_Research_Agent101/README.md",
                    "Livecodebench: Holistic and contamination free evaluation of large language models for code",
                    "Unsupervised translation of programming languages",
                    "Leveraging automated unit tests for unsupervised code translation",
                    "Repairagent: An autonomous, llm-based agent for program repair",
                    "SWE-bench: Can language models resolve real-world github issues",
                    "Starcoder 2 and the stack v2: The next generation",
                    "https://opencsg.com/product?class=StarShip/",
                    "Magicoder: Source code is all you need",
                    "Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt",
                    "The living review on automated program repair",
                    "How to understand whole software repository",
                    "Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models",
                    "Starcoder: may the source be with you",
                    "Universal fuzzing via large language models",
                    "Automated program repair in the era of large pre-trained language models",
                    "Ds-1000: A natural and reliable benchmark for data science code generation",
                    "https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240622_Lingma_Agent",
                    "Mapping language to code in programmatic context",
                    "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x",
                    "https://www.marscode.com",
                    "Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models",
                    "Wizardcoder: Empowering code large language models with evol-instruct",
                    "Natural language to code generation in interactive data science notebooks",
                    "Evaluating large language models trained on code",
                    "https://www.swebench.com/lite.html",
                    "Competition-level code generation with alphacode",
                    "Measuring coding challenge competence with apps",
                    "https://aws.amazon.com/q/developer",
                    "Program synthesis with large language models",
                    "Autocoderover: Autonomous program improvement",
                    "https://www.cognition.ai/introducing-devin",
                    "Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation",
                    "Repocoder: Repository-level code completion through iterative retrieval and generation",
                    "https://github.com/OpenDevin/OpenDevin/",
                    "Patch generation with language models: Feasibility and scaling behavior",
                    "Code llama: Open foundation models for code",
                    "Deepseek-coder: When the large language model meets programming-the rise of code intelligence",
                    "Lost in translation: A study of bugs introduced by large language models while translating code",
                    "Repobench: Benchmarking repository-level code auto-completion systems",
                    "Aider is ai pair programming in your terminal",
                    "Large language models are few-shot testers: Exploring llm-based general bug reproduction",
                    "Opencodeinterpreter: Integrating code generation with execution and refinement",
                    "Multipl-e: A scalable and polyglot approach to benchmarking neural code generation",
                    "Coder: Issue resolving with multi-agent and task graphs",
                    "https://www.factory.ai",
                    "Swe-agent: Agent-computer interfaces enable automated software engineering"
                ]
            },
            {
                "name": "Prioritizing Software Requirements Using Large Language Models",
                "arxiv_id": "2405.01564",
                "reference": [
                    "The scope of chatgpt in software engineering: A thorough investigation",
                    "Prius: Applying gamification to user stories prioritization",
                    "Navigating complexity in software engineering: A prototype for comparing gpt-n solutions",
                    "Large language model evaluation via multi ai agents: Preliminary results",
                    "Artificial Intelligence-driven web development and agile project management using OpenAI API and GPT technology: A detailed report on technical integration and implementation of GPT models in CMS with API and agile web development for quality user-centered AI chat service experience",
                    "Autonomous agents in software development: A vision paper",
                    "Is chatgpt leading generative ai? what is beyond expectations? What is beyond expectations",
                    "Self-collaboration code generation via chatgpt",
                    "Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis",
                    "Not all requirements prioritization criteria are equal at all times: A quantitative analysis",
                    "Regulating chatgpt and other large generative ai models",
                    "Investigating code generation performance of chat-gpt with crowdsourcing social data",
                    "Towards human-bot collaborative software architecting with chatgpt",
                    "Gpt understands, too"
                ]
            },
            {
                "name": "EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation",
                "arxiv_id": "2408.11198",
                "reference": [
                    "React: Synergizing reasoning and acting in language models",
                    "Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions",
                    "Language models are few-shot learners",
                    "Connecting large language models with evolutionary algorithms yields powerful prompt optimizers",
                    "A systematic survey of prompt engineering in large language models: Techniques and applications",
                    "Instoptima: Evolutionary multi-objective instruc- tion optimization via large language model-based instruction opera- tors",
                    "Automatic prompt optimization with\" gradient descent\" and beam search",
                    "Automatic code generation using pre-trained language models",
                    "Promptbreeder: Self-referential self-improvement via prompt evolution",
                    "A comprehensive review of State-of-The-Art methods for Java code generation from Natural Language Text",
                    "Code llama: Open foundation models for code",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Automatic chain of thought prompting in large language models",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Spell: Semantic prompt evolution based on a llm",
                    "Language models are unsupervised multitask learners",
                    "Evaluating large language models trained on code",
                    "Magicoder: Source Code Is All You Need",
                    "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
                    "Generated knowledge prompting for commonsense reasoning",
                    "Retrieval-augmented generation for knowledge-intensive nlp tasks",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
                    "Language agent tree search unifies reasoning acting and planning in language models",
                    "Llama 2: Open foundation and fine-tuned chat models",
                    "Large language models are human-level prompt engineers",
                    "Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources",
                    "Reflexion: Language Agents with Verbal Reinforcement Learning",
                    "Bert: Pre-training of deep bidirectional transformers for language understanding",
                    "Codesearchnet challenge: Evaluating the state of semantic code search",
                    "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization",
                    "Codexglue: A machine learning benchmark dataset for code understanding and generation"
                ]
            },
            {
                "name": "No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair",
                "arxiv_id": "2409.03267",
                "reference": [
                    "Pre-Trained Model-Based Automated Software Vulnerability Repair: How Far are We",
                    "A Survey of Learning-based Automated Program Repair",
                    "A Survey on Large Language Models for Code Generation",
                    "Automated Program Repair in the Era of Large Pre-trained Language Models",
                    "Teaching Large Language Models to Self-Debug",
                    "Gamma: Revisiting Template-Based Automated Program Repair Via Mask Prediction",
                    "Impact of Code Language Models on Automated Program Repair",
                    "A syntax-guided edit decoder for neural program repair",
                    "ChatGPT",
                    "Software Testing With Large Language Models: Survey, Landscape, and Vision",
                    "An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation",
                    "Is Self-Repair a Silver Bullet for Code Generation",
                    "CoCoNuT: combining context-aware neural translation models using ensemble for program repair",
                    "A Systematic Literature Review on Large Language Models for Automated Program Repair",
                    "Apache Lucene",
                    "SkCoder: A Sketch-based Approach for Automatic Code Generation",
                    "Code Llama: Open Foundation Models for Code",
                    "GPT-4 Technical Report",
                    "A Survey on Large Language Models for Software Engineering",
                    "CodeAgent: Collaborative Agents for Software Engineering",
                    "A Survey of Source Code Search: A 3-Dimensional Perspective",
                    "AceCoder: An Effective Prompting Technique Specialized in Code Generation",
                    "GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search"
                ]
            },
            {
                "name": "Towards Integrating Emerging AI Applications in SE Education",
                "arxiv_id": "2405.18062",
                "reference": [
                    "Chatbots applications in education: A systematic review",
                    "Artificial intelligence in education: A review",
                    "Computing education in the era of generative AI",
                    "Systematic review of research on artificial intelligence applications in higher education-where are the educators",
                    "How chatgpt will change software engineering education",
                    "Positive artificial intelligence in education (P-AIED) : A roadmap",
                    "Toward computer science curricular guidelines 2023 (cs",
                    "Artificial intelligence meets software engineering in computing education",
                    "Interacting with educational chatbots: A systematic review",
                    "SWEBOK Guide Version 4.0 beta",
                    "A review of artificial intelligence (AI) in education from 2010 to 2020"
                ]
            },
            {
                "name": "A Qualitative Study on Using ChatGPT for Software Security: Perception vs. Practicality",
                "arxiv_id": "2408.00435",
                "reference": [
                    "Considerations for evaluating large language models for cybersecurity tasks",
                    "Transformer-based vulnerability detection in code at edittime: Zero-shot, few-shot, or fine-tuning",
                    "When chatgpt meets smart contract vulnerability detection: How far are we",
                    "Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities",
                    "Apibot: question answering bot for api documentation",
                    "New tricks to old codes: can ai chatbots replace static code analysis tools",
                    "Data quality for software vulnerability datasets",
                    "An empirical comparison of transformer-based models in vulnerability prediction",
                    "Transformer-based language models for software vulnerability detection",
                    "Conversational devbots for secure programming: An empirical study on skf chatbot",
                    "A new approach to web application security: Utilizing gpt language models for source code inspection",
                    "Codebert: A pre-trained model for programming and natural languages",
                    "Can large language models identify and reason about security vulnerabilities? not yet",
                    "Llbezpeky: Leveraging large language models for vulnerability detection",
                    "Breaking the silence: the threats of using llms in software engineering",
                    "Investigating user perceptions of conversational agents for software-related exploratory web search",
                    "Chatgpt for vulnerability detection, classification, and repair: How far are we",
                    "Prompt-enhanced software vulnerability detection using chatgpt"
                ]
            },
            {
                "name": "Harnessing the Power of LLMs: Automating Unit Test Generation for High-Performance Computing",
                "arxiv_id": "2407.05202",
                "reference": [
                    "Towards automatic and flexible unit test generation for legacy hpc code",
                    "A study on robustness and reliability of large language model code generation",
                    "Unit test case generation with transformers and focal context",
                    "Integrating mpi-based numerical software into an advanced parallel computing environment",
                    "No more manual tests? evaluating and improving chatgpt for unit test generation",
                    "Towards test driven development for computational science with pfunit",
                    "Optimizing the parallel scheme of the poisson solver for the reduced kinetic code teresa",
                    "Evaluating large language models trained on code",
                    "Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code",
                    "A systematic evaluation of large language models of code",
                    "Asleep at the keyboard? assessing the security of github copilot's code contributions",
                    "Test-driven development in hpc science: A case study",
                    "Assessing the quality of github copilot's code generation",
                    "Large language models and simple, stupid bugs",
                    "Do users write more insecure code with ai assistants"
                ]
            },
            {
                "name": "Studying LLM Performance on Closed- and Open-source Data",
                "arxiv_id": "2402.15100",
                "reference": [
                    "Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction",
                    "CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models",
                    "Memorization and generalization in neural code intelligence models",
                    "Learning to Represent Programs with Graphs",
                    "code2seq: Generating sequences from structured representations of code",
                    "Impact of Code Language Models on Automated Program Repair",
                    "code2vec: Learning distributed representations of code",
                    "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
                    "Few-shot training LLMs for project-specific code-summarization",
                    "Typewriter: Neural type prediction with search-based validation",
                    "Evaluating large language models trained on code",
                    "Better patching using LLM prompting, via Self-Consistency",
                    "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
                    "Chain-of-thought prompting elicits reasoning in large language models",
                    "Copiloting the copilots: Fusing large language models with completion engines for automated program repair",
                    "Codegen2: Lessons for training llms on programming and natural languages",
                    "A systematic evaluation of large language models of code",
                    "Code llama: Open foundation models for code",
                    "Codegen: An open large language model for code with multi-turn program synthesis",
                    "Automated repair of programs from large language models",
                    "Self-consistency improves chain of thought reasoning in language models",
                    "Attention is all you need",
                    "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization",
                    "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
                    "Towards Autonomous Testing Agents via Conversational Large Language Models",
                    "Global relational models of source code"
                ]
            },
            {
                "name": "CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios",
                "arxiv_id": "2403.19287",
                "reference": [
                    "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
                    "No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation",
                    "Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction",
                    "StarCoder: may the source be with you",
                    "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
                    "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation",
                    "MultiPL-E: a scalable and polyglot approach to benchmarking neural code generation",
                    "CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models",
                    "Defects4J: a database of existing faults to enable controlled testing studies for Java programs",
                    "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis",
                    "Code Llama: Open Foundation Models for Code",
                    "Attention is All you Need",
                    "Program Synthesis with Large Language Models",
                    "Evaluating Large Language Models Trained on Code",
                    "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"
                ]
            }
        ]
    }
]